{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I24JPlcOpBrS",
    "outputId": "9b1bec08-e54b-41b3-bce7-c3f666b6239e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KtcxjNjqf-6a",
    "outputId": "5cf5cc52-dd14-4ef0-9d8b-394e85053828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/projects/OCOR\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/projects/OCOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV6zgv3BNvV6"
   },
   "source": [
    "验证当前环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "Tw7hLcLiNjEh",
    "outputId": "2509397c-a421-4c2e-ae8a-2c56a58d384a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/featurize/work/OCOR\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmcv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpwd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# !pip list\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmmcv\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)         \u001b[38;5;66;03m# eg. 2.0.1\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;241m.\u001b[39mcuda)        \u001b[38;5;66;03m# eg. 11.8\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmcv'"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "# !pip list\n",
    "\n",
    "import torch, mmcv, numpy\n",
    "print(torch.__version__)         # eg. 2.0.1\n",
    "print(torch.version.cuda)        # eg. 11.8\n",
    "print(mmcv.__version__)\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpSAgRGdrNqM",
    "outputId": "2dbcaef8-21b6-4a03-82f8-bdb9263a81ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 16 08:58:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:10:00.0 Off |                  Off |\n",
      "| 30%   26C    P8             21W /  450W |       2MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# ========================== 1. 使用 GPU ==========================\n",
    "# 如果你不是 GPU 环境，点击上方菜单：修改 > 笔记本设置 > 硬件加速器 选 GPU\n",
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "2l6K1HxktFTc",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "9012bee0-5f4c-4fb1-a71d-37ff88128bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'OCOR'...\n",
      "remote: Enumerating objects: 1093, done.\u001b[K\n",
      "remote: Counting objects: 100% (972/972), done.\u001b[K\n",
      "remote: Compressing objects: 100% (603/603), done.\u001b[K\n",
      "remote: Total 1093 (delta 382), reused 941 (delta 369), pack-reused 121 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1093/1093), 7.40 MiB | 13.53 MiB/s, done.\n",
      "Resolving deltas: 100% (389/389), done.\n",
      "/content/OCOR\n",
      "\u001b[0m\u001b[01;34mconfigs\u001b[0m/        \u001b[01;34mdocs\u001b[0m/            \u001b[01;34mmmcv_custom\u001b[0m/   requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
      "DatasetTest.py  evaluate_SOR.py  \u001b[01;34mmmdet\u001b[0m/         \u001b[01;34mresources\u001b[0m/        \u001b[01;34mtools\u001b[0m/\n",
      "\u001b[01;34mdemo\u001b[0m/           inference.py     README.md      setup.cfg         utils.py\n",
      "\u001b[01;34mdocker\u001b[0m/         LICENSE          \u001b[01;34mrequirements\u001b[0m/  setup.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================== 2. 克隆 OCOR 项目 ==========================\n",
    "# !git clone https://github.com/AliasJeff/OCOR.git\n",
    "# %cd OCOR\n",
    "# %ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting scipy\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/af/2c/40108915fd340c830aee332bb85a9160f99e90893e58008b659b9f3dddc0/scipy-1.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.6,>=1.25.2 in /environment/miniconda3/lib/python3.11/site-packages (from scipy) (1.26.4)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "gxVHPwKDqDy_",
    "outputId": "34f4c6f4-859c-4490-d6e2-8fe101dee655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 2.2.6\n",
      "Uninstalling numpy-2.2.6:\n",
      "  Successfully uninstalled numpy-2.2.6\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting numpy==1.25.2\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/32/6a/65dbc57a89078af9ff8bfcd4c0761a50172d90192eaeb1b6f56e5fbf1c3d/numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.25.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y numpy\n",
    "\n",
    "# 安装 NumPy 1.x（避免 ABI 问题）\n",
    "!pip install numpy==1.25.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7FUPq6btFdf",
    "outputId": "31028bcc-6853-425c-e7cc-cde3f6c511c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping mmcv as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping mmcv-full as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping mmdet as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: torch 2.2.2\n",
      "Uninstalling torch-2.2.2:\n",
      "  Successfully uninstalled torch-2.2.2\n",
      "Found existing installation: torchvision 0.17.2\n",
      "Uninstalling torchvision-0.17.2:\n",
      "  Successfully uninstalled torchvision-0.17.2\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.0.1\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp311-cp311-linux_x86_64.whl (2267.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision==0.15.2\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.15.2%2Bcu118-cp311-cp311-linux_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.0.1) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.0.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.0.1) (3.3)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.11/site-packages (from torch==2.0.1) (3.1.3)\n",
      "Collecting triton==2.0.0 (from torch==2.0.1)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /environment/miniconda3/lib/python3.11/site-packages (from torchvision==0.15.2) (1.25.2)\n",
      "Requirement already satisfied: requests in /environment/miniconda3/lib/python3.11/site-packages (from torchvision==0.15.2) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /environment/miniconda3/lib/python3.11/site-packages (from torchvision==0.15.2) (10.3.0)\n",
      "Collecting cmake (from triton==2.0.0->torch==2.0.1)\n",
      "  Downloading https://download.pytorch.org/whl/cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lit (from triton==2.0.0->torch==2.0.1)\n",
      "  Downloading https://download.pytorch.org/whl/lit-15.0.7.tar.gz (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torchvision==0.15.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torchvision==0.15.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torchvision==0.15.2) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.11/site-packages (from requests->torchvision==0.15.2) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /environment/miniconda3/lib/python3.11/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-15.0.7-py3-none-any.whl size=89989 sha256=2c4b3c8d59166995b615321a938378e0c047108258d3d5dca3e9dfc2f351c632\n",
      "  Stored in directory: /home/featurize/.cache/pip/wheels/fc/5d/45/34fe9945d5e45e261134e72284395be36c2d4828af38e2b0fe\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, triton, torch, torchvision\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.2.0\n",
      "    Uninstalling triton-2.2.0:\n",
      "      Successfully uninstalled triton-2.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.2.2 requires torch==2.2.2, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed cmake-3.25.0 lit-15.0.7 torch-2.0.1+cu118 torchvision-0.15.2+cu118 triton-2.0.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Looking in links: https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html\n",
      "Collecting mmcv-full==1.7.2\n",
      "  Downloading https://download.openmmlab.com/mmcv/dist/cu118/torch2.0.0/mmcv_full-1.7.2-cp311-cp311-manylinux1_x86_64.whl (70.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting addict (from mmcv-full==1.7.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6a/00/b08f23b7d7e1e14ce01419a467b583edbb93c6cdb8654e54a9cc579cd61f/addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
      "Requirement already satisfied: numpy in /environment/miniconda3/lib/python3.11/site-packages (from mmcv-full==1.7.2) (1.25.2)\n",
      "Requirement already satisfied: packaging in /environment/miniconda3/lib/python3.11/site-packages (from mmcv-full==1.7.2) (23.2)\n",
      "Requirement already satisfied: Pillow in /environment/miniconda3/lib/python3.11/site-packages (from mmcv-full==1.7.2) (10.3.0)\n",
      "Requirement already satisfied: pyyaml in /environment/miniconda3/lib/python3.11/site-packages (from mmcv-full==1.7.2) (6.0.1)\n",
      "Collecting yapf (from mmcv-full==1.7.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/37/81/6acd6601f61e31cfb8729d3da6d5df966f80f374b78eff83760714487338/yapf-0.43.0-py3-none-any.whl (256 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m938.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opencv-python>=3 (from mmcv-full==1.7.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/68/1f/795e7f4aa2eacc59afa4fb61a2e35e510d06414dd5a802b51a012d691b37/opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numpy (from mmcv-full==1.7.2)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/b3/dd/2238b898e51bd6d389b7389ffb20d7f4c10066d80351187ec8e303a5a475/numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: platformdirs>=3.5.1 in /environment/miniconda3/lib/python3.11/site-packages (from yapf->mmcv-full==1.7.2) (3.10.0)\n",
      "Installing collected packages: addict, yapf, numpy, opencv-python, mmcv-full\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed addict-2.4.0 mmcv-full-1.7.2 numpy-2.2.6 opencv-python-4.12.0.88 yapf-0.43.0\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting mmdet==2.12.0\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/98/c9/107507a9a66394f8d031c11b01de1384be24fc629c989fec15b765cbf3a4/mmdet-2.12.0-py3-none-any.whl (592 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m592.1/592.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (3.9.0)\n",
      "Requirement already satisfied: numpy in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (2.2.6)\n",
      "Requirement already satisfied: six in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (1.16.0)\n",
      "Collecting terminaltables (from mmdet==2.12.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c4/fb/ea621e0a19733e01fe4005d46087d383693c0f4a8f824b47d8d4122c87e0/terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pycocotools (from mmdet==2.12.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a7/ec/7827cd9ce6e80f739fab0163ecb3765df54af744a9bab64b0058bdce47ef/pycocotools-2.0.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (477 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m477.3/477.3 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (2.9.0.post0)\n",
      "Installing collected packages: terminaltables, pycocotools, mmdet\n",
      "Successfully installed mmdet-2.12.0 pycocotools-2.0.10 terminaltables-3.1.10\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting ninja\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/eb/7a/455d2877fe6cf99886849c7f9755d897df32eaf3a0fba47b56e615f880f7/ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ninja\n",
      "Successfully installed ninja-1.11.1.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================== 3. 安装依赖 ==========================\n",
    "\n",
    "# Step 1: 清理旧依赖\n",
    "!pip uninstall -y mmcv mmcv-full mmdet torch torchvision\n",
    "\n",
    "# Step 2: 安装 PyTorch 2.0.1 + CUDA 11.8（与 mmcv-full 兼容）\n",
    "!pip install torch==2.0.1 torchvision==0.15.2 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Step 3: 安装 mmcv-full 1.7.1（适配 mmdet 2.12.0 和 torch 2.0）\n",
    "# !pip install mmcv==1.3.9 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html\n",
    "!pip install mmcv-full==1.7.2 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.0/index.html\n",
    "\n",
    "# Step 4: 安装 MMDetection 2.12.0\n",
    "!pip install mmdet==2.12.0\n",
    "\n",
    "# 安装 ninja\n",
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OaOSY0Yu5rd",
    "outputId": "9ab69662-bbd4-4697-daa2-ed6fb139440c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Obtaining file:///home/featurize/work/OCOR\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (3.9.0)\n",
      "Requirement already satisfied: numpy in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (2.2.6)\n",
      "Requirement already satisfied: pycocotools in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (2.0.10)\n",
      "Requirement already satisfied: six in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (1.16.0)\n",
      "Requirement already satisfied: terminaltables in /environment/miniconda3/lib/python3.11/site-packages (from mmdet==2.12.0) (3.1.10)\n",
      "Collecting timm (from mmdet==2.12.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/2a/48/732f0a5637d2486e6548a302cf0e6427856827a21f7555349d3e49319368/timm-1.0.17-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /environment/miniconda3/lib/python3.11/site-packages (from matplotlib->mmdet==2.12.0) (2.9.0.post0)\n",
      "Requirement already satisfied: torch in /environment/miniconda3/lib/python3.11/site-packages (from timm->mmdet==2.12.0) (2.0.1+cu118)\n",
      "Requirement already satisfied: torchvision in /environment/miniconda3/lib/python3.11/site-packages (from timm->mmdet==2.12.0) (0.15.2+cu118)\n",
      "Requirement already satisfied: pyyaml in /environment/miniconda3/lib/python3.11/site-packages (from timm->mmdet==2.12.0) (6.0.1)\n",
      "Collecting huggingface_hub (from timm->mmdet==2.12.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/46/7b/98daa50a2db034cab6cd23a3de04fa2358cb691593d28e9130203eb7a805/huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting safetensors (from timm->mmdet==2.12.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub->timm->mmdet==2.12.0) (3.13.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub->timm->mmdet==2.12.0) (2024.3.1)\n",
      "Requirement already satisfied: requests in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub->timm->mmdet==2.12.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub->timm->mmdet==2.12.0) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /environment/miniconda3/lib/python3.11/site-packages (from huggingface_hub->timm->mmdet==2.12.0) (4.11.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub->timm->mmdet==2.12.0)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/6d/2f/6cad7b5fe86b7652579346cb7f85156c11761df26435651cbba89376cd2c/hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /environment/miniconda3/lib/python3.11/site-packages (from torch->timm->mmdet==2.12.0) (1.12)\n",
      "Requirement already satisfied: networkx in /environment/miniconda3/lib/python3.11/site-packages (from torch->timm->mmdet==2.12.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /environment/miniconda3/lib/python3.11/site-packages (from torch->timm->mmdet==2.12.0) (3.1.3)\n",
      "Requirement already satisfied: triton==2.0.0 in /environment/miniconda3/lib/python3.11/site-packages (from torch->timm->mmdet==2.12.0) (2.0.0)\n",
      "Requirement already satisfied: cmake in /environment/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch->timm->mmdet==2.12.0) (3.25.0)\n",
      "Requirement already satisfied: lit in /environment/miniconda3/lib/python3.11/site-packages (from triton==2.0.0->torch->timm->mmdet==2.12.0) (15.0.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /environment/miniconda3/lib/python3.11/site-packages (from jinja2->torch->timm->mmdet==2.12.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm->mmdet==2.12.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm->mmdet==2.12.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm->mmdet==2.12.0) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /environment/miniconda3/lib/python3.11/site-packages (from requests->huggingface_hub->timm->mmdet==2.12.0) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /environment/miniconda3/lib/python3.11/site-packages (from sympy->torch->timm->mmdet==2.12.0) (1.3.0)\n",
      "Installing collected packages: safetensors, hf-xet, huggingface_hub, timm, mmdet\n",
      "  Attempting uninstall: mmdet\n",
      "    Found existing installation: mmdet 2.12.0\n",
      "    Uninstalling mmdet-2.12.0:\n",
      "      Successfully uninstalled mmdet-2.12.0\n",
      "  Running setup.py develop for mmdet\n",
      "Successfully installed hf-xet-1.1.5 huggingface_hub-0.33.4 mmdet-2.12.0 safetensors-0.5.3 timm-1.0.17\n"
     ]
    }
   ],
   "source": [
    "# 安装 mmdet + 本地项目（OCOR）\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1iR0HIf8zGwy",
    "outputId": "61f53b54-d023-4aac-c774-d0b87eae095f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/setuptools/__init__.py:80: _DeprecatedInstaller: setuptools.installer and fetch_build_eggs are deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Requirements should be satisfied by a PEP 517 installer.\n",
      "        If you are using pip, you can try `pip install --use-pep517`.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  dist.fetch_build_eggs(dist.setup_requires)\n",
      "running develop\n",
      "/environment/miniconda3/lib/python3.11/site-packages/setuptools/command/develop.py:40: EasyInstallDeprecationWarning: easy_install command is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` and ``easy_install``.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://github.com/pypa/setuptools/issues/917 for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  easy_install.initialize_options(self)\n",
      "/environment/miniconda3/lib/python3.11/site-packages/setuptools/_distutils/cmd.py:66: SetuptoolsDeprecationWarning: setup.py install is deprecated.\n",
      "!!\n",
      "\n",
      "        ********************************************************************************\n",
      "        Please avoid running ``setup.py`` directly.\n",
      "        Instead, use pypa/build, pypa/installer or other\n",
      "        standards-based tools.\n",
      "\n",
      "        See https://blog.ganssle.io/articles/2021/10/setup-py-deprecated.html for details.\n",
      "        ********************************************************************************\n",
      "\n",
      "!!\n",
      "  self.initialize_options()\n",
      "running egg_info\n",
      "writing mmdet.egg-info/PKG-INFO\n",
      "writing dependency_links to mmdet.egg-info/dependency_links.txt\n",
      "writing requirements to mmdet.egg-info/requires.txt\n",
      "writing top-level names to mmdet.egg-info/top_level.txt\n",
      "reading manifest file 'mmdet.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'mmdet.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "Creating /environment/miniconda3/lib/python3.11/site-packages/mmdet.egg-link (link to .)\n",
      "mmdet 2.12.0 is already the active version in easy-install.pth\n",
      "\n",
      "Installed /home/featurize/work/OCOR\n",
      "Processing dependencies for mmdet==2.12.0\n",
      "Searching for pycocotools==2.0.10\n",
      "Best match: pycocotools 2.0.10\n",
      "Adding pycocotools 2.0.10 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for timm==1.0.17\n",
      "Best match: timm 1.0.17\n",
      "Adding timm 1.0.17 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for terminaltables==3.1.10\n",
      "Best match: terminaltables 3.1.10\n",
      "Adding terminaltables 3.1.10 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for six==1.16.0\n",
      "Best match: six 1.16.0\n",
      "Adding six 1.16.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for numpy==2.2.6\n",
      "Best match: numpy 2.2.6\n",
      "Adding numpy 2.2.6 to easy-install.pth file\n",
      "Installing f2py script to /environment/miniconda3/bin\n",
      "Installing numpy-config script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for matplotlib==3.9.0\n",
      "Best match: matplotlib 3.9.0\n",
      "Adding matplotlib 3.9.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for safetensors==0.5.3\n",
      "Best match: safetensors 0.5.3\n",
      "Adding safetensors 0.5.3 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for huggingface-hub==0.33.4\n",
      "Best match: huggingface-hub 0.33.4\n",
      "Adding huggingface-hub 0.33.4 to easy-install.pth file\n",
      "Installing huggingface-cli script to /environment/miniconda3/bin\n",
      "Installing tiny-agents script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for PyYAML==6.0.1\n",
      "Best match: PyYAML 6.0.1\n",
      "Adding PyYAML 6.0.1 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for torchvision==0.15.2+cu118\n",
      "Best match: torchvision 0.15.2+cu118\n",
      "Adding torchvision 0.15.2+cu118 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for torch==2.0.1+cu118\n",
      "Best match: torch 2.0.1+cu118\n",
      "Adding torch 2.0.1+cu118 to easy-install.pth file\n",
      "Installing convert-caffe2-to-onnx script to /environment/miniconda3/bin\n",
      "Installing convert-onnx-to-caffe2 script to /environment/miniconda3/bin\n",
      "Installing torchrun script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for python-dateutil==2.9.0.post0\n",
      "Best match: python-dateutil 2.9.0.post0\n",
      "Adding python-dateutil 2.9.0.post0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for pyparsing==3.1.2\n",
      "Best match: pyparsing 3.1.2\n",
      "Adding pyparsing 3.1.2 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for pillow==10.3.0\n",
      "Best match: pillow 10.3.0\n",
      "Adding pillow 10.3.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for packaging==23.2\n",
      "Best match: packaging 23.2\n",
      "Adding packaging 23.2 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for kiwisolver==1.4.5\n",
      "Best match: kiwisolver 1.4.5\n",
      "Adding kiwisolver 1.4.5 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for fonttools==4.53.0\n",
      "Best match: fonttools 4.53.0\n",
      "Adding fonttools 4.53.0 to easy-install.pth file\n",
      "Installing fonttools script to /environment/miniconda3/bin\n",
      "Installing pyftmerge script to /environment/miniconda3/bin\n",
      "Installing pyftsubset script to /environment/miniconda3/bin\n",
      "Installing ttx script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for cycler==0.12.1\n",
      "Best match: cycler 0.12.1\n",
      "Adding cycler 0.12.1 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for contourpy==1.2.1\n",
      "Best match: contourpy 1.2.1\n",
      "Adding contourpy 1.2.1 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for hf-xet==1.1.5\n",
      "Best match: hf-xet 1.1.5\n",
      "Adding hf-xet 1.1.5 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for typing-extensions==4.11.0\n",
      "Best match: typing-extensions 4.11.0\n",
      "Adding typing-extensions 4.11.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for tqdm==4.65.0\n",
      "Best match: tqdm 4.65.0\n",
      "Adding tqdm 4.65.0 to easy-install.pth file\n",
      "Installing tqdm script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for requests==2.31.0\n",
      "Best match: requests 2.31.0\n",
      "Adding requests 2.31.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for fsspec==2024.3.1\n",
      "Best match: fsspec 2024.3.1\n",
      "Adding fsspec 2024.3.1 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for filelock==3.13.4\n",
      "Best match: filelock 3.13.4\n",
      "Adding filelock 3.13.4 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for triton==2.0.0\n",
      "Best match: triton 2.0.0\n",
      "Adding triton 2.0.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for Jinja2==3.1.3\n",
      "Best match: Jinja2 3.1.3\n",
      "Adding Jinja2 3.1.3 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for networkx==3.3\n",
      "Best match: networkx 3.3\n",
      "Adding networkx 3.3 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for sympy==1.12\n",
      "Best match: sympy 1.12\n",
      "Adding sympy 1.12 to easy-install.pth file\n",
      "Installing isympy script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for certifi==2024.2.2\n",
      "Best match: certifi 2024.2.2\n",
      "Adding certifi 2024.2.2 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for urllib3==2.1.0\n",
      "Best match: urllib3 2.1.0\n",
      "Adding urllib3 2.1.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for idna==3.4\n",
      "Best match: idna 3.4\n",
      "Adding idna 3.4 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for charset-normalizer==2.0.4\n",
      "Best match: charset-normalizer 2.0.4\n",
      "Adding charset-normalizer 2.0.4 to easy-install.pth file\n",
      "Installing normalizer script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for lit==15.0.7\n",
      "Best match: lit 15.0.7\n",
      "Adding lit 15.0.7 to easy-install.pth file\n",
      "Installing lit script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for cmake==3.25.0\n",
      "Best match: cmake 3.25.0\n",
      "Adding cmake 3.25.0 to easy-install.pth file\n",
      "Installing cmake script to /environment/miniconda3/bin\n",
      "Installing cpack script to /environment/miniconda3/bin\n",
      "Installing ctest script to /environment/miniconda3/bin\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for MarkupSafe==2.1.5\n",
      "Best match: MarkupSafe 2.1.5\n",
      "Adding MarkupSafe 2.1.5 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Searching for mpmath==1.3.0\n",
      "Best match: mpmath 1.3.0\n",
      "Adding mpmath 1.3.0 to easy-install.pth file\n",
      "\n",
      "Using /environment/miniconda3/lib/python3.11/site-packages\n",
      "Finished processing dependencies for mmdet==2.12.0\n"
     ]
    }
   ],
   "source": [
    "!python setup.py develop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从头开始用预训练模型训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rGalUGfltFrP",
    "outputId": "656bd5c0-f171-49c2-a05e-60b7757dc176",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "apex is not installed\n",
      "/environment/miniconda3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "apex is not installed\n",
      "apex is not installed\n",
      "apex is not installed\n",
      ">>> RUNTIME PYTHON: /environment/miniconda3/bin/python\n",
      ">>> TORCH: 2.0.1+cu118\n",
      ">>> NUMPY: 1.25.2\n",
      "2025-07-16 12:13:57,246 - mmdet - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0]\n",
      "CUDA available: True\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "CUDA_HOME: /usr/local/cuda\n",
      "NVCC: Cuda compilation tools, release 12.3, V12.3.107\n",
      "GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "PyTorch: 2.0.1+cu118\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "TorchVision: 0.15.2+cu118\n",
      "OpenCV: 4.12.0\n",
      "MMCV: 1.7.2\n",
      "MMCV Compiler: GCC 9.3\n",
      "MMCV CUDA Compiler: 11.8\n",
      "MMDetection: 2.12.0+0c8ce2c\n",
      "------------------------------------------------------------\n",
      "\n",
      "2025-07-16 12:13:57,814 - mmdet - INFO - Distributed training: False\n",
      "2025-07-16 12:13:58,502 - mmdet - INFO - Config:\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = './Datasets/ASSR/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='AutoAugment',\n",
      "        policies=[[{\n",
      "            'type': 'Resize',\n",
      "            'img_scale': [(800, 800)],\n",
      "            'multiscale_mode': 'range',\n",
      "            'keep_ratio': True\n",
      "        }]]),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=1,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='./Datasets/ASSR/instances_train.json',\n",
      "        img_prefix='./Datasets/ASSR/images/train/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='AutoAugment',\n",
      "                policies=[[{\n",
      "                    'type': 'Resize',\n",
      "                    'img_scale': [(800, 800)],\n",
      "                    'multiscale_mode': 'range',\n",
      "                    'keep_ratio': True\n",
      "                }]]),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='./Datasets/ASSR/instances_test.json',\n",
      "        img_prefix='./Datasets/ASSR/images/test/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='./Datasets/ASSR/instances_test.json',\n",
      "        img_prefix='./Datasets/ASSR/images/test/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric=['bbox', 'segm'])\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=2.5e-05,\n",
      "    weight_decay=0.0001,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "optimizer_config = dict(\n",
      "    grad_clip=dict(max_norm=1, norm_type=2),\n",
      "    type='DistOptimizerHook',\n",
      "    update_interval=1,\n",
      "    coalesce=True,\n",
      "    bucket_size_mb=-1,\n",
      "    use_fp16=False)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=1000,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[40])\n",
      "runner = dict(type='EpochBasedRunnerAmp', max_epochs=65)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = 'checkpoint/model.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "num_stages = 6\n",
      "num_proposals = 300\n",
      "model = dict(\n",
      "    type='QueryInst',\n",
      "    pretrained=None,\n",
      "    backbone=dict(\n",
      "        type='SwinTransformer',\n",
      "        embed_dim=192,\n",
      "        depths=[2, 2, 18, 2],\n",
      "        num_heads=[6, 12, 24, 48],\n",
      "        window_size=7,\n",
      "        mlp_ratio=4.0,\n",
      "        qkv_bias=True,\n",
      "        qk_scale=None,\n",
      "        drop_rate=0.0,\n",
      "        attn_drop_rate=0.0,\n",
      "        drop_path_rate=0.3,\n",
      "        ape=False,\n",
      "        patch_norm=True,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        use_checkpoint=False),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[192, 384, 768, 1536],\n",
      "        out_channels=256,\n",
      "        start_level=0,\n",
      "        add_extra_convs='on_input',\n",
      "        num_outs=4),\n",
      "    rpn_head=dict(\n",
      "        type='EmbeddingRPNHead',\n",
      "        num_proposals=300,\n",
      "        proposal_feature_channel=256),\n",
      "    roi_head=dict(\n",
      "        type='QueryRoIHead',\n",
      "        num_stages=6,\n",
      "        stage_loss_weights=[1, 1, 1, 1, 1, 1],\n",
      "        proposal_feature_channel=256,\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=2),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        mask_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=2),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0]))\n",
      "        ],\n",
      "        mask_head=[\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0))\n",
      "        ]),\n",
      "    train_cfg=dict(\n",
      "        rpn=None,\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False)\n",
      "        ]),\n",
      "    test_cfg=dict(rpn=None, rcnn=dict(max_per_img=300, mask_thr_binary=0.5)))\n",
      "total_epochs = 65\n",
      "min_values = (480, 512, 544, 576, 608, 640)\n",
      "fp16 = None\n",
      "work_dir = './work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals'\n",
      "gpu_ids = range(0, 1)\n",
      "\n",
      "/environment/miniconda3/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/environment/miniconda3/lib/python3.11/site-packages/mmcv/cnn/bricks/conv_module.py:153: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n",
      "2025-07-16 12:14:02,990 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}\n",
      "2025-07-16 12:14:03,011 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 12:14:03,200 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 12:14:03,381 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 12:14:03,571 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 12:14:03,774 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 12:14:03,950 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.patch_embed.proj.weight - torch.Size([192, 3, 4, 4]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.patch_embed.proj.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.patch_embed.norm.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.patch_embed.norm.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm1.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm1.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.relative_position_bias_table - torch.Size([169, 6]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.qkv.weight - torch.Size([576, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.qkv.bias - torch.Size([576]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.proj.weight - torch.Size([192, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.proj.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm2.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm2.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc1.weight - torch.Size([768, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc1.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc2.weight - torch.Size([192, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc2.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm1.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm1.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.relative_position_bias_table - torch.Size([169, 6]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.qkv.weight - torch.Size([576, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.qkv.bias - torch.Size([576]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.proj.weight - torch.Size([192, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.proj.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm2.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm2.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,160 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc1.weight - torch.Size([768, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc1.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc2.weight - torch.Size([192, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc2.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.0.downsample.reduction.weight - torch.Size([384, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.0.downsample.norm.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.0.downsample.norm.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm1.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm1.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.relative_position_bias_table - torch.Size([169, 12]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.qkv.weight - torch.Size([1152, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.qkv.bias - torch.Size([1152]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.proj.weight - torch.Size([384, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.proj.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm2.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm2.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc1.weight - torch.Size([1536, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc1.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc2.weight - torch.Size([384, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc2.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm1.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm1.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.relative_position_bias_table - torch.Size([169, 12]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.qkv.weight - torch.Size([1152, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.qkv.bias - torch.Size([1152]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.proj.weight - torch.Size([384, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.proj.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm2.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm2.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc1.weight - torch.Size([1536, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc1.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc2.weight - torch.Size([384, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc2.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.downsample.reduction.weight - torch.Size([768, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,161 - mmcv - INFO - \n",
      "backbone.layers.1.downsample.norm.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.1.downsample.norm.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,162 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,163 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,164 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,165 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,166 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,167 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,173 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.downsample.reduction.weight - torch.Size([1536, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.downsample.norm.weight - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.2.downsample.norm.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm1.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm1.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.relative_position_bias_table - torch.Size([169, 48]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.qkv.weight - torch.Size([4608, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.qkv.bias - torch.Size([4608]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.proj.weight - torch.Size([1536, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.proj.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm2.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm2.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc1.weight - torch.Size([6144, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc1.bias - torch.Size([6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc2.weight - torch.Size([1536, 6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc2.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm1.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm1.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.relative_position_bias_table - torch.Size([169, 48]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.qkv.weight - torch.Size([4608, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.qkv.bias - torch.Size([4608]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.proj.weight - torch.Size([1536, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,174 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.proj.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm2.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm2.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc1.weight - torch.Size([6144, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc1.bias - torch.Size([6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc2.weight - torch.Size([1536, 6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc2.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm0.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm0.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm1.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm1.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm3.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "backbone.norm3.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.weight - torch.Size([256, 192, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.weight - torch.Size([256, 384, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.weight - torch.Size([256, 768, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.3.conv.weight - torch.Size([256, 1536, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.lateral_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "neck.fpn_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "rpn_head.init_proposal_bboxes.weight - torch.Size([300, 4]): \n",
      "Initialized by user-defined `init_weights` in EmbeddingRPNHead  \n",
      " \n",
      "2025-07-16 12:14:04,175 - mmcv - INFO - \n",
      "rpn_head.init_proposal_features.weight - torch.Size([300, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,176 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,177 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,178 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,179 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,180 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,181 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,182 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,183 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,184 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,185 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,186 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 12:14:04,187 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "2025-07-16 12:14:04,615 - mmdet - INFO - load checkpoint from checkpoint/model.pth\n",
      "2025-07-16 12:14:04,616 - mmdet - INFO - load checkpoint from local path: checkpoint/model.pth\n",
      "2025-07-16 12:14:05,443 - mmdet - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/OCOR/work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals\n",
      "2025-07-16 12:14:05,444 - mmdet - INFO - workflow: [('train', 1)], max: 65 epochs\n",
      "2025-07-16 12:14:05,444 - mmdet - INFO - Checkpoints will be saved to /home/featurize/work/OCOR/work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals by HardDiskBackend.\n",
      "2025-07-16 12:14:25,077 - mmdet - INFO - Epoch [1][50/750]\tlr: 1.249e-06, eta: 5:18:35, time: 0.393, data_time: 0.051, memory: 10418, stage0_loss_cls: 1.2923, stage0_pos_acc: 29.4032, stage0_loss_bbox: 2.0042, stage0_loss_iou: 1.8078, stage0_loss_mask: 4.8837, stage1_loss_cls: 1.4586, stage1_pos_acc: 31.6359, stage1_loss_bbox: 1.7160, stage1_loss_iou: 1.6636, stage1_loss_mask: 4.4119, stage2_loss_cls: 1.4905, stage2_pos_acc: 25.3779, stage2_loss_bbox: 1.6027, stage2_loss_iou: 1.6065, stage2_loss_mask: 4.2992, stage3_loss_cls: 1.5168, stage3_pos_acc: 22.2905, stage3_loss_bbox: 1.5733, stage3_loss_iou: 1.5712, stage3_loss_mask: 4.2548, stage4_loss_cls: 1.4800, stage4_pos_acc: 23.2660, stage4_loss_bbox: 1.5237, stage4_loss_iou: 1.5555, stage4_loss_mask: 4.1957, stage5_loss_cls: 1.4944, stage5_pos_acc: 24.6310, stage5_loss_bbox: 1.4732, stage5_loss_iou: 1.5386, stage5_loss_mask: 4.1159, loss: 54.5303\n",
      "2025-07-16 12:14:41,673 - mmdet - INFO - Epoch [1][100/750]\tlr: 2.498e-06, eta: 4:53:41, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.2999, stage0_pos_acc: 24.6102, stage0_loss_bbox: 1.6313, stage0_loss_iou: 1.6942, stage0_loss_mask: 4.7122, stage1_loss_cls: 1.4139, stage1_pos_acc: 23.8270, stage1_loss_bbox: 1.3042, stage1_loss_iou: 1.4761, stage1_loss_mask: 3.9168, stage2_loss_cls: 1.4216, stage2_pos_acc: 21.6550, stage2_loss_bbox: 1.1761, stage2_loss_iou: 1.3718, stage2_loss_mask: 3.7180, stage3_loss_cls: 1.4150, stage3_pos_acc: 23.3426, stage3_loss_bbox: 1.1535, stage3_loss_iou: 1.3388, stage3_loss_mask: 3.6337, stage4_loss_cls: 1.3593, stage4_pos_acc: 21.1283, stage4_loss_bbox: 1.1472, stage4_loss_iou: 1.3435, stage4_loss_mask: 3.6602, stage5_loss_cls: 1.3384, stage5_pos_acc: 19.7276, stage5_loss_bbox: 1.1367, stage5_loss_iou: 1.3290, stage5_loss_mask: 3.5491, loss: 47.5405\n",
      "2025-07-16 12:14:57,956 - mmdet - INFO - Epoch [1][150/750]\tlr: 3.746e-06, eta: 4:43:30, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1832, stage0_pos_acc: 30.1104, stage0_loss_bbox: 1.3973, stage0_loss_iou: 1.5136, stage0_loss_mask: 4.0524, stage1_loss_cls: 1.2229, stage1_pos_acc: 26.0107, stage1_loss_bbox: 0.9856, stage1_loss_iou: 1.2098, stage1_loss_mask: 3.0105, stage2_loss_cls: 1.2075, stage2_pos_acc: 21.6281, stage2_loss_bbox: 0.8860, stage2_loss_iou: 1.1183, stage2_loss_mask: 2.7613, stage3_loss_cls: 1.1904, stage3_pos_acc: 24.7884, stage3_loss_bbox: 0.8725, stage3_loss_iou: 1.0972, stage3_loss_mask: 2.7290, stage4_loss_cls: 1.1602, stage4_pos_acc: 24.5524, stage4_loss_bbox: 0.8719, stage4_loss_iou: 1.0928, stage4_loss_mask: 2.6782, stage5_loss_cls: 1.1450, stage5_pos_acc: 25.7836, stage5_loss_bbox: 0.8623, stage5_loss_iou: 1.0938, stage5_loss_mask: 2.6140, loss: 37.9557\n",
      "2025-07-16 12:15:14,388 - mmdet - INFO - Epoch [1][200/750]\tlr: 4.995e-06, eta: 4:38:53, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.2148, stage0_pos_acc: 28.8698, stage0_loss_bbox: 1.2277, stage0_loss_iou: 1.5283, stage0_loss_mask: 3.9828, stage1_loss_cls: 1.2319, stage1_pos_acc: 23.6378, stage1_loss_bbox: 0.8748, stage1_loss_iou: 1.2543, stage1_loss_mask: 2.9172, stage2_loss_cls: 1.1979, stage2_pos_acc: 25.7136, stage2_loss_bbox: 0.8084, stage2_loss_iou: 1.1699, stage2_loss_mask: 2.6498, stage3_loss_cls: 1.1862, stage3_pos_acc: 24.4057, stage3_loss_bbox: 0.7938, stage3_loss_iou: 1.1606, stage3_loss_mask: 2.6010, stage4_loss_cls: 1.1776, stage4_pos_acc: 25.5183, stage4_loss_bbox: 0.7717, stage4_loss_iou: 1.1580, stage4_loss_mask: 2.5966, stage5_loss_cls: 1.1749, stage5_pos_acc: 26.4001, stage5_loss_bbox: 0.7654, stage5_loss_iou: 1.1572, stage5_loss_mask: 2.5527, loss: 37.1535\n",
      "2025-07-16 12:15:30,913 - mmdet - INFO - Epoch [1][250/750]\tlr: 6.244e-06, eta: 4:36:18, time: 0.330, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1749, stage0_pos_acc: 35.0933, stage0_loss_bbox: 1.1851, stage0_loss_iou: 1.4265, stage0_loss_mask: 3.1609, stage1_loss_cls: 1.1740, stage1_pos_acc: 26.2837, stage1_loss_bbox: 0.8699, stage1_loss_iou: 1.0961, stage1_loss_mask: 2.2142, stage2_loss_cls: 1.1569, stage2_pos_acc: 34.3651, stage2_loss_bbox: 0.7852, stage2_loss_iou: 1.0292, stage2_loss_mask: 2.1050, stage3_loss_cls: 1.1358, stage3_pos_acc: 30.2012, stage3_loss_bbox: 0.7754, stage3_loss_iou: 1.0174, stage3_loss_mask: 2.1333, stage4_loss_cls: 1.1442, stage4_pos_acc: 27.2325, stage4_loss_bbox: 0.7718, stage4_loss_iou: 1.0123, stage4_loss_mask: 2.1389, stage5_loss_cls: 1.1524, stage5_pos_acc: 31.6103, stage5_loss_bbox: 0.7648, stage5_loss_iou: 1.0095, stage5_loss_mask: 2.1193, loss: 32.5531\n",
      "2025-07-16 12:15:47,175 - mmdet - INFO - Epoch [1][300/750]\tlr: 7.493e-06, eta: 4:33:47, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1672, stage0_pos_acc: 29.1206, stage0_loss_bbox: 1.0321, stage0_loss_iou: 1.4205, stage0_loss_mask: 3.2693, stage1_loss_cls: 1.1539, stage1_pos_acc: 26.5389, stage1_loss_bbox: 0.6731, stage1_loss_iou: 1.0138, stage1_loss_mask: 2.0336, stage2_loss_cls: 1.1519, stage2_pos_acc: 26.0754, stage2_loss_bbox: 0.6100, stage2_loss_iou: 0.9206, stage2_loss_mask: 1.8629, stage3_loss_cls: 1.1458, stage3_pos_acc: 23.6595, stage3_loss_bbox: 0.5975, stage3_loss_iou: 0.9106, stage3_loss_mask: 1.9153, stage4_loss_cls: 1.1280, stage4_pos_acc: 24.3802, stage4_loss_bbox: 0.5850, stage4_loss_iou: 0.9007, stage4_loss_mask: 1.8688, stage5_loss_cls: 1.1342, stage5_pos_acc: 25.1833, stage5_loss_bbox: 0.5811, stage5_loss_iou: 0.8995, stage5_loss_mask: 1.8667, loss: 29.8421\n",
      "2025-07-16 12:16:03,513 - mmdet - INFO - Epoch [1][350/750]\tlr: 8.741e-06, eta: 4:32:05, time: 0.327, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1512, stage0_pos_acc: 36.7854, stage0_loss_bbox: 1.0787, stage0_loss_iou: 1.4049, stage0_loss_mask: 3.0755, stage1_loss_cls: 1.1249, stage1_pos_acc: 27.7843, stage1_loss_bbox: 0.7612, stage1_loss_iou: 1.0358, stage1_loss_mask: 1.8678, stage2_loss_cls: 1.0977, stage2_pos_acc: 30.4934, stage2_loss_bbox: 0.6831, stage2_loss_iou: 0.9670, stage2_loss_mask: 1.7886, stage3_loss_cls: 1.0755, stage3_pos_acc: 29.8626, stage3_loss_bbox: 0.6766, stage3_loss_iou: 0.9550, stage3_loss_mask: 1.8014, stage4_loss_cls: 1.0715, stage4_pos_acc: 31.5980, stage4_loss_bbox: 0.6643, stage4_loss_iou: 0.9515, stage4_loss_mask: 1.8207, stage5_loss_cls: 1.0803, stage5_pos_acc: 30.7056, stage5_loss_bbox: 0.6564, stage5_loss_iou: 0.9433, stage5_loss_mask: 1.7851, loss: 29.5178\n",
      "2025-07-16 12:16:19,907 - mmdet - INFO - Epoch [1][400/750]\tlr: 9.990e-06, eta: 4:30:51, time: 0.328, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1708, stage0_pos_acc: 31.3732, stage0_loss_bbox: 0.9652, stage0_loss_iou: 1.3310, stage0_loss_mask: 2.7267, stage1_loss_cls: 1.1039, stage1_pos_acc: 31.5880, stage1_loss_bbox: 0.7457, stage1_loss_iou: 1.0267, stage1_loss_mask: 2.0054, stage2_loss_cls: 1.0868, stage2_pos_acc: 26.4985, stage2_loss_bbox: 0.6912, stage2_loss_iou: 0.9816, stage2_loss_mask: 1.9567, stage3_loss_cls: 1.0671, stage3_pos_acc: 28.1183, stage3_loss_bbox: 0.6746, stage3_loss_iou: 0.9779, stage3_loss_mask: 1.9522, stage4_loss_cls: 1.0670, stage4_pos_acc: 27.3063, stage4_loss_bbox: 0.6672, stage4_loss_iou: 0.9570, stage4_loss_mask: 1.8559, stage5_loss_cls: 1.0651, stage5_pos_acc: 28.8488, stage5_loss_bbox: 0.6544, stage5_loss_iou: 0.9594, stage5_loss_mask: 1.8746, loss: 29.5640\n",
      "2025-07-16 12:16:36,408 - mmdet - INFO - Epoch [1][450/750]\tlr: 1.124e-05, eta: 4:30:02, time: 0.330, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1813, stage0_pos_acc: 26.0682, stage0_loss_bbox: 0.8713, stage0_loss_iou: 1.3380, stage0_loss_mask: 2.6697, stage1_loss_cls: 1.1056, stage1_pos_acc: 29.8583, stage1_loss_bbox: 0.6128, stage1_loss_iou: 1.0099, stage1_loss_mask: 1.8082, stage2_loss_cls: 1.1079, stage2_pos_acc: 32.4944, stage2_loss_bbox: 0.5401, stage2_loss_iou: 0.9049, stage2_loss_mask: 1.6467, stage3_loss_cls: 1.0950, stage3_pos_acc: 28.1597, stage3_loss_bbox: 0.5199, stage3_loss_iou: 0.8779, stage3_loss_mask: 1.6320, stage4_loss_cls: 1.0853, stage4_pos_acc: 27.5120, stage4_loss_bbox: 0.5068, stage4_loss_iou: 0.8669, stage4_loss_mask: 1.6304, stage5_loss_cls: 1.0868, stage5_pos_acc: 26.0473, stage5_loss_bbox: 0.4989, stage5_loss_iou: 0.8528, stage5_loss_mask: 1.5594, loss: 27.0085\n",
      "2025-07-16 12:16:53,081 - mmdet - INFO - Epoch [1][500/750]\tlr: 1.249e-05, eta: 4:29:35, time: 0.333, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1579, stage0_pos_acc: 24.8458, stage0_loss_bbox: 0.8584, stage0_loss_iou: 1.3947, stage0_loss_mask: 2.8345, stage1_loss_cls: 1.1041, stage1_pos_acc: 31.1856, stage1_loss_bbox: 0.6206, stage1_loss_iou: 1.0108, stage1_loss_mask: 1.9899, stage2_loss_cls: 1.0894, stage2_pos_acc: 29.1249, stage2_loss_bbox: 0.5648, stage2_loss_iou: 0.9353, stage2_loss_mask: 1.8276, stage3_loss_cls: 1.0795, stage3_pos_acc: 30.0695, stage3_loss_bbox: 0.5437, stage3_loss_iou: 0.9175, stage3_loss_mask: 1.8067, stage4_loss_cls: 1.0738, stage4_pos_acc: 28.9337, stage4_loss_bbox: 0.5389, stage4_loss_iou: 0.9055, stage4_loss_mask: 1.7641, stage5_loss_cls: 1.0802, stage5_pos_acc: 30.3822, stage5_loss_bbox: 0.5349, stage5_loss_iou: 0.8998, stage5_loss_mask: 1.7480, loss: 28.2807\n",
      "2025-07-16 12:17:09,577 - mmdet - INFO - Epoch [1][550/750]\tlr: 1.374e-05, eta: 4:28:55, time: 0.330, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1738, stage0_pos_acc: 29.4956, stage0_loss_bbox: 0.8731, stage0_loss_iou: 1.2852, stage0_loss_mask: 2.3737, stage1_loss_cls: 1.0677, stage1_pos_acc: 29.7620, stage1_loss_bbox: 0.6239, stage1_loss_iou: 0.9078, stage1_loss_mask: 1.5007, stage2_loss_cls: 1.0777, stage2_pos_acc: 29.3976, stage2_loss_bbox: 0.5439, stage2_loss_iou: 0.8243, stage2_loss_mask: 1.4189, stage3_loss_cls: 1.0636, stage3_pos_acc: 29.5530, stage3_loss_bbox: 0.5293, stage3_loss_iou: 0.8070, stage3_loss_mask: 1.3936, stage4_loss_cls: 1.0552, stage4_pos_acc: 31.9091, stage4_loss_bbox: 0.5325, stage4_loss_iou: 0.8098, stage4_loss_mask: 1.3801, stage5_loss_cls: 1.0511, stage5_pos_acc: 28.8479, stage5_loss_bbox: 0.5380, stage5_loss_iou: 0.8102, stage5_loss_mask: 1.3746, loss: 25.0156\n",
      "2025-07-16 12:17:25,984 - mmdet - INFO - Epoch [1][600/750]\tlr: 1.499e-05, eta: 4:28:12, time: 0.328, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1238, stage0_pos_acc: 28.5154, stage0_loss_bbox: 0.8873, stage0_loss_iou: 1.2834, stage0_loss_mask: 2.2947, stage1_loss_cls: 1.0275, stage1_pos_acc: 27.9202, stage1_loss_bbox: 0.6170, stage1_loss_iou: 0.9158, stage1_loss_mask: 1.4651, stage2_loss_cls: 1.0533, stage2_pos_acc: 28.8699, stage2_loss_bbox: 0.5105, stage2_loss_iou: 0.7866, stage2_loss_mask: 1.2892, stage3_loss_cls: 1.0439, stage3_pos_acc: 27.7708, stage3_loss_bbox: 0.4829, stage3_loss_iou: 0.7742, stage3_loss_mask: 1.2648, stage4_loss_cls: 1.0483, stage4_pos_acc: 29.7279, stage4_loss_bbox: 0.4724, stage4_loss_iou: 0.7665, stage4_loss_mask: 1.2716, stage5_loss_cls: 1.0553, stage5_pos_acc: 27.4909, stage5_loss_bbox: 0.4608, stage5_loss_iou: 0.7616, stage5_loss_mask: 1.2816, loss: 23.9381\n",
      "2025-07-16 12:17:42,639 - mmdet - INFO - Epoch [1][650/750]\tlr: 1.623e-05, eta: 4:27:51, time: 0.333, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1960, stage0_pos_acc: 30.5193, stage0_loss_bbox: 0.8037, stage0_loss_iou: 1.3505, stage0_loss_mask: 2.2403, stage1_loss_cls: 1.0842, stage1_pos_acc: 33.2502, stage1_loss_bbox: 0.5247, stage1_loss_iou: 0.8785, stage1_loss_mask: 1.4048, stage2_loss_cls: 1.0664, stage2_pos_acc: 33.0390, stage2_loss_bbox: 0.4395, stage2_loss_iou: 0.7706, stage2_loss_mask: 1.3069, stage3_loss_cls: 1.0400, stage3_pos_acc: 30.9080, stage3_loss_bbox: 0.4350, stage3_loss_iou: 0.7672, stage3_loss_mask: 1.2584, stage4_loss_cls: 1.0366, stage4_pos_acc: 27.4946, stage4_loss_bbox: 0.4280, stage4_loss_iou: 0.7608, stage4_loss_mask: 1.2375, stage5_loss_cls: 1.0335, stage5_pos_acc: 28.2327, stage5_loss_bbox: 0.4154, stage5_loss_iou: 0.7530, stage5_loss_mask: 1.2721, loss: 23.5035\n",
      "2025-07-16 12:17:58,936 - mmdet - INFO - Epoch [1][700/750]\tlr: 1.748e-05, eta: 4:27:06, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1223, stage0_pos_acc: 33.4694, stage0_loss_bbox: 0.7560, stage0_loss_iou: 1.2762, stage0_loss_mask: 2.2189, stage1_loss_cls: 1.0500, stage1_pos_acc: 30.0177, stage1_loss_bbox: 0.5294, stage1_loss_iou: 0.8792, stage1_loss_mask: 1.4058, stage2_loss_cls: 1.0581, stage2_pos_acc: 29.8601, stage2_loss_bbox: 0.4590, stage2_loss_iou: 0.7628, stage2_loss_mask: 1.2450, stage3_loss_cls: 1.0448, stage3_pos_acc: 31.5236, stage3_loss_bbox: 0.4443, stage3_loss_iou: 0.7502, stage3_loss_mask: 1.2943, stage4_loss_cls: 1.0336, stage4_pos_acc: 30.9955, stage4_loss_bbox: 0.4393, stage4_loss_iou: 0.7409, stage4_loss_mask: 1.2949, stage5_loss_cls: 1.0394, stage5_pos_acc: 32.2772, stage5_loss_bbox: 0.4298, stage5_loss_iou: 0.7356, stage5_loss_mask: 1.3009, loss: 23.3109\n",
      "2025-07-16 12:18:15,210 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:18:15,210 - mmdet - INFO - Epoch [1][750/750]\tlr: 1.873e-05, eta: 4:26:24, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1260, stage0_pos_acc: 33.1888, stage0_loss_bbox: 0.8233, stage0_loss_iou: 1.2934, stage0_loss_mask: 2.5785, stage1_loss_cls: 1.0740, stage1_pos_acc: 26.6983, stage1_loss_bbox: 0.6178, stage1_loss_iou: 0.9310, stage1_loss_mask: 1.6340, stage2_loss_cls: 1.0576, stage2_pos_acc: 32.1868, stage2_loss_bbox: 0.5233, stage2_loss_iou: 0.8194, stage2_loss_mask: 1.5439, stage3_loss_cls: 1.0483, stage3_pos_acc: 32.0621, stage3_loss_bbox: 0.5174, stage3_loss_iou: 0.8035, stage3_loss_mask: 1.5598, stage4_loss_cls: 1.0448, stage4_pos_acc: 33.7507, stage4_loss_bbox: 0.5028, stage4_loss_iou: 0.7909, stage4_loss_mask: 1.5353, stage5_loss_cls: 1.0394, stage5_pos_acc: 33.2240, stage5_loss_bbox: 0.5011, stage5_loss_iou: 0.7893, stage5_loss_mask: 1.5564, loss: 25.7114\n",
      "2025-07-16 12:18:15,261 - mmdet - INFO - Saving checkpoint at 1 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 89s, ETA:     0s2025-07-16 12:21:21,882 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.13s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.027\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.382\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.177\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.428\n",
      "2025-07-16 12:21:23,669 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.70s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.026\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.179\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.422\n",
      "2025-07-16 12:21:26,610 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:21:26,611 - mmdet - INFO - Epoch(val) [1][750]\tbbox_mAP: 0.0210, bbox_mAP_50: 0.0370, bbox_mAP_75: 0.0210, bbox_mAP_s: 0.0260, bbox_mAP_m: 0.0050, bbox_mAP_l: 0.0270, bbox_mAP_copypaste: 0.021 0.037 0.021 0.026 0.005 0.027, segm_mAP: 0.0200, segm_mAP_50: 0.0370, segm_mAP_75: 0.0190, segm_mAP_s: 0.0270, segm_mAP_m: 0.0050, segm_mAP_l: 0.0260, segm_mAP_copypaste: 0.020 0.037 0.019 0.027 0.005 0.026\n",
      "2025-07-16 12:21:45,109 - mmdet - INFO - Epoch [2][50/750]\tlr: 1.998e-05, eta: 4:27:57, time: 0.370, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.1369, stage0_pos_acc: 29.6645, stage0_loss_bbox: 0.7648, stage0_loss_iou: 1.2711, stage0_loss_mask: 2.4383, stage1_loss_cls: 1.0780, stage1_pos_acc: 28.0726, stage1_loss_bbox: 0.5347, stage1_loss_iou: 0.9371, stage1_loss_mask: 1.6910, stage2_loss_cls: 1.0650, stage2_pos_acc: 33.3727, stage2_loss_bbox: 0.4696, stage2_loss_iou: 0.8255, stage2_loss_mask: 1.5431, stage3_loss_cls: 1.0627, stage3_pos_acc: 32.2326, stage3_loss_bbox: 0.4565, stage3_loss_iou: 0.7941, stage3_loss_mask: 1.4775, stage4_loss_cls: 1.0494, stage4_pos_acc: 31.7483, stage4_loss_bbox: 0.4580, stage4_loss_iou: 0.7895, stage4_loss_mask: 1.4769, stage5_loss_cls: 1.0624, stage5_pos_acc: 29.2867, stage5_loss_bbox: 0.4418, stage5_loss_iou: 0.7785, stage5_loss_mask: 1.4703, loss: 25.0727\n",
      "2025-07-16 12:22:01,267 - mmdet - INFO - Epoch [2][100/750]\tlr: 2.123e-05, eta: 4:27:06, time: 0.323, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1370, stage0_pos_acc: 32.9330, stage0_loss_bbox: 0.7966, stage0_loss_iou: 1.2106, stage0_loss_mask: 2.1272, stage1_loss_cls: 1.0401, stage1_pos_acc: 33.9961, stage1_loss_bbox: 0.5360, stage1_loss_iou: 0.8596, stage1_loss_mask: 1.3585, stage2_loss_cls: 1.0320, stage2_pos_acc: 36.9003, stage2_loss_bbox: 0.4702, stage2_loss_iou: 0.7619, stage2_loss_mask: 1.2902, stage3_loss_cls: 1.0250, stage3_pos_acc: 33.4033, stage3_loss_bbox: 0.4495, stage3_loss_iou: 0.7331, stage3_loss_mask: 1.2715, stage4_loss_cls: 1.0279, stage4_pos_acc: 36.4216, stage4_loss_bbox: 0.4481, stage4_loss_iou: 0.7232, stage4_loss_mask: 1.2544, stage5_loss_cls: 1.0290, stage5_pos_acc: 36.5365, stage5_loss_bbox: 0.4476, stage5_loss_iou: 0.7169, stage5_loss_mask: 1.2286, loss: 22.9746\n",
      "2025-07-16 12:22:17,836 - mmdet - INFO - Epoch [2][150/750]\tlr: 2.248e-05, eta: 4:26:41, time: 0.331, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1736, stage0_pos_acc: 30.0906, stage0_loss_bbox: 0.8450, stage0_loss_iou: 1.3305, stage0_loss_mask: 2.3257, stage1_loss_cls: 1.0824, stage1_pos_acc: 25.8363, stage1_loss_bbox: 0.5627, stage1_loss_iou: 0.9399, stage1_loss_mask: 1.6677, stage2_loss_cls: 1.0642, stage2_pos_acc: 28.5097, stage2_loss_bbox: 0.4604, stage2_loss_iou: 0.8314, stage2_loss_mask: 1.4863, stage3_loss_cls: 1.0580, stage3_pos_acc: 32.2320, stage3_loss_bbox: 0.4573, stage3_loss_iou: 0.7903, stage3_loss_mask: 1.4575, stage4_loss_cls: 1.0441, stage4_pos_acc: 28.6147, stage4_loss_bbox: 0.4482, stage4_loss_iou: 0.7820, stage4_loss_mask: 1.4560, stage5_loss_cls: 1.0405, stage5_pos_acc: 30.0637, stage5_loss_bbox: 0.4436, stage5_loss_iou: 0.7761, stage5_loss_mask: 1.4396, loss: 24.9630\n",
      "2025-07-16 12:22:34,188 - mmdet - INFO - Epoch [2][200/750]\tlr: 2.373e-05, eta: 4:26:06, time: 0.327, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1699, stage0_pos_acc: 28.8046, stage0_loss_bbox: 0.8305, stage0_loss_iou: 1.3179, stage0_loss_mask: 2.3060, stage1_loss_cls: 1.0867, stage1_pos_acc: 31.2103, stage1_loss_bbox: 0.5134, stage1_loss_iou: 0.8947, stage1_loss_mask: 1.5448, stage2_loss_cls: 1.0777, stage2_pos_acc: 30.5745, stage2_loss_bbox: 0.4399, stage2_loss_iou: 0.7769, stage2_loss_mask: 1.4508, stage3_loss_cls: 1.0303, stage3_pos_acc: 33.6974, stage3_loss_bbox: 0.4310, stage3_loss_iou: 0.7629, stage3_loss_mask: 1.3995, stage4_loss_cls: 1.0112, stage4_pos_acc: 33.5537, stage4_loss_bbox: 0.4192, stage4_loss_iou: 0.7467, stage4_loss_mask: 1.3864, stage5_loss_cls: 1.0060, stage5_pos_acc: 31.4855, stage5_loss_bbox: 0.4169, stage5_loss_iou: 0.7500, stage5_loss_mask: 1.3806, loss: 24.1501\n",
      "2025-07-16 12:22:50,573 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:22:50,573 - mmdet - INFO - Epoch [2][250/750]\tlr: 2.498e-05, eta: 4:25:34, time: 0.328, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1064, stage0_pos_acc: 30.7365, stage0_loss_bbox: 0.7799, stage0_loss_iou: 1.2107, stage0_loss_mask: 2.1398, stage1_loss_cls: 1.0392, stage1_pos_acc: 30.8516, stage1_loss_bbox: 0.5579, stage1_loss_iou: 0.9080, stage1_loss_mask: 1.5740, stage2_loss_cls: 1.0219, stage2_pos_acc: 27.7460, stage2_loss_bbox: 0.5275, stage2_loss_iou: 0.8315, stage2_loss_mask: 1.4450, stage3_loss_cls: 0.9899, stage3_pos_acc: 29.5325, stage3_loss_bbox: 0.5230, stage3_loss_iou: 0.8170, stage3_loss_mask: 1.4094, stage4_loss_cls: 0.9873, stage4_pos_acc: 31.2016, stage4_loss_bbox: 0.5255, stage4_loss_iou: 0.8163, stage4_loss_mask: 1.4074, stage5_loss_cls: 0.9838, stage5_pos_acc: 29.6992, stage5_loss_bbox: 0.5228, stage5_loss_iou: 0.8199, stage5_loss_mask: 1.4034, loss: 24.3473\n",
      "2025-07-16 12:23:07,072 - mmdet - INFO - Epoch [2][300/750]\tlr: 2.500e-05, eta: 4:25:09, time: 0.330, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.2227, stage0_pos_acc: 29.6653, stage0_loss_bbox: 0.8002, stage0_loss_iou: 1.3397, stage0_loss_mask: 2.2544, stage1_loss_cls: 1.1370, stage1_pos_acc: 27.8588, stage1_loss_bbox: 0.5810, stage1_loss_iou: 0.9390, stage1_loss_mask: 1.4933, stage2_loss_cls: 1.1318, stage2_pos_acc: 29.8399, stage2_loss_bbox: 0.4835, stage2_loss_iou: 0.8181, stage2_loss_mask: 1.3715, stage3_loss_cls: 1.1256, stage3_pos_acc: 32.5059, stage3_loss_bbox: 0.4704, stage3_loss_iou: 0.7818, stage3_loss_mask: 1.3499, stage4_loss_cls: 1.0991, stage4_pos_acc: 31.4450, stage4_loss_bbox: 0.4616, stage4_loss_iou: 0.7726, stage4_loss_mask: 1.3429, stage5_loss_cls: 1.1020, stage5_pos_acc: 28.5628, stage5_loss_bbox: 0.4524, stage5_loss_iou: 0.7663, stage5_loss_mask: 1.3233, loss: 24.6200\n",
      "2025-07-16 12:23:23,549 - mmdet - INFO - Epoch [2][350/750]\tlr: 2.500e-05, eta: 4:24:43, time: 0.330, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1646, stage0_pos_acc: 26.6864, stage0_loss_bbox: 0.7520, stage0_loss_iou: 1.1045, stage0_loss_mask: 1.8013, stage1_loss_cls: 1.0453, stage1_pos_acc: 28.6699, stage1_loss_bbox: 0.4944, stage1_loss_iou: 0.7572, stage1_loss_mask: 1.2392, stage2_loss_cls: 1.0238, stage2_pos_acc: 29.7410, stage2_loss_bbox: 0.4242, stage2_loss_iou: 0.6636, stage2_loss_mask: 1.0937, stage3_loss_cls: 1.0266, stage3_pos_acc: 35.3672, stage3_loss_bbox: 0.4186, stage3_loss_iou: 0.6465, stage3_loss_mask: 1.0667, stage4_loss_cls: 0.9995, stage4_pos_acc: 33.8201, stage4_loss_bbox: 0.3954, stage4_loss_iou: 0.6385, stage4_loss_mask: 1.0631, stage5_loss_cls: 0.9835, stage5_pos_acc: 35.4049, stage5_loss_bbox: 0.3998, stage5_loss_iou: 0.6432, stage5_loss_mask: 1.0690, loss: 20.9141\n",
      "2025-07-16 12:23:39,942 - mmdet - INFO - Epoch [2][400/750]\tlr: 2.500e-05, eta: 4:24:15, time: 0.328, data_time: 0.005, memory: 11264, stage0_loss_cls: 1.1454, stage0_pos_acc: 26.3640, stage0_loss_bbox: 0.7922, stage0_loss_iou: 1.2784, stage0_loss_mask: 2.3050, stage1_loss_cls: 1.0583, stage1_pos_acc: 29.2002, stage1_loss_bbox: 0.5531, stage1_loss_iou: 0.9424, stage1_loss_mask: 1.6613, stage2_loss_cls: 1.0852, stage2_pos_acc: 28.7228, stage2_loss_bbox: 0.4740, stage2_loss_iou: 0.8204, stage2_loss_mask: 1.4198, stage3_loss_cls: 1.0812, stage3_pos_acc: 28.8345, stage3_loss_bbox: 0.4678, stage3_loss_iou: 0.7933, stage3_loss_mask: 1.3237, stage4_loss_cls: 1.0855, stage4_pos_acc: 27.3837, stage4_loss_bbox: 0.4547, stage4_loss_iou: 0.7828, stage4_loss_mask: 1.3017, stage5_loss_cls: 1.0774, stage5_pos_acc: 27.9397, stage5_loss_bbox: 0.4546, stage5_loss_iou: 0.7810, stage5_loss_mask: 1.3194, loss: 24.4585\n",
      "2025-07-16 12:23:56,410 - mmdet - INFO - Epoch [2][450/750]\tlr: 2.500e-05, eta: 4:23:51, time: 0.329, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1580, stage0_pos_acc: 33.1870, stage0_loss_bbox: 0.7479, stage0_loss_iou: 1.2210, stage0_loss_mask: 2.0276, stage1_loss_cls: 1.0378, stage1_pos_acc: 33.1698, stage1_loss_bbox: 0.5561, stage1_loss_iou: 0.8335, stage1_loss_mask: 1.3363, stage2_loss_cls: 1.0432, stage2_pos_acc: 34.7697, stage2_loss_bbox: 0.4512, stage2_loss_iou: 0.7265, stage2_loss_mask: 1.2029, stage3_loss_cls: 1.0348, stage3_pos_acc: 31.9019, stage3_loss_bbox: 0.4516, stage3_loss_iou: 0.6984, stage3_loss_mask: 1.1950, stage4_loss_cls: 1.0541, stage4_pos_acc: 34.7223, stage4_loss_bbox: 0.4240, stage4_loss_iou: 0.6597, stage4_loss_mask: 1.1781, stage5_loss_cls: 1.0586, stage5_pos_acc: 31.3675, stage5_loss_bbox: 0.4159, stage5_loss_iou: 0.6537, stage5_loss_mask: 1.1730, loss: 22.3389\n",
      "2025-07-16 12:24:12,718 - mmdet - INFO - Epoch [2][500/750]\tlr: 2.500e-05, eta: 4:23:21, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1174, stage0_pos_acc: 33.2949, stage0_loss_bbox: 0.7632, stage0_loss_iou: 1.2582, stage0_loss_mask: 2.1281, stage1_loss_cls: 1.0100, stage1_pos_acc: 33.7451, stage1_loss_bbox: 0.5465, stage1_loss_iou: 0.8811, stage1_loss_mask: 1.5396, stage2_loss_cls: 0.9968, stage2_pos_acc: 33.7800, stage2_loss_bbox: 0.4867, stage2_loss_iou: 0.7941, stage2_loss_mask: 1.4898, stage3_loss_cls: 0.9734, stage3_pos_acc: 32.9959, stage3_loss_bbox: 0.4552, stage3_loss_iou: 0.7761, stage3_loss_mask: 1.4662, stage4_loss_cls: 0.9754, stage4_pos_acc: 32.6144, stage4_loss_bbox: 0.4551, stage4_loss_iou: 0.7667, stage4_loss_mask: 1.4445, stage5_loss_cls: 0.9698, stage5_pos_acc: 33.3453, stage5_loss_bbox: 0.4555, stage5_loss_iou: 0.7637, stage5_loss_mask: 1.4523, loss: 23.9653\n",
      "2025-07-16 12:24:29,022 - mmdet - INFO - Epoch [2][550/750]\tlr: 2.500e-05, eta: 4:22:53, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1825, stage0_pos_acc: 34.4588, stage0_loss_bbox: 0.7900, stage0_loss_iou: 1.2242, stage0_loss_mask: 1.9827, stage1_loss_cls: 1.0229, stage1_pos_acc: 31.5817, stage1_loss_bbox: 0.5267, stage1_loss_iou: 0.8098, stage1_loss_mask: 1.3423, stage2_loss_cls: 1.0210, stage2_pos_acc: 31.5124, stage2_loss_bbox: 0.4604, stage2_loss_iou: 0.7066, stage2_loss_mask: 1.2185, stage3_loss_cls: 0.9967, stage3_pos_acc: 33.4346, stage3_loss_bbox: 0.4565, stage3_loss_iou: 0.6948, stage3_loss_mask: 1.1315, stage4_loss_cls: 0.9720, stage4_pos_acc: 33.4363, stage4_loss_bbox: 0.4450, stage4_loss_iou: 0.6822, stage4_loss_mask: 1.1282, stage5_loss_cls: 0.9651, stage5_pos_acc: 34.2136, stage5_loss_bbox: 0.4424, stage5_loss_iou: 0.6914, stage5_loss_mask: 1.1337, loss: 22.0272\n",
      "2025-07-16 12:24:45,587 - mmdet - INFO - Epoch [2][600/750]\tlr: 2.500e-05, eta: 4:22:34, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1790, stage0_pos_acc: 30.3683, stage0_loss_bbox: 0.8287, stage0_loss_iou: 1.2460, stage0_loss_mask: 2.0744, stage1_loss_cls: 1.0792, stage1_pos_acc: 30.3952, stage1_loss_bbox: 0.5450, stage1_loss_iou: 0.8806, stage1_loss_mask: 1.6027, stage2_loss_cls: 1.0472, stage2_pos_acc: 28.4325, stage2_loss_bbox: 0.5061, stage2_loss_iou: 0.7889, stage2_loss_mask: 1.4979, stage3_loss_cls: 1.0342, stage3_pos_acc: 29.0071, stage3_loss_bbox: 0.4879, stage3_loss_iou: 0.7590, stage3_loss_mask: 1.4297, stage4_loss_cls: 1.0197, stage4_pos_acc: 29.3659, stage4_loss_bbox: 0.4743, stage4_loss_iou: 0.7561, stage4_loss_mask: 1.3902, stage5_loss_cls: 1.0231, stage5_pos_acc: 27.3778, stage5_loss_bbox: 0.4783, stage5_loss_iou: 0.7462, stage5_loss_mask: 1.3839, loss: 24.2584\n",
      "2025-07-16 12:25:02,142 - mmdet - INFO - Epoch [2][650/750]\tlr: 2.500e-05, eta: 4:22:15, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1912, stage0_pos_acc: 25.3883, stage0_loss_bbox: 0.7931, stage0_loss_iou: 1.3897, stage0_loss_mask: 2.3183, stage1_loss_cls: 1.0431, stage1_pos_acc: 36.2093, stage1_loss_bbox: 0.5310, stage1_loss_iou: 1.0021, stage1_loss_mask: 1.8180, stage2_loss_cls: 1.0414, stage2_pos_acc: 39.1753, stage2_loss_bbox: 0.4343, stage2_loss_iou: 0.9113, stage2_loss_mask: 1.6357, stage3_loss_cls: 1.0017, stage3_pos_acc: 36.4961, stage3_loss_bbox: 0.4191, stage3_loss_iou: 0.8767, stage3_loss_mask: 1.6200, stage4_loss_cls: 1.0040, stage4_pos_acc: 36.1697, stage4_loss_bbox: 0.4021, stage4_loss_iou: 0.8587, stage4_loss_mask: 1.5978, stage5_loss_cls: 1.0086, stage5_pos_acc: 34.2689, stage5_loss_bbox: 0.4021, stage5_loss_iou: 0.8545, stage5_loss_mask: 1.5917, loss: 25.7461\n",
      "2025-07-16 12:25:18,918 - mmdet - INFO - Epoch [2][700/750]\tlr: 2.500e-05, eta: 4:22:04, time: 0.335, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1569, stage0_pos_acc: 28.3948, stage0_loss_bbox: 0.7327, stage0_loss_iou: 1.2572, stage0_loss_mask: 2.0755, stage1_loss_cls: 1.0130, stage1_pos_acc: 26.3877, stage1_loss_bbox: 0.4779, stage1_loss_iou: 0.8663, stage1_loss_mask: 1.2241, stage2_loss_cls: 1.0044, stage2_pos_acc: 25.3639, stage2_loss_bbox: 0.3957, stage2_loss_iou: 0.7243, stage2_loss_mask: 1.0964, stage3_loss_cls: 0.9719, stage3_pos_acc: 29.6632, stage3_loss_bbox: 0.3788, stage3_loss_iou: 0.6955, stage3_loss_mask: 1.1200, stage4_loss_cls: 0.9631, stage4_pos_acc: 26.3433, stage4_loss_bbox: 0.3707, stage4_loss_iou: 0.6841, stage4_loss_mask: 1.0953, stage5_loss_cls: 0.9702, stage5_pos_acc: 28.8361, stage5_loss_bbox: 0.3691, stage5_loss_iou: 0.6743, stage5_loss_mask: 1.0873, loss: 21.4046\n",
      "2025-07-16 12:25:35,832 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:25:35,832 - mmdet - INFO - Epoch [2][750/750]\tlr: 2.500e-05, eta: 4:21:57, time: 0.338, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1305, stage0_pos_acc: 27.1585, stage0_loss_bbox: 0.7999, stage0_loss_iou: 1.2294, stage0_loss_mask: 2.0771, stage1_loss_cls: 1.0234, stage1_pos_acc: 34.3039, stage1_loss_bbox: 0.5560, stage1_loss_iou: 0.8493, stage1_loss_mask: 1.4453, stage2_loss_cls: 1.0002, stage2_pos_acc: 33.8168, stage2_loss_bbox: 0.4821, stage2_loss_iou: 0.7432, stage2_loss_mask: 1.2679, stage3_loss_cls: 0.9706, stage3_pos_acc: 33.9589, stage3_loss_bbox: 0.4691, stage3_loss_iou: 0.7116, stage3_loss_mask: 1.1655, stage4_loss_cls: 0.9579, stage4_pos_acc: 34.0342, stage4_loss_bbox: 0.4644, stage4_loss_iou: 0.7020, stage4_loss_mask: 1.1433, stage5_loss_cls: 0.9623, stage5_pos_acc: 37.3067, stage5_loss_bbox: 0.4710, stage5_loss_iou: 0.7082, stage5_loss_mask: 1.1648, loss: 22.4948\n",
      "2025-07-16 12:25:35,930 - mmdet - INFO - Saving checkpoint at 2 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 96s, ETA:     0s2025-07-16 12:28:47,774 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.16s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.123\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.450\n",
      "2025-07-16 12:28:49,576 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.49s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.147\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.488\n",
      "2025-07-16 12:28:52,760 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:28:52,760 - mmdet - INFO - Epoch(val) [2][750]\tbbox_mAP: 0.0280, bbox_mAP_50: 0.0530, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.0080, bbox_mAP_m: 0.0050, bbox_mAP_l: 0.0330, bbox_mAP_copypaste: 0.028 0.053 0.028 0.008 0.005 0.033, segm_mAP: 0.0310, segm_mAP_50: 0.0550, segm_mAP_75: 0.0300, segm_mAP_s: 0.0080, segm_mAP_m: 0.0050, segm_mAP_l: 0.0370, segm_mAP_copypaste: 0.031 0.055 0.030 0.008 0.005 0.037\n",
      "2025-07-16 12:29:11,888 - mmdet - INFO - Epoch [3][50/750]\tlr: 2.500e-05, eta: 4:22:56, time: 0.382, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.2185, stage0_pos_acc: 30.8659, stage0_loss_bbox: 0.7621, stage0_loss_iou: 1.2237, stage0_loss_mask: 1.7981, stage1_loss_cls: 1.0469, stage1_pos_acc: 30.3194, stage1_loss_bbox: 0.4576, stage1_loss_iou: 0.8260, stage1_loss_mask: 1.2112, stage2_loss_cls: 1.0146, stage2_pos_acc: 35.0270, stage2_loss_bbox: 0.3958, stage2_loss_iou: 0.7086, stage2_loss_mask: 1.0078, stage3_loss_cls: 0.9911, stage3_pos_acc: 35.5845, stage3_loss_bbox: 0.3780, stage3_loss_iou: 0.6862, stage3_loss_mask: 1.0123, stage4_loss_cls: 0.9773, stage4_pos_acc: 32.8849, stage4_loss_bbox: 0.3685, stage4_loss_iou: 0.6709, stage4_loss_mask: 0.9698, stage5_loss_cls: 0.9725, stage5_pos_acc: 29.9040, stage5_loss_bbox: 0.3588, stage5_loss_iou: 0.6572, stage5_loss_mask: 0.9499, loss: 20.6634\n",
      "2025-07-16 12:29:28,806 - mmdet - INFO - Epoch [3][100/750]\tlr: 2.500e-05, eta: 4:22:45, time: 0.338, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1448, stage0_pos_acc: 31.1498, stage0_loss_bbox: 0.7426, stage0_loss_iou: 1.1930, stage0_loss_mask: 1.7185, stage1_loss_cls: 0.9883, stage1_pos_acc: 33.7241, stage1_loss_bbox: 0.4748, stage1_loss_iou: 0.8018, stage1_loss_mask: 1.1352, stage2_loss_cls: 0.9828, stage2_pos_acc: 32.0360, stage2_loss_bbox: 0.4039, stage2_loss_iou: 0.7026, stage2_loss_mask: 1.0554, stage3_loss_cls: 0.9542, stage3_pos_acc: 27.1317, stage3_loss_bbox: 0.3948, stage3_loss_iou: 0.6828, stage3_loss_mask: 1.0230, stage4_loss_cls: 0.9456, stage4_pos_acc: 30.2205, stage4_loss_bbox: 0.3901, stage4_loss_iou: 0.6845, stage4_loss_mask: 1.0224, stage5_loss_cls: 0.9458, stage5_pos_acc: 32.6063, stage5_loss_bbox: 0.3965, stage5_loss_iou: 0.6736, stage5_loss_mask: 1.0173, loss: 20.4743\n",
      "2025-07-16 12:29:45,705 - mmdet - INFO - Epoch [3][150/750]\tlr: 2.500e-05, eta: 4:22:33, time: 0.338, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1480, stage0_pos_acc: 32.4026, stage0_loss_bbox: 0.8007, stage0_loss_iou: 1.1955, stage0_loss_mask: 2.0435, stage1_loss_cls: 0.9911, stage1_pos_acc: 32.3078, stage1_loss_bbox: 0.5438, stage1_loss_iou: 0.8831, stage1_loss_mask: 1.4247, stage2_loss_cls: 0.9825, stage2_pos_acc: 37.6759, stage2_loss_bbox: 0.4867, stage2_loss_iou: 0.7796, stage2_loss_mask: 1.3029, stage3_loss_cls: 0.9646, stage3_pos_acc: 38.2618, stage3_loss_bbox: 0.4715, stage3_loss_iou: 0.7589, stage3_loss_mask: 1.2698, stage4_loss_cls: 0.9670, stage4_pos_acc: 37.8883, stage4_loss_bbox: 0.4448, stage4_loss_iou: 0.7343, stage4_loss_mask: 1.2376, stage5_loss_cls: 0.9746, stage5_pos_acc: 38.3144, stage5_loss_bbox: 0.4570, stage5_loss_iou: 0.7292, stage5_loss_mask: 1.2323, loss: 22.8239\n",
      "2025-07-16 12:30:02,535 - mmdet - INFO - Epoch [3][200/750]\tlr: 2.500e-05, eta: 4:22:20, time: 0.337, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1368, stage0_pos_acc: 28.1810, stage0_loss_bbox: 0.8163, stage0_loss_iou: 1.2323, stage0_loss_mask: 1.9338, stage1_loss_cls: 1.0105, stage1_pos_acc: 34.4178, stage1_loss_bbox: 0.5720, stage1_loss_iou: 0.8524, stage1_loss_mask: 1.3596, stage2_loss_cls: 0.9718, stage2_pos_acc: 32.5779, stage2_loss_bbox: 0.5048, stage2_loss_iou: 0.7589, stage2_loss_mask: 1.2361, stage3_loss_cls: 0.9476, stage3_pos_acc: 34.0612, stage3_loss_bbox: 0.4793, stage3_loss_iou: 0.7348, stage3_loss_mask: 1.1841, stage4_loss_cls: 0.9345, stage4_pos_acc: 31.0484, stage4_loss_bbox: 0.4725, stage4_loss_iou: 0.7322, stage4_loss_mask: 1.1617, stage5_loss_cls: 0.9470, stage5_pos_acc: 31.9502, stage5_loss_bbox: 0.4832, stage5_loss_iou: 0.7360, stage5_loss_mask: 1.1667, loss: 22.3648\n",
      "2025-07-16 12:30:19,748 - mmdet - INFO - Epoch [3][250/750]\tlr: 2.500e-05, eta: 4:22:16, time: 0.344, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1503, stage0_pos_acc: 30.3168, stage0_loss_bbox: 0.7877, stage0_loss_iou: 1.2925, stage0_loss_mask: 1.9332, stage1_loss_cls: 0.9936, stage1_pos_acc: 31.2008, stage1_loss_bbox: 0.4661, stage1_loss_iou: 0.8286, stage1_loss_mask: 1.3211, stage2_loss_cls: 0.9677, stage2_pos_acc: 29.7505, stage2_loss_bbox: 0.4037, stage2_loss_iou: 0.7042, stage2_loss_mask: 1.2036, stage3_loss_cls: 0.9535, stage3_pos_acc: 30.5186, stage3_loss_bbox: 0.3777, stage3_loss_iou: 0.6713, stage3_loss_mask: 1.1836, stage4_loss_cls: 0.9425, stage4_pos_acc: 30.9749, stage4_loss_bbox: 0.3755, stage4_loss_iou: 0.6671, stage4_loss_mask: 1.1617, stage5_loss_cls: 0.9268, stage5_pos_acc: 31.6383, stage5_loss_bbox: 0.3745, stage5_loss_iou: 0.6641, stage5_loss_mask: 1.1727, loss: 21.5233\n",
      "2025-07-16 12:30:36,177 - mmdet - INFO - Epoch [3][300/750]\tlr: 2.500e-05, eta: 4:21:51, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1704, stage0_pos_acc: 32.2273, stage0_loss_bbox: 0.8005, stage0_loss_iou: 1.2395, stage0_loss_mask: 2.0529, stage1_loss_cls: 1.0228, stage1_pos_acc: 33.7851, stage1_loss_bbox: 0.5391, stage1_loss_iou: 0.8660, stage1_loss_mask: 1.6576, stage2_loss_cls: 0.9800, stage2_pos_acc: 32.3606, stage2_loss_bbox: 0.4865, stage2_loss_iou: 0.7827, stage2_loss_mask: 1.5857, stage3_loss_cls: 0.9715, stage3_pos_acc: 33.6694, stage3_loss_bbox: 0.4702, stage3_loss_iou: 0.7728, stage3_loss_mask: 1.5715, stage4_loss_cls: 0.9641, stage4_pos_acc: 34.5338, stage4_loss_bbox: 0.4720, stage4_loss_iou: 0.7627, stage4_loss_mask: 1.5620, stage5_loss_cls: 0.9602, stage5_pos_acc: 34.1219, stage5_loss_bbox: 0.4675, stage5_loss_iou: 0.7635, stage5_loss_mask: 1.5445, loss: 24.4663\n",
      "2025-07-16 12:30:52,709 - mmdet - INFO - Epoch [3][350/750]\tlr: 2.500e-05, eta: 4:21:29, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1598, stage0_pos_acc: 29.5083, stage0_loss_bbox: 0.7021, stage0_loss_iou: 1.2558, stage0_loss_mask: 1.8102, stage1_loss_cls: 1.0030, stage1_pos_acc: 28.4694, stage1_loss_bbox: 0.4167, stage1_loss_iou: 0.8185, stage1_loss_mask: 1.2651, stage2_loss_cls: 0.9492, stage2_pos_acc: 35.8011, stage2_loss_bbox: 0.3651, stage2_loss_iou: 0.7140, stage2_loss_mask: 1.1524, stage3_loss_cls: 0.9447, stage3_pos_acc: 34.7418, stage3_loss_bbox: 0.3379, stage3_loss_iou: 0.6795, stage3_loss_mask: 1.0908, stage4_loss_cls: 0.9303, stage4_pos_acc: 32.8341, stage4_loss_bbox: 0.3330, stage4_loss_iou: 0.6674, stage4_loss_mask: 1.0598, stage5_loss_cls: 0.9232, stage5_pos_acc: 33.6669, stage5_loss_bbox: 0.3393, stage5_loss_iou: 0.6615, stage5_loss_mask: 1.0541, loss: 20.6332\n",
      "2025-07-16 12:31:09,040 - mmdet - INFO - Epoch [3][400/750]\tlr: 2.500e-05, eta: 4:21:03, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1408, stage0_pos_acc: 31.5457, stage0_loss_bbox: 0.6987, stage0_loss_iou: 1.1377, stage0_loss_mask: 1.8167, stage1_loss_cls: 1.0051, stage1_pos_acc: 31.9784, stage1_loss_bbox: 0.4581, stage1_loss_iou: 0.7627, stage1_loss_mask: 1.2175, stage2_loss_cls: 1.0170, stage2_pos_acc: 37.7698, stage2_loss_bbox: 0.3890, stage2_loss_iou: 0.6569, stage2_loss_mask: 1.1406, stage3_loss_cls: 0.9948, stage3_pos_acc: 35.5417, stage3_loss_bbox: 0.3993, stage3_loss_iou: 0.6406, stage3_loss_mask: 1.1186, stage4_loss_cls: 0.9753, stage4_pos_acc: 34.6178, stage4_loss_bbox: 0.3909, stage4_loss_iou: 0.6299, stage4_loss_mask: 1.1495, stage5_loss_cls: 0.9755, stage5_pos_acc: 33.9226, stage5_loss_bbox: 0.3851, stage5_loss_iou: 0.6258, stage5_loss_mask: 1.1119, loss: 20.8381\n",
      "2025-07-16 12:31:25,281 - mmdet - INFO - Epoch [3][450/750]\tlr: 2.500e-05, eta: 4:20:35, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1391, stage0_pos_acc: 29.5525, stage0_loss_bbox: 0.6877, stage0_loss_iou: 1.1618, stage0_loss_mask: 1.7245, stage1_loss_cls: 1.0217, stage1_pos_acc: 34.5440, stage1_loss_bbox: 0.4378, stage1_loss_iou: 0.7303, stage1_loss_mask: 1.1283, stage2_loss_cls: 1.0048, stage2_pos_acc: 37.9635, stage2_loss_bbox: 0.3702, stage2_loss_iou: 0.6274, stage2_loss_mask: 1.0395, stage3_loss_cls: 0.9878, stage3_pos_acc: 36.4794, stage3_loss_bbox: 0.3573, stage3_loss_iou: 0.6103, stage3_loss_mask: 1.0293, stage4_loss_cls: 0.9820, stage4_pos_acc: 33.6468, stage4_loss_bbox: 0.3459, stage4_loss_iou: 0.5964, stage4_loss_mask: 1.0714, stage5_loss_cls: 0.9839, stage5_pos_acc: 33.3931, stage5_loss_bbox: 0.3581, stage5_loss_iou: 0.5975, stage5_loss_mask: 1.0391, loss: 20.0320\n",
      "2025-07-16 12:31:41,382 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:31:41,382 - mmdet - INFO - Epoch [3][500/750]\tlr: 2.500e-05, eta: 4:20:04, time: 0.322, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1671, stage0_pos_acc: 30.3113, stage0_loss_bbox: 0.6550, stage0_loss_iou: 1.2095, stage0_loss_mask: 1.7791, stage1_loss_cls: 1.0143, stage1_pos_acc: 30.4652, stage1_loss_bbox: 0.4213, stage1_loss_iou: 0.7844, stage1_loss_mask: 1.1758, stage2_loss_cls: 0.9902, stage2_pos_acc: 32.4247, stage2_loss_bbox: 0.3518, stage2_loss_iou: 0.6501, stage2_loss_mask: 1.0822, stage3_loss_cls: 0.9708, stage3_pos_acc: 33.7842, stage3_loss_bbox: 0.3269, stage3_loss_iou: 0.6273, stage3_loss_mask: 1.0153, stage4_loss_cls: 0.9492, stage4_pos_acc: 34.8597, stage4_loss_bbox: 0.3240, stage4_loss_iou: 0.6174, stage4_loss_mask: 1.0089, stage5_loss_cls: 0.9503, stage5_pos_acc: 35.2011, stage5_loss_bbox: 0.3293, stage5_loss_iou: 0.6158, stage5_loss_mask: 0.9968, loss: 20.0127\n",
      "2025-07-16 12:31:57,465 - mmdet - INFO - Epoch [3][550/750]\tlr: 2.500e-05, eta: 4:19:33, time: 0.322, data_time: 0.005, memory: 11264, stage0_loss_cls: 1.1659, stage0_pos_acc: 28.8697, stage0_loss_bbox: 0.6682, stage0_loss_iou: 1.2167, stage0_loss_mask: 1.4755, stage1_loss_cls: 1.0131, stage1_pos_acc: 30.9521, stage1_loss_bbox: 0.3648, stage1_loss_iou: 0.7214, stage1_loss_mask: 0.8373, stage2_loss_cls: 0.9994, stage2_pos_acc: 32.7234, stage2_loss_bbox: 0.2854, stage2_loss_iou: 0.5839, stage2_loss_mask: 0.7295, stage3_loss_cls: 0.9597, stage3_pos_acc: 38.2377, stage3_loss_bbox: 0.2706, stage3_loss_iou: 0.5625, stage3_loss_mask: 0.7421, stage4_loss_cls: 0.9469, stage4_pos_acc: 34.6059, stage4_loss_bbox: 0.2700, stage4_loss_iou: 0.5503, stage4_loss_mask: 0.7043, stage5_loss_cls: 0.9435, stage5_pos_acc: 35.3235, stage5_loss_bbox: 0.2712, stage5_loss_iou: 0.5496, stage5_loss_mask: 0.7319, loss: 17.5638\n",
      "2025-07-16 12:32:13,831 - mmdet - INFO - Epoch [3][600/750]\tlr: 2.500e-05, eta: 4:19:10, time: 0.327, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1755, stage0_pos_acc: 24.2539, stage0_loss_bbox: 0.6663, stage0_loss_iou: 1.1464, stage0_loss_mask: 1.4372, stage1_loss_cls: 0.9953, stage1_pos_acc: 36.4687, stage1_loss_bbox: 0.3788, stage1_loss_iou: 0.7069, stage1_loss_mask: 0.9474, stage2_loss_cls: 0.9891, stage2_pos_acc: 32.9809, stage2_loss_bbox: 0.3183, stage2_loss_iou: 0.5816, stage2_loss_mask: 0.8337, stage3_loss_cls: 0.9427, stage3_pos_acc: 31.9817, stage3_loss_bbox: 0.3028, stage3_loss_iou: 0.5640, stage3_loss_mask: 0.7988, stage4_loss_cls: 0.9280, stage4_pos_acc: 31.8253, stage4_loss_bbox: 0.3004, stage4_loss_iou: 0.5519, stage4_loss_mask: 0.7750, stage5_loss_cls: 0.9212, stage5_pos_acc: 32.9960, stage5_loss_bbox: 0.2975, stage5_loss_iou: 0.5398, stage5_loss_mask: 0.7523, loss: 17.8508\n",
      "2025-07-16 12:32:30,299 - mmdet - INFO - Epoch [3][650/750]\tlr: 2.500e-05, eta: 4:18:49, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1817, stage0_pos_acc: 33.0960, stage0_loss_bbox: 0.7274, stage0_loss_iou: 1.2641, stage0_loss_mask: 1.8418, stage1_loss_cls: 1.0285, stage1_pos_acc: 30.5071, stage1_loss_bbox: 0.4488, stage1_loss_iou: 0.8127, stage1_loss_mask: 1.2405, stage2_loss_cls: 1.0102, stage2_pos_acc: 30.5230, stage2_loss_bbox: 0.3872, stage2_loss_iou: 0.6955, stage2_loss_mask: 1.1426, stage3_loss_cls: 0.9867, stage3_pos_acc: 30.1873, stage3_loss_bbox: 0.3544, stage3_loss_iou: 0.6645, stage3_loss_mask: 1.1196, stage4_loss_cls: 0.9850, stage4_pos_acc: 30.8222, stage4_loss_bbox: 0.3361, stage4_loss_iou: 0.6479, stage4_loss_mask: 1.0892, stage5_loss_cls: 0.9721, stage5_pos_acc: 32.8960, stage5_loss_bbox: 0.3451, stage5_loss_iou: 0.6526, stage5_loss_mask: 1.0769, loss: 21.0111\n",
      "2025-07-16 12:32:46,891 - mmdet - INFO - Epoch [3][700/750]\tlr: 2.500e-05, eta: 4:18:31, time: 0.332, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1327, stage0_pos_acc: 29.0168, stage0_loss_bbox: 0.7022, stage0_loss_iou: 1.2064, stage0_loss_mask: 1.7227, stage1_loss_cls: 1.0066, stage1_pos_acc: 35.6401, stage1_loss_bbox: 0.4323, stage1_loss_iou: 0.7732, stage1_loss_mask: 1.1395, stage2_loss_cls: 0.9880, stage2_pos_acc: 37.6370, stage2_loss_bbox: 0.3614, stage2_loss_iou: 0.6495, stage2_loss_mask: 1.0669, stage3_loss_cls: 0.9478, stage3_pos_acc: 33.7163, stage3_loss_bbox: 0.3378, stage3_loss_iou: 0.6299, stage3_loss_mask: 1.0019, stage4_loss_cls: 0.9424, stage4_pos_acc: 37.7822, stage4_loss_bbox: 0.3226, stage4_loss_iou: 0.6155, stage4_loss_mask: 0.9628, stage5_loss_cls: 0.9383, stage5_pos_acc: 35.9409, stage5_loss_bbox: 0.3286, stage5_loss_iou: 0.6171, stage5_loss_mask: 0.9614, loss: 19.7875\n",
      "2025-07-16 12:33:03,539 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:33:03,539 - mmdet - INFO - Epoch [3][750/750]\tlr: 2.500e-05, eta: 4:18:14, time: 0.333, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0710, stage0_pos_acc: 34.5508, stage0_loss_bbox: 0.7490, stage0_loss_iou: 1.1978, stage0_loss_mask: 1.8106, stage1_loss_cls: 0.9682, stage1_pos_acc: 33.6609, stage1_loss_bbox: 0.5301, stage1_loss_iou: 0.8016, stage1_loss_mask: 1.3728, stage2_loss_cls: 0.9652, stage2_pos_acc: 34.4607, stage2_loss_bbox: 0.4773, stage2_loss_iou: 0.7235, stage2_loss_mask: 1.3188, stage3_loss_cls: 0.9299, stage3_pos_acc: 33.9114, stage3_loss_bbox: 0.4680, stage3_loss_iou: 0.7146, stage3_loss_mask: 1.2902, stage4_loss_cls: 0.9212, stage4_pos_acc: 33.2881, stage4_loss_bbox: 0.4629, stage4_loss_iou: 0.7069, stage4_loss_mask: 1.2299, stage5_loss_cls: 0.9221, stage5_pos_acc: 37.5577, stage5_loss_bbox: 0.4509, stage5_loss_iou: 0.7050, stage5_loss_mask: 1.2457, loss: 22.0332\n",
      "2025-07-16 12:33:03,650 - mmdet - INFO - Saving checkpoint at 3 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 12:36:07,274 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.37s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.412\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.412\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.151\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.468\n",
      "2025-07-16 12:36:09,289 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.419\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.160\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.476\n",
      "2025-07-16 12:36:12,283 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:36:12,283 - mmdet - INFO - Epoch(val) [3][750]\tbbox_mAP: 0.0330, bbox_mAP_50: 0.0600, bbox_mAP_75: 0.0330, bbox_mAP_s: 0.0040, bbox_mAP_m: 0.0040, bbox_mAP_l: 0.0410, bbox_mAP_copypaste: 0.033 0.060 0.033 0.004 0.004 0.041, segm_mAP: 0.0340, segm_mAP_50: 0.0600, segm_mAP_75: 0.0320, segm_mAP_s: 0.0040, segm_mAP_m: 0.0030, segm_mAP_l: 0.0420, segm_mAP_copypaste: 0.034 0.060 0.032 0.004 0.003 0.042\n",
      "2025-07-16 12:36:30,991 - mmdet - INFO - Epoch [4][50/750]\tlr: 2.500e-05, eta: 4:18:38, time: 0.374, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.1498, stage0_pos_acc: 29.6697, stage0_loss_bbox: 0.7341, stage0_loss_iou: 1.1611, stage0_loss_mask: 1.5505, stage1_loss_cls: 0.9731, stage1_pos_acc: 33.6749, stage1_loss_bbox: 0.4576, stage1_loss_iou: 0.7152, stage1_loss_mask: 1.0994, stage2_loss_cls: 0.9665, stage2_pos_acc: 38.1434, stage2_loss_bbox: 0.3851, stage2_loss_iou: 0.6126, stage2_loss_mask: 1.0324, stage3_loss_cls: 0.9258, stage3_pos_acc: 39.7501, stage3_loss_bbox: 0.3994, stage3_loss_iou: 0.6075, stage3_loss_mask: 1.0297, stage4_loss_cls: 0.9197, stage4_pos_acc: 36.2326, stage4_loss_bbox: 0.3944, stage4_loss_iou: 0.6035, stage4_loss_mask: 1.0179, stage5_loss_cls: 0.9218, stage5_pos_acc: 39.2898, stage5_loss_bbox: 0.3888, stage5_loss_iou: 0.6006, stage5_loss_mask: 0.9982, loss: 19.6450\n",
      "2025-07-16 12:36:47,234 - mmdet - INFO - Epoch [4][100/750]\tlr: 2.500e-05, eta: 4:18:12, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1389, stage0_pos_acc: 30.0987, stage0_loss_bbox: 0.6834, stage0_loss_iou: 1.1699, stage0_loss_mask: 1.7931, stage1_loss_cls: 1.0070, stage1_pos_acc: 33.5771, stage1_loss_bbox: 0.4328, stage1_loss_iou: 0.7406, stage1_loss_mask: 1.1112, stage2_loss_cls: 1.0042, stage2_pos_acc: 33.9225, stage2_loss_bbox: 0.3764, stage2_loss_iou: 0.6319, stage2_loss_mask: 0.9724, stage3_loss_cls: 0.9435, stage3_pos_acc: 33.4185, stage3_loss_bbox: 0.3751, stage3_loss_iou: 0.6286, stage3_loss_mask: 0.9467, stage4_loss_cls: 0.9398, stage4_pos_acc: 40.9115, stage4_loss_bbox: 0.3692, stage4_loss_iou: 0.6088, stage4_loss_mask: 0.9244, stage5_loss_cls: 0.9362, stage5_pos_acc: 38.4241, stage5_loss_bbox: 0.3576, stage5_loss_iou: 0.6158, stage5_loss_mask: 0.9353, loss: 19.6430\n",
      "2025-07-16 12:37:03,535 - mmdet - INFO - Epoch [4][150/750]\tlr: 2.500e-05, eta: 4:17:48, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1773, stage0_pos_acc: 30.6775, stage0_loss_bbox: 0.7300, stage0_loss_iou: 1.1377, stage0_loss_mask: 1.2974, stage1_loss_cls: 0.9901, stage1_pos_acc: 34.7413, stage1_loss_bbox: 0.4026, stage1_loss_iou: 0.6471, stage1_loss_mask: 0.7426, stage2_loss_cls: 0.9600, stage2_pos_acc: 30.6342, stage2_loss_bbox: 0.3492, stage2_loss_iou: 0.5387, stage2_loss_mask: 0.7021, stage3_loss_cls: 0.9223, stage3_pos_acc: 31.5390, stage3_loss_bbox: 0.3364, stage3_loss_iou: 0.5214, stage3_loss_mask: 0.6515, stage4_loss_cls: 0.9272, stage4_pos_acc: 33.2931, stage4_loss_bbox: 0.3216, stage4_loss_iou: 0.5061, stage4_loss_mask: 0.6251, stage5_loss_cls: 0.9331, stage5_pos_acc: 30.6570, stage5_loss_bbox: 0.3223, stage5_loss_iou: 0.5040, stage5_loss_mask: 0.6402, loss: 16.8861\n",
      "2025-07-16 12:37:19,855 - mmdet - INFO - Epoch [4][200/750]\tlr: 2.500e-05, eta: 4:17:24, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1817, stage0_pos_acc: 29.7266, stage0_loss_bbox: 0.6905, stage0_loss_iou: 1.2870, stage0_loss_mask: 1.8123, stage1_loss_cls: 0.9871, stage1_pos_acc: 35.7311, stage1_loss_bbox: 0.3725, stage1_loss_iou: 0.7867, stage1_loss_mask: 1.1494, stage2_loss_cls: 0.9747, stage2_pos_acc: 34.9257, stage2_loss_bbox: 0.2835, stage2_loss_iou: 0.6326, stage2_loss_mask: 1.0098, stage3_loss_cls: 0.9313, stage3_pos_acc: 34.3114, stage3_loss_bbox: 0.2707, stage3_loss_iou: 0.6012, stage3_loss_mask: 1.0187, stage4_loss_cls: 0.9188, stage4_pos_acc: 36.0407, stage4_loss_bbox: 0.2544, stage4_loss_iou: 0.5801, stage4_loss_mask: 0.9456, stage5_loss_cls: 0.9305, stage5_pos_acc: 34.8933, stage5_loss_bbox: 0.2535, stage5_loss_iou: 0.5737, stage5_loss_mask: 0.9268, loss: 19.3731\n",
      "2025-07-16 12:37:36,098 - mmdet - INFO - Epoch [4][250/750]\tlr: 2.500e-05, eta: 4:17:00, time: 0.325, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1077, stage0_pos_acc: 35.0642, stage0_loss_bbox: 0.7082, stage0_loss_iou: 1.2003, stage0_loss_mask: 1.4969, stage1_loss_cls: 0.9648, stage1_pos_acc: 31.8765, stage1_loss_bbox: 0.4609, stage1_loss_iou: 0.7552, stage1_loss_mask: 1.0183, stage2_loss_cls: 0.9562, stage2_pos_acc: 35.6091, stage2_loss_bbox: 0.3880, stage2_loss_iou: 0.6368, stage2_loss_mask: 0.9361, stage3_loss_cls: 0.9411, stage3_pos_acc: 38.2147, stage3_loss_bbox: 0.3789, stage3_loss_iou: 0.6144, stage3_loss_mask: 0.9142, stage4_loss_cls: 0.9214, stage4_pos_acc: 35.6914, stage4_loss_bbox: 0.3673, stage4_loss_iou: 0.5976, stage4_loss_mask: 0.9277, stage5_loss_cls: 0.9315, stage5_pos_acc: 35.8947, stage5_loss_bbox: 0.3556, stage5_loss_iou: 0.5939, stage5_loss_mask: 0.9117, loss: 19.0848\n",
      "2025-07-16 12:37:52,548 - mmdet - INFO - Epoch [4][300/750]\tlr: 2.500e-05, eta: 4:16:39, time: 0.329, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1550, stage0_pos_acc: 29.4585, stage0_loss_bbox: 0.7503, stage0_loss_iou: 1.2333, stage0_loss_mask: 1.7735, stage1_loss_cls: 1.0177, stage1_pos_acc: 36.4821, stage1_loss_bbox: 0.4503, stage1_loss_iou: 0.7601, stage1_loss_mask: 1.1262, stage2_loss_cls: 0.9836, stage2_pos_acc: 39.4843, stage2_loss_bbox: 0.4026, stage2_loss_iou: 0.6610, stage2_loss_mask: 1.1026, stage3_loss_cls: 0.9537, stage3_pos_acc: 37.3499, stage3_loss_bbox: 0.3754, stage3_loss_iou: 0.6360, stage3_loss_mask: 1.0692, stage4_loss_cls: 0.9410, stage4_pos_acc: 37.7430, stage4_loss_bbox: 0.3689, stage4_loss_iou: 0.6247, stage4_loss_mask: 1.0748, stage5_loss_cls: 0.9405, stage5_pos_acc: 37.8943, stage5_loss_bbox: 0.3769, stage5_loss_iou: 0.6303, stage5_loss_mask: 1.0742, loss: 20.4817\n",
      "2025-07-16 12:38:09,004 - mmdet - INFO - Epoch [4][350/750]\tlr: 2.500e-05, eta: 4:16:19, time: 0.329, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1332, stage0_pos_acc: 32.6601, stage0_loss_bbox: 0.6797, stage0_loss_iou: 1.1218, stage0_loss_mask: 1.6459, stage1_loss_cls: 0.9777, stage1_pos_acc: 35.2995, stage1_loss_bbox: 0.4622, stage1_loss_iou: 0.7586, stage1_loss_mask: 1.2582, stage2_loss_cls: 0.9619, stage2_pos_acc: 35.5259, stage2_loss_bbox: 0.4217, stage2_loss_iou: 0.6870, stage2_loss_mask: 1.2400, stage3_loss_cls: 0.9448, stage3_pos_acc: 34.1201, stage3_loss_bbox: 0.4017, stage3_loss_iou: 0.6691, stage3_loss_mask: 1.2305, stage4_loss_cls: 0.9352, stage4_pos_acc: 39.9338, stage4_loss_bbox: 0.3971, stage4_loss_iou: 0.6544, stage4_loss_mask: 1.2208, stage5_loss_cls: 0.9380, stage5_pos_acc: 41.1419, stage5_loss_bbox: 0.4021, stage5_loss_iou: 0.6536, stage5_loss_mask: 1.2327, loss: 21.0279\n",
      "2025-07-16 12:38:25,346 - mmdet - INFO - Epoch [4][400/750]\tlr: 2.500e-05, eta: 4:15:56, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1883, stage0_pos_acc: 25.6845, stage0_loss_bbox: 0.8389, stage0_loss_iou: 1.2488, stage0_loss_mask: 1.8672, stage1_loss_cls: 0.9796, stage1_pos_acc: 31.4004, stage1_loss_bbox: 0.5293, stage1_loss_iou: 0.8003, stage1_loss_mask: 1.2427, stage2_loss_cls: 0.9510, stage2_pos_acc: 38.3885, stage2_loss_bbox: 0.4735, stage2_loss_iou: 0.7109, stage2_loss_mask: 1.1847, stage3_loss_cls: 0.9376, stage3_pos_acc: 34.0012, stage3_loss_bbox: 0.4544, stage3_loss_iou: 0.6662, stage3_loss_mask: 1.1593, stage4_loss_cls: 0.9117, stage4_pos_acc: 36.5313, stage4_loss_bbox: 0.4418, stage4_loss_iou: 0.6589, stage4_loss_mask: 1.1501, stage5_loss_cls: 0.9191, stage5_pos_acc: 37.9534, stage5_loss_bbox: 0.4399, stage5_loss_iou: 0.6592, stage5_loss_mask: 1.1515, loss: 21.5649\n",
      "2025-07-16 12:38:41,651 - mmdet - INFO - Epoch [4][450/750]\tlr: 2.500e-05, eta: 4:15:34, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1721, stage0_pos_acc: 25.9574, stage0_loss_bbox: 0.7611, stage0_loss_iou: 1.2008, stage0_loss_mask: 1.7099, stage1_loss_cls: 0.9789, stage1_pos_acc: 29.8701, stage1_loss_bbox: 0.4900, stage1_loss_iou: 0.7580, stage1_loss_mask: 1.1158, stage2_loss_cls: 0.9747, stage2_pos_acc: 33.7496, stage2_loss_bbox: 0.4206, stage2_loss_iou: 0.6570, stage2_loss_mask: 1.0061, stage3_loss_cls: 0.9415, stage3_pos_acc: 31.8576, stage3_loss_bbox: 0.3983, stage3_loss_iou: 0.6282, stage3_loss_mask: 0.9945, stage4_loss_cls: 0.9225, stage4_pos_acc: 32.7797, stage4_loss_bbox: 0.3889, stage4_loss_iou: 0.6181, stage4_loss_mask: 0.9838, stage5_loss_cls: 0.9225, stage5_pos_acc: 33.2019, stage5_loss_bbox: 0.3847, stage5_loss_iou: 0.6086, stage5_loss_mask: 0.9660, loss: 20.0025\n",
      "2025-07-16 12:38:57,927 - mmdet - INFO - Epoch [4][500/750]\tlr: 2.500e-05, eta: 4:15:11, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1531, stage0_pos_acc: 29.7416, stage0_loss_bbox: 0.7444, stage0_loss_iou: 1.2410, stage0_loss_mask: 1.6957, stage1_loss_cls: 0.9845, stage1_pos_acc: 33.1550, stage1_loss_bbox: 0.4263, stage1_loss_iou: 0.7604, stage1_loss_mask: 1.0111, stage2_loss_cls: 0.9407, stage2_pos_acc: 33.8661, stage2_loss_bbox: 0.3730, stage2_loss_iou: 0.6421, stage2_loss_mask: 0.8909, stage3_loss_cls: 0.9095, stage3_pos_acc: 32.7928, stage3_loss_bbox: 0.3590, stage3_loss_iou: 0.6209, stage3_loss_mask: 0.8827, stage4_loss_cls: 0.8934, stage4_pos_acc: 34.9394, stage4_loss_bbox: 0.3521, stage4_loss_iou: 0.6142, stage4_loss_mask: 0.8631, stage5_loss_cls: 0.8843, stage5_pos_acc: 36.8984, stage5_loss_bbox: 0.3602, stage5_loss_iou: 0.6142, stage5_loss_mask: 0.8637, loss: 19.0806\n",
      "2025-07-16 12:39:14,280 - mmdet - INFO - Epoch [4][550/750]\tlr: 2.500e-05, eta: 4:14:49, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.2098, stage0_pos_acc: 25.1297, stage0_loss_bbox: 0.7179, stage0_loss_iou: 1.1590, stage0_loss_mask: 1.6738, stage1_loss_cls: 1.0400, stage1_pos_acc: 30.0215, stage1_loss_bbox: 0.4164, stage1_loss_iou: 0.7186, stage1_loss_mask: 0.9698, stage2_loss_cls: 1.0083, stage2_pos_acc: 36.4858, stage2_loss_bbox: 0.3521, stage2_loss_iou: 0.6201, stage2_loss_mask: 0.9130, stage3_loss_cls: 0.9525, stage3_pos_acc: 34.7264, stage3_loss_bbox: 0.3443, stage3_loss_iou: 0.6056, stage3_loss_mask: 0.9221, stage4_loss_cls: 0.9420, stage4_pos_acc: 34.0292, stage4_loss_bbox: 0.3276, stage4_loss_iou: 0.5923, stage4_loss_mask: 0.8956, stage5_loss_cls: 0.9471, stage5_pos_acc: 33.0798, stage5_loss_bbox: 0.3239, stage5_loss_iou: 0.5871, stage5_loss_mask: 0.8868, loss: 19.1257\n",
      "2025-07-16 12:39:30,626 - mmdet - INFO - Epoch [4][600/750]\tlr: 2.500e-05, eta: 4:14:28, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1283, stage0_pos_acc: 35.9570, stage0_loss_bbox: 0.7304, stage0_loss_iou: 1.1643, stage0_loss_mask: 1.4677, stage1_loss_cls: 0.9896, stage1_pos_acc: 30.0553, stage1_loss_bbox: 0.4750, stage1_loss_iou: 0.7144, stage1_loss_mask: 0.8785, stage2_loss_cls: 0.9660, stage2_pos_acc: 37.9256, stage2_loss_bbox: 0.4031, stage2_loss_iou: 0.6233, stage2_loss_mask: 0.7966, stage3_loss_cls: 0.9439, stage3_pos_acc: 35.8087, stage3_loss_bbox: 0.3685, stage3_loss_iou: 0.5803, stage3_loss_mask: 0.7693, stage4_loss_cls: 0.9278, stage4_pos_acc: 38.6725, stage4_loss_bbox: 0.3676, stage4_loss_iou: 0.5723, stage4_loss_mask: 0.7795, stage5_loss_cls: 0.9163, stage5_pos_acc: 36.6130, stage5_loss_bbox: 0.3592, stage5_loss_iou: 0.5729, stage5_loss_mask: 0.7866, loss: 18.2816\n",
      "2025-07-16 12:39:47,014 - mmdet - INFO - Epoch [4][650/750]\tlr: 2.500e-05, eta: 4:14:07, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1589, stage0_pos_acc: 27.2663, stage0_loss_bbox: 0.6973, stage0_loss_iou: 1.1979, stage0_loss_mask: 1.4970, stage1_loss_cls: 0.9885, stage1_pos_acc: 36.5300, stage1_loss_bbox: 0.4170, stage1_loss_iou: 0.7449, stage1_loss_mask: 1.0120, stage2_loss_cls: 0.9516, stage2_pos_acc: 37.4977, stage2_loss_bbox: 0.3700, stage2_loss_iou: 0.5972, stage2_loss_mask: 0.9746, stage3_loss_cls: 0.8953, stage3_pos_acc: 34.5513, stage3_loss_bbox: 0.3582, stage3_loss_iou: 0.5842, stage3_loss_mask: 0.9860, stage4_loss_cls: 0.8802, stage4_pos_acc: 38.7659, stage4_loss_bbox: 0.3504, stage4_loss_iou: 0.5677, stage4_loss_mask: 0.9980, stage5_loss_cls: 0.8788, stage5_pos_acc: 36.4849, stage5_loss_bbox: 0.3487, stage5_loss_iou: 0.5663, stage5_loss_mask: 0.9694, loss: 18.9903\n",
      "2025-07-16 12:40:03,375 - mmdet - INFO - Epoch [4][700/750]\tlr: 2.500e-05, eta: 4:13:47, time: 0.327, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1710, stage0_pos_acc: 30.0070, stage0_loss_bbox: 0.6895, stage0_loss_iou: 1.2159, stage0_loss_mask: 1.7385, stage1_loss_cls: 0.9838, stage1_pos_acc: 35.5788, stage1_loss_bbox: 0.4291, stage1_loss_iou: 0.7578, stage1_loss_mask: 1.1729, stage2_loss_cls: 0.9570, stage2_pos_acc: 33.3643, stage2_loss_bbox: 0.3911, stage2_loss_iou: 0.6430, stage2_loss_mask: 1.0599, stage3_loss_cls: 0.8979, stage3_pos_acc: 38.2683, stage3_loss_bbox: 0.3659, stage3_loss_iou: 0.6236, stage3_loss_mask: 1.0358, stage4_loss_cls: 0.8874, stage4_pos_acc: 36.2818, stage4_loss_bbox: 0.3484, stage4_loss_iou: 0.6018, stage4_loss_mask: 0.9868, stage5_loss_cls: 0.8854, stage5_pos_acc: 35.3692, stage5_loss_bbox: 0.3514, stage5_loss_iou: 0.5957, stage5_loss_mask: 0.9929, loss: 19.7827\n",
      "2025-07-16 12:40:19,685 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:40:19,685 - mmdet - INFO - Epoch [4][750/750]\tlr: 2.500e-05, eta: 4:13:25, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1160, stage0_pos_acc: 31.0055, stage0_loss_bbox: 0.7812, stage0_loss_iou: 1.2753, stage0_loss_mask: 1.9109, stage1_loss_cls: 0.9829, stage1_pos_acc: 34.4859, stage1_loss_bbox: 0.4752, stage1_loss_iou: 0.8417, stage1_loss_mask: 1.3845, stage2_loss_cls: 0.9553, stage2_pos_acc: 35.1359, stage2_loss_bbox: 0.4379, stage2_loss_iou: 0.7184, stage2_loss_mask: 1.2936, stage3_loss_cls: 0.9356, stage3_pos_acc: 37.4692, stage3_loss_bbox: 0.3982, stage3_loss_iou: 0.6812, stage3_loss_mask: 1.2497, stage4_loss_cls: 0.9283, stage4_pos_acc: 40.6094, stage4_loss_bbox: 0.3920, stage4_loss_iou: 0.6667, stage4_loss_mask: 1.2232, stage5_loss_cls: 0.9252, stage5_pos_acc: 34.0682, stage5_loss_bbox: 0.3917, stage5_loss_iou: 0.6559, stage5_loss_mask: 1.2179, loss: 21.8384\n",
      "2025-07-16 12:40:19,781 - mmdet - INFO - Saving checkpoint at 4 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 87s, ETA:     0s2025-07-16 12:43:24,089 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.42s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.044\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.168\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.470\n",
      "2025-07-16 12:43:26,002 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.42s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.003\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.044\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.177\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.473\n",
      "2025-07-16 12:43:29,099 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:43:29,100 - mmdet - INFO - Epoch(val) [4][750]\tbbox_mAP: 0.0330, bbox_mAP_50: 0.0590, bbox_mAP_75: 0.0320, bbox_mAP_s: 0.0030, bbox_mAP_m: 0.0060, bbox_mAP_l: 0.0440, bbox_mAP_copypaste: 0.033 0.059 0.032 0.003 0.006 0.044, segm_mAP: 0.0330, segm_mAP_50: 0.0590, segm_mAP_75: 0.0300, segm_mAP_s: 0.0030, segm_mAP_m: 0.0060, segm_mAP_l: 0.0440, segm_mAP_copypaste: 0.033 0.059 0.030 0.003 0.006 0.044\n",
      "2025-07-16 12:43:48,302 - mmdet - INFO - Epoch [5][50/750]\tlr: 2.500e-05, eta: 4:13:47, time: 0.384, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.1661, stage0_pos_acc: 31.7025, stage0_loss_bbox: 0.7794, stage0_loss_iou: 1.2090, stage0_loss_mask: 1.8882, stage1_loss_cls: 0.9915, stage1_pos_acc: 34.8017, stage1_loss_bbox: 0.5015, stage1_loss_iou: 0.7921, stage1_loss_mask: 1.4319, stage2_loss_cls: 0.9536, stage2_pos_acc: 38.3859, stage2_loss_bbox: 0.4251, stage2_loss_iou: 0.6719, stage2_loss_mask: 1.3563, stage3_loss_cls: 0.9174, stage3_pos_acc: 39.7764, stage3_loss_bbox: 0.3993, stage3_loss_iou: 0.6583, stage3_loss_mask: 1.2731, stage4_loss_cls: 0.8918, stage4_pos_acc: 39.7740, stage4_loss_bbox: 0.3828, stage4_loss_iou: 0.6322, stage4_loss_mask: 1.2507, stage5_loss_cls: 0.8969, stage5_pos_acc: 39.8264, stage5_loss_bbox: 0.3724, stage5_loss_iou: 0.6190, stage5_loss_mask: 1.2335, loss: 21.6943\n",
      "2025-07-16 12:44:05,040 - mmdet - INFO - Epoch [5][100/750]\tlr: 2.500e-05, eta: 4:13:32, time: 0.335, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.2022, stage0_pos_acc: 25.7021, stage0_loss_bbox: 0.7471, stage0_loss_iou: 1.2908, stage0_loss_mask: 1.7290, stage1_loss_cls: 0.9816, stage1_pos_acc: 33.9358, stage1_loss_bbox: 0.4118, stage1_loss_iou: 0.7744, stage1_loss_mask: 1.0881, stage2_loss_cls: 0.9253, stage2_pos_acc: 40.5357, stage2_loss_bbox: 0.3392, stage2_loss_iou: 0.6361, stage2_loss_mask: 0.9647, stage3_loss_cls: 0.8612, stage3_pos_acc: 37.7167, stage3_loss_bbox: 0.3288, stage3_loss_iou: 0.5795, stage3_loss_mask: 0.9089, stage4_loss_cls: 0.8355, stage4_pos_acc: 41.4649, stage4_loss_bbox: 0.3181, stage4_loss_iou: 0.5506, stage4_loss_mask: 0.9019, stage5_loss_cls: 0.8320, stage5_pos_acc: 42.3811, stage5_loss_bbox: 0.3284, stage5_loss_iou: 0.5461, stage5_loss_mask: 0.8983, loss: 18.9796\n",
      "2025-07-16 12:44:21,337 - mmdet - INFO - Epoch [5][150/750]\tlr: 2.500e-05, eta: 4:13:10, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1844, stage0_pos_acc: 28.2220, stage0_loss_bbox: 0.6521, stage0_loss_iou: 1.1241, stage0_loss_mask: 1.3403, stage1_loss_cls: 1.0083, stage1_pos_acc: 34.2304, stage1_loss_bbox: 0.3812, stage1_loss_iou: 0.6444, stage1_loss_mask: 0.7945, stage2_loss_cls: 0.9672, stage2_pos_acc: 42.0167, stage2_loss_bbox: 0.3296, stage2_loss_iou: 0.5516, stage2_loss_mask: 0.7336, stage3_loss_cls: 0.9375, stage3_pos_acc: 36.8976, stage3_loss_bbox: 0.3243, stage3_loss_iou: 0.5403, stage3_loss_mask: 0.7074, stage4_loss_cls: 0.8881, stage4_pos_acc: 38.0018, stage4_loss_bbox: 0.3213, stage4_loss_iou: 0.5329, stage4_loss_mask: 0.7122, stage5_loss_cls: 0.8913, stage5_pos_acc: 38.8101, stage5_loss_bbox: 0.3151, stage5_loss_iou: 0.5252, stage5_loss_mask: 0.7139, loss: 17.1207\n",
      "2025-07-16 12:44:37,498 - mmdet - INFO - Epoch [5][200/750]\tlr: 2.500e-05, eta: 4:12:46, time: 0.323, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1848, stage0_pos_acc: 28.2338, stage0_loss_bbox: 0.6517, stage0_loss_iou: 1.0439, stage0_loss_mask: 1.2491, stage1_loss_cls: 0.9953, stage1_pos_acc: 34.1534, stage1_loss_bbox: 0.3418, stage1_loss_iou: 0.5996, stage1_loss_mask: 0.7469, stage2_loss_cls: 0.9463, stage2_pos_acc: 38.1008, stage2_loss_bbox: 0.2986, stage2_loss_iou: 0.5149, stage2_loss_mask: 0.7314, stage3_loss_cls: 0.9123, stage3_pos_acc: 38.8030, stage3_loss_bbox: 0.2781, stage3_loss_iou: 0.4988, stage3_loss_mask: 0.7047, stage4_loss_cls: 0.9042, stage4_pos_acc: 38.3054, stage4_loss_bbox: 0.2714, stage4_loss_iou: 0.4909, stage4_loss_mask: 0.6776, stage5_loss_cls: 0.8913, stage5_pos_acc: 38.2901, stage5_loss_bbox: 0.2755, stage5_loss_iou: 0.4907, stage5_loss_mask: 0.6747, loss: 16.3746\n",
      "2025-07-16 12:44:53,712 - mmdet - INFO - Epoch [5][250/750]\tlr: 2.500e-05, eta: 4:12:23, time: 0.324, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1585, stage0_pos_acc: 27.9135, stage0_loss_bbox: 0.6690, stage0_loss_iou: 1.1036, stage0_loss_mask: 1.4197, stage1_loss_cls: 0.9755, stage1_pos_acc: 35.7770, stage1_loss_bbox: 0.3860, stage1_loss_iou: 0.6566, stage1_loss_mask: 0.8638, stage2_loss_cls: 0.9682, stage2_pos_acc: 37.9460, stage2_loss_bbox: 0.3325, stage2_loss_iou: 0.5568, stage2_loss_mask: 0.7562, stage3_loss_cls: 0.9040, stage3_pos_acc: 34.8579, stage3_loss_bbox: 0.3172, stage3_loss_iou: 0.5459, stage3_loss_mask: 0.7820, stage4_loss_cls: 0.9082, stage4_pos_acc: 35.1310, stage4_loss_bbox: 0.3106, stage4_loss_iou: 0.5215, stage4_loss_mask: 0.7227, stage5_loss_cls: 0.9124, stage5_pos_acc: 32.9452, stage5_loss_bbox: 0.3019, stage5_loss_iou: 0.5052, stage5_loss_mask: 0.7082, loss: 17.2861\n",
      "2025-07-16 12:45:10,093 - mmdet - INFO - Epoch [5][300/750]\tlr: 2.500e-05, eta: 4:12:03, time: 0.328, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1808, stage0_pos_acc: 27.2602, stage0_loss_bbox: 0.6886, stage0_loss_iou: 1.1865, stage0_loss_mask: 1.5671, stage1_loss_cls: 0.9720, stage1_pos_acc: 34.5309, stage1_loss_bbox: 0.4261, stage1_loss_iou: 0.7241, stage1_loss_mask: 1.1561, stage2_loss_cls: 0.9327, stage2_pos_acc: 33.4798, stage2_loss_bbox: 0.3726, stage2_loss_iou: 0.6116, stage2_loss_mask: 1.0900, stage3_loss_cls: 0.9084, stage3_pos_acc: 34.1221, stage3_loss_bbox: 0.3565, stage3_loss_iou: 0.5903, stage3_loss_mask: 1.0655, stage4_loss_cls: 0.8824, stage4_pos_acc: 33.8440, stage4_loss_bbox: 0.3395, stage4_loss_iou: 0.5788, stage4_loss_mask: 1.0608, stage5_loss_cls: 0.8917, stage5_pos_acc: 32.5637, stage5_loss_bbox: 0.3362, stage5_loss_iou: 0.5751, stage5_loss_mask: 1.0543, loss: 19.5475\n",
      "2025-07-16 12:45:26,710 - mmdet - INFO - Epoch [5][350/750]\tlr: 2.500e-05, eta: 4:11:46, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1262, stage0_pos_acc: 35.6794, stage0_loss_bbox: 0.8030, stage0_loss_iou: 1.3222, stage0_loss_mask: 2.0045, stage1_loss_cls: 0.9579, stage1_pos_acc: 34.5960, stage1_loss_bbox: 0.5060, stage1_loss_iou: 0.8259, stage1_loss_mask: 1.2610, stage2_loss_cls: 0.9428, stage2_pos_acc: 37.0778, stage2_loss_bbox: 0.4304, stage2_loss_iou: 0.7195, stage2_loss_mask: 1.1531, stage3_loss_cls: 0.9054, stage3_pos_acc: 38.4317, stage3_loss_bbox: 0.4146, stage3_loss_iou: 0.6853, stage3_loss_mask: 1.1128, stage4_loss_cls: 0.9016, stage4_pos_acc: 39.2921, stage4_loss_bbox: 0.4122, stage4_loss_iou: 0.6738, stage4_loss_mask: 1.1095, stage5_loss_cls: 0.8938, stage5_pos_acc: 37.8984, stage5_loss_bbox: 0.4107, stage5_loss_iou: 0.6683, stage5_loss_mask: 1.1177, loss: 21.3577\n",
      "2025-07-16 12:45:42,994 - mmdet - INFO - Epoch [5][400/750]\tlr: 2.500e-05, eta: 4:11:25, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1232, stage0_pos_acc: 27.2620, stage0_loss_bbox: 0.7104, stage0_loss_iou: 1.1780, stage0_loss_mask: 1.7183, stage1_loss_cls: 0.9511, stage1_pos_acc: 33.7313, stage1_loss_bbox: 0.4128, stage1_loss_iou: 0.7393, stage1_loss_mask: 1.0623, stage2_loss_cls: 0.9239, stage2_pos_acc: 39.4351, stage2_loss_bbox: 0.3613, stage2_loss_iou: 0.6345, stage2_loss_mask: 1.0323, stage3_loss_cls: 0.8803, stage3_pos_acc: 35.4145, stage3_loss_bbox: 0.3383, stage3_loss_iou: 0.6131, stage3_loss_mask: 0.9733, stage4_loss_cls: 0.8666, stage4_pos_acc: 38.3217, stage4_loss_bbox: 0.3362, stage4_loss_iou: 0.6010, stage4_loss_mask: 0.9851, stage5_loss_cls: 0.8738, stage5_pos_acc: 39.5185, stage5_loss_bbox: 0.3265, stage5_loss_iou: 0.5887, stage5_loss_mask: 0.9669, loss: 19.1970\n",
      "2025-07-16 12:45:59,375 - mmdet - INFO - Epoch [5][450/750]\tlr: 2.500e-05, eta: 4:11:05, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1607, stage0_pos_acc: 31.9116, stage0_loss_bbox: 0.6355, stage0_loss_iou: 1.1608, stage0_loss_mask: 1.4518, stage1_loss_cls: 0.9895, stage1_pos_acc: 34.2485, stage1_loss_bbox: 0.3651, stage1_loss_iou: 0.6918, stage1_loss_mask: 0.8610, stage2_loss_cls: 0.9407, stage2_pos_acc: 34.5835, stage2_loss_bbox: 0.3181, stage2_loss_iou: 0.5579, stage2_loss_mask: 0.7970, stage3_loss_cls: 0.9000, stage3_pos_acc: 38.0397, stage3_loss_bbox: 0.3118, stage3_loss_iou: 0.5496, stage3_loss_mask: 0.7480, stage4_loss_cls: 0.8791, stage4_pos_acc: 37.6633, stage4_loss_bbox: 0.3098, stage4_loss_iou: 0.5338, stage4_loss_mask: 0.7657, stage5_loss_cls: 0.8793, stage5_pos_acc: 36.8691, stage5_loss_bbox: 0.3144, stage5_loss_iou: 0.5276, stage5_loss_mask: 0.7713, loss: 17.4201\n",
      "2025-07-16 12:46:16,054 - mmdet - INFO - Epoch [5][500/750]\tlr: 2.500e-05, eta: 4:10:49, time: 0.334, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.1683, stage0_pos_acc: 30.2832, stage0_loss_bbox: 0.6713, stage0_loss_iou: 1.1946, stage0_loss_mask: 1.5587, stage1_loss_cls: 0.9878, stage1_pos_acc: 36.7825, stage1_loss_bbox: 0.4330, stage1_loss_iou: 0.7409, stage1_loss_mask: 1.1292, stage2_loss_cls: 0.9505, stage2_pos_acc: 39.3229, stage2_loss_bbox: 0.3706, stage2_loss_iou: 0.6266, stage2_loss_mask: 1.0136, stage3_loss_cls: 0.9079, stage3_pos_acc: 39.3844, stage3_loss_bbox: 0.3541, stage3_loss_iou: 0.6000, stage3_loss_mask: 0.9763, stage4_loss_cls: 0.8870, stage4_pos_acc: 35.7242, stage4_loss_bbox: 0.3551, stage4_loss_iou: 0.5876, stage4_loss_mask: 0.9492, stage5_loss_cls: 0.8804, stage5_pos_acc: 41.2557, stage5_loss_bbox: 0.3518, stage5_loss_iou: 0.5855, stage5_loss_mask: 0.9451, loss: 19.2250\n",
      "2025-07-16 12:46:32,319 - mmdet - INFO - Epoch [5][550/750]\tlr: 2.500e-05, eta: 4:10:28, time: 0.325, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1766, stage0_pos_acc: 26.3381, stage0_loss_bbox: 0.6925, stage0_loss_iou: 1.1428, stage0_loss_mask: 1.5061, stage1_loss_cls: 0.9657, stage1_pos_acc: 35.2714, stage1_loss_bbox: 0.4011, stage1_loss_iou: 0.7379, stage1_loss_mask: 1.0357, stage2_loss_cls: 0.9697, stage2_pos_acc: 32.0722, stage2_loss_bbox: 0.3129, stage2_loss_iou: 0.5861, stage2_loss_mask: 0.8990, stage3_loss_cls: 0.9187, stage3_pos_acc: 32.6167, stage3_loss_bbox: 0.3077, stage3_loss_iou: 0.5522, stage3_loss_mask: 0.8493, stage4_loss_cls: 0.8907, stage4_pos_acc: 35.3373, stage4_loss_bbox: 0.3043, stage4_loss_iou: 0.5504, stage4_loss_mask: 0.8428, stage5_loss_cls: 0.8880, stage5_pos_acc: 36.3778, stage5_loss_bbox: 0.3004, stage5_loss_iou: 0.5493, stage5_loss_mask: 0.8301, loss: 18.2101\n",
      "2025-07-16 12:46:48,667 - mmdet - INFO - Epoch [5][600/750]\tlr: 2.500e-05, eta: 4:10:07, time: 0.327, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1402, stage0_pos_acc: 29.7607, stage0_loss_bbox: 0.6503, stage0_loss_iou: 1.0857, stage0_loss_mask: 1.4429, stage1_loss_cls: 0.9824, stage1_pos_acc: 32.7710, stage1_loss_bbox: 0.4084, stage1_loss_iou: 0.7008, stage1_loss_mask: 1.0937, stage2_loss_cls: 0.9453, stage2_pos_acc: 36.1974, stage2_loss_bbox: 0.3754, stage2_loss_iou: 0.6404, stage2_loss_mask: 1.0264, stage3_loss_cls: 0.9405, stage3_pos_acc: 34.1227, stage3_loss_bbox: 0.3535, stage3_loss_iou: 0.6001, stage3_loss_mask: 0.9755, stage4_loss_cls: 0.9157, stage4_pos_acc: 33.5624, stage4_loss_bbox: 0.3365, stage4_loss_iou: 0.5950, stage4_loss_mask: 0.9748, stage5_loss_cls: 0.9167, stage5_pos_acc: 34.9276, stage5_loss_bbox: 0.3350, stage5_loss_iou: 0.5933, stage5_loss_mask: 0.9603, loss: 18.9887\n",
      "2025-07-16 12:47:04,932 - mmdet - INFO - Epoch [5][650/750]\tlr: 2.500e-05, eta: 4:09:46, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1815, stage0_pos_acc: 29.3321, stage0_loss_bbox: 0.7168, stage0_loss_iou: 1.1242, stage0_loss_mask: 1.6391, stage1_loss_cls: 1.0043, stage1_pos_acc: 33.6504, stage1_loss_bbox: 0.4418, stage1_loss_iou: 0.7297, stage1_loss_mask: 1.1164, stage2_loss_cls: 0.9736, stage2_pos_acc: 36.5841, stage2_loss_bbox: 0.3954, stage2_loss_iou: 0.6161, stage2_loss_mask: 1.0144, stage3_loss_cls: 0.9570, stage3_pos_acc: 32.5175, stage3_loss_bbox: 0.3812, stage3_loss_iou: 0.5897, stage3_loss_mask: 1.0103, stage4_loss_cls: 0.9442, stage4_pos_acc: 36.0254, stage4_loss_bbox: 0.3655, stage4_loss_iou: 0.5674, stage4_loss_mask: 1.0074, stage5_loss_cls: 0.9448, stage5_pos_acc: 32.8202, stage5_loss_bbox: 0.3743, stage5_loss_iou: 0.5629, stage5_loss_mask: 0.9860, loss: 19.6439\n",
      "2025-07-16 12:47:21,240 - mmdet - INFO - Epoch [5][700/750]\tlr: 2.500e-05, eta: 4:09:26, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1435, stage0_pos_acc: 29.3613, stage0_loss_bbox: 0.7092, stage0_loss_iou: 1.1552, stage0_loss_mask: 1.5484, stage1_loss_cls: 0.9744, stage1_pos_acc: 35.3097, stage1_loss_bbox: 0.4538, stage1_loss_iou: 0.7285, stage1_loss_mask: 1.0635, stage2_loss_cls: 0.9475, stage2_pos_acc: 39.3819, stage2_loss_bbox: 0.4063, stage2_loss_iou: 0.6442, stage2_loss_mask: 0.9680, stage3_loss_cls: 0.9296, stage3_pos_acc: 37.6076, stage3_loss_bbox: 0.3932, stage3_loss_iou: 0.6252, stage3_loss_mask: 0.9450, stage4_loss_cls: 0.9181, stage4_pos_acc: 41.3332, stage4_loss_bbox: 0.3876, stage4_loss_iou: 0.6191, stage4_loss_mask: 0.9571, stage5_loss_cls: 0.9182, stage5_pos_acc: 35.4089, stage5_loss_bbox: 0.3972, stage5_loss_iou: 0.6147, stage5_loss_mask: 0.9760, loss: 19.4237\n",
      "2025-07-16 12:47:37,609 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:47:37,609 - mmdet - INFO - Epoch [5][750/750]\tlr: 2.500e-05, eta: 4:09:07, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1251, stage0_pos_acc: 27.4198, stage0_loss_bbox: 0.7389, stage0_loss_iou: 1.2713, stage0_loss_mask: 1.6871, stage1_loss_cls: 0.9325, stage1_pos_acc: 35.0761, stage1_loss_bbox: 0.4702, stage1_loss_iou: 0.7830, stage1_loss_mask: 1.1156, stage2_loss_cls: 0.8869, stage2_pos_acc: 36.9368, stage2_loss_bbox: 0.3850, stage2_loss_iou: 0.6414, stage2_loss_mask: 1.0293, stage3_loss_cls: 0.8572, stage3_pos_acc: 41.4007, stage3_loss_bbox: 0.3558, stage3_loss_iou: 0.6038, stage3_loss_mask: 1.0018, stage4_loss_cls: 0.8490, stage4_pos_acc: 39.6985, stage4_loss_bbox: 0.3504, stage4_loss_iou: 0.5944, stage4_loss_mask: 1.0100, stage5_loss_cls: 0.8473, stage5_pos_acc: 37.3603, stage5_loss_bbox: 0.3512, stage5_loss_iou: 0.5848, stage5_loss_mask: 0.9896, loss: 19.4618\n",
      "2025-07-16 12:47:37,711 - mmdet - INFO - Saving checkpoint at 5 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 12:50:40,458 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.419\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.421\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.421\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.067\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.458\n",
      "2025-07-16 12:50:42,369 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.75s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.429\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.431\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.431\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.067\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.251\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.470\n",
      "2025-07-16 12:50:45,256 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:50:45,256 - mmdet - INFO - Epoch(val) [5][750]\tbbox_mAP: 0.0290, bbox_mAP_50: 0.0540, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.0530, bbox_mAP_m: 0.0100, bbox_mAP_l: 0.0370, bbox_mAP_copypaste: 0.029 0.054 0.028 0.053 0.010 0.037, segm_mAP: 0.0290, segm_mAP_50: 0.0540, segm_mAP_75: 0.0290, segm_mAP_s: 0.0530, segm_mAP_m: 0.0100, segm_mAP_l: 0.0380, segm_mAP_copypaste: 0.029 0.054 0.029 0.053 0.010 0.038\n",
      "2025-07-16 12:51:04,096 - mmdet - INFO - Epoch [6][50/750]\tlr: 2.500e-05, eta: 4:09:16, time: 0.377, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.1787, stage0_pos_acc: 27.2659, stage0_loss_bbox: 0.6759, stage0_loss_iou: 1.1308, stage0_loss_mask: 1.3117, stage1_loss_cls: 0.9869, stage1_pos_acc: 41.9918, stage1_loss_bbox: 0.3860, stage1_loss_iou: 0.6865, stage1_loss_mask: 0.9103, stage2_loss_cls: 0.9429, stage2_pos_acc: 39.5727, stage2_loss_bbox: 0.3236, stage2_loss_iou: 0.5515, stage2_loss_mask: 0.7863, stage3_loss_cls: 0.8976, stage3_pos_acc: 41.4179, stage3_loss_bbox: 0.3035, stage3_loss_iou: 0.5122, stage3_loss_mask: 0.7543, stage4_loss_cls: 0.8748, stage4_pos_acc: 40.8108, stage4_loss_bbox: 0.2908, stage4_loss_iou: 0.4990, stage4_loss_mask: 0.7405, stage5_loss_cls: 0.8688, stage5_pos_acc: 40.4822, stage5_loss_bbox: 0.2971, stage5_loss_iou: 0.4996, stage5_loss_mask: 0.7374, loss: 17.1467\n",
      "2025-07-16 12:51:20,651 - mmdet - INFO - Epoch [6][100/750]\tlr: 2.500e-05, eta: 4:08:59, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1933, stage0_pos_acc: 28.8251, stage0_loss_bbox: 0.6469, stage0_loss_iou: 1.0916, stage0_loss_mask: 1.2683, stage1_loss_cls: 0.9743, stage1_pos_acc: 32.0502, stage1_loss_bbox: 0.3667, stage1_loss_iou: 0.6330, stage1_loss_mask: 0.8724, stage2_loss_cls: 0.9397, stage2_pos_acc: 38.2046, stage2_loss_bbox: 0.3205, stage2_loss_iou: 0.5404, stage2_loss_mask: 0.7960, stage3_loss_cls: 0.9027, stage3_pos_acc: 38.9546, stage3_loss_bbox: 0.3096, stage3_loss_iou: 0.5273, stage3_loss_mask: 0.7567, stage4_loss_cls: 0.8646, stage4_pos_acc: 38.7826, stage4_loss_bbox: 0.3188, stage4_loss_iou: 0.5329, stage4_loss_mask: 0.7779, stage5_loss_cls: 0.8640, stage5_pos_acc: 39.0017, stage5_loss_bbox: 0.3149, stage5_loss_iou: 0.5319, stage5_loss_mask: 0.7522, loss: 17.0966\n",
      "2025-07-16 12:51:37,212 - mmdet - INFO - Epoch [6][150/750]\tlr: 2.500e-05, eta: 4:08:41, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1727, stage0_pos_acc: 29.0789, stage0_loss_bbox: 0.6420, stage0_loss_iou: 1.1568, stage0_loss_mask: 1.2813, stage1_loss_cls: 0.9718, stage1_pos_acc: 28.8584, stage1_loss_bbox: 0.3358, stage1_loss_iou: 0.6492, stage1_loss_mask: 0.8814, stage2_loss_cls: 0.9116, stage2_pos_acc: 32.9202, stage2_loss_bbox: 0.2853, stage2_loss_iou: 0.5396, stage2_loss_mask: 0.7991, stage3_loss_cls: 0.8744, stage3_pos_acc: 36.2843, stage3_loss_bbox: 0.2777, stage3_loss_iou: 0.5145, stage3_loss_mask: 0.7200, stage4_loss_cls: 0.8429, stage4_pos_acc: 38.8996, stage4_loss_bbox: 0.2737, stage4_loss_iou: 0.5066, stage4_loss_mask: 0.7254, stage5_loss_cls: 0.8298, stage5_pos_acc: 41.0108, stage5_loss_bbox: 0.2818, stage5_loss_iou: 0.5157, stage5_loss_mask: 0.7272, loss: 16.7164\n",
      "2025-07-16 12:51:53,907 - mmdet - INFO - Epoch [6][200/750]\tlr: 2.500e-05, eta: 4:08:25, time: 0.334, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1575, stage0_pos_acc: 34.6255, stage0_loss_bbox: 0.5717, stage0_loss_iou: 1.0488, stage0_loss_mask: 1.1504, stage1_loss_cls: 0.9371, stage1_pos_acc: 37.2047, stage1_loss_bbox: 0.3131, stage1_loss_iou: 0.5875, stage1_loss_mask: 0.6894, stage2_loss_cls: 0.8947, stage2_pos_acc: 41.4880, stage2_loss_bbox: 0.2650, stage2_loss_iou: 0.5016, stage2_loss_mask: 0.6100, stage3_loss_cls: 0.8680, stage3_pos_acc: 39.9827, stage3_loss_bbox: 0.2540, stage3_loss_iou: 0.4742, stage3_loss_mask: 0.5726, stage4_loss_cls: 0.8430, stage4_pos_acc: 39.2098, stage4_loss_bbox: 0.2372, stage4_loss_iou: 0.4624, stage4_loss_mask: 0.5739, stage5_loss_cls: 0.8483, stage5_pos_acc: 40.6491, stage5_loss_bbox: 0.2358, stage5_loss_iou: 0.4581, stage5_loss_mask: 0.5785, loss: 15.1327\n",
      "2025-07-16 12:52:10,451 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:52:10,451 - mmdet - INFO - Epoch [6][250/750]\tlr: 2.500e-05, eta: 4:08:08, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1334, stage0_pos_acc: 35.0392, stage0_loss_bbox: 0.6311, stage0_loss_iou: 1.1177, stage0_loss_mask: 1.4444, stage1_loss_cls: 0.8995, stage1_pos_acc: 38.3962, stage1_loss_bbox: 0.4206, stage1_loss_iou: 0.6856, stage1_loss_mask: 1.0145, stage2_loss_cls: 0.8786, stage2_pos_acc: 39.5435, stage2_loss_bbox: 0.3619, stage2_loss_iou: 0.5878, stage2_loss_mask: 0.9868, stage3_loss_cls: 0.8268, stage3_pos_acc: 39.1927, stage3_loss_bbox: 0.3659, stage3_loss_iou: 0.5738, stage3_loss_mask: 0.9696, stage4_loss_cls: 0.8090, stage4_pos_acc: 40.0991, stage4_loss_bbox: 0.3539, stage4_loss_iou: 0.5630, stage4_loss_mask: 0.9394, stage5_loss_cls: 0.8118, stage5_pos_acc: 39.9543, stage5_loss_bbox: 0.3435, stage5_loss_iou: 0.5607, stage5_loss_mask: 0.9504, loss: 18.2298\n",
      "2025-07-16 12:52:27,084 - mmdet - INFO - Epoch [6][300/750]\tlr: 2.500e-05, eta: 4:07:51, time: 0.333, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1755, stage0_pos_acc: 33.7700, stage0_loss_bbox: 0.6575, stage0_loss_iou: 1.1523, stage0_loss_mask: 1.5156, stage1_loss_cls: 0.9926, stage1_pos_acc: 38.3098, stage1_loss_bbox: 0.3762, stage1_loss_iou: 0.7096, stage1_loss_mask: 1.0280, stage2_loss_cls: 0.9444, stage2_pos_acc: 45.5167, stage2_loss_bbox: 0.3289, stage2_loss_iou: 0.6058, stage2_loss_mask: 0.9747, stage3_loss_cls: 0.9045, stage3_pos_acc: 44.9480, stage3_loss_bbox: 0.3287, stage3_loss_iou: 0.5997, stage3_loss_mask: 0.9965, stage4_loss_cls: 0.8903, stage4_pos_acc: 43.5563, stage4_loss_bbox: 0.3178, stage4_loss_iou: 0.5834, stage4_loss_mask: 0.9829, stage5_loss_cls: 0.8905, stage5_pos_acc: 45.7775, stage5_loss_bbox: 0.3202, stage5_loss_iou: 0.5840, stage5_loss_mask: 0.9857, loss: 18.8453\n",
      "2025-07-16 12:52:43,800 - mmdet - INFO - Epoch [6][350/750]\tlr: 2.500e-05, eta: 4:07:35, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1879, stage0_pos_acc: 33.2215, stage0_loss_bbox: 0.7056, stage0_loss_iou: 1.2313, stage0_loss_mask: 1.6627, stage1_loss_cls: 0.9371, stage1_pos_acc: 33.5200, stage1_loss_bbox: 0.4058, stage1_loss_iou: 0.7433, stage1_loss_mask: 1.0468, stage2_loss_cls: 0.8714, stage2_pos_acc: 39.4073, stage2_loss_bbox: 0.3433, stage2_loss_iou: 0.6244, stage2_loss_mask: 0.9669, stage3_loss_cls: 0.8281, stage3_pos_acc: 39.7763, stage3_loss_bbox: 0.3177, stage3_loss_iou: 0.5835, stage3_loss_mask: 0.9648, stage4_loss_cls: 0.8181, stage4_pos_acc: 42.2418, stage4_loss_bbox: 0.3020, stage4_loss_iou: 0.5529, stage4_loss_mask: 0.9396, stage5_loss_cls: 0.8074, stage5_pos_acc: 41.9489, stage5_loss_bbox: 0.2983, stage5_loss_iou: 0.5488, stage5_loss_mask: 0.9487, loss: 18.6365\n",
      "2025-07-16 12:53:00,932 - mmdet - INFO - Epoch [6][400/750]\tlr: 2.500e-05, eta: 4:07:24, time: 0.343, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1014, stage0_pos_acc: 32.2947, stage0_loss_bbox: 0.6225, stage0_loss_iou: 1.0896, stage0_loss_mask: 1.5775, stage1_loss_cls: 0.9070, stage1_pos_acc: 40.0833, stage1_loss_bbox: 0.3712, stage1_loss_iou: 0.6763, stage1_loss_mask: 0.9986, stage2_loss_cls: 0.8874, stage2_pos_acc: 44.2925, stage2_loss_bbox: 0.3307, stage2_loss_iou: 0.6015, stage2_loss_mask: 0.9560, stage3_loss_cls: 0.8483, stage3_pos_acc: 41.0412, stage3_loss_bbox: 0.3257, stage3_loss_iou: 0.5835, stage3_loss_mask: 0.8655, stage4_loss_cls: 0.8288, stage4_pos_acc: 40.3199, stage4_loss_bbox: 0.3203, stage4_loss_iou: 0.5776, stage4_loss_mask: 0.8556, stage5_loss_cls: 0.8318, stage5_pos_acc: 41.8387, stage5_loss_bbox: 0.3116, stage5_loss_iou: 0.5665, stage5_loss_mask: 0.8046, loss: 17.8396\n",
      "2025-07-16 12:53:17,755 - mmdet - INFO - Epoch [6][450/750]\tlr: 2.500e-05, eta: 4:07:09, time: 0.336, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1564, stage0_pos_acc: 25.1179, stage0_loss_bbox: 0.6877, stage0_loss_iou: 1.1687, stage0_loss_mask: 1.7496, stage1_loss_cls: 0.9613, stage1_pos_acc: 35.4961, stage1_loss_bbox: 0.4338, stage1_loss_iou: 0.7424, stage1_loss_mask: 1.1741, stage2_loss_cls: 0.9264, stage2_pos_acc: 36.5515, stage2_loss_bbox: 0.3826, stage2_loss_iou: 0.6513, stage2_loss_mask: 1.0584, stage3_loss_cls: 0.9070, stage3_pos_acc: 38.8951, stage3_loss_bbox: 0.3710, stage3_loss_iou: 0.6122, stage3_loss_mask: 0.9869, stage4_loss_cls: 0.8921, stage4_pos_acc: 35.5157, stage4_loss_bbox: 0.3616, stage4_loss_iou: 0.6053, stage4_loss_mask: 0.9977, stage5_loss_cls: 0.8859, stage5_pos_acc: 38.9276, stage5_loss_bbox: 0.3658, stage5_loss_iou: 0.6061, stage5_loss_mask: 1.0014, loss: 19.6858\n",
      "2025-07-16 12:53:34,070 - mmdet - INFO - Epoch [6][500/750]\tlr: 2.500e-05, eta: 4:06:49, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1218, stage0_pos_acc: 31.2388, stage0_loss_bbox: 0.6098, stage0_loss_iou: 1.0463, stage0_loss_mask: 1.1826, stage1_loss_cls: 0.9571, stage1_pos_acc: 34.7577, stage1_loss_bbox: 0.3552, stage1_loss_iou: 0.6075, stage1_loss_mask: 0.8315, stage2_loss_cls: 0.9413, stage2_pos_acc: 31.9210, stage2_loss_bbox: 0.3168, stage2_loss_iou: 0.5205, stage2_loss_mask: 0.7728, stage3_loss_cls: 0.8862, stage3_pos_acc: 33.9520, stage3_loss_bbox: 0.2960, stage3_loss_iou: 0.5147, stage3_loss_mask: 0.7879, stage4_loss_cls: 0.8718, stage4_pos_acc: 36.4170, stage4_loss_bbox: 0.2858, stage4_loss_iou: 0.4999, stage4_loss_mask: 0.7636, stage5_loss_cls: 0.8728, stage5_pos_acc: 37.9337, stage5_loss_bbox: 0.2893, stage5_loss_iou: 0.4933, stage5_loss_mask: 0.7529, loss: 16.5773\n",
      "2025-07-16 12:53:50,329 - mmdet - INFO - Epoch [6][550/750]\tlr: 2.500e-05, eta: 4:06:28, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1272, stage0_pos_acc: 32.0928, stage0_loss_bbox: 0.6299, stage0_loss_iou: 1.1268, stage0_loss_mask: 1.2333, stage1_loss_cls: 0.8927, stage1_pos_acc: 37.3807, stage1_loss_bbox: 0.3337, stage1_loss_iou: 0.6493, stage1_loss_mask: 0.7821, stage2_loss_cls: 0.8604, stage2_pos_acc: 40.0920, stage2_loss_bbox: 0.2820, stage2_loss_iou: 0.5356, stage2_loss_mask: 0.7264, stage3_loss_cls: 0.8247, stage3_pos_acc: 39.9626, stage3_loss_bbox: 0.2778, stage3_loss_iou: 0.5223, stage3_loss_mask: 0.6929, stage4_loss_cls: 0.7903, stage4_pos_acc: 35.7618, stage4_loss_bbox: 0.2817, stage4_loss_iou: 0.5246, stage4_loss_mask: 0.7168, stage5_loss_cls: 0.7933, stage5_pos_acc: 39.6917, stage5_loss_bbox: 0.2855, stage5_loss_iou: 0.5215, stage5_loss_mask: 0.6978, loss: 16.1086\n",
      "2025-07-16 12:54:06,716 - mmdet - INFO - Epoch [6][600/750]\tlr: 2.500e-05, eta: 4:06:09, time: 0.328, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1387, stage0_pos_acc: 29.3744, stage0_loss_bbox: 0.6126, stage0_loss_iou: 1.0719, stage0_loss_mask: 1.1344, stage1_loss_cls: 0.9243, stage1_pos_acc: 36.7456, stage1_loss_bbox: 0.3557, stage1_loss_iou: 0.6163, stage1_loss_mask: 0.7163, stage2_loss_cls: 0.8858, stage2_pos_acc: 34.0352, stage2_loss_bbox: 0.3224, stage2_loss_iou: 0.5174, stage2_loss_mask: 0.6713, stage3_loss_cls: 0.8283, stage3_pos_acc: 36.1154, stage3_loss_bbox: 0.3081, stage3_loss_iou: 0.4962, stage3_loss_mask: 0.7021, stage4_loss_cls: 0.8319, stage4_pos_acc: 35.8108, stage4_loss_bbox: 0.2972, stage4_loss_iou: 0.4792, stage4_loss_mask: 0.6663, stage5_loss_cls: 0.8252, stage5_pos_acc: 33.6243, stage5_loss_bbox: 0.3034, stage5_loss_iou: 0.4728, stage5_loss_mask: 0.6593, loss: 15.8371\n",
      "2025-07-16 12:54:22,919 - mmdet - INFO - Epoch [6][650/750]\tlr: 2.500e-05, eta: 4:05:48, time: 0.324, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1629, stage0_pos_acc: 30.8591, stage0_loss_bbox: 0.5910, stage0_loss_iou: 1.1141, stage0_loss_mask: 1.3448, stage1_loss_cls: 0.9574, stage1_pos_acc: 33.8581, stage1_loss_bbox: 0.3420, stage1_loss_iou: 0.7019, stage1_loss_mask: 0.9033, stage2_loss_cls: 0.9136, stage2_pos_acc: 39.6662, stage2_loss_bbox: 0.3070, stage2_loss_iou: 0.6080, stage2_loss_mask: 0.8984, stage3_loss_cls: 0.8652, stage3_pos_acc: 39.5566, stage3_loss_bbox: 0.3142, stage3_loss_iou: 0.5927, stage3_loss_mask: 0.8916, stage4_loss_cls: 0.8434, stage4_pos_acc: 36.9247, stage4_loss_bbox: 0.3026, stage4_loss_iou: 0.5749, stage4_loss_mask: 0.8432, stage5_loss_cls: 0.8361, stage5_pos_acc: 38.0010, stage5_loss_bbox: 0.3064, stage5_loss_iou: 0.5696, stage5_loss_mask: 0.8154, loss: 17.5997\n",
      "2025-07-16 12:54:39,194 - mmdet - INFO - Epoch [6][700/750]\tlr: 2.500e-05, eta: 4:05:28, time: 0.325, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1311, stage0_pos_acc: 37.9335, stage0_loss_bbox: 0.6468, stage0_loss_iou: 1.1748, stage0_loss_mask: 1.4475, stage1_loss_cls: 0.9276, stage1_pos_acc: 41.8973, stage1_loss_bbox: 0.3734, stage1_loss_iou: 0.7173, stage1_loss_mask: 1.0442, stage2_loss_cls: 0.9124, stage2_pos_acc: 38.2215, stage2_loss_bbox: 0.3210, stage2_loss_iou: 0.6228, stage2_loss_mask: 0.9681, stage3_loss_cls: 0.8681, stage3_pos_acc: 40.5685, stage3_loss_bbox: 0.3071, stage3_loss_iou: 0.5905, stage3_loss_mask: 0.9776, stage4_loss_cls: 0.8513, stage4_pos_acc: 40.0411, stage4_loss_bbox: 0.3053, stage4_loss_iou: 0.5755, stage4_loss_mask: 0.9508, stage5_loss_cls: 0.8337, stage5_pos_acc: 41.7684, stage5_loss_bbox: 0.3100, stage5_loss_iou: 0.5729, stage5_loss_mask: 0.9617, loss: 18.3915\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 12:57:59,280 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.431\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.151\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.498\n",
      "2025-07-16 12:58:01,022 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.46s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.84s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.043\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.157\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.519\n",
      "2025-07-16 12:58:04,220 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 12:58:04,220 - mmdet - INFO - Epoch(val) [6][750]\tbbox_mAP: 0.0330, bbox_mAP_50: 0.0580, bbox_mAP_75: 0.0340, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0100, bbox_mAP_l: 0.0400, bbox_mAP_copypaste: 0.033 0.058 0.034 0.005 0.010 0.040, segm_mAP: 0.0350, segm_mAP_50: 0.0590, segm_mAP_75: 0.0360, segm_mAP_s: 0.0060, segm_mAP_m: 0.0090, segm_mAP_l: 0.0430, segm_mAP_copypaste: 0.035 0.059 0.036 0.006 0.009 0.043\n",
      "2025-07-16 12:58:23,040 - mmdet - INFO - Epoch [7][50/750]\tlr: 2.500e-05, eta: 4:05:14, time: 0.376, data_time: 0.050, memory: 11264, stage0_loss_cls: 1.1593, stage0_pos_acc: 32.4404, stage0_loss_bbox: 0.6265, stage0_loss_iou: 1.1380, stage0_loss_mask: 1.3519, stage1_loss_cls: 0.9106, stage1_pos_acc: 38.0865, stage1_loss_bbox: 0.3409, stage1_loss_iou: 0.6629, stage1_loss_mask: 0.8653, stage2_loss_cls: 0.8985, stage2_pos_acc: 40.8520, stage2_loss_bbox: 0.2718, stage2_loss_iou: 0.5378, stage2_loss_mask: 0.7761, stage3_loss_cls: 0.8392, stage3_pos_acc: 41.8187, stage3_loss_bbox: 0.2568, stage3_loss_iou: 0.5056, stage3_loss_mask: 0.7513, stage4_loss_cls: 0.8228, stage4_pos_acc: 42.8503, stage4_loss_bbox: 0.2437, stage4_loss_iou: 0.4888, stage4_loss_mask: 0.7173, stage5_loss_cls: 0.8141, stage5_pos_acc: 39.7850, stage5_loss_bbox: 0.2464, stage5_loss_iou: 0.4881, stage5_loss_mask: 0.7205, loss: 16.4343\n",
      "2025-07-16 12:58:39,680 - mmdet - INFO - Epoch [7][100/750]\tlr: 2.500e-05, eta: 4:04:57, time: 0.333, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1669, stage0_pos_acc: 26.6342, stage0_loss_bbox: 0.6604, stage0_loss_iou: 1.2120, stage0_loss_mask: 1.5214, stage1_loss_cls: 0.9016, stage1_pos_acc: 36.2231, stage1_loss_bbox: 0.3796, stage1_loss_iou: 0.7164, stage1_loss_mask: 1.0389, stage2_loss_cls: 0.8708, stage2_pos_acc: 38.2834, stage2_loss_bbox: 0.3031, stage2_loss_iou: 0.5985, stage2_loss_mask: 0.9404, stage3_loss_cls: 0.8390, stage3_pos_acc: 36.5532, stage3_loss_bbox: 0.2938, stage3_loss_iou: 0.5598, stage3_loss_mask: 0.8812, stage4_loss_cls: 0.7959, stage4_pos_acc: 36.1515, stage4_loss_bbox: 0.2987, stage4_loss_iou: 0.5485, stage4_loss_mask: 0.8845, stage5_loss_cls: 0.7972, stage5_pos_acc: 36.9325, stage5_loss_bbox: 0.2903, stage5_loss_iou: 0.5409, stage5_loss_mask: 0.8756, loss: 17.9153\n",
      "2025-07-16 12:58:56,274 - mmdet - INFO - Epoch [7][150/750]\tlr: 2.500e-05, eta: 4:04:40, time: 0.332, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1406, stage0_pos_acc: 35.2540, stage0_loss_bbox: 0.6506, stage0_loss_iou: 1.0504, stage0_loss_mask: 1.3720, stage1_loss_cls: 0.9456, stage1_pos_acc: 33.6909, stage1_loss_bbox: 0.3967, stage1_loss_iou: 0.6853, stage1_loss_mask: 1.0130, stage2_loss_cls: 0.9328, stage2_pos_acc: 38.2748, stage2_loss_bbox: 0.3359, stage2_loss_iou: 0.5892, stage2_loss_mask: 0.9000, stage3_loss_cls: 0.8975, stage3_pos_acc: 38.6539, stage3_loss_bbox: 0.3311, stage3_loss_iou: 0.5684, stage3_loss_mask: 0.8679, stage4_loss_cls: 0.8716, stage4_pos_acc: 43.9435, stage4_loss_bbox: 0.3185, stage4_loss_iou: 0.5492, stage4_loss_mask: 0.8184, stage5_loss_cls: 0.8845, stage5_pos_acc: 40.8882, stage5_loss_bbox: 0.3175, stage5_loss_iou: 0.5502, stage5_loss_mask: 0.8390, loss: 17.8259\n",
      "2025-07-16 12:59:12,945 - mmdet - INFO - Epoch [7][200/750]\tlr: 2.500e-05, eta: 4:04:24, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1626, stage0_pos_acc: 31.2905, stage0_loss_bbox: 0.5887, stage0_loss_iou: 1.1070, stage0_loss_mask: 1.2446, stage1_loss_cls: 0.9274, stage1_pos_acc: 32.4571, stage1_loss_bbox: 0.3213, stage1_loss_iou: 0.5949, stage1_loss_mask: 0.7564, stage2_loss_cls: 0.8688, stage2_pos_acc: 38.1143, stage2_loss_bbox: 0.2879, stage2_loss_iou: 0.5186, stage2_loss_mask: 0.7299, stage3_loss_cls: 0.8238, stage3_pos_acc: 38.2190, stage3_loss_bbox: 0.2674, stage3_loss_iou: 0.5046, stage3_loss_mask: 0.7325, stage4_loss_cls: 0.8085, stage4_pos_acc: 41.5048, stage4_loss_bbox: 0.2666, stage4_loss_iou: 0.5042, stage4_loss_mask: 0.7181, stage5_loss_cls: 0.8062, stage5_pos_acc: 43.5810, stage5_loss_bbox: 0.2523, stage5_loss_iou: 0.4935, stage5_loss_mask: 0.7349, loss: 16.0208\n",
      "2025-07-16 12:59:29,644 - mmdet - INFO - Epoch [7][250/750]\tlr: 2.500e-05, eta: 4:04:08, time: 0.334, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1552, stage0_pos_acc: 27.9857, stage0_loss_bbox: 0.6381, stage0_loss_iou: 1.1191, stage0_loss_mask: 1.1854, stage1_loss_cls: 0.9066, stage1_pos_acc: 36.3619, stage1_loss_bbox: 0.3517, stage1_loss_iou: 0.6103, stage1_loss_mask: 0.7513, stage2_loss_cls: 0.8615, stage2_pos_acc: 37.5714, stage2_loss_bbox: 0.3057, stage2_loss_iou: 0.5020, stage2_loss_mask: 0.6503, stage3_loss_cls: 0.8048, stage3_pos_acc: 37.6071, stage3_loss_bbox: 0.2753, stage3_loss_iou: 0.4564, stage3_loss_mask: 0.6257, stage4_loss_cls: 0.7781, stage4_pos_acc: 40.0929, stage4_loss_bbox: 0.2778, stage4_loss_iou: 0.4451, stage4_loss_mask: 0.6216, stage5_loss_cls: 0.7721, stage5_pos_acc: 43.7143, stage5_loss_bbox: 0.2815, stage5_loss_iou: 0.4437, stage5_loss_mask: 0.6121, loss: 15.4313\n",
      "2025-07-16 12:59:46,311 - mmdet - INFO - Epoch [7][300/750]\tlr: 2.500e-05, eta: 4:03:51, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1466, stage0_pos_acc: 28.4364, stage0_loss_bbox: 0.6691, stage0_loss_iou: 1.2194, stage0_loss_mask: 1.5219, stage1_loss_cls: 0.9438, stage1_pos_acc: 31.3907, stage1_loss_bbox: 0.4079, stage1_loss_iou: 0.7214, stage1_loss_mask: 0.9768, stage2_loss_cls: 0.9259, stage2_pos_acc: 44.0415, stage2_loss_bbox: 0.3522, stage2_loss_iou: 0.6054, stage2_loss_mask: 0.9149, stage3_loss_cls: 0.8802, stage3_pos_acc: 45.3097, stage3_loss_bbox: 0.3307, stage3_loss_iou: 0.5750, stage3_loss_mask: 0.8977, stage4_loss_cls: 0.8563, stage4_pos_acc: 40.7460, stage4_loss_bbox: 0.3249, stage4_loss_iou: 0.5678, stage4_loss_mask: 0.9020, stage5_loss_cls: 0.8473, stage5_pos_acc: 40.9904, stage5_loss_bbox: 0.3305, stage5_loss_iou: 0.5672, stage5_loss_mask: 0.9081, loss: 18.3931\n",
      "2025-07-16 13:00:02,849 - mmdet - INFO - Epoch [7][350/750]\tlr: 2.500e-05, eta: 4:03:34, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1917, stage0_pos_acc: 25.4377, stage0_loss_bbox: 0.6487, stage0_loss_iou: 1.0939, stage0_loss_mask: 1.3095, stage1_loss_cls: 0.9849, stage1_pos_acc: 38.6020, stage1_loss_bbox: 0.3935, stage1_loss_iou: 0.6486, stage1_loss_mask: 0.9613, stage2_loss_cls: 0.9746, stage2_pos_acc: 40.1172, stage2_loss_bbox: 0.3172, stage2_loss_iou: 0.5435, stage2_loss_mask: 0.8506, stage3_loss_cls: 0.9119, stage3_pos_acc: 40.1084, stage3_loss_bbox: 0.3194, stage3_loss_iou: 0.5433, stage3_loss_mask: 0.8665, stage4_loss_cls: 0.8910, stage4_pos_acc: 39.3449, stage4_loss_bbox: 0.3113, stage4_loss_iou: 0.5334, stage4_loss_mask: 0.8665, stage5_loss_cls: 0.8851, stage5_pos_acc: 39.5973, stage5_loss_bbox: 0.3050, stage5_loss_iou: 0.5360, stage5_loss_mask: 0.8661, loss: 17.7534\n",
      "2025-07-16 13:00:19,558 - mmdet - INFO - Epoch [7][400/750]\tlr: 2.500e-05, eta: 4:03:17, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1761, stage0_pos_acc: 32.0339, stage0_loss_bbox: 0.6661, stage0_loss_iou: 1.1461, stage0_loss_mask: 1.4010, stage1_loss_cls: 0.9359, stage1_pos_acc: 40.7610, stage1_loss_bbox: 0.3706, stage1_loss_iou: 0.6561, stage1_loss_mask: 0.9193, stage2_loss_cls: 0.9161, stage2_pos_acc: 41.0141, stage2_loss_bbox: 0.2963, stage2_loss_iou: 0.5405, stage2_loss_mask: 0.8460, stage3_loss_cls: 0.8372, stage3_pos_acc: 41.9382, stage3_loss_bbox: 0.2886, stage3_loss_iou: 0.5110, stage3_loss_mask: 0.7937, stage4_loss_cls: 0.8166, stage4_pos_acc: 42.8057, stage4_loss_bbox: 0.2914, stage4_loss_iou: 0.5064, stage4_loss_mask: 0.7640, stage5_loss_cls: 0.8213, stage5_pos_acc: 42.0415, stage5_loss_bbox: 0.2905, stage5_loss_iou: 0.5014, stage5_loss_mask: 0.7448, loss: 17.0371\n",
      "2025-07-16 13:00:36,514 - mmdet - INFO - Epoch [7][450/750]\tlr: 2.500e-05, eta: 4:03:04, time: 0.339, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1456, stage0_pos_acc: 38.1065, stage0_loss_bbox: 0.6506, stage0_loss_iou: 1.1552, stage0_loss_mask: 1.4892, stage1_loss_cls: 0.9497, stage1_pos_acc: 39.8575, stage1_loss_bbox: 0.4178, stage1_loss_iou: 0.6647, stage1_loss_mask: 1.0204, stage2_loss_cls: 0.9069, stage2_pos_acc: 45.7380, stage2_loss_bbox: 0.3766, stage2_loss_iou: 0.5759, stage2_loss_mask: 0.9670, stage3_loss_cls: 0.8701, stage3_pos_acc: 43.6160, stage3_loss_bbox: 0.3608, stage3_loss_iou: 0.5544, stage3_loss_mask: 0.9666, stage4_loss_cls: 0.8509, stage4_pos_acc: 42.8466, stage4_loss_bbox: 0.3630, stage4_loss_iou: 0.5488, stage4_loss_mask: 0.9314, stage5_loss_cls: 0.8476, stage5_pos_acc: 44.8853, stage5_loss_bbox: 0.3696, stage5_loss_iou: 0.5596, stage5_loss_mask: 0.9552, loss: 18.4976\n",
      "2025-07-16 13:00:53,123 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:00:53,123 - mmdet - INFO - Epoch [7][500/750]\tlr: 2.500e-05, eta: 4:02:47, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1522, stage0_pos_acc: 30.5348, stage0_loss_bbox: 0.6293, stage0_loss_iou: 1.1911, stage0_loss_mask: 1.5377, stage1_loss_cls: 0.9252, stage1_pos_acc: 42.4553, stage1_loss_bbox: 0.3311, stage1_loss_iou: 0.6757, stage1_loss_mask: 0.9819, stage2_loss_cls: 0.8894, stage2_pos_acc: 41.6890, stage2_loss_bbox: 0.2972, stage2_loss_iou: 0.5818, stage2_loss_mask: 0.9296, stage3_loss_cls: 0.8198, stage3_pos_acc: 43.7987, stage3_loss_bbox: 0.2863, stage3_loss_iou: 0.5833, stage3_loss_mask: 0.8904, stage4_loss_cls: 0.8078, stage4_pos_acc: 45.4820, stage4_loss_bbox: 0.2723, stage4_loss_iou: 0.5658, stage4_loss_mask: 0.8562, stage5_loss_cls: 0.7988, stage5_pos_acc: 45.3695, stage5_loss_bbox: 0.2747, stage5_loss_iou: 0.5665, stage5_loss_mask: 0.8438, loss: 17.6876\n",
      "2025-07-16 13:01:09,871 - mmdet - INFO - Epoch [7][550/750]\tlr: 2.500e-05, eta: 4:02:31, time: 0.335, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1219, stage0_pos_acc: 35.6600, stage0_loss_bbox: 0.6456, stage0_loss_iou: 1.1335, stage0_loss_mask: 1.4777, stage1_loss_cls: 0.9459, stage1_pos_acc: 37.3326, stage1_loss_bbox: 0.3829, stage1_loss_iou: 0.6845, stage1_loss_mask: 0.9977, stage2_loss_cls: 0.9057, stage2_pos_acc: 41.6936, stage2_loss_bbox: 0.3280, stage2_loss_iou: 0.5814, stage2_loss_mask: 0.8746, stage3_loss_cls: 0.8508, stage3_pos_acc: 44.7797, stage3_loss_bbox: 0.3125, stage3_loss_iou: 0.5666, stage3_loss_mask: 0.8880, stage4_loss_cls: 0.8298, stage4_pos_acc: 47.1106, stage4_loss_bbox: 0.3133, stage4_loss_iou: 0.5633, stage4_loss_mask: 0.8945, stage5_loss_cls: 0.8337, stage5_pos_acc: 42.3441, stage5_loss_bbox: 0.3119, stage5_loss_iou: 0.5601, stage5_loss_mask: 0.8840, loss: 17.8878\n",
      "2025-07-16 13:01:26,211 - mmdet - INFO - Epoch [7][600/750]\tlr: 2.500e-05, eta: 4:02:12, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1377, stage0_pos_acc: 33.7221, stage0_loss_bbox: 0.6235, stage0_loss_iou: 1.0734, stage0_loss_mask: 1.5964, stage1_loss_cls: 0.9621, stage1_pos_acc: 39.4392, stage1_loss_bbox: 0.3845, stage1_loss_iou: 0.6784, stage1_loss_mask: 1.1116, stage2_loss_cls: 0.9227, stage2_pos_acc: 38.8191, stage2_loss_bbox: 0.3510, stage2_loss_iou: 0.6065, stage2_loss_mask: 1.0690, stage3_loss_cls: 0.8821, stage3_pos_acc: 40.3440, stage3_loss_bbox: 0.3495, stage3_loss_iou: 0.5972, stage3_loss_mask: 1.0629, stage4_loss_cls: 0.8588, stage4_pos_acc: 44.2206, stage4_loss_bbox: 0.3412, stage4_loss_iou: 0.5909, stage4_loss_mask: 1.0650, stage5_loss_cls: 0.8546, stage5_pos_acc: 42.9192, stage5_loss_bbox: 0.3441, stage5_loss_iou: 0.5863, stage5_loss_mask: 1.0683, loss: 19.1177\n",
      "2025-07-16 13:01:42,523 - mmdet - INFO - Epoch [7][650/750]\tlr: 2.500e-05, eta: 4:01:52, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1155, stage0_pos_acc: 32.9963, stage0_loss_bbox: 0.6621, stage0_loss_iou: 1.0799, stage0_loss_mask: 1.1783, stage1_loss_cls: 0.8999, stage1_pos_acc: 35.7032, stage1_loss_bbox: 0.3715, stage1_loss_iou: 0.6071, stage1_loss_mask: 0.7160, stage2_loss_cls: 0.8626, stage2_pos_acc: 42.1072, stage2_loss_bbox: 0.2951, stage2_loss_iou: 0.4884, stage2_loss_mask: 0.6535, stage3_loss_cls: 0.8076, stage3_pos_acc: 40.3051, stage3_loss_bbox: 0.3009, stage3_loss_iou: 0.4741, stage3_loss_mask: 0.6322, stage4_loss_cls: 0.7956, stage4_pos_acc: 42.7853, stage4_loss_bbox: 0.2899, stage4_loss_iou: 0.4568, stage4_loss_mask: 0.6388, stage5_loss_cls: 0.7934, stage5_pos_acc: 42.5043, stage5_loss_bbox: 0.3100, stage5_loss_iou: 0.4646, stage5_loss_mask: 0.6315, loss: 15.5252\n",
      "2025-07-16 13:01:58,772 - mmdet - INFO - Epoch [7][700/750]\tlr: 2.500e-05, eta: 4:01:32, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1448, stage0_pos_acc: 29.0520, stage0_loss_bbox: 0.6208, stage0_loss_iou: 1.0990, stage0_loss_mask: 1.0941, stage1_loss_cls: 0.8951, stage1_pos_acc: 34.7918, stage1_loss_bbox: 0.3235, stage1_loss_iou: 0.5973, stage1_loss_mask: 0.7553, stage2_loss_cls: 0.8487, stage2_pos_acc: 36.2536, stage2_loss_bbox: 0.2664, stage2_loss_iou: 0.4739, stage2_loss_mask: 0.7027, stage3_loss_cls: 0.7977, stage3_pos_acc: 37.6902, stage3_loss_bbox: 0.2590, stage3_loss_iou: 0.4507, stage3_loss_mask: 0.6757, stage4_loss_cls: 0.7762, stage4_pos_acc: 38.6917, stage4_loss_bbox: 0.2600, stage4_loss_iou: 0.4433, stage4_loss_mask: 0.6492, stage5_loss_cls: 0.7691, stage5_pos_acc: 38.2592, stage5_loss_bbox: 0.2559, stage5_loss_iou: 0.4369, stage5_loss_mask: 0.6527, loss: 15.2481\n",
      "2025-07-16 13:02:15,153 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:02:15,153 - mmdet - INFO - Epoch [7][750/750]\tlr: 2.500e-05, eta: 4:01:13, time: 0.328, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1462, stage0_pos_acc: 31.2271, stage0_loss_bbox: 0.6608, stage0_loss_iou: 1.1123, stage0_loss_mask: 1.3718, stage1_loss_cls: 0.9302, stage1_pos_acc: 37.3501, stage1_loss_bbox: 0.3894, stage1_loss_iou: 0.6530, stage1_loss_mask: 0.8778, stage2_loss_cls: 0.9043, stage2_pos_acc: 42.6820, stage2_loss_bbox: 0.3258, stage2_loss_iou: 0.5619, stage2_loss_mask: 0.7878, stage3_loss_cls: 0.8391, stage3_pos_acc: 42.2677, stage3_loss_bbox: 0.3374, stage3_loss_iou: 0.5591, stage3_loss_mask: 0.8177, stage4_loss_cls: 0.8341, stage4_pos_acc: 40.6375, stage4_loss_bbox: 0.3214, stage4_loss_iou: 0.5440, stage4_loss_mask: 0.7973, stage5_loss_cls: 0.8334, stage5_pos_acc: 41.2573, stage5_loss_bbox: 0.3166, stage5_loss_iou: 0.5262, stage5_loss_mask: 0.7763, loss: 17.2240\n",
      "2025-07-16 13:02:15,235 - mmdet - INFO - Saving checkpoint at 7 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 88s, ETA:     0s2025-07-16 13:05:21,605 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.042\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.169\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.477\n",
      "2025-07-16 13:05:23,322 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.73s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.061\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.421\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.182\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.480\n",
      "2025-07-16 13:05:26,307 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:05:26,307 - mmdet - INFO - Epoch(val) [7][750]\tbbox_mAP: 0.0340, bbox_mAP_50: 0.0620, bbox_mAP_75: 0.0330, bbox_mAP_s: 0.0420, bbox_mAP_m: 0.0050, bbox_mAP_l: 0.0470, bbox_mAP_copypaste: 0.034 0.062 0.033 0.042 0.005 0.047, segm_mAP: 0.0340, segm_mAP_50: 0.0610, segm_mAP_75: 0.0320, segm_mAP_s: 0.0430, segm_mAP_m: 0.0050, segm_mAP_l: 0.0470, segm_mAP_copypaste: 0.034 0.061 0.032 0.043 0.005 0.047\n",
      "2025-07-16 13:05:45,228 - mmdet - INFO - Epoch [8][50/750]\tlr: 2.500e-05, eta: 4:01:15, time: 0.378, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.1054, stage0_pos_acc: 28.7790, stage0_loss_bbox: 0.7009, stage0_loss_iou: 1.1940, stage0_loss_mask: 1.5026, stage1_loss_cls: 0.8789, stage1_pos_acc: 40.8773, stage1_loss_bbox: 0.3695, stage1_loss_iou: 0.6776, stage1_loss_mask: 1.0495, stage2_loss_cls: 0.8192, stage2_pos_acc: 41.8454, stage2_loss_bbox: 0.2998, stage2_loss_iou: 0.5638, stage2_loss_mask: 0.9367, stage3_loss_cls: 0.7735, stage3_pos_acc: 42.5380, stage3_loss_bbox: 0.2756, stage3_loss_iou: 0.5178, stage3_loss_mask: 0.8665, stage4_loss_cls: 0.7450, stage4_pos_acc: 46.6255, stage4_loss_bbox: 0.2704, stage4_loss_iou: 0.4989, stage4_loss_mask: 0.8768, stage5_loss_cls: 0.7572, stage5_pos_acc: 44.6810, stage5_loss_bbox: 0.2535, stage5_loss_iou: 0.4785, stage5_loss_mask: 0.8218, loss: 17.2335\n",
      "2025-07-16 13:06:01,867 - mmdet - INFO - Epoch [8][100/750]\tlr: 2.500e-05, eta: 4:00:58, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1115, stage0_pos_acc: 32.1041, stage0_loss_bbox: 0.6128, stage0_loss_iou: 1.0506, stage0_loss_mask: 1.1272, stage1_loss_cls: 0.9325, stage1_pos_acc: 38.0957, stage1_loss_bbox: 0.3431, stage1_loss_iou: 0.5983, stage1_loss_mask: 0.8276, stage2_loss_cls: 0.9109, stage2_pos_acc: 41.2392, stage2_loss_bbox: 0.3021, stage2_loss_iou: 0.5177, stage2_loss_mask: 0.7644, stage3_loss_cls: 0.8623, stage3_pos_acc: 38.3884, stage3_loss_bbox: 0.3060, stage3_loss_iou: 0.5168, stage3_loss_mask: 0.7711, stage4_loss_cls: 0.8446, stage4_pos_acc: 41.4513, stage4_loss_bbox: 0.2966, stage4_loss_iou: 0.5105, stage4_loss_mask: 0.7404, stage5_loss_cls: 0.8319, stage5_pos_acc: 44.9466, stage5_loss_bbox: 0.2890, stage5_loss_iou: 0.5085, stage5_loss_mask: 0.7649, loss: 16.3415\n",
      "2025-07-16 13:06:18,419 - mmdet - INFO - Epoch [8][150/750]\tlr: 2.500e-05, eta: 4:00:41, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1611, stage0_pos_acc: 32.0519, stage0_loss_bbox: 0.6521, stage0_loss_iou: 1.1975, stage0_loss_mask: 1.5440, stage1_loss_cls: 0.9142, stage1_pos_acc: 38.5297, stage1_loss_bbox: 0.3501, stage1_loss_iou: 0.7171, stage1_loss_mask: 1.1194, stage2_loss_cls: 0.8549, stage2_pos_acc: 46.2058, stage2_loss_bbox: 0.3161, stage2_loss_iou: 0.6186, stage2_loss_mask: 1.0560, stage3_loss_cls: 0.8270, stage3_pos_acc: 44.1669, stage3_loss_bbox: 0.3109, stage3_loss_iou: 0.5861, stage3_loss_mask: 0.9952, stage4_loss_cls: 0.7951, stage4_pos_acc: 45.3550, stage4_loss_bbox: 0.3041, stage4_loss_iou: 0.5717, stage4_loss_mask: 0.9885, stage5_loss_cls: 0.7976, stage5_pos_acc: 45.5621, stage5_loss_bbox: 0.2898, stage5_loss_iou: 0.5583, stage5_loss_mask: 0.9568, loss: 18.4821\n",
      "2025-07-16 13:06:35,248 - mmdet - INFO - Epoch [8][200/750]\tlr: 2.500e-05, eta: 4:00:26, time: 0.337, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1391, stage0_pos_acc: 31.7703, stage0_loss_bbox: 0.5704, stage0_loss_iou: 1.0150, stage0_loss_mask: 1.0837, stage1_loss_cls: 0.9146, stage1_pos_acc: 34.1075, stage1_loss_bbox: 0.3144, stage1_loss_iou: 0.5764, stage1_loss_mask: 0.7323, stage2_loss_cls: 0.8671, stage2_pos_acc: 39.3988, stage2_loss_bbox: 0.2820, stage2_loss_iou: 0.4866, stage2_loss_mask: 0.6685, stage3_loss_cls: 0.8362, stage3_pos_acc: 40.5392, stage3_loss_bbox: 0.2592, stage3_loss_iou: 0.4672, stage3_loss_mask: 0.6193, stage4_loss_cls: 0.7896, stage4_pos_acc: 42.9409, stage4_loss_bbox: 0.2710, stage4_loss_iou: 0.4790, stage4_loss_mask: 0.6482, stage5_loss_cls: 0.7925, stage5_pos_acc: 40.9686, stage5_loss_bbox: 0.2637, stage5_loss_iou: 0.4737, stage5_loss_mask: 0.6442, loss: 15.1941\n",
      "2025-07-16 13:06:52,322 - mmdet - INFO - Epoch [8][250/750]\tlr: 2.500e-05, eta: 4:00:12, time: 0.341, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1528, stage0_pos_acc: 31.8411, stage0_loss_bbox: 0.6385, stage0_loss_iou: 1.1085, stage0_loss_mask: 1.1833, stage1_loss_cls: 0.8963, stage1_pos_acc: 41.0187, stage1_loss_bbox: 0.3501, stage1_loss_iou: 0.6109, stage1_loss_mask: 0.8189, stage2_loss_cls: 0.8350, stage2_pos_acc: 45.3397, stage2_loss_bbox: 0.3019, stage2_loss_iou: 0.5087, stage2_loss_mask: 0.7599, stage3_loss_cls: 0.7839, stage3_pos_acc: 47.9164, stage3_loss_bbox: 0.2761, stage3_loss_iou: 0.4847, stage3_loss_mask: 0.7115, stage4_loss_cls: 0.7493, stage4_pos_acc: 48.4370, stage4_loss_bbox: 0.2769, stage4_loss_iou: 0.4877, stage4_loss_mask: 0.7134, stage5_loss_cls: 0.7379, stage5_pos_acc: 46.2444, stage5_loss_bbox: 0.2840, stage5_loss_iou: 0.4895, stage5_loss_mask: 0.7368, loss: 15.8965\n",
      "2025-07-16 13:07:08,975 - mmdet - INFO - Epoch [8][300/750]\tlr: 2.500e-05, eta: 3:59:56, time: 0.333, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1704, stage0_pos_acc: 35.1250, stage0_loss_bbox: 0.5860, stage0_loss_iou: 1.1044, stage0_loss_mask: 1.2885, stage1_loss_cls: 0.8969, stage1_pos_acc: 42.5013, stage1_loss_bbox: 0.3188, stage1_loss_iou: 0.6309, stage1_loss_mask: 0.8988, stage2_loss_cls: 0.8695, stage2_pos_acc: 41.3014, stage2_loss_bbox: 0.2703, stage2_loss_iou: 0.5143, stage2_loss_mask: 0.7938, stage3_loss_cls: 0.8194, stage3_pos_acc: 42.0107, stage3_loss_bbox: 0.2670, stage3_loss_iou: 0.4956, stage3_loss_mask: 0.7683, stage4_loss_cls: 0.7989, stage4_pos_acc: 43.3448, stage4_loss_bbox: 0.2592, stage4_loss_iou: 0.4934, stage4_loss_mask: 0.7600, stage5_loss_cls: 0.7914, stage5_pos_acc: 46.2008, stage5_loss_bbox: 0.2597, stage5_loss_iou: 0.4891, stage5_loss_mask: 0.7518, loss: 16.2965\n",
      "2025-07-16 13:07:25,574 - mmdet - INFO - Epoch [8][350/750]\tlr: 2.500e-05, eta: 3:59:38, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1504, stage0_pos_acc: 33.1199, stage0_loss_bbox: 0.5913, stage0_loss_iou: 1.0262, stage0_loss_mask: 1.2838, stage1_loss_cls: 0.9429, stage1_pos_acc: 41.5396, stage1_loss_bbox: 0.3656, stage1_loss_iou: 0.6250, stage1_loss_mask: 0.8053, stage2_loss_cls: 0.9085, stage2_pos_acc: 40.6301, stage2_loss_bbox: 0.3136, stage2_loss_iou: 0.5472, stage2_loss_mask: 0.7630, stage3_loss_cls: 0.8555, stage3_pos_acc: 43.6450, stage3_loss_bbox: 0.3091, stage3_loss_iou: 0.5262, stage3_loss_mask: 0.7591, stage4_loss_cls: 0.8398, stage4_pos_acc: 41.7998, stage4_loss_bbox: 0.3084, stage4_loss_iou: 0.5121, stage4_loss_mask: 0.7624, stage5_loss_cls: 0.8425, stage5_pos_acc: 43.5348, stage5_loss_bbox: 0.3028, stage5_loss_iou: 0.5058, stage5_loss_mask: 0.7833, loss: 16.6297\n",
      "2025-07-16 13:07:42,203 - mmdet - INFO - Epoch [8][400/750]\tlr: 2.500e-05, eta: 3:59:21, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1694, stage0_pos_acc: 33.3623, stage0_loss_bbox: 0.6130, stage0_loss_iou: 1.0515, stage0_loss_mask: 1.0512, stage1_loss_cls: 0.9457, stage1_pos_acc: 40.5091, stage1_loss_bbox: 0.3273, stage1_loss_iou: 0.5608, stage1_loss_mask: 0.5998, stage2_loss_cls: 0.8841, stage2_pos_acc: 42.4891, stage2_loss_bbox: 0.2630, stage2_loss_iou: 0.4563, stage2_loss_mask: 0.5587, stage3_loss_cls: 0.8111, stage3_pos_acc: 42.9661, stage3_loss_bbox: 0.2583, stage3_loss_iou: 0.4416, stage3_loss_mask: 0.5531, stage4_loss_cls: 0.7848, stage4_pos_acc: 44.7883, stage4_loss_bbox: 0.2566, stage4_loss_iou: 0.4421, stage4_loss_mask: 0.5348, stage5_loss_cls: 0.7981, stage5_pos_acc: 41.8213, stage5_loss_bbox: 0.2619, stage5_loss_iou: 0.4426, stage5_loss_mask: 0.5397, loss: 14.6053\n",
      "2025-07-16 13:07:59,107 - mmdet - INFO - Epoch [8][450/750]\tlr: 2.500e-05, eta: 3:59:07, time: 0.338, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1347, stage0_pos_acc: 26.4958, stage0_loss_bbox: 0.6524, stage0_loss_iou: 1.0929, stage0_loss_mask: 1.2398, stage1_loss_cls: 0.9022, stage1_pos_acc: 39.1036, stage1_loss_bbox: 0.3751, stage1_loss_iou: 0.6457, stage1_loss_mask: 0.8605, stage2_loss_cls: 0.8430, stage2_pos_acc: 41.1950, stage2_loss_bbox: 0.3166, stage2_loss_iou: 0.5309, stage2_loss_mask: 0.8403, stage3_loss_cls: 0.8054, stage3_pos_acc: 42.8069, stage3_loss_bbox: 0.2858, stage3_loss_iou: 0.4788, stage3_loss_mask: 0.7674, stage4_loss_cls: 0.7727, stage4_pos_acc: 42.4736, stage4_loss_bbox: 0.2806, stage4_loss_iou: 0.4735, stage4_loss_mask: 0.7716, stage5_loss_cls: 0.7693, stage5_pos_acc: 45.3426, stage5_loss_bbox: 0.2880, stage5_loss_iou: 0.4746, stage5_loss_mask: 0.7646, loss: 16.3663\n",
      "2025-07-16 13:08:16,278 - mmdet - INFO - Epoch [8][500/750]\tlr: 2.500e-05, eta: 3:58:54, time: 0.343, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1442, stage0_pos_acc: 31.6849, stage0_loss_bbox: 0.6694, stage0_loss_iou: 1.0713, stage0_loss_mask: 1.0603, stage1_loss_cls: 0.9160, stage1_pos_acc: 37.5413, stage1_loss_bbox: 0.3869, stage1_loss_iou: 0.5951, stage1_loss_mask: 0.7891, stage2_loss_cls: 0.8781, stage2_pos_acc: 36.9325, stage2_loss_bbox: 0.3155, stage2_loss_iou: 0.4797, stage2_loss_mask: 0.7537, stage3_loss_cls: 0.8312, stage3_pos_acc: 36.5548, stage3_loss_bbox: 0.3005, stage3_loss_iou: 0.4721, stage3_loss_mask: 0.6974, stage4_loss_cls: 0.8184, stage4_pos_acc: 40.6079, stage4_loss_bbox: 0.2886, stage4_loss_iou: 0.4523, stage4_loss_mask: 0.7129, stage5_loss_cls: 0.8096, stage5_pos_acc: 43.4849, stage5_loss_bbox: 0.2817, stage5_loss_iou: 0.4447, stage5_loss_mask: 0.7108, loss: 15.8795\n",
      "2025-07-16 13:08:33,754 - mmdet - INFO - Epoch [8][550/750]\tlr: 2.500e-05, eta: 3:58:43, time: 0.350, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1162, stage0_pos_acc: 33.4292, stage0_loss_bbox: 0.6309, stage0_loss_iou: 1.0948, stage0_loss_mask: 1.2921, stage1_loss_cls: 0.9147, stage1_pos_acc: 39.3379, stage1_loss_bbox: 0.3590, stage1_loss_iou: 0.6907, stage1_loss_mask: 0.9316, stage2_loss_cls: 0.8490, stage2_pos_acc: 44.8246, stage2_loss_bbox: 0.3131, stage2_loss_iou: 0.5932, stage2_loss_mask: 0.8999, stage3_loss_cls: 0.8014, stage3_pos_acc: 44.4832, stage3_loss_bbox: 0.3023, stage3_loss_iou: 0.5693, stage3_loss_mask: 0.8449, stage4_loss_cls: 0.7858, stage4_pos_acc: 45.8645, stage4_loss_bbox: 0.2873, stage4_loss_iou: 0.5500, stage4_loss_mask: 0.8179, stage5_loss_cls: 0.7890, stage5_pos_acc: 45.1238, stage5_loss_bbox: 0.2853, stage5_loss_iou: 0.5373, stage5_loss_mask: 0.8128, loss: 17.0685\n",
      "2025-07-16 13:08:50,883 - mmdet - INFO - Epoch [8][600/750]\tlr: 2.500e-05, eta: 3:58:30, time: 0.343, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1635, stage0_pos_acc: 32.1212, stage0_loss_bbox: 0.5607, stage0_loss_iou: 1.0834, stage0_loss_mask: 1.1694, stage1_loss_cls: 0.8631, stage1_pos_acc: 37.7212, stage1_loss_bbox: 0.2915, stage1_loss_iou: 0.5964, stage1_loss_mask: 0.6916, stage2_loss_cls: 0.8155, stage2_pos_acc: 42.6657, stage2_loss_bbox: 0.2325, stage2_loss_iou: 0.5057, stage2_loss_mask: 0.5929, stage3_loss_cls: 0.7847, stage3_pos_acc: 41.6657, stage3_loss_bbox: 0.2300, stage3_loss_iou: 0.4721, stage3_loss_mask: 0.5323, stage4_loss_cls: 0.7476, stage4_pos_acc: 43.4848, stage4_loss_bbox: 0.2203, stage4_loss_iou: 0.4512, stage4_loss_mask: 0.5096, stage5_loss_cls: 0.7349, stage5_pos_acc: 41.5561, stage5_loss_bbox: 0.2246, stage5_loss_iou: 0.4471, stage5_loss_mask: 0.5071, loss: 14.4277\n",
      "2025-07-16 13:09:07,896 - mmdet - INFO - Epoch [8][650/750]\tlr: 2.500e-05, eta: 3:58:15, time: 0.340, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1698, stage0_pos_acc: 30.5167, stage0_loss_bbox: 0.5943, stage0_loss_iou: 1.0887, stage0_loss_mask: 1.3190, stage1_loss_cls: 0.9466, stage1_pos_acc: 37.2353, stage1_loss_bbox: 0.3475, stage1_loss_iou: 0.6512, stage1_loss_mask: 0.8683, stage2_loss_cls: 0.8989, stage2_pos_acc: 40.9878, stage2_loss_bbox: 0.3133, stage2_loss_iou: 0.5656, stage2_loss_mask: 0.7948, stage3_loss_cls: 0.8539, stage3_pos_acc: 42.2045, stage3_loss_bbox: 0.2963, stage3_loss_iou: 0.5439, stage3_loss_mask: 0.7293, stage4_loss_cls: 0.8293, stage4_pos_acc: 43.1211, stage4_loss_bbox: 0.2831, stage4_loss_iou: 0.5278, stage4_loss_mask: 0.7268, stage5_loss_cls: 0.8439, stage5_pos_acc: 44.9227, stage5_loss_bbox: 0.2854, stage5_loss_iou: 0.5132, stage5_loss_mask: 0.7095, loss: 16.7005\n",
      "2025-07-16 13:09:24,818 - mmdet - INFO - Epoch [8][700/750]\tlr: 2.500e-05, eta: 3:58:01, time: 0.338, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1867, stage0_pos_acc: 31.2503, stage0_loss_bbox: 0.6260, stage0_loss_iou: 1.1505, stage0_loss_mask: 1.4064, stage1_loss_cls: 0.9465, stage1_pos_acc: 36.4748, stage1_loss_bbox: 0.3570, stage1_loss_iou: 0.6723, stage1_loss_mask: 1.0372, stage2_loss_cls: 0.8957, stage2_pos_acc: 45.7763, stage2_loss_bbox: 0.3043, stage2_loss_iou: 0.5782, stage2_loss_mask: 0.9611, stage3_loss_cls: 0.8488, stage3_pos_acc: 45.6271, stage3_loss_bbox: 0.3080, stage3_loss_iou: 0.5483, stage3_loss_mask: 0.8688, stage4_loss_cls: 0.8280, stage4_pos_acc: 43.6233, stage4_loss_bbox: 0.2945, stage4_loss_iou: 0.5266, stage4_loss_mask: 0.8655, stage5_loss_cls: 0.8264, stage5_pos_acc: 44.4066, stage5_loss_bbox: 0.2879, stage5_loss_iou: 0.5303, stage5_loss_mask: 0.8895, loss: 17.7446\n",
      "2025-07-16 13:09:42,140 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:09:42,141 - mmdet - INFO - Epoch [8][750/750]\tlr: 2.500e-05, eta: 3:57:48, time: 0.346, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1623, stage0_pos_acc: 31.3230, stage0_loss_bbox: 0.6947, stage0_loss_iou: 1.2704, stage0_loss_mask: 1.8446, stage1_loss_cls: 0.9554, stage1_pos_acc: 39.5663, stage1_loss_bbox: 0.3977, stage1_loss_iou: 0.7946, stage1_loss_mask: 1.3471, stage2_loss_cls: 0.9046, stage2_pos_acc: 42.8500, stage2_loss_bbox: 0.3349, stage2_loss_iou: 0.6604, stage2_loss_mask: 1.1408, stage3_loss_cls: 0.8375, stage3_pos_acc: 43.2849, stage3_loss_bbox: 0.3067, stage3_loss_iou: 0.6265, stage3_loss_mask: 1.0962, stage4_loss_cls: 0.8140, stage4_pos_acc: 43.8095, stage4_loss_bbox: 0.2995, stage4_loss_iou: 0.5994, stage4_loss_mask: 1.0624, stage5_loss_cls: 0.8101, stage5_pos_acc: 41.8857, stage5_loss_bbox: 0.2928, stage5_loss_iou: 0.5896, stage5_loss_mask: 1.0743, loss: 19.9165\n",
      "2025-07-16 13:09:42,224 - mmdet - INFO - Saving checkpoint at 8 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  ] 94/100, 1.2 task/s, elapsed: 78s, ETA:     5s2025-07-16 13:14:33,997 - mmdet - INFO - Epoch [9][300/750]\tlr: 2.500e-05, eta: 3:56:26, time: 0.330, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1692, stage0_pos_acc: 29.2942, stage0_loss_bbox: 0.6542, stage0_loss_iou: 1.1503, stage0_loss_mask: 1.1765, stage1_loss_cls: 0.8940, stage1_pos_acc: 36.5498, stage1_loss_bbox: 0.3304, stage1_loss_iou: 0.6058, stage1_loss_mask: 0.6069, stage2_loss_cls: 0.8311, stage2_pos_acc: 39.9688, stage2_loss_bbox: 0.2531, stage2_loss_iou: 0.4831, stage2_loss_mask: 0.5670, stage3_loss_cls: 0.7743, stage3_pos_acc: 43.0061, stage3_loss_bbox: 0.2336, stage3_loss_iou: 0.4487, stage3_loss_mask: 0.5718, stage4_loss_cls: 0.7470, stage4_pos_acc: 45.2061, stage4_loss_bbox: 0.2256, stage4_loss_iou: 0.4342, stage4_loss_mask: 0.5654, stage5_loss_cls: 0.7408, stage5_pos_acc: 45.3013, stage5_loss_bbox: 0.2305, stage5_loss_iou: 0.4334, stage5_loss_mask: 0.5589, loss: 14.6858\n",
      "2025-07-16 13:14:51,005 - mmdet - INFO - Epoch [9][350/750]\tlr: 2.500e-05, eta: 3:56:11, time: 0.340, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1377, stage0_pos_acc: 31.4936, stage0_loss_bbox: 0.6421, stage0_loss_iou: 1.1742, stage0_loss_mask: 1.3701, stage1_loss_cls: 0.8899, stage1_pos_acc: 38.2511, stage1_loss_bbox: 0.3504, stage1_loss_iou: 0.6295, stage1_loss_mask: 0.7819, stage2_loss_cls: 0.8182, stage2_pos_acc: 42.9238, stage2_loss_bbox: 0.2983, stage2_loss_iou: 0.5239, stage2_loss_mask: 0.7362, stage3_loss_cls: 0.7581, stage3_pos_acc: 41.1027, stage3_loss_bbox: 0.2903, stage3_loss_iou: 0.5095, stage3_loss_mask: 0.7540, stage4_loss_cls: 0.7225, stage4_pos_acc: 44.3824, stage4_loss_bbox: 0.2829, stage4_loss_iou: 0.5024, stage4_loss_mask: 0.7764, stage5_loss_cls: 0.7120, stage5_pos_acc: 44.7098, stage5_loss_bbox: 0.2743, stage5_loss_iou: 0.4996, stage5_loss_mask: 0.7761, loss: 16.2104\n",
      "2025-07-16 13:15:07,680 - mmdet - INFO - Epoch [9][400/750]\tlr: 2.500e-05, eta: 3:55:54, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1213, stage0_pos_acc: 28.5776, stage0_loss_bbox: 0.6618, stage0_loss_iou: 1.1472, stage0_loss_mask: 1.3554, stage1_loss_cls: 0.9302, stage1_pos_acc: 32.0325, stage1_loss_bbox: 0.3778, stage1_loss_iou: 0.6305, stage1_loss_mask: 0.7743, stage2_loss_cls: 0.8688, stage2_pos_acc: 37.5850, stage2_loss_bbox: 0.3093, stage2_loss_iou: 0.5392, stage2_loss_mask: 0.7664, stage3_loss_cls: 0.8258, stage3_pos_acc: 38.8612, stage3_loss_bbox: 0.2940, stage3_loss_iou: 0.5205, stage3_loss_mask: 0.7813, stage4_loss_cls: 0.7952, stage4_pos_acc: 37.7603, stage4_loss_bbox: 0.2921, stage4_loss_iou: 0.5176, stage4_loss_mask: 0.7799, stage5_loss_cls: 0.8027, stage5_pos_acc: 37.9563, stage5_loss_bbox: 0.2903, stage5_loss_iou: 0.5018, stage5_loss_mask: 0.7664, loss: 16.6497\n",
      "2025-07-16 13:15:23,954 - mmdet - INFO - Epoch [9][450/750]\tlr: 2.500e-05, eta: 3:55:34, time: 0.325, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1116, stage0_pos_acc: 30.6137, stage0_loss_bbox: 0.6185, stage0_loss_iou: 1.0952, stage0_loss_mask: 1.1828, stage1_loss_cls: 0.8812, stage1_pos_acc: 38.4477, stage1_loss_bbox: 0.3518, stage1_loss_iou: 0.6102, stage1_loss_mask: 0.7450, stage2_loss_cls: 0.8375, stage2_pos_acc: 39.2664, stage2_loss_bbox: 0.2933, stage2_loss_iou: 0.5045, stage2_loss_mask: 0.6637, stage3_loss_cls: 0.7957, stage3_pos_acc: 41.3924, stage3_loss_bbox: 0.2722, stage3_loss_iou: 0.4883, stage3_loss_mask: 0.6508, stage4_loss_cls: 0.7783, stage4_pos_acc: 41.0004, stage4_loss_bbox: 0.2657, stage4_loss_iou: 0.4861, stage4_loss_mask: 0.6544, stage5_loss_cls: 0.7746, stage5_pos_acc: 41.4282, stage5_loss_bbox: 0.2665, stage5_loss_iou: 0.4904, stage5_loss_mask: 0.6479, loss: 15.4660\n",
      "2025-07-16 13:15:40,610 - mmdet - INFO - Epoch [9][500/750]\tlr: 2.500e-05, eta: 3:55:17, time: 0.333, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1092, stage0_pos_acc: 32.6471, stage0_loss_bbox: 0.7353, stage0_loss_iou: 1.1168, stage0_loss_mask: 1.4923, stage1_loss_cls: 0.8972, stage1_pos_acc: 38.8578, stage1_loss_bbox: 0.4689, stage1_loss_iou: 0.6808, stage1_loss_mask: 1.0518, stage2_loss_cls: 0.8408, stage2_pos_acc: 45.9854, stage2_loss_bbox: 0.4068, stage2_loss_iou: 0.5787, stage2_loss_mask: 1.0031, stage3_loss_cls: 0.7886, stage3_pos_acc: 40.5174, stage3_loss_bbox: 0.3914, stage3_loss_iou: 0.5478, stage3_loss_mask: 0.9023, stage4_loss_cls: 0.7616, stage4_pos_acc: 41.1009, stage4_loss_bbox: 0.3806, stage4_loss_iou: 0.5254, stage4_loss_mask: 0.8752, stage5_loss_cls: 0.7550, stage5_pos_acc: 42.1586, stage5_loss_bbox: 0.3792, stage5_loss_iou: 0.5174, stage5_loss_mask: 0.8895, loss: 18.0956\n",
      "2025-07-16 13:15:57,079 - mmdet - INFO - Epoch [9][550/750]\tlr: 2.500e-05, eta: 3:54:59, time: 0.329, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0896, stage0_pos_acc: 34.7372, stage0_loss_bbox: 0.6426, stage0_loss_iou: 1.1013, stage0_loss_mask: 1.2116, stage1_loss_cls: 0.8548, stage1_pos_acc: 46.8405, stage1_loss_bbox: 0.3362, stage1_loss_iou: 0.6139, stage1_loss_mask: 0.7542, stage2_loss_cls: 0.8071, stage2_pos_acc: 46.7039, stage2_loss_bbox: 0.2663, stage2_loss_iou: 0.4947, stage2_loss_mask: 0.7153, stage3_loss_cls: 0.7457, stage3_pos_acc: 48.2771, stage3_loss_bbox: 0.2607, stage3_loss_iou: 0.4744, stage3_loss_mask: 0.6655, stage4_loss_cls: 0.7203, stage4_pos_acc: 48.3937, stage4_loss_bbox: 0.2495, stage4_loss_iou: 0.4615, stage4_loss_mask: 0.6472, stage5_loss_cls: 0.7152, stage5_pos_acc: 49.0857, stage5_loss_bbox: 0.2381, stage5_loss_iou: 0.4573, stage5_loss_mask: 0.6320, loss: 15.1552\n",
      "2025-07-16 13:16:13,603 - mmdet - INFO - Epoch [9][600/750]\tlr: 2.500e-05, eta: 3:54:41, time: 0.330, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1132, stage0_pos_acc: 37.8863, stage0_loss_bbox: 0.6463, stage0_loss_iou: 1.1926, stage0_loss_mask: 1.5221, stage1_loss_cls: 0.8713, stage1_pos_acc: 42.2493, stage1_loss_bbox: 0.4050, stage1_loss_iou: 0.7109, stage1_loss_mask: 1.0984, stage2_loss_cls: 0.8540, stage2_pos_acc: 50.0576, stage2_loss_bbox: 0.3310, stage2_loss_iou: 0.6141, stage2_loss_mask: 1.0414, stage3_loss_cls: 0.7838, stage3_pos_acc: 49.6517, stage3_loss_bbox: 0.3295, stage3_loss_iou: 0.6022, stage3_loss_mask: 1.0450, stage4_loss_cls: 0.7589, stage4_pos_acc: 49.7072, stage4_loss_bbox: 0.3176, stage4_loss_iou: 0.5877, stage4_loss_mask: 1.0184, stage5_loss_cls: 0.7652, stage5_pos_acc: 49.5100, stage5_loss_bbox: 0.3014, stage5_loss_iou: 0.5751, stage5_loss_mask: 0.9990, loss: 18.4841\n",
      "2025-07-16 13:16:29,951 - mmdet - INFO - Epoch [9][650/750]\tlr: 2.500e-05, eta: 3:54:22, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1239, stage0_pos_acc: 33.2797, stage0_loss_bbox: 0.6910, stage0_loss_iou: 1.1779, stage0_loss_mask: 1.4609, stage1_loss_cls: 0.9026, stage1_pos_acc: 38.6664, stage1_loss_bbox: 0.4171, stage1_loss_iou: 0.6864, stage1_loss_mask: 0.8410, stage2_loss_cls: 0.8552, stage2_pos_acc: 42.1972, stage2_loss_bbox: 0.3318, stage2_loss_iou: 0.5582, stage2_loss_mask: 0.7893, stage3_loss_cls: 0.7890, stage3_pos_acc: 45.1538, stage3_loss_bbox: 0.3130, stage3_loss_iou: 0.5233, stage3_loss_mask: 0.7445, stage4_loss_cls: 0.7752, stage4_pos_acc: 43.1915, stage4_loss_bbox: 0.2933, stage4_loss_iou: 0.5056, stage4_loss_mask: 0.7122, stage5_loss_cls: 0.7713, stage5_pos_acc: 45.2203, stage5_loss_bbox: 0.2967, stage5_loss_iou: 0.5042, stage5_loss_mask: 0.7247, loss: 16.7883\n",
      "2025-07-16 13:16:46,343 - mmdet - INFO - Epoch [9][700/750]\tlr: 2.500e-05, eta: 3:54:04, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1543, stage0_pos_acc: 32.4483, stage0_loss_bbox: 0.6708, stage0_loss_iou: 1.0743, stage0_loss_mask: 1.1914, stage1_loss_cls: 0.8900, stage1_pos_acc: 38.9530, stage1_loss_bbox: 0.3651, stage1_loss_iou: 0.5867, stage1_loss_mask: 0.7905, stage2_loss_cls: 0.8170, stage2_pos_acc: 52.2923, stage2_loss_bbox: 0.2996, stage2_loss_iou: 0.4908, stage2_loss_mask: 0.7286, stage3_loss_cls: 0.7660, stage3_pos_acc: 50.6788, stage3_loss_bbox: 0.2873, stage3_loss_iou: 0.4639, stage3_loss_mask: 0.6744, stage4_loss_cls: 0.7477, stage4_pos_acc: 52.7133, stage4_loss_bbox: 0.2849, stage4_loss_iou: 0.4552, stage4_loss_mask: 0.6586, stage5_loss_cls: 0.7486, stage5_pos_acc: 53.8911, stage5_loss_bbox: 0.2784, stage5_loss_iou: 0.4481, stage5_loss_mask: 0.6485, loss: 15.5207\n",
      "2025-07-16 13:17:02,685 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:17:02,685 - mmdet - INFO - Epoch [9][750/750]\tlr: 2.500e-05, eta: 3:53:45, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1454, stage0_pos_acc: 31.0709, stage0_loss_bbox: 0.6068, stage0_loss_iou: 1.1195, stage0_loss_mask: 1.2863, stage1_loss_cls: 0.8838, stage1_pos_acc: 35.4697, stage1_loss_bbox: 0.3100, stage1_loss_iou: 0.6375, stage1_loss_mask: 0.8291, stage2_loss_cls: 0.8486, stage2_pos_acc: 41.6775, stage2_loss_bbox: 0.2640, stage2_loss_iou: 0.5320, stage2_loss_mask: 0.7436, stage3_loss_cls: 0.7984, stage3_pos_acc: 44.6379, stage3_loss_bbox: 0.2661, stage3_loss_iou: 0.5161, stage3_loss_mask: 0.7326, stage4_loss_cls: 0.7822, stage4_pos_acc: 46.8951, stage4_loss_bbox: 0.2571, stage4_loss_iou: 0.5039, stage4_loss_mask: 0.7429, stage5_loss_cls: 0.7776, stage5_pos_acc: 47.2933, stage5_loss_bbox: 0.2608, stage5_loss_iou: 0.5081, stage5_loss_mask: 0.7353, loss: 16.0878\n",
      "2025-07-16 13:17:02,792 - mmdet - INFO - Saving checkpoint at 9 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 13:20:07,178 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.25s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.046\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.486\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.486\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.153\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.273\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.535\n",
      "2025-07-16 13:20:08,954 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.75s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.061\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.046\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.483\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.527\n",
      "2025-07-16 13:20:11,946 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:20:11,947 - mmdet - INFO - Epoch(val) [9][750]\tbbox_mAP: 0.0350, bbox_mAP_50: 0.0620, bbox_mAP_75: 0.0340, bbox_mAP_s: 0.0160, bbox_mAP_m: 0.0090, bbox_mAP_l: 0.0460, bbox_mAP_copypaste: 0.035 0.062 0.034 0.016 0.009 0.046, segm_mAP: 0.0350, segm_mAP_50: 0.0610, segm_mAP_75: 0.0350, segm_mAP_s: 0.0180, segm_mAP_m: 0.0090, segm_mAP_l: 0.0460, segm_mAP_copypaste: 0.035 0.061 0.035 0.018 0.009 0.046\n",
      "2025-07-16 13:20:30,713 - mmdet - INFO - Epoch [10][50/750]\tlr: 2.500e-05, eta: 3:53:41, time: 0.375, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.1138, stage0_pos_acc: 33.4944, stage0_loss_bbox: 0.6512, stage0_loss_iou: 1.0937, stage0_loss_mask: 1.4937, stage1_loss_cls: 0.8979, stage1_pos_acc: 39.8211, stage1_loss_bbox: 0.3822, stage1_loss_iou: 0.6931, stage1_loss_mask: 1.0061, stage2_loss_cls: 0.8658, stage2_pos_acc: 49.1160, stage2_loss_bbox: 0.3190, stage2_loss_iou: 0.5906, stage2_loss_mask: 0.8855, stage3_loss_cls: 0.8015, stage3_pos_acc: 46.5272, stage3_loss_bbox: 0.3177, stage3_loss_iou: 0.5799, stage3_loss_mask: 0.9047, stage4_loss_cls: 0.7888, stage4_pos_acc: 47.6300, stage4_loss_bbox: 0.3076, stage4_loss_iou: 0.5665, stage4_loss_mask: 0.8846, stage5_loss_cls: 0.7901, stage5_pos_acc: 46.9947, stage5_loss_bbox: 0.2999, stage5_loss_iou: 0.5594, stage5_loss_mask: 0.8767, loss: 17.6702\n",
      "2025-07-16 13:20:47,176 - mmdet - INFO - Epoch [10][100/750]\tlr: 2.500e-05, eta: 3:53:22, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1159, stage0_pos_acc: 29.5700, stage0_loss_bbox: 0.6675, stage0_loss_iou: 1.1281, stage0_loss_mask: 1.3902, stage1_loss_cls: 0.8903, stage1_pos_acc: 35.7420, stage1_loss_bbox: 0.3583, stage1_loss_iou: 0.6282, stage1_loss_mask: 0.7759, stage2_loss_cls: 0.8268, stage2_pos_acc: 39.5604, stage2_loss_bbox: 0.2962, stage2_loss_iou: 0.5173, stage2_loss_mask: 0.7977, stage3_loss_cls: 0.7609, stage3_pos_acc: 39.6992, stage3_loss_bbox: 0.2760, stage3_loss_iou: 0.4817, stage3_loss_mask: 0.7889, stage4_loss_cls: 0.7360, stage4_pos_acc: 43.2804, stage4_loss_bbox: 0.2691, stage4_loss_iou: 0.4673, stage4_loss_mask: 0.7730, stage5_loss_cls: 0.7287, stage5_pos_acc: 41.1297, stage5_loss_bbox: 0.2681, stage5_loss_iou: 0.4549, stage5_loss_mask: 0.7756, loss: 16.1727\n",
      "2025-07-16 13:21:04,052 - mmdet - INFO - Epoch [10][150/750]\tlr: 2.500e-05, eta: 3:53:07, time: 0.337, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1468, stage0_pos_acc: 34.3979, stage0_loss_bbox: 0.6282, stage0_loss_iou: 1.1640, stage0_loss_mask: 1.3644, stage1_loss_cls: 0.9042, stage1_pos_acc: 44.8990, stage1_loss_bbox: 0.3414, stage1_loss_iou: 0.6585, stage1_loss_mask: 1.0109, stage2_loss_cls: 0.8264, stage2_pos_acc: 47.6343, stage2_loss_bbox: 0.2709, stage2_loss_iou: 0.5322, stage2_loss_mask: 0.8756, stage3_loss_cls: 0.7803, stage3_pos_acc: 50.6137, stage3_loss_bbox: 0.2591, stage3_loss_iou: 0.5256, stage3_loss_mask: 0.9306, stage4_loss_cls: 0.7542, stage4_pos_acc: 48.7206, stage4_loss_bbox: 0.2627, stage4_loss_iou: 0.5233, stage4_loss_mask: 0.9215, stage5_loss_cls: 0.7527, stage5_pos_acc: 48.6750, stage5_loss_bbox: 0.2677, stage5_loss_iou: 0.5241, stage5_loss_mask: 0.9291, loss: 17.1544\n",
      "2025-07-16 13:21:21,304 - mmdet - INFO - Epoch [10][200/750]\tlr: 2.500e-05, eta: 3:52:53, time: 0.345, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1026, stage0_pos_acc: 37.8618, stage0_loss_bbox: 0.6105, stage0_loss_iou: 1.0773, stage0_loss_mask: 1.2029, stage1_loss_cls: 0.8620, stage1_pos_acc: 49.1316, stage1_loss_bbox: 0.3448, stage1_loss_iou: 0.5989, stage1_loss_mask: 0.7136, stage2_loss_cls: 0.8299, stage2_pos_acc: 49.9102, stage2_loss_bbox: 0.2822, stage2_loss_iou: 0.4911, stage2_loss_mask: 0.6613, stage3_loss_cls: 0.7553, stage3_pos_acc: 51.6257, stage3_loss_bbox: 0.2783, stage3_loss_iou: 0.4677, stage3_loss_mask: 0.6496, stage4_loss_cls: 0.7235, stage4_pos_acc: 52.3588, stage4_loss_bbox: 0.2684, stage4_loss_iou: 0.4503, stage4_loss_mask: 0.6289, stage5_loss_cls: 0.7306, stage5_pos_acc: 57.5291, stage5_loss_bbox: 0.2560, stage5_loss_iou: 0.4357, stage5_loss_mask: 0.6077, loss: 15.0292\n",
      "2025-07-16 13:21:38,237 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:21:38,237 - mmdet - INFO - Epoch [10][250/750]\tlr: 2.500e-05, eta: 3:52:38, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1326, stage0_pos_acc: 31.2405, stage0_loss_bbox: 0.6590, stage0_loss_iou: 1.1298, stage0_loss_mask: 1.2860, stage1_loss_cls: 0.8779, stage1_pos_acc: 39.4889, stage1_loss_bbox: 0.3729, stage1_loss_iou: 0.6040, stage1_loss_mask: 0.8299, stage2_loss_cls: 0.8354, stage2_pos_acc: 44.1675, stage2_loss_bbox: 0.3057, stage2_loss_iou: 0.4835, stage2_loss_mask: 0.7581, stage3_loss_cls: 0.7680, stage3_pos_acc: 41.8484, stage3_loss_bbox: 0.3036, stage3_loss_iou: 0.4581, stage3_loss_mask: 0.7349, stage4_loss_cls: 0.7531, stage4_pos_acc: 41.8325, stage4_loss_bbox: 0.2923, stage4_loss_iou: 0.4467, stage4_loss_mask: 0.7347, stage5_loss_cls: 0.7539, stage5_pos_acc: 43.2444, stage5_loss_bbox: 0.2852, stage5_loss_iou: 0.4430, stage5_loss_mask: 0.7202, loss: 15.9685\n",
      "2025-07-16 13:21:54,776 - mmdet - INFO - Epoch [10][300/750]\tlr: 2.500e-05, eta: 3:52:20, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1246, stage0_pos_acc: 32.7706, stage0_loss_bbox: 0.5832, stage0_loss_iou: 1.0723, stage0_loss_mask: 0.9970, stage1_loss_cls: 0.8254, stage1_pos_acc: 41.7745, stage1_loss_bbox: 0.2843, stage1_loss_iou: 0.5184, stage1_loss_mask: 0.6954, stage2_loss_cls: 0.7757, stage2_pos_acc: 47.1359, stage2_loss_bbox: 0.2236, stage2_loss_iou: 0.4000, stage2_loss_mask: 0.6133, stage3_loss_cls: 0.7049, stage3_pos_acc: 47.6087, stage3_loss_bbox: 0.2180, stage3_loss_iou: 0.3720, stage3_loss_mask: 0.5925, stage4_loss_cls: 0.6737, stage4_pos_acc: 48.7284, stage4_loss_bbox: 0.2177, stage4_loss_iou: 0.3692, stage4_loss_mask: 0.5783, stage5_loss_cls: 0.6657, stage5_pos_acc: 50.5444, stage5_loss_bbox: 0.2131, stage5_loss_iou: 0.3644, stage5_loss_mask: 0.5701, loss: 13.6528\n",
      "2025-07-16 13:22:11,302 - mmdet - INFO - Epoch [10][350/750]\tlr: 2.500e-05, eta: 3:52:02, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1017, stage0_pos_acc: 36.3842, stage0_loss_bbox: 0.5695, stage0_loss_iou: 1.0764, stage0_loss_mask: 1.1445, stage1_loss_cls: 0.8806, stage1_pos_acc: 40.0478, stage1_loss_bbox: 0.3156, stage1_loss_iou: 0.6150, stage1_loss_mask: 0.8096, stage2_loss_cls: 0.8267, stage2_pos_acc: 47.2651, stage2_loss_bbox: 0.2818, stage2_loss_iou: 0.5169, stage2_loss_mask: 0.7900, stage3_loss_cls: 0.7588, stage3_pos_acc: 48.7796, stage3_loss_bbox: 0.2763, stage3_loss_iou: 0.4854, stage3_loss_mask: 0.7352, stage4_loss_cls: 0.7277, stage4_pos_acc: 48.4053, stage4_loss_bbox: 0.2638, stage4_loss_iou: 0.4606, stage4_loss_mask: 0.7059, stage5_loss_cls: 0.7209, stage5_pos_acc: 50.6195, stage5_loss_bbox: 0.2636, stage5_loss_iou: 0.4563, stage5_loss_mask: 0.7271, loss: 15.5098\n",
      "2025-07-16 13:22:27,966 - mmdet - INFO - Epoch [10][400/750]\tlr: 2.500e-05, eta: 3:51:45, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1826, stage0_pos_acc: 30.1619, stage0_loss_bbox: 0.6519, stage0_loss_iou: 1.2094, stage0_loss_mask: 1.4432, stage1_loss_cls: 0.8940, stage1_pos_acc: 32.7190, stage1_loss_bbox: 0.3496, stage1_loss_iou: 0.7018, stage1_loss_mask: 1.0419, stage2_loss_cls: 0.8554, stage2_pos_acc: 38.3786, stage2_loss_bbox: 0.2727, stage2_loss_iou: 0.5677, stage2_loss_mask: 0.9389, stage3_loss_cls: 0.7968, stage3_pos_acc: 40.6190, stage3_loss_bbox: 0.2587, stage3_loss_iou: 0.5414, stage3_loss_mask: 0.9082, stage4_loss_cls: 0.7727, stage4_pos_acc: 38.9548, stage4_loss_bbox: 0.2559, stage4_loss_iou: 0.5270, stage4_loss_mask: 0.9245, stage5_loss_cls: 0.7761, stage5_pos_acc: 38.6452, stage5_loss_bbox: 0.2671, stage5_loss_iou: 0.5299, stage5_loss_mask: 0.9005, loss: 17.5681\n",
      "2025-07-16 13:22:44,484 - mmdet - INFO - Epoch [10][450/750]\tlr: 2.500e-05, eta: 3:51:27, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1321, stage0_pos_acc: 33.5346, stage0_loss_bbox: 0.6088, stage0_loss_iou: 1.0677, stage0_loss_mask: 1.0303, stage1_loss_cls: 0.8659, stage1_pos_acc: 42.1700, stage1_loss_bbox: 0.2934, stage1_loss_iou: 0.5767, stage1_loss_mask: 0.6587, stage2_loss_cls: 0.8250, stage2_pos_acc: 44.7738, stage2_loss_bbox: 0.2328, stage2_loss_iou: 0.4614, stage2_loss_mask: 0.5783, stage3_loss_cls: 0.7606, stage3_pos_acc: 46.5574, stage3_loss_bbox: 0.2133, stage3_loss_iou: 0.4329, stage3_loss_mask: 0.5441, stage4_loss_cls: 0.7318, stage4_pos_acc: 47.3577, stage4_loss_bbox: 0.2064, stage4_loss_iou: 0.4296, stage4_loss_mask: 0.5827, stage5_loss_cls: 0.7277, stage5_pos_acc: 47.4430, stage5_loss_bbox: 0.2034, stage5_loss_iou: 0.4239, stage5_loss_mask: 0.5594, loss: 14.1469\n",
      "2025-07-16 13:23:00,942 - mmdet - INFO - Epoch [10][500/750]\tlr: 2.500e-05, eta: 3:51:09, time: 0.329, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1174, stage0_pos_acc: 35.5218, stage0_loss_bbox: 0.5936, stage0_loss_iou: 1.0885, stage0_loss_mask: 1.2354, stage1_loss_cls: 0.8600, stage1_pos_acc: 36.8501, stage1_loss_bbox: 0.3227, stage1_loss_iou: 0.6108, stage1_loss_mask: 0.8695, stage2_loss_cls: 0.8496, stage2_pos_acc: 38.3384, stage2_loss_bbox: 0.2642, stage2_loss_iou: 0.4953, stage2_loss_mask: 0.7732, stage3_loss_cls: 0.7838, stage3_pos_acc: 40.0155, stage3_loss_bbox: 0.2614, stage3_loss_iou: 0.4903, stage3_loss_mask: 0.7651, stage4_loss_cls: 0.7655, stage4_pos_acc: 46.1801, stage4_loss_bbox: 0.2546, stage4_loss_iou: 0.4658, stage4_loss_mask: 0.7382, stage5_loss_cls: 0.7705, stage5_pos_acc: 47.0792, stage5_loss_bbox: 0.2555, stage5_loss_iou: 0.4585, stage5_loss_mask: 0.7369, loss: 15.8263\n",
      "2025-07-16 13:23:17,402 - mmdet - INFO - Epoch [10][550/750]\tlr: 2.500e-05, eta: 3:50:51, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1222, stage0_pos_acc: 32.1833, stage0_loss_bbox: 0.6558, stage0_loss_iou: 1.1572, stage0_loss_mask: 1.2554, stage1_loss_cls: 0.8797, stage1_pos_acc: 37.9405, stage1_loss_bbox: 0.3225, stage1_loss_iou: 0.6056, stage1_loss_mask: 0.7675, stage2_loss_cls: 0.8233, stage2_pos_acc: 44.5881, stage2_loss_bbox: 0.2532, stage2_loss_iou: 0.4786, stage2_loss_mask: 0.6305, stage3_loss_cls: 0.7452, stage3_pos_acc: 43.4476, stage3_loss_bbox: 0.2365, stage3_loss_iou: 0.4569, stage3_loss_mask: 0.6277, stage4_loss_cls: 0.7271, stage4_pos_acc: 39.9857, stage4_loss_bbox: 0.2339, stage4_loss_iou: 0.4451, stage4_loss_mask: 0.6105, stage5_loss_cls: 0.7259, stage5_pos_acc: 41.6000, stage5_loss_bbox: 0.2292, stage5_loss_iou: 0.4387, stage5_loss_mask: 0.6002, loss: 15.0286\n",
      "2025-07-16 13:23:33,936 - mmdet - INFO - Epoch [10][600/750]\tlr: 2.500e-05, eta: 3:50:33, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1059, stage0_pos_acc: 34.7317, stage0_loss_bbox: 0.6015, stage0_loss_iou: 1.0458, stage0_loss_mask: 0.9287, stage1_loss_cls: 0.8211, stage1_pos_acc: 39.3336, stage1_loss_bbox: 0.2924, stage1_loss_iou: 0.5368, stage1_loss_mask: 0.6208, stage2_loss_cls: 0.7753, stage2_pos_acc: 45.7901, stage2_loss_bbox: 0.2516, stage2_loss_iou: 0.4654, stage2_loss_mask: 0.6096, stage3_loss_cls: 0.7276, stage3_pos_acc: 46.7577, stage3_loss_bbox: 0.2350, stage3_loss_iou: 0.4296, stage3_loss_mask: 0.5247, stage4_loss_cls: 0.6920, stage4_pos_acc: 47.9124, stage4_loss_bbox: 0.2375, stage4_loss_iou: 0.4359, stage4_loss_mask: 0.5367, stage5_loss_cls: 0.6831, stage5_pos_acc: 51.1537, stage5_loss_bbox: 0.2350, stage5_loss_iou: 0.4317, stage5_loss_mask: 0.5448, loss: 13.7683\n",
      "2025-07-16 13:23:50,557 - mmdet - INFO - Epoch [10][650/750]\tlr: 2.500e-05, eta: 3:50:16, time: 0.332, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1104, stage0_pos_acc: 32.1722, stage0_loss_bbox: 0.5799, stage0_loss_iou: 1.0401, stage0_loss_mask: 1.0966, stage1_loss_cls: 0.8562, stage1_pos_acc: 37.8111, stage1_loss_bbox: 0.3176, stage1_loss_iou: 0.5871, stage1_loss_mask: 0.6912, stage2_loss_cls: 0.8123, stage2_pos_acc: 37.2333, stage2_loss_bbox: 0.2662, stage2_loss_iou: 0.4972, stage2_loss_mask: 0.5789, stage3_loss_cls: 0.7524, stage3_pos_acc: 41.4833, stage3_loss_bbox: 0.2538, stage3_loss_iou: 0.4721, stage3_loss_mask: 0.5359, stage4_loss_cls: 0.7366, stage4_pos_acc: 45.1056, stage4_loss_bbox: 0.2437, stage4_loss_iou: 0.4484, stage4_loss_mask: 0.4975, stage5_loss_cls: 0.7304, stage5_pos_acc: 46.1000, stage5_loss_bbox: 0.2387, stage5_loss_iou: 0.4482, stage5_loss_mask: 0.5163, loss: 14.3076\n",
      "2025-07-16 13:24:07,673 - mmdet - INFO - Epoch [10][700/750]\tlr: 2.500e-05, eta: 3:50:02, time: 0.342, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1197, stage0_pos_acc: 34.3137, stage0_loss_bbox: 0.6375, stage0_loss_iou: 1.1446, stage0_loss_mask: 1.3029, stage1_loss_cls: 0.8304, stage1_pos_acc: 44.6734, stage1_loss_bbox: 0.3776, stage1_loss_iou: 0.7037, stage1_loss_mask: 0.9195, stage2_loss_cls: 0.7806, stage2_pos_acc: 49.3304, stage2_loss_bbox: 0.3278, stage2_loss_iou: 0.5987, stage2_loss_mask: 0.8514, stage3_loss_cls: 0.7389, stage3_pos_acc: 48.2590, stage3_loss_bbox: 0.3196, stage3_loss_iou: 0.5661, stage3_loss_mask: 0.8529, stage4_loss_cls: 0.7107, stage4_pos_acc: 51.7053, stage4_loss_bbox: 0.3174, stage4_loss_iou: 0.5565, stage4_loss_mask: 0.8862, stage5_loss_cls: 0.7132, stage5_pos_acc: 51.1974, stage5_loss_bbox: 0.3068, stage5_loss_iou: 0.5491, stage5_loss_mask: 0.8645, loss: 16.9763\n",
      "2025-07-16 13:24:24,764 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:24:24,764 - mmdet - INFO - Epoch [10][750/750]\tlr: 2.500e-05, eta: 3:49:47, time: 0.342, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0937, stage0_pos_acc: 34.1278, stage0_loss_bbox: 0.6482, stage0_loss_iou: 1.1357, stage0_loss_mask: 1.3304, stage1_loss_cls: 0.8666, stage1_pos_acc: 35.2250, stage1_loss_bbox: 0.3519, stage1_loss_iou: 0.6499, stage1_loss_mask: 0.9123, stage2_loss_cls: 0.8093, stage2_pos_acc: 42.7583, stage2_loss_bbox: 0.2960, stage2_loss_iou: 0.5349, stage2_loss_mask: 0.7977, stage3_loss_cls: 0.7570, stage3_pos_acc: 44.3972, stage3_loss_bbox: 0.2657, stage3_loss_iou: 0.5055, stage3_loss_mask: 0.7649, stage4_loss_cls: 0.7287, stage4_pos_acc: 43.7667, stage4_loss_bbox: 0.2692, stage4_loss_iou: 0.4943, stage4_loss_mask: 0.7300, stage5_loss_cls: 0.7235, stage5_pos_acc: 43.6111, stage5_loss_bbox: 0.2751, stage5_loss_iou: 0.4962, stage5_loss_mask: 0.7646, loss: 16.2014\n",
      "2025-07-16 13:24:24,865 - mmdet - INFO - Saving checkpoint at 10 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 87s, ETA:     0s2025-07-16 13:27:29,443 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.447\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.447\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.060\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.203\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.503\n",
      "2025-07-16 13:27:31,169 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.73s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.461\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.462\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.462\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.087\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.214\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.519\n",
      "2025-07-16 13:27:34,150 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:27:34,150 - mmdet - INFO - Epoch(val) [10][750]\tbbox_mAP: 0.0290, bbox_mAP_50: 0.0540, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0070, bbox_mAP_l: 0.0360, bbox_mAP_copypaste: 0.029 0.054 0.028 0.005 0.007 0.036, segm_mAP: 0.0300, segm_mAP_50: 0.0540, segm_mAP_75: 0.0290, segm_mAP_s: 0.0060, segm_mAP_m: 0.0080, segm_mAP_l: 0.0380, segm_mAP_copypaste: 0.030 0.054 0.029 0.006 0.008 0.038\n",
      "2025-07-16 13:27:53,414 - mmdet - INFO - Epoch [11][50/750]\tlr: 2.500e-05, eta: 3:49:44, time: 0.385, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.1541, stage0_pos_acc: 29.9633, stage0_loss_bbox: 0.6296, stage0_loss_iou: 1.2303, stage0_loss_mask: 1.3872, stage1_loss_cls: 0.8388, stage1_pos_acc: 39.1950, stage1_loss_bbox: 0.2998, stage1_loss_iou: 0.6201, stage1_loss_mask: 0.7399, stage2_loss_cls: 0.7810, stage2_pos_acc: 46.5444, stage2_loss_bbox: 0.2237, stage2_loss_iou: 0.4599, stage2_loss_mask: 0.5840, stage3_loss_cls: 0.7248, stage3_pos_acc: 49.5132, stage3_loss_bbox: 0.2161, stage3_loss_iou: 0.4245, stage3_loss_mask: 0.5615, stage4_loss_cls: 0.6721, stage4_pos_acc: 54.2317, stage4_loss_bbox: 0.1991, stage4_loss_iou: 0.4003, stage4_loss_mask: 0.5574, stage5_loss_cls: 0.6688, stage5_pos_acc: 53.6332, stage5_loss_bbox: 0.1961, stage5_loss_iou: 0.3952, stage5_loss_mask: 0.5299, loss: 14.4942\n",
      "2025-07-16 13:28:10,107 - mmdet - INFO - Epoch [11][100/750]\tlr: 2.500e-05, eta: 3:49:27, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1401, stage0_pos_acc: 34.7015, stage0_loss_bbox: 0.6221, stage0_loss_iou: 1.1019, stage0_loss_mask: 1.2504, stage1_loss_cls: 0.8841, stage1_pos_acc: 41.2794, stage1_loss_bbox: 0.3146, stage1_loss_iou: 0.6065, stage1_loss_mask: 0.8027, stage2_loss_cls: 0.7843, stage2_pos_acc: 51.4140, stage2_loss_bbox: 0.2664, stage2_loss_iou: 0.5177, stage2_loss_mask: 0.7568, stage3_loss_cls: 0.7065, stage3_pos_acc: 55.5589, stage3_loss_bbox: 0.2615, stage3_loss_iou: 0.5006, stage3_loss_mask: 0.7564, stage4_loss_cls: 0.6768, stage4_pos_acc: 52.7003, stage4_loss_bbox: 0.2445, stage4_loss_iou: 0.4942, stage4_loss_mask: 0.7347, stage5_loss_cls: 0.6809, stage5_pos_acc: 54.0705, stage5_loss_bbox: 0.2427, stage5_loss_iou: 0.4841, stage5_loss_mask: 0.7161, loss: 15.5465\n",
      "2025-07-16 13:28:27,027 - mmdet - INFO - Epoch [11][150/750]\tlr: 2.500e-05, eta: 3:49:12, time: 0.338, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1089, stage0_pos_acc: 26.2608, stage0_loss_bbox: 0.6203, stage0_loss_iou: 1.0528, stage0_loss_mask: 0.9006, stage1_loss_cls: 0.8580, stage1_pos_acc: 38.2486, stage1_loss_bbox: 0.3113, stage1_loss_iou: 0.5358, stage1_loss_mask: 0.5256, stage2_loss_cls: 0.7979, stage2_pos_acc: 42.2272, stage2_loss_bbox: 0.2432, stage2_loss_iou: 0.4187, stage2_loss_mask: 0.4731, stage3_loss_cls: 0.7246, stage3_pos_acc: 46.8403, stage3_loss_bbox: 0.2289, stage3_loss_iou: 0.3926, stage3_loss_mask: 0.4559, stage4_loss_cls: 0.6933, stage4_pos_acc: 46.8998, stage4_loss_bbox: 0.2270, stage4_loss_iou: 0.3769, stage4_loss_mask: 0.4716, stage5_loss_cls: 0.6865, stage5_pos_acc: 48.0483, stage5_loss_bbox: 0.2205, stage5_loss_iou: 0.3706, stage5_loss_mask: 0.4488, loss: 13.1433\n",
      "2025-07-16 13:28:44,200 - mmdet - INFO - Epoch [11][200/750]\tlr: 2.500e-05, eta: 3:48:57, time: 0.343, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1186, stage0_pos_acc: 30.6859, stage0_loss_bbox: 0.5787, stage0_loss_iou: 1.0880, stage0_loss_mask: 1.0857, stage1_loss_cls: 0.8174, stage1_pos_acc: 48.7566, stage1_loss_bbox: 0.2931, stage1_loss_iou: 0.5510, stage1_loss_mask: 0.5949, stage2_loss_cls: 0.7627, stage2_pos_acc: 48.7551, stage2_loss_bbox: 0.2399, stage2_loss_iou: 0.4311, stage2_loss_mask: 0.4969, stage3_loss_cls: 0.6971, stage3_pos_acc: 52.0520, stage3_loss_bbox: 0.2305, stage3_loss_iou: 0.4102, stage3_loss_mask: 0.4891, stage4_loss_cls: 0.6621, stage4_pos_acc: 51.4929, stage4_loss_bbox: 0.2212, stage4_loss_iou: 0.4010, stage4_loss_mask: 0.4682, stage5_loss_cls: 0.6490, stage5_pos_acc: 53.4470, stage5_loss_bbox: 0.2223, stage5_loss_iou: 0.3998, stage5_loss_mask: 0.4761, loss: 13.3844\n",
      "2025-07-16 13:29:01,146 - mmdet - INFO - Epoch [11][250/750]\tlr: 2.500e-05, eta: 3:48:42, time: 0.339, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0774, stage0_pos_acc: 34.7065, stage0_loss_bbox: 0.5938, stage0_loss_iou: 1.0732, stage0_loss_mask: 1.1718, stage1_loss_cls: 0.8362, stage1_pos_acc: 44.1160, stage1_loss_bbox: 0.3191, stage1_loss_iou: 0.5698, stage1_loss_mask: 0.7513, stage2_loss_cls: 0.7792, stage2_pos_acc: 46.1303, stage2_loss_bbox: 0.2561, stage2_loss_iou: 0.4758, stage2_loss_mask: 0.6749, stage3_loss_cls: 0.7038, stage3_pos_acc: 49.7915, stage3_loss_bbox: 0.2613, stage3_loss_iou: 0.4747, stage3_loss_mask: 0.6884, stage4_loss_cls: 0.6666, stage4_pos_acc: 50.6534, stage4_loss_bbox: 0.2604, stage4_loss_iou: 0.4700, stage4_loss_mask: 0.6535, stage5_loss_cls: 0.6663, stage5_pos_acc: 54.1212, stage5_loss_bbox: 0.2648, stage5_loss_iou: 0.4631, stage5_loss_mask: 0.7033, loss: 14.8548\n",
      "2025-07-16 13:29:17,670 - mmdet - INFO - Epoch [11][300/750]\tlr: 2.500e-05, eta: 3:48:24, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1382, stage0_pos_acc: 28.7641, stage0_loss_bbox: 0.5925, stage0_loss_iou: 1.0481, stage0_loss_mask: 1.0423, stage1_loss_cls: 0.8914, stage1_pos_acc: 36.3602, stage1_loss_bbox: 0.3323, stage1_loss_iou: 0.5923, stage1_loss_mask: 0.7782, stage2_loss_cls: 0.8350, stage2_pos_acc: 41.5745, stage2_loss_bbox: 0.2716, stage2_loss_iou: 0.4813, stage2_loss_mask: 0.7256, stage3_loss_cls: 0.7759, stage3_pos_acc: 44.4641, stage3_loss_bbox: 0.2544, stage3_loss_iou: 0.4589, stage3_loss_mask: 0.7035, stage4_loss_cls: 0.7451, stage4_pos_acc: 45.7118, stage4_loss_bbox: 0.2606, stage4_loss_iou: 0.4598, stage4_loss_mask: 0.7222, stage5_loss_cls: 0.7407, stage5_pos_acc: 41.5100, stage5_loss_bbox: 0.2668, stage5_loss_iou: 0.4597, stage5_loss_mask: 0.7056, loss: 15.2819\n",
      "2025-07-16 13:29:34,720 - mmdet - INFO - Epoch [11][350/750]\tlr: 2.500e-05, eta: 3:48:09, time: 0.341, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1372, stage0_pos_acc: 34.9811, stage0_loss_bbox: 0.6178, stage0_loss_iou: 1.1080, stage0_loss_mask: 1.2037, stage1_loss_cls: 0.8608, stage1_pos_acc: 38.1695, stage1_loss_bbox: 0.3180, stage1_loss_iou: 0.5983, stage1_loss_mask: 0.8085, stage2_loss_cls: 0.8075, stage2_pos_acc: 42.6846, stage2_loss_bbox: 0.2861, stage2_loss_iou: 0.4822, stage2_loss_mask: 0.6628, stage3_loss_cls: 0.7596, stage3_pos_acc: 47.1037, stage3_loss_bbox: 0.2642, stage3_loss_iou: 0.4575, stage3_loss_mask: 0.6526, stage4_loss_cls: 0.7264, stage4_pos_acc: 51.9831, stage4_loss_bbox: 0.2508, stage4_loss_iou: 0.4360, stage4_loss_mask: 0.5968, stage5_loss_cls: 0.7198, stage5_pos_acc: 51.6198, stage5_loss_bbox: 0.2461, stage5_loss_iou: 0.4255, stage5_loss_mask: 0.5991, loss: 15.0255\n",
      "2025-07-16 13:29:51,785 - mmdet - INFO - Epoch [11][400/750]\tlr: 2.500e-05, eta: 3:47:54, time: 0.341, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1143, stage0_pos_acc: 30.8959, stage0_loss_bbox: 0.5990, stage0_loss_iou: 1.0490, stage0_loss_mask: 1.0559, stage1_loss_cls: 0.8801, stage1_pos_acc: 43.2976, stage1_loss_bbox: 0.2880, stage1_loss_iou: 0.5278, stage1_loss_mask: 0.5688, stage2_loss_cls: 0.8355, stage2_pos_acc: 45.5508, stage2_loss_bbox: 0.2370, stage2_loss_iou: 0.4126, stage2_loss_mask: 0.5139, stage3_loss_cls: 0.7736, stage3_pos_acc: 46.1583, stage3_loss_bbox: 0.2227, stage3_loss_iou: 0.3854, stage3_loss_mask: 0.4686, stage4_loss_cls: 0.7443, stage4_pos_acc: 48.9751, stage4_loss_bbox: 0.2191, stage4_loss_iou: 0.3776, stage4_loss_mask: 0.4492, stage5_loss_cls: 0.7291, stage5_pos_acc: 51.7796, stage5_loss_bbox: 0.2070, stage5_loss_iou: 0.3729, stage5_loss_mask: 0.4560, loss: 13.4872\n",
      "2025-07-16 13:30:08,720 - mmdet - INFO - Epoch [11][450/750]\tlr: 2.500e-05, eta: 3:47:38, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1201, stage0_pos_acc: 33.3500, stage0_loss_bbox: 0.6171, stage0_loss_iou: 1.1432, stage0_loss_mask: 1.3443, stage1_loss_cls: 0.8801, stage1_pos_acc: 43.1762, stage1_loss_bbox: 0.3442, stage1_loss_iou: 0.6791, stage1_loss_mask: 0.9372, stage2_loss_cls: 0.8282, stage2_pos_acc: 46.3286, stage2_loss_bbox: 0.2873, stage2_loss_iou: 0.5582, stage2_loss_mask: 0.8468, stage3_loss_cls: 0.7452, stage3_pos_acc: 49.4357, stage3_loss_bbox: 0.2754, stage3_loss_iou: 0.5363, stage3_loss_mask: 0.8673, stage4_loss_cls: 0.7154, stage4_pos_acc: 53.1214, stage4_loss_bbox: 0.2687, stage4_loss_iou: 0.5148, stage4_loss_mask: 0.8593, stage5_loss_cls: 0.7034, stage5_pos_acc: 53.0881, stage5_loss_bbox: 0.2740, stage5_loss_iou: 0.5040, stage5_loss_mask: 0.8566, loss: 16.7062\n",
      "2025-07-16 13:30:25,666 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:30:25,666 - mmdet - INFO - Epoch [11][500/750]\tlr: 2.500e-05, eta: 3:47:22, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1253, stage0_pos_acc: 33.2287, stage0_loss_bbox: 0.5642, stage0_loss_iou: 1.1072, stage0_loss_mask: 1.2290, stage1_loss_cls: 0.8670, stage1_pos_acc: 44.2140, stage1_loss_bbox: 0.2814, stage1_loss_iou: 0.5919, stage1_loss_mask: 0.7647, stage2_loss_cls: 0.8138, stage2_pos_acc: 49.6217, stage2_loss_bbox: 0.2283, stage2_loss_iou: 0.4648, stage2_loss_mask: 0.6552, stage3_loss_cls: 0.7528, stage3_pos_acc: 51.0254, stage3_loss_bbox: 0.2040, stage3_loss_iou: 0.4296, stage3_loss_mask: 0.6015, stage4_loss_cls: 0.7308, stage4_pos_acc: 50.9692, stage4_loss_bbox: 0.2000, stage4_loss_iou: 0.4190, stage4_loss_mask: 0.6238, stage5_loss_cls: 0.7133, stage5_pos_acc: 54.7637, stage5_loss_bbox: 0.2093, stage5_loss_iou: 0.4245, stage5_loss_mask: 0.6109, loss: 14.6124\n",
      "2025-07-16 13:30:42,707 - mmdet - INFO - Epoch [11][550/750]\tlr: 2.500e-05, eta: 3:47:07, time: 0.341, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1392, stage0_pos_acc: 31.3082, stage0_loss_bbox: 0.5475, stage0_loss_iou: 1.0663, stage0_loss_mask: 0.9958, stage1_loss_cls: 0.8668, stage1_pos_acc: 42.5301, stage1_loss_bbox: 0.2681, stage1_loss_iou: 0.5246, stage1_loss_mask: 0.6420, stage2_loss_cls: 0.8069, stage2_pos_acc: 45.2054, stage2_loss_bbox: 0.2094, stage2_loss_iou: 0.4051, stage2_loss_mask: 0.6022, stage3_loss_cls: 0.7324, stage3_pos_acc: 46.4100, stage3_loss_bbox: 0.2114, stage3_loss_iou: 0.3918, stage3_loss_mask: 0.6052, stage4_loss_cls: 0.7018, stage4_pos_acc: 49.7769, stage4_loss_bbox: 0.2012, stage4_loss_iou: 0.3784, stage4_loss_mask: 0.5874, stage5_loss_cls: 0.6978, stage5_pos_acc: 50.5068, stage5_loss_bbox: 0.2115, stage5_loss_iou: 0.3829, stage5_loss_mask: 0.5889, loss: 13.7645\n",
      "2025-07-16 13:30:59,745 - mmdet - INFO - Epoch [11][600/750]\tlr: 2.500e-05, eta: 3:46:52, time: 0.341, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1153, stage0_pos_acc: 32.2561, stage0_loss_bbox: 0.6087, stage0_loss_iou: 1.0752, stage0_loss_mask: 1.1711, stage1_loss_cls: 0.8447, stage1_pos_acc: 44.8001, stage1_loss_bbox: 0.3093, stage1_loss_iou: 0.5859, stage1_loss_mask: 0.7341, stage2_loss_cls: 0.7882, stage2_pos_acc: 56.8109, stage2_loss_bbox: 0.2546, stage2_loss_iou: 0.4747, stage2_loss_mask: 0.6924, stage3_loss_cls: 0.6878, stage3_pos_acc: 52.2331, stage3_loss_bbox: 0.2545, stage3_loss_iou: 0.4664, stage3_loss_mask: 0.7180, stage4_loss_cls: 0.6723, stage4_pos_acc: 57.0310, stage4_loss_bbox: 0.2514, stage4_loss_iou: 0.4502, stage4_loss_mask: 0.7099, stage5_loss_cls: 0.6713, stage5_pos_acc: 56.0815, stage5_loss_bbox: 0.2521, stage5_loss_iou: 0.4486, stage5_loss_mask: 0.6960, loss: 14.9325\n",
      "2025-07-16 13:31:17,046 - mmdet - INFO - Epoch [11][650/750]\tlr: 2.500e-05, eta: 3:46:38, time: 0.346, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1134, stage0_pos_acc: 27.3302, stage0_loss_bbox: 0.5976, stage0_loss_iou: 1.0893, stage0_loss_mask: 1.1786, stage1_loss_cls: 0.8502, stage1_pos_acc: 43.2484, stage1_loss_bbox: 0.3232, stage1_loss_iou: 0.6268, stage1_loss_mask: 0.8343, stage2_loss_cls: 0.8098, stage2_pos_acc: 44.5833, stage2_loss_bbox: 0.2574, stage2_loss_iou: 0.5079, stage2_loss_mask: 0.7477, stage3_loss_cls: 0.7466, stage3_pos_acc: 44.9857, stage3_loss_bbox: 0.2508, stage3_loss_iou: 0.4804, stage3_loss_mask: 0.6865, stage4_loss_cls: 0.7032, stage4_pos_acc: 47.8317, stage4_loss_bbox: 0.2473, stage4_loss_iou: 0.4805, stage4_loss_mask: 0.6856, stage5_loss_cls: 0.7013, stage5_pos_acc: 48.5127, stage5_loss_bbox: 0.2422, stage5_loss_iou: 0.4641, stage5_loss_mask: 0.6531, loss: 15.2779\n",
      "2025-07-16 13:31:33,666 - mmdet - INFO - Epoch [11][700/750]\tlr: 2.500e-05, eta: 3:46:21, time: 0.332, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0984, stage0_pos_acc: 28.2500, stage0_loss_bbox: 0.5887, stage0_loss_iou: 1.0044, stage0_loss_mask: 0.9233, stage1_loss_cls: 0.8260, stage1_pos_acc: 38.8939, stage1_loss_bbox: 0.3460, stage1_loss_iou: 0.5301, stage1_loss_mask: 0.6867, stage2_loss_cls: 0.7827, stage2_pos_acc: 41.3210, stage2_loss_bbox: 0.2871, stage2_loss_iou: 0.4329, stage2_loss_mask: 0.6254, stage3_loss_cls: 0.7287, stage3_pos_acc: 42.2043, stage3_loss_bbox: 0.2795, stage3_loss_iou: 0.4217, stage3_loss_mask: 0.6191, stage4_loss_cls: 0.7000, stage4_pos_acc: 45.9716, stage4_loss_bbox: 0.2791, stage4_loss_iou: 0.4116, stage4_loss_mask: 0.6093, stage5_loss_cls: 0.6941, stage5_pos_acc: 50.8462, stage5_loss_bbox: 0.2727, stage5_loss_iou: 0.4124, stage5_loss_mask: 0.6057, loss: 14.1656\n",
      "2025-07-16 13:31:50,445 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:31:50,445 - mmdet - INFO - Epoch [11][750/750]\tlr: 2.500e-05, eta: 3:46:04, time: 0.336, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0971, stage0_pos_acc: 37.0687, stage0_loss_bbox: 0.5750, stage0_loss_iou: 0.9787, stage0_loss_mask: 1.0156, stage1_loss_cls: 0.8777, stage1_pos_acc: 44.2986, stage1_loss_bbox: 0.3220, stage1_loss_iou: 0.5526, stage1_loss_mask: 0.6850, stage2_loss_cls: 0.8281, stage2_pos_acc: 45.4220, stage2_loss_bbox: 0.2680, stage2_loss_iou: 0.4797, stage2_loss_mask: 0.6461, stage3_loss_cls: 0.7568, stage3_pos_acc: 51.0808, stage3_loss_bbox: 0.2649, stage3_loss_iou: 0.4609, stage3_loss_mask: 0.6300, stage4_loss_cls: 0.7480, stage4_pos_acc: 50.0887, stage4_loss_bbox: 0.2577, stage4_loss_iou: 0.4525, stage4_loss_mask: 0.6024, stage5_loss_cls: 0.7431, stage5_pos_acc: 53.9030, stage5_loss_bbox: 0.2541, stage5_loss_iou: 0.4517, stage5_loss_mask: 0.6192, loss: 14.5668\n",
      "2025-07-16 13:31:50,543 - mmdet - INFO - Saving checkpoint at 11 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 13:34:54,517 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.066\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.044\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.270\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.537\n",
      "2025-07-16 13:34:56,306 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.82s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.040\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.045\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.489\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.491\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.491\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.140\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.275\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.541\n",
      "2025-07-16 13:34:59,405 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:34:59,405 - mmdet - INFO - Epoch(val) [11][750]\tbbox_mAP: 0.0350, bbox_mAP_50: 0.0660, bbox_mAP_75: 0.0330, bbox_mAP_s: 0.0650, bbox_mAP_m: 0.0190, bbox_mAP_l: 0.0440, bbox_mAP_copypaste: 0.035 0.066 0.033 0.065 0.019 0.044, segm_mAP: 0.0360, segm_mAP_50: 0.0650, segm_mAP_75: 0.0350, segm_mAP_s: 0.0400, segm_mAP_m: 0.0190, segm_mAP_l: 0.0450, segm_mAP_copypaste: 0.036 0.065 0.035 0.040 0.019 0.045\n",
      "2025-07-16 13:35:18,284 - mmdet - INFO - Epoch [12][50/750]\tlr: 2.500e-05, eta: 3:45:58, time: 0.377, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.1324, stage0_pos_acc: 34.6621, stage0_loss_bbox: 0.5873, stage0_loss_iou: 1.0953, stage0_loss_mask: 1.2205, stage1_loss_cls: 0.8193, stage1_pos_acc: 47.1341, stage1_loss_bbox: 0.2818, stage1_loss_iou: 0.5577, stage1_loss_mask: 0.6884, stage2_loss_cls: 0.7618, stage2_pos_acc: 49.7303, stage2_loss_bbox: 0.2238, stage2_loss_iou: 0.4555, stage2_loss_mask: 0.7049, stage3_loss_cls: 0.6960, stage3_pos_acc: 51.3077, stage3_loss_bbox: 0.2058, stage3_loss_iou: 0.4164, stage3_loss_mask: 0.6664, stage4_loss_cls: 0.6435, stage4_pos_acc: 50.4735, stage4_loss_bbox: 0.2114, stage4_loss_iou: 0.4156, stage4_loss_mask: 0.6561, stage5_loss_cls: 0.6442, stage5_pos_acc: 54.9955, stage5_loss_bbox: 0.1997, stage5_loss_iou: 0.4063, stage5_loss_mask: 0.6585, loss: 14.3486\n",
      "2025-07-16 13:35:34,523 - mmdet - INFO - Epoch [12][100/750]\tlr: 2.500e-05, eta: 3:45:38, time: 0.325, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0935, stage0_pos_acc: 36.5831, stage0_loss_bbox: 0.5772, stage0_loss_iou: 1.0181, stage0_loss_mask: 0.9590, stage1_loss_cls: 0.8296, stage1_pos_acc: 43.9486, stage1_loss_bbox: 0.3036, stage1_loss_iou: 0.5451, stage1_loss_mask: 0.6648, stage2_loss_cls: 0.7891, stage2_pos_acc: 46.3826, stage2_loss_bbox: 0.2446, stage2_loss_iou: 0.4387, stage2_loss_mask: 0.6142, stage3_loss_cls: 0.7303, stage3_pos_acc: 47.9128, stage3_loss_bbox: 0.2393, stage3_loss_iou: 0.4242, stage3_loss_mask: 0.6193, stage4_loss_cls: 0.6921, stage4_pos_acc: 49.4603, stage4_loss_bbox: 0.2399, stage4_loss_iou: 0.4194, stage4_loss_mask: 0.5986, stage5_loss_cls: 0.6764, stage5_pos_acc: 53.9508, stage5_loss_bbox: 0.2403, stage5_loss_iou: 0.4181, stage5_loss_mask: 0.5929, loss: 13.9686\n",
      "2025-07-16 13:35:50,869 - mmdet - INFO - Epoch [12][150/750]\tlr: 2.500e-05, eta: 3:45:20, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1225, stage0_pos_acc: 30.3701, stage0_loss_bbox: 0.5855, stage0_loss_iou: 1.0165, stage0_loss_mask: 1.0670, stage1_loss_cls: 0.8643, stage1_pos_acc: 43.2033, stage1_loss_bbox: 0.3170, stage1_loss_iou: 0.5481, stage1_loss_mask: 0.7543, stage2_loss_cls: 0.8092, stage2_pos_acc: 46.5106, stage2_loss_bbox: 0.2670, stage2_loss_iou: 0.4626, stage2_loss_mask: 0.7045, stage3_loss_cls: 0.7488, stage3_pos_acc: 49.9681, stage3_loss_bbox: 0.2456, stage3_loss_iou: 0.4440, stage3_loss_mask: 0.6655, stage4_loss_cls: 0.7291, stage4_pos_acc: 52.3897, stage4_loss_bbox: 0.2400, stage4_loss_iou: 0.4364, stage4_loss_mask: 0.6459, stage5_loss_cls: 0.7223, stage5_pos_acc: 51.0057, stage5_loss_bbox: 0.2400, stage5_loss_iou: 0.4322, stage5_loss_mask: 0.6475, loss: 14.7156\n",
      "2025-07-16 13:36:07,535 - mmdet - INFO - Epoch [12][200/750]\tlr: 2.500e-05, eta: 3:45:02, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1320, stage0_pos_acc: 37.5335, stage0_loss_bbox: 0.6295, stage0_loss_iou: 1.2063, stage0_loss_mask: 1.3661, stage1_loss_cls: 0.8509, stage1_pos_acc: 46.3488, stage1_loss_bbox: 0.2765, stage1_loss_iou: 0.6156, stage1_loss_mask: 0.6701, stage2_loss_cls: 0.7926, stage2_pos_acc: 50.6737, stage2_loss_bbox: 0.2301, stage2_loss_iou: 0.4748, stage2_loss_mask: 0.5840, stage3_loss_cls: 0.7115, stage3_pos_acc: 55.6287, stage3_loss_bbox: 0.2303, stage3_loss_iou: 0.4660, stage3_loss_mask: 0.5822, stage4_loss_cls: 0.6819, stage4_pos_acc: 56.3816, stage4_loss_bbox: 0.2224, stage4_loss_iou: 0.4552, stage4_loss_mask: 0.5434, stage5_loss_cls: 0.6710, stage5_pos_acc: 59.3372, stage5_loss_bbox: 0.2224, stage5_loss_iou: 0.4580, stage5_loss_mask: 0.5641, loss: 14.6370\n",
      "2025-07-16 13:36:24,185 - mmdet - INFO - Epoch [12][250/750]\tlr: 2.500e-05, eta: 3:44:45, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1263, stage0_pos_acc: 33.2552, stage0_loss_bbox: 0.5763, stage0_loss_iou: 1.0827, stage0_loss_mask: 0.9940, stage1_loss_cls: 0.8571, stage1_pos_acc: 38.5156, stage1_loss_bbox: 0.2627, stage1_loss_iou: 0.5211, stage1_loss_mask: 0.5299, stage2_loss_cls: 0.7918, stage2_pos_acc: 42.5561, stage2_loss_bbox: 0.2066, stage2_loss_iou: 0.3991, stage2_loss_mask: 0.4386, stage3_loss_cls: 0.7210, stage3_pos_acc: 46.0370, stage3_loss_bbox: 0.1972, stage3_loss_iou: 0.3930, stage3_loss_mask: 0.4560, stage4_loss_cls: 0.6745, stage4_pos_acc: 46.9465, stage4_loss_bbox: 0.1873, stage4_loss_iou: 0.3735, stage4_loss_mask: 0.4527, stage5_loss_cls: 0.6725, stage5_pos_acc: 51.3323, stage5_loss_bbox: 0.1865, stage5_loss_iou: 0.3743, stage5_loss_mask: 0.4247, loss: 12.8996\n",
      "2025-07-16 13:36:40,897 - mmdet - INFO - Epoch [12][300/750]\tlr: 2.500e-05, eta: 3:44:28, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1431, stage0_pos_acc: 32.7720, stage0_loss_bbox: 0.6328, stage0_loss_iou: 1.0752, stage0_loss_mask: 1.1023, stage1_loss_cls: 0.8724, stage1_pos_acc: 48.7175, stage1_loss_bbox: 0.3160, stage1_loss_iou: 0.5643, stage1_loss_mask: 0.8171, stage2_loss_cls: 0.7827, stage2_pos_acc: 50.1935, stage2_loss_bbox: 0.2942, stage2_loss_iou: 0.4749, stage2_loss_mask: 0.7819, stage3_loss_cls: 0.7145, stage3_pos_acc: 51.0514, stage3_loss_bbox: 0.2811, stage3_loss_iou: 0.4417, stage3_loss_mask: 0.7234, stage4_loss_cls: 0.6734, stage4_pos_acc: 57.1978, stage4_loss_bbox: 0.2766, stage4_loss_iou: 0.4280, stage4_loss_mask: 0.7002, stage5_loss_cls: 0.6708, stage5_pos_acc: 56.3352, stage5_loss_bbox: 0.2807, stage5_loss_iou: 0.4248, stage5_loss_mask: 0.7094, loss: 15.1814\n",
      "2025-07-16 13:36:57,256 - mmdet - INFO - Epoch [12][350/750]\tlr: 2.500e-05, eta: 3:44:10, time: 0.327, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1318, stage0_pos_acc: 33.9810, stage0_loss_bbox: 0.5277, stage0_loss_iou: 1.0187, stage0_loss_mask: 0.8818, stage1_loss_cls: 0.8217, stage1_pos_acc: 47.7239, stage1_loss_bbox: 0.2483, stage1_loss_iou: 0.4898, stage1_loss_mask: 0.5795, stage2_loss_cls: 0.7352, stage2_pos_acc: 54.1678, stage2_loss_bbox: 0.1968, stage2_loss_iou: 0.3909, stage2_loss_mask: 0.5320, stage3_loss_cls: 0.6448, stage3_pos_acc: 50.0110, stage3_loss_bbox: 0.1952, stage3_loss_iou: 0.3839, stage3_loss_mask: 0.5142, stage4_loss_cls: 0.6217, stage4_pos_acc: 52.2309, stage4_loss_bbox: 0.1845, stage4_loss_iou: 0.3689, stage4_loss_mask: 0.5034, stage5_loss_cls: 0.6174, stage5_pos_acc: 53.4143, stage5_loss_bbox: 0.1901, stage5_loss_iou: 0.3644, stage5_loss_mask: 0.5074, loss: 12.6499\n",
      "2025-07-16 13:37:13,801 - mmdet - INFO - Epoch [12][400/750]\tlr: 2.500e-05, eta: 3:43:52, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1072, stage0_pos_acc: 33.4610, stage0_loss_bbox: 0.5485, stage0_loss_iou: 1.0160, stage0_loss_mask: 1.1384, stage1_loss_cls: 0.8427, stage1_pos_acc: 44.9266, stage1_loss_bbox: 0.3161, stage1_loss_iou: 0.5623, stage1_loss_mask: 0.8057, stage2_loss_cls: 0.7903, stage2_pos_acc: 45.8543, stage2_loss_bbox: 0.2702, stage2_loss_iou: 0.4802, stage2_loss_mask: 0.7287, stage3_loss_cls: 0.7197, stage3_pos_acc: 47.6845, stage3_loss_bbox: 0.2657, stage3_loss_iou: 0.4579, stage3_loss_mask: 0.6959, stage4_loss_cls: 0.6911, stage4_pos_acc: 49.9750, stage4_loss_bbox: 0.2555, stage4_loss_iou: 0.4532, stage4_loss_mask: 0.7140, stage5_loss_cls: 0.6818, stage5_pos_acc: 51.4580, stage5_loss_bbox: 0.2565, stage5_loss_iou: 0.4542, stage5_loss_mask: 0.7191, loss: 14.9710\n",
      "2025-07-16 13:37:30,195 - mmdet - INFO - Epoch [12][450/750]\tlr: 2.500e-05, eta: 3:43:33, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1088, stage0_pos_acc: 29.9885, stage0_loss_bbox: 0.6103, stage0_loss_iou: 1.0742, stage0_loss_mask: 1.0748, stage1_loss_cls: 0.8551, stage1_pos_acc: 38.6105, stage1_loss_bbox: 0.2857, stage1_loss_iou: 0.5251, stage1_loss_mask: 0.6825, stage2_loss_cls: 0.7816, stage2_pos_acc: 45.4315, stage2_loss_bbox: 0.2368, stage2_loss_iou: 0.4286, stage2_loss_mask: 0.6493, stage3_loss_cls: 0.7294, stage3_pos_acc: 47.0911, stage3_loss_bbox: 0.2230, stage3_loss_iou: 0.4109, stage3_loss_mask: 0.5885, stage4_loss_cls: 0.6759, stage4_pos_acc: 48.5208, stage4_loss_bbox: 0.2255, stage4_loss_iou: 0.4069, stage4_loss_mask: 0.5804, stage5_loss_cls: 0.6773, stage5_pos_acc: 52.1545, stage5_loss_bbox: 0.2210, stage5_loss_iou: 0.3980, stage5_loss_mask: 0.5587, loss: 14.0084\n",
      "2025-07-16 13:37:46,904 - mmdet - INFO - Epoch [12][500/750]\tlr: 2.500e-05, eta: 3:43:17, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1024, stage0_pos_acc: 33.9794, stage0_loss_bbox: 0.5479, stage0_loss_iou: 1.0594, stage0_loss_mask: 1.1705, stage1_loss_cls: 0.7991, stage1_pos_acc: 48.7527, stage1_loss_bbox: 0.3051, stage1_loss_iou: 0.5858, stage1_loss_mask: 0.8326, stage2_loss_cls: 0.7469, stage2_pos_acc: 49.3268, stage2_loss_bbox: 0.2472, stage2_loss_iou: 0.4840, stage2_loss_mask: 0.7453, stage3_loss_cls: 0.6894, stage3_pos_acc: 50.1772, stage3_loss_bbox: 0.2289, stage3_loss_iou: 0.4569, stage3_loss_mask: 0.7299, stage4_loss_cls: 0.6532, stage4_pos_acc: 54.2008, stage4_loss_bbox: 0.2261, stage4_loss_iou: 0.4535, stage4_loss_mask: 0.7211, stage5_loss_cls: 0.6565, stage5_pos_acc: 52.5174, stage5_loss_bbox: 0.2270, stage5_loss_iou: 0.4543, stage5_loss_mask: 0.7168, loss: 14.8399\n",
      "2025-07-16 13:38:04,046 - mmdet - INFO - Epoch [12][550/750]\tlr: 2.500e-05, eta: 3:43:02, time: 0.343, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1053, stage0_pos_acc: 33.6834, stage0_loss_bbox: 0.5475, stage0_loss_iou: 0.9732, stage0_loss_mask: 0.8774, stage1_loss_cls: 0.8542, stage1_pos_acc: 38.7310, stage1_loss_bbox: 0.2830, stage1_loss_iou: 0.4963, stage1_loss_mask: 0.5231, stage2_loss_cls: 0.7927, stage2_pos_acc: 44.4027, stage2_loss_bbox: 0.2364, stage2_loss_iou: 0.4115, stage2_loss_mask: 0.5066, stage3_loss_cls: 0.7303, stage3_pos_acc: 45.2174, stage3_loss_bbox: 0.2137, stage3_loss_iou: 0.3848, stage3_loss_mask: 0.4301, stage4_loss_cls: 0.7067, stage4_pos_acc: 44.3024, stage4_loss_bbox: 0.2059, stage4_loss_iou: 0.3728, stage4_loss_mask: 0.4255, stage5_loss_cls: 0.6958, stage5_pos_acc: 44.6357, stage5_loss_bbox: 0.2221, stage5_loss_iou: 0.3752, stage5_loss_mask: 0.4322, loss: 12.8021\n",
      "2025-07-16 13:38:20,820 - mmdet - INFO - Epoch [12][600/750]\tlr: 2.500e-05, eta: 3:42:45, time: 0.335, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1305, stage0_pos_acc: 38.3556, stage0_loss_bbox: 0.6100, stage0_loss_iou: 1.1069, stage0_loss_mask: 1.1497, stage1_loss_cls: 0.8605, stage1_pos_acc: 47.0504, stage1_loss_bbox: 0.3321, stage1_loss_iou: 0.6283, stage1_loss_mask: 0.8373, stage2_loss_cls: 0.7952, stage2_pos_acc: 50.8853, stage2_loss_bbox: 0.2747, stage2_loss_iou: 0.5236, stage2_loss_mask: 0.8041, stage3_loss_cls: 0.7289, stage3_pos_acc: 53.3329, stage3_loss_bbox: 0.2604, stage3_loss_iou: 0.4907, stage3_loss_mask: 0.7795, stage4_loss_cls: 0.7051, stage4_pos_acc: 55.0195, stage4_loss_bbox: 0.2477, stage4_loss_iou: 0.4767, stage4_loss_mask: 0.7576, stage5_loss_cls: 0.7063, stage5_pos_acc: 55.6338, stage5_loss_bbox: 0.2472, stage5_loss_iou: 0.4726, stage5_loss_mask: 0.7517, loss: 15.6772\n",
      "2025-07-16 13:38:37,683 - mmdet - INFO - Epoch [12][650/750]\tlr: 2.500e-05, eta: 3:42:29, time: 0.337, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1029, stage0_pos_acc: 32.7838, stage0_loss_bbox: 0.5806, stage0_loss_iou: 1.0161, stage0_loss_mask: 0.9014, stage1_loss_cls: 0.7872, stage1_pos_acc: 42.6548, stage1_loss_bbox: 0.2926, stage1_loss_iou: 0.5178, stage1_loss_mask: 0.5817, stage2_loss_cls: 0.7262, stage2_pos_acc: 48.1794, stage2_loss_bbox: 0.2383, stage2_loss_iou: 0.4208, stage2_loss_mask: 0.5122, stage3_loss_cls: 0.6708, stage3_pos_acc: 50.0903, stage3_loss_bbox: 0.2173, stage3_loss_iou: 0.3914, stage3_loss_mask: 0.4722, stage4_loss_cls: 0.6271, stage4_pos_acc: 55.1997, stage4_loss_bbox: 0.2169, stage4_loss_iou: 0.3830, stage4_loss_mask: 0.4697, stage5_loss_cls: 0.6203, stage5_pos_acc: 56.1719, stage5_loss_bbox: 0.2183, stage5_loss_iou: 0.3799, stage5_loss_mask: 0.4707, loss: 12.8153\n",
      "2025-07-16 13:38:54,354 - mmdet - INFO - Epoch [12][700/750]\tlr: 2.500e-05, eta: 3:42:12, time: 0.333, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1147, stage0_pos_acc: 32.8019, stage0_loss_bbox: 0.6161, stage0_loss_iou: 1.1145, stage0_loss_mask: 1.2791, stage1_loss_cls: 0.8131, stage1_pos_acc: 40.8732, stage1_loss_bbox: 0.3198, stage1_loss_iou: 0.5867, stage1_loss_mask: 0.7487, stage2_loss_cls: 0.7607, stage2_pos_acc: 41.5346, stage2_loss_bbox: 0.2590, stage2_loss_iou: 0.4720, stage2_loss_mask: 0.6261, stage3_loss_cls: 0.6997, stage3_pos_acc: 49.9634, stage3_loss_bbox: 0.2455, stage3_loss_iou: 0.4561, stage3_loss_mask: 0.6147, stage4_loss_cls: 0.6620, stage4_pos_acc: 55.0548, stage4_loss_bbox: 0.2382, stage4_loss_iou: 0.4454, stage4_loss_mask: 0.5849, stage5_loss_cls: 0.6594, stage5_pos_acc: 51.6937, stage5_loss_bbox: 0.2322, stage5_loss_iou: 0.4370, stage5_loss_mask: 0.5927, loss: 14.5783\n",
      "2025-07-16 13:39:11,392 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:39:11,392 - mmdet - INFO - Epoch [12][750/750]\tlr: 2.500e-05, eta: 3:41:56, time: 0.341, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1538, stage0_pos_acc: 29.1592, stage0_loss_bbox: 0.5687, stage0_loss_iou: 1.0737, stage0_loss_mask: 1.0523, stage1_loss_cls: 0.8727, stage1_pos_acc: 38.1481, stage1_loss_bbox: 0.2789, stage1_loss_iou: 0.5333, stage1_loss_mask: 0.6150, stage2_loss_cls: 0.7945, stage2_pos_acc: 45.7830, stage2_loss_bbox: 0.2449, stage2_loss_iou: 0.4557, stage2_loss_mask: 0.5483, stage3_loss_cls: 0.7210, stage3_pos_acc: 50.8133, stage3_loss_bbox: 0.2268, stage3_loss_iou: 0.4369, stage3_loss_mask: 0.5095, stage4_loss_cls: 0.6982, stage4_pos_acc: 46.1014, stage4_loss_bbox: 0.2188, stage4_loss_iou: 0.4238, stage4_loss_mask: 0.5181, stage5_loss_cls: 0.7026, stage5_pos_acc: 47.6244, stage5_loss_bbox: 0.2195, stage5_loss_iou: 0.4187, stage5_loss_mask: 0.5049, loss: 13.7904\n",
      "2025-07-16 13:39:11,524 - mmdet - INFO - Saving checkpoint at 12 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 89s, ETA:     0s2025-07-16 13:42:18,523 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.104\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.046\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.474\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.476\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.476\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.230\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.510\n",
      "2025-07-16 13:42:20,290 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.76s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.122\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.047\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.470\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.471\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.471\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.306\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.505\n",
      "2025-07-16 13:42:23,327 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:42:23,327 - mmdet - INFO - Epoch(val) [12][750]\tbbox_mAP: 0.0340, bbox_mAP_50: 0.0620, bbox_mAP_75: 0.0330, bbox_mAP_s: 0.1040, bbox_mAP_m: 0.0100, bbox_mAP_l: 0.0460, bbox_mAP_copypaste: 0.034 0.062 0.033 0.104 0.010 0.046, segm_mAP: 0.0340, segm_mAP_50: 0.0620, segm_mAP_75: 0.0330, segm_mAP_s: 0.1220, segm_mAP_m: 0.0090, segm_mAP_l: 0.0470, segm_mAP_copypaste: 0.034 0.062 0.033 0.122 0.009 0.047\n",
      "2025-07-16 13:42:43,158 - mmdet - INFO - Epoch [13][50/750]\tlr: 2.500e-05, eta: 3:41:53, time: 0.396, data_time: 0.055, memory: 11264, stage0_loss_cls: 1.1124, stage0_pos_acc: 35.1669, stage0_loss_bbox: 0.5567, stage0_loss_iou: 1.0615, stage0_loss_mask: 0.9904, stage1_loss_cls: 0.8146, stage1_pos_acc: 50.4377, stage1_loss_bbox: 0.2685, stage1_loss_iou: 0.5565, stage1_loss_mask: 0.6307, stage2_loss_cls: 0.7270, stage2_pos_acc: 50.5403, stage2_loss_bbox: 0.2330, stage2_loss_iou: 0.4525, stage2_loss_mask: 0.6089, stage3_loss_cls: 0.6573, stage3_pos_acc: 54.7529, stage3_loss_bbox: 0.2210, stage3_loss_iou: 0.4211, stage3_loss_mask: 0.5976, stage4_loss_cls: 0.6098, stage4_pos_acc: 56.6749, stage4_loss_bbox: 0.2231, stage4_loss_iou: 0.4213, stage4_loss_mask: 0.6018, stage5_loss_cls: 0.6031, stage5_pos_acc: 57.1277, stage5_loss_bbox: 0.2193, stage5_loss_iou: 0.4162, stage5_loss_mask: 0.6068, loss: 13.6111\n",
      "2025-07-16 13:42:59,938 - mmdet - INFO - Epoch [13][100/750]\tlr: 2.500e-05, eta: 3:41:36, time: 0.336, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1231, stage0_pos_acc: 35.0859, stage0_loss_bbox: 0.5454, stage0_loss_iou: 1.0530, stage0_loss_mask: 0.9815, stage1_loss_cls: 0.8286, stage1_pos_acc: 41.0343, stage1_loss_bbox: 0.2536, stage1_loss_iou: 0.5295, stage1_loss_mask: 0.6460, stage2_loss_cls: 0.7701, stage2_pos_acc: 46.0310, stage2_loss_bbox: 0.2184, stage2_loss_iou: 0.4387, stage2_loss_mask: 0.5977, stage3_loss_cls: 0.6907, stage3_pos_acc: 49.5848, stage3_loss_bbox: 0.2095, stage3_loss_iou: 0.4200, stage3_loss_mask: 0.5645, stage4_loss_cls: 0.6492, stage4_pos_acc: 56.2400, stage4_loss_bbox: 0.2119, stage4_loss_iou: 0.4150, stage4_loss_mask: 0.5698, stage5_loss_cls: 0.6564, stage5_pos_acc: 56.3859, stage5_loss_bbox: 0.2120, stage5_loss_iou: 0.4038, stage5_loss_mask: 0.5582, loss: 13.5467\n",
      "2025-07-16 13:43:16,746 - mmdet - INFO - Epoch [13][150/750]\tlr: 2.500e-05, eta: 3:41:20, time: 0.336, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1561, stage0_pos_acc: 29.8077, stage0_loss_bbox: 0.5833, stage0_loss_iou: 1.0581, stage0_loss_mask: 1.0177, stage1_loss_cls: 0.8402, stage1_pos_acc: 42.5364, stage1_loss_bbox: 0.3080, stage1_loss_iou: 0.5425, stage1_loss_mask: 0.6646, stage2_loss_cls: 0.7933, stage2_pos_acc: 46.8083, stage2_loss_bbox: 0.2447, stage2_loss_iou: 0.4454, stage2_loss_mask: 0.6229, stage3_loss_cls: 0.6989, stage3_pos_acc: 51.7707, stage3_loss_bbox: 0.2314, stage3_loss_iou: 0.4241, stage3_loss_mask: 0.5971, stage4_loss_cls: 0.6545, stage4_pos_acc: 50.4496, stage4_loss_bbox: 0.2250, stage4_loss_iou: 0.4068, stage4_loss_mask: 0.6037, stage5_loss_cls: 0.6533, stage5_pos_acc: 52.5903, stage5_loss_bbox: 0.2221, stage5_loss_iou: 0.4029, stage5_loss_mask: 0.6135, loss: 14.0100\n",
      "2025-07-16 13:43:33,478 - mmdet - INFO - Epoch [13][200/750]\tlr: 2.500e-05, eta: 3:41:03, time: 0.335, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1219, stage0_pos_acc: 29.4817, stage0_loss_bbox: 0.6220, stage0_loss_iou: 1.0720, stage0_loss_mask: 0.9694, stage1_loss_cls: 0.7924, stage1_pos_acc: 43.8056, stage1_loss_bbox: 0.2973, stage1_loss_iou: 0.5223, stage1_loss_mask: 0.6079, stage2_loss_cls: 0.7082, stage2_pos_acc: 50.9302, stage2_loss_bbox: 0.2279, stage2_loss_iou: 0.4184, stage2_loss_mask: 0.4899, stage3_loss_cls: 0.6393, stage3_pos_acc: 53.1365, stage3_loss_bbox: 0.2150, stage3_loss_iou: 0.3852, stage3_loss_mask: 0.4621, stage4_loss_cls: 0.5961, stage4_pos_acc: 55.1246, stage4_loss_bbox: 0.1997, stage4_loss_iou: 0.3668, stage4_loss_mask: 0.4393, stage5_loss_cls: 0.5777, stage5_pos_acc: 57.9222, stage5_loss_bbox: 0.2082, stage5_loss_iou: 0.3716, stage5_loss_mask: 0.4491, loss: 12.7597\n",
      "2025-07-16 13:43:50,411 - mmdet - INFO - Epoch [13][250/750]\tlr: 2.500e-05, eta: 3:40:47, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0997, stage0_pos_acc: 32.9893, stage0_loss_bbox: 0.6028, stage0_loss_iou: 1.0270, stage0_loss_mask: 1.0611, stage1_loss_cls: 0.7916, stage1_pos_acc: 45.8544, stage1_loss_bbox: 0.3386, stage1_loss_iou: 0.5469, stage1_loss_mask: 0.7093, stage2_loss_cls: 0.7443, stage2_pos_acc: 46.4897, stage2_loss_bbox: 0.2854, stage2_loss_iou: 0.4495, stage2_loss_mask: 0.6288, stage3_loss_cls: 0.6803, stage3_pos_acc: 48.1325, stage3_loss_bbox: 0.2730, stage3_loss_iou: 0.4220, stage3_loss_mask: 0.6321, stage4_loss_cls: 0.6448, stage4_pos_acc: 54.0206, stage4_loss_bbox: 0.2664, stage4_loss_iou: 0.4179, stage4_loss_mask: 0.6330, stage5_loss_cls: 0.6405, stage5_pos_acc: 52.9718, stage5_loss_bbox: 0.2678, stage5_loss_iou: 0.4136, stage5_loss_mask: 0.6199, loss: 14.1962\n",
      "2025-07-16 13:44:07,289 - mmdet - INFO - Epoch [13][300/750]\tlr: 2.500e-05, eta: 3:40:30, time: 0.338, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1333, stage0_pos_acc: 32.7752, stage0_loss_bbox: 0.6177, stage0_loss_iou: 1.1697, stage0_loss_mask: 1.4155, stage1_loss_cls: 0.8373, stage1_pos_acc: 43.5891, stage1_loss_bbox: 0.3209, stage1_loss_iou: 0.6300, stage1_loss_mask: 0.8567, stage2_loss_cls: 0.7535, stage2_pos_acc: 49.9922, stage2_loss_bbox: 0.2669, stage2_loss_iou: 0.5165, stage2_loss_mask: 0.8128, stage3_loss_cls: 0.6948, stage3_pos_acc: 50.5494, stage3_loss_bbox: 0.2610, stage3_loss_iou: 0.4932, stage3_loss_mask: 0.7688, stage4_loss_cls: 0.6807, stage4_pos_acc: 53.3970, stage4_loss_bbox: 0.2484, stage4_loss_iou: 0.4800, stage4_loss_mask: 0.7838, stage5_loss_cls: 0.6717, stage5_pos_acc: 55.6613, stage5_loss_bbox: 0.2408, stage5_loss_iou: 0.4731, stage5_loss_mask: 0.7573, loss: 15.8847\n",
      "2025-07-16 13:44:24,247 - mmdet - INFO - Epoch [13][350/750]\tlr: 2.500e-05, eta: 3:40:14, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0870, stage0_pos_acc: 35.3590, stage0_loss_bbox: 0.6742, stage0_loss_iou: 1.2220, stage0_loss_mask: 1.6528, stage1_loss_cls: 0.8078, stage1_pos_acc: 45.6269, stage1_loss_bbox: 0.3967, stage1_loss_iou: 0.7101, stage1_loss_mask: 1.0484, stage2_loss_cls: 0.7534, stage2_pos_acc: 50.5911, stage2_loss_bbox: 0.3182, stage2_loss_iou: 0.5569, stage2_loss_mask: 0.9738, stage3_loss_cls: 0.7078, stage3_pos_acc: 55.0210, stage3_loss_bbox: 0.2907, stage3_loss_iou: 0.5171, stage3_loss_mask: 0.8994, stage4_loss_cls: 0.6796, stage4_pos_acc: 58.4191, stage4_loss_bbox: 0.2778, stage4_loss_iou: 0.4967, stage4_loss_mask: 0.8680, stage5_loss_cls: 0.6773, stage5_pos_acc: 58.2588, stage5_loss_bbox: 0.2712, stage5_loss_iou: 0.4866, stage5_loss_mask: 0.8737, loss: 17.2474\n",
      "2025-07-16 13:44:41,695 - mmdet - INFO - Epoch [13][400/750]\tlr: 2.500e-05, eta: 3:40:00, time: 0.349, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1287, stage0_pos_acc: 33.8339, stage0_loss_bbox: 0.6421, stage0_loss_iou: 1.2017, stage0_loss_mask: 1.1256, stage1_loss_cls: 0.8031, stage1_pos_acc: 49.9021, stage1_loss_bbox: 0.3108, stage1_loss_iou: 0.5918, stage1_loss_mask: 0.5273, stage2_loss_cls: 0.7342, stage2_pos_acc: 55.3694, stage2_loss_bbox: 0.2365, stage2_loss_iou: 0.4455, stage2_loss_mask: 0.4411, stage3_loss_cls: 0.6447, stage3_pos_acc: 56.7862, stage3_loss_bbox: 0.2152, stage3_loss_iou: 0.4029, stage3_loss_mask: 0.4242, stage4_loss_cls: 0.6052, stage4_pos_acc: 53.9181, stage4_loss_bbox: 0.2024, stage4_loss_iou: 0.3883, stage4_loss_mask: 0.4193, stage5_loss_cls: 0.5878, stage5_pos_acc: 57.9426, stage5_loss_bbox: 0.2051, stage5_loss_iou: 0.3814, stage5_loss_mask: 0.4097, loss: 13.0747\n",
      "2025-07-16 13:44:58,580 - mmdet - INFO - Epoch [13][450/750]\tlr: 2.500e-05, eta: 3:39:44, time: 0.338, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0659, stage0_pos_acc: 30.5389, stage0_loss_bbox: 0.6203, stage0_loss_iou: 0.9987, stage0_loss_mask: 1.0576, stage1_loss_cls: 0.8313, stage1_pos_acc: 39.3644, stage1_loss_bbox: 0.3308, stage1_loss_iou: 0.5093, stage1_loss_mask: 0.7028, stage2_loss_cls: 0.7755, stage2_pos_acc: 44.5978, stage2_loss_bbox: 0.2727, stage2_loss_iou: 0.4309, stage2_loss_mask: 0.6399, stage3_loss_cls: 0.7251, stage3_pos_acc: 44.3469, stage3_loss_bbox: 0.2606, stage3_loss_iou: 0.4141, stage3_loss_mask: 0.6107, stage4_loss_cls: 0.6875, stage4_pos_acc: 51.6511, stage4_loss_bbox: 0.2570, stage4_loss_iou: 0.4092, stage4_loss_mask: 0.6221, stage5_loss_cls: 0.6858, stage5_pos_acc: 49.6493, stage5_loss_bbox: 0.2505, stage5_loss_iou: 0.4056, stage5_loss_mask: 0.6172, loss: 14.1813\n",
      "2025-07-16 13:45:15,729 - mmdet - INFO - Epoch [13][500/750]\tlr: 2.500e-05, eta: 3:39:29, time: 0.343, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1333, stage0_pos_acc: 29.1850, stage0_loss_bbox: 0.5516, stage0_loss_iou: 1.1087, stage0_loss_mask: 1.1656, stage1_loss_cls: 0.8297, stage1_pos_acc: 39.0140, stage1_loss_bbox: 0.2835, stage1_loss_iou: 0.5583, stage1_loss_mask: 0.6759, stage2_loss_cls: 0.7696, stage2_pos_acc: 45.3426, stage2_loss_bbox: 0.2215, stage2_loss_iou: 0.4355, stage2_loss_mask: 0.6345, stage3_loss_cls: 0.6966, stage3_pos_acc: 48.4822, stage3_loss_bbox: 0.2027, stage3_loss_iou: 0.4112, stage3_loss_mask: 0.6057, stage4_loss_cls: 0.6609, stage4_pos_acc: 54.6674, stage4_loss_bbox: 0.2028, stage4_loss_iou: 0.3982, stage4_loss_mask: 0.6022, stage5_loss_cls: 0.6526, stage5_pos_acc: 52.2598, stage5_loss_bbox: 0.2034, stage5_loss_iou: 0.3993, stage5_loss_mask: 0.6100, loss: 14.0133\n",
      "2025-07-16 13:45:33,466 - mmdet - INFO - Epoch [13][550/750]\tlr: 2.500e-05, eta: 3:39:16, time: 0.355, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0606, stage0_pos_acc: 33.4366, stage0_loss_bbox: 0.6074, stage0_loss_iou: 1.0797, stage0_loss_mask: 1.0730, stage1_loss_cls: 0.7998, stage1_pos_acc: 44.1598, stage1_loss_bbox: 0.2945, stage1_loss_iou: 0.5551, stage1_loss_mask: 0.6450, stage2_loss_cls: 0.7223, stage2_pos_acc: 50.9743, stage2_loss_bbox: 0.2423, stage2_loss_iou: 0.4537, stage2_loss_mask: 0.6143, stage3_loss_cls: 0.6458, stage3_pos_acc: 51.3356, stage3_loss_bbox: 0.2315, stage3_loss_iou: 0.4191, stage3_loss_mask: 0.5915, stage4_loss_cls: 0.6244, stage4_pos_acc: 51.9422, stage4_loss_bbox: 0.2210, stage4_loss_iou: 0.4055, stage4_loss_mask: 0.5839, stage5_loss_cls: 0.6111, stage5_pos_acc: 55.7675, stage5_loss_bbox: 0.2251, stage5_loss_iou: 0.4125, stage5_loss_mask: 0.5698, loss: 13.6891\n",
      "2025-07-16 13:45:51,070 - mmdet - INFO - Epoch [13][600/750]\tlr: 2.500e-05, eta: 3:39:03, time: 0.352, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0812, stage0_pos_acc: 36.6667, stage0_loss_bbox: 0.6224, stage0_loss_iou: 1.1255, stage0_loss_mask: 1.4115, stage1_loss_cls: 0.8715, stage1_pos_acc: 44.9730, stage1_loss_bbox: 0.3489, stage1_loss_iou: 0.6639, stage1_loss_mask: 0.8591, stage2_loss_cls: 0.8120, stage2_pos_acc: 47.2929, stage2_loss_bbox: 0.2975, stage2_loss_iou: 0.5447, stage2_loss_mask: 0.7637, stage3_loss_cls: 0.7472, stage3_pos_acc: 48.6500, stage3_loss_bbox: 0.2742, stage3_loss_iou: 0.5168, stage3_loss_mask: 0.7399, stage4_loss_cls: 0.7200, stage4_pos_acc: 51.7103, stage4_loss_bbox: 0.2603, stage4_loss_iou: 0.4987, stage4_loss_mask: 0.7158, stage5_loss_cls: 0.6995, stage5_pos_acc: 51.5000, stage5_loss_bbox: 0.2593, stage5_loss_iou: 0.4872, stage5_loss_mask: 0.7092, loss: 16.0299\n",
      "2025-07-16 13:46:08,609 - mmdet - INFO - Epoch [13][650/750]\tlr: 2.500e-05, eta: 3:38:49, time: 0.351, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1173, stage0_pos_acc: 29.2457, stage0_loss_bbox: 0.6455, stage0_loss_iou: 1.1719, stage0_loss_mask: 1.3604, stage1_loss_cls: 0.8036, stage1_pos_acc: 50.4686, stage1_loss_bbox: 0.3523, stage1_loss_iou: 0.6267, stage1_loss_mask: 0.7781, stage2_loss_cls: 0.7567, stage2_pos_acc: 51.4821, stage2_loss_bbox: 0.2851, stage2_loss_iou: 0.4949, stage2_loss_mask: 0.6910, stage3_loss_cls: 0.6905, stage3_pos_acc: 50.1427, stage3_loss_bbox: 0.2473, stage3_loss_iou: 0.4512, stage3_loss_mask: 0.6193, stage4_loss_cls: 0.6451, stage4_pos_acc: 50.0688, stage4_loss_bbox: 0.2398, stage4_loss_iou: 0.4343, stage4_loss_mask: 0.6076, stage5_loss_cls: 0.6410, stage5_pos_acc: 52.0244, stage5_loss_bbox: 0.2307, stage5_loss_iou: 0.4286, stage5_loss_mask: 0.6378, loss: 14.9569\n",
      "2025-07-16 13:46:26,313 - mmdet - INFO - Epoch [13][700/750]\tlr: 2.500e-05, eta: 3:38:36, time: 0.354, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1307, stage0_pos_acc: 36.7573, stage0_loss_bbox: 0.5985, stage0_loss_iou: 1.0875, stage0_loss_mask: 1.1158, stage1_loss_cls: 0.8406, stage1_pos_acc: 41.3097, stage1_loss_bbox: 0.2852, stage1_loss_iou: 0.5505, stage1_loss_mask: 0.6451, stage2_loss_cls: 0.7666, stage2_pos_acc: 47.0807, stage2_loss_bbox: 0.2334, stage2_loss_iou: 0.4279, stage2_loss_mask: 0.6062, stage3_loss_cls: 0.7065, stage3_pos_acc: 49.6231, stage3_loss_bbox: 0.2271, stage3_loss_iou: 0.4016, stage3_loss_mask: 0.5219, stage4_loss_cls: 0.6738, stage4_pos_acc: 49.6922, stage4_loss_bbox: 0.2167, stage4_loss_iou: 0.3888, stage4_loss_mask: 0.5066, stage5_loss_cls: 0.6619, stage5_pos_acc: 51.5202, stage5_loss_bbox: 0.2127, stage5_loss_iou: 0.3861, stage5_loss_mask: 0.4954, loss: 13.6868\n",
      "2025-07-16 13:46:43,926 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:46:43,927 - mmdet - INFO - Epoch [13][750/750]\tlr: 2.500e-05, eta: 3:38:22, time: 0.352, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1624, stage0_pos_acc: 33.4876, stage0_loss_bbox: 0.6244, stage0_loss_iou: 1.1046, stage0_loss_mask: 1.0780, stage1_loss_cls: 0.8481, stage1_pos_acc: 41.8211, stage1_loss_bbox: 0.2901, stage1_loss_iou: 0.5634, stage1_loss_mask: 0.6615, stage2_loss_cls: 0.7871, stage2_pos_acc: 49.8521, stage2_loss_bbox: 0.2288, stage2_loss_iou: 0.4466, stage2_loss_mask: 0.6254, stage3_loss_cls: 0.6892, stage3_pos_acc: 48.1615, stage3_loss_bbox: 0.2206, stage3_loss_iou: 0.4182, stage3_loss_mask: 0.6066, stage4_loss_cls: 0.6451, stage4_pos_acc: 52.7346, stage4_loss_bbox: 0.2216, stage4_loss_iou: 0.4258, stage4_loss_mask: 0.6105, stage5_loss_cls: 0.6449, stage5_pos_acc: 53.4426, stage5_loss_bbox: 0.2158, stage5_loss_iou: 0.4222, stage5_loss_mask: 0.6060, loss: 14.1469\n",
      "2025-07-16 13:46:44,053 - mmdet - INFO - Saving checkpoint at 13 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 13:49:48,911 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.20s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.079\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.479\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.480\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.480\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.290\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.522\n",
      "2025-07-16 13:49:50,638 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.71s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.079\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.043\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.487\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.529\n",
      "2025-07-16 13:49:53,597 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:49:53,597 - mmdet - INFO - Epoch(val) [13][750]\tbbox_mAP: 0.0330, bbox_mAP_50: 0.0600, bbox_mAP_75: 0.0320, bbox_mAP_s: 0.0790, bbox_mAP_m: 0.0090, bbox_mAP_l: 0.0420, bbox_mAP_copypaste: 0.033 0.060 0.032 0.079 0.009 0.042, segm_mAP: 0.0340, segm_mAP_50: 0.0600, segm_mAP_75: 0.0350, segm_mAP_s: 0.0790, segm_mAP_m: 0.0090, segm_mAP_l: 0.0430, segm_mAP_copypaste: 0.034 0.060 0.035 0.079 0.009 0.043\n",
      "2025-07-16 13:50:12,552 - mmdet - INFO - Epoch [14][50/750]\tlr: 2.500e-05, eta: 3:38:14, time: 0.379, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.1640, stage0_pos_acc: 31.6745, stage0_loss_bbox: 0.5409, stage0_loss_iou: 1.1066, stage0_loss_mask: 1.0191, stage1_loss_cls: 0.7869, stage1_pos_acc: 46.5894, stage1_loss_bbox: 0.2377, stage1_loss_iou: 0.5413, stage1_loss_mask: 0.6204, stage2_loss_cls: 0.7375, stage2_pos_acc: 54.4980, stage2_loss_bbox: 0.1829, stage2_loss_iou: 0.4248, stage2_loss_mask: 0.5492, stage3_loss_cls: 0.6484, stage3_pos_acc: 51.1386, stage3_loss_bbox: 0.1622, stage3_loss_iou: 0.3935, stage3_loss_mask: 0.5484, stage4_loss_cls: 0.6133, stage4_pos_acc: 57.7148, stage4_loss_bbox: 0.1535, stage4_loss_iou: 0.3797, stage4_loss_mask: 0.5369, stage5_loss_cls: 0.6033, stage5_pos_acc: 55.4021, stage5_loss_bbox: 0.1475, stage5_loss_iou: 0.3686, stage5_loss_mask: 0.5235, loss: 12.9900\n",
      "2025-07-16 13:50:29,167 - mmdet - INFO - Epoch [14][100/750]\tlr: 2.500e-05, eta: 3:37:56, time: 0.332, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1374, stage0_pos_acc: 30.1822, stage0_loss_bbox: 0.5955, stage0_loss_iou: 1.1448, stage0_loss_mask: 1.1248, stage1_loss_cls: 0.8344, stage1_pos_acc: 46.7575, stage1_loss_bbox: 0.3031, stage1_loss_iou: 0.5612, stage1_loss_mask: 0.7073, stage2_loss_cls: 0.7469, stage2_pos_acc: 52.5853, stage2_loss_bbox: 0.2447, stage2_loss_iou: 0.4529, stage2_loss_mask: 0.6336, stage3_loss_cls: 0.6716, stage3_pos_acc: 51.8275, stage3_loss_bbox: 0.2328, stage3_loss_iou: 0.4256, stage3_loss_mask: 0.6208, stage4_loss_cls: 0.6361, stage4_pos_acc: 53.0534, stage4_loss_bbox: 0.2516, stage4_loss_iou: 0.4113, stage4_loss_mask: 0.5322, stage5_loss_cls: 0.6175, stage5_pos_acc: 55.1908, stage5_loss_bbox: 0.2506, stage5_loss_iou: 0.4092, stage5_loss_mask: 0.5350, loss: 14.0807\n",
      "2025-07-16 13:50:45,701 - mmdet - INFO - Epoch [14][150/750]\tlr: 2.500e-05, eta: 3:37:38, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1320, stage0_pos_acc: 33.1714, stage0_loss_bbox: 0.5895, stage0_loss_iou: 1.0468, stage0_loss_mask: 0.6981, stage1_loss_cls: 0.7870, stage1_pos_acc: 42.3476, stage1_loss_bbox: 0.2569, stage1_loss_iou: 0.4706, stage1_loss_mask: 0.3843, stage2_loss_cls: 0.7035, stage2_pos_acc: 49.5238, stage2_loss_bbox: 0.2027, stage2_loss_iou: 0.3472, stage2_loss_mask: 0.3626, stage3_loss_cls: 0.6000, stage3_pos_acc: 52.6214, stage3_loss_bbox: 0.1834, stage3_loss_iou: 0.3278, stage3_loss_mask: 0.3554, stage4_loss_cls: 0.5706, stage4_pos_acc: 56.2690, stage4_loss_bbox: 0.1759, stage4_loss_iou: 0.3168, stage4_loss_mask: 0.3362, stage5_loss_cls: 0.5524, stage5_pos_acc: 60.4214, stage5_loss_bbox: 0.1798, stage5_loss_iou: 0.3094, stage5_loss_mask: 0.3285, loss: 11.2176\n",
      "2025-07-16 13:51:02,549 - mmdet - INFO - Epoch [14][200/750]\tlr: 2.500e-05, eta: 3:37:22, time: 0.337, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1202, stage0_pos_acc: 34.6333, stage0_loss_bbox: 0.5746, stage0_loss_iou: 1.0886, stage0_loss_mask: 1.0225, stage1_loss_cls: 0.7845, stage1_pos_acc: 51.2556, stage1_loss_bbox: 0.2693, stage1_loss_iou: 0.5367, stage1_loss_mask: 0.6423, stage2_loss_cls: 0.7084, stage2_pos_acc: 52.9556, stage2_loss_bbox: 0.2223, stage2_loss_iou: 0.4232, stage2_loss_mask: 0.5888, stage3_loss_cls: 0.6263, stage3_pos_acc: 56.5778, stage3_loss_bbox: 0.2064, stage3_loss_iou: 0.4033, stage3_loss_mask: 0.5870, stage4_loss_cls: 0.6019, stage4_pos_acc: 55.1222, stage4_loss_bbox: 0.1972, stage4_loss_iou: 0.3843, stage4_loss_mask: 0.5613, stage5_loss_cls: 0.6029, stage5_pos_acc: 59.1778, stage5_loss_bbox: 0.1953, stage5_loss_iou: 0.3780, stage5_loss_mask: 0.5335, loss: 13.2589\n",
      "2025-07-16 13:51:19,660 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:51:19,660 - mmdet - INFO - Epoch [14][250/750]\tlr: 2.500e-05, eta: 3:37:06, time: 0.342, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1311, stage0_pos_acc: 36.4041, stage0_loss_bbox: 0.5766, stage0_loss_iou: 1.0696, stage0_loss_mask: 1.1268, stage1_loss_cls: 0.7928, stage1_pos_acc: 47.9240, stage1_loss_bbox: 0.3310, stage1_loss_iou: 0.5759, stage1_loss_mask: 0.7399, stage2_loss_cls: 0.7222, stage2_pos_acc: 54.6180, stage2_loss_bbox: 0.2603, stage2_loss_iou: 0.4626, stage2_loss_mask: 0.6782, stage3_loss_cls: 0.6635, stage3_pos_acc: 51.3105, stage3_loss_bbox: 0.2317, stage3_loss_iou: 0.4337, stage3_loss_mask: 0.6643, stage4_loss_cls: 0.6278, stage4_pos_acc: 55.6478, stage4_loss_bbox: 0.2242, stage4_loss_iou: 0.4166, stage4_loss_mask: 0.6459, stage5_loss_cls: 0.6148, stage5_pos_acc: 57.3939, stage5_loss_bbox: 0.2207, stage5_loss_iou: 0.4142, stage5_loss_mask: 0.6530, loss: 14.2774\n",
      "2025-07-16 13:51:36,708 - mmdet - INFO - Epoch [14][300/750]\tlr: 2.500e-05, eta: 3:36:50, time: 0.341, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1629, stage0_pos_acc: 29.7346, stage0_loss_bbox: 0.5777, stage0_loss_iou: 1.0996, stage0_loss_mask: 1.0501, stage1_loss_cls: 0.8040, stage1_pos_acc: 47.6346, stage1_loss_bbox: 0.3238, stage1_loss_iou: 0.5851, stage1_loss_mask: 0.6840, stage2_loss_cls: 0.7569, stage2_pos_acc: 51.5052, stage2_loss_bbox: 0.2456, stage2_loss_iou: 0.4656, stage2_loss_mask: 0.6777, stage3_loss_cls: 0.6732, stage3_pos_acc: 55.1903, stage3_loss_bbox: 0.2333, stage3_loss_iou: 0.4298, stage3_loss_mask: 0.6247, stage4_loss_cls: 0.6395, stage4_pos_acc: 55.9656, stage4_loss_bbox: 0.2275, stage4_loss_iou: 0.4196, stage4_loss_mask: 0.6128, stage5_loss_cls: 0.6465, stage5_pos_acc: 58.9751, stage5_loss_bbox: 0.2231, stage5_loss_iou: 0.4086, stage5_loss_mask: 0.5908, loss: 14.1625\n",
      "2025-07-16 13:51:53,255 - mmdet - INFO - Epoch [14][350/750]\tlr: 2.500e-05, eta: 3:36:32, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1198, stage0_pos_acc: 31.1512, stage0_loss_bbox: 0.6222, stage0_loss_iou: 1.1191, stage0_loss_mask: 1.1019, stage1_loss_cls: 0.7894, stage1_pos_acc: 43.8635, stage1_loss_bbox: 0.3322, stage1_loss_iou: 0.6180, stage1_loss_mask: 0.7181, stage2_loss_cls: 0.7344, stage2_pos_acc: 44.9873, stage2_loss_bbox: 0.2654, stage2_loss_iou: 0.4642, stage2_loss_mask: 0.6102, stage3_loss_cls: 0.6615, stage3_pos_acc: 53.5452, stage3_loss_bbox: 0.2526, stage3_loss_iou: 0.4316, stage3_loss_mask: 0.5985, stage4_loss_cls: 0.6233, stage4_pos_acc: 55.0444, stage4_loss_bbox: 0.2449, stage4_loss_iou: 0.4184, stage4_loss_mask: 0.6013, stage5_loss_cls: 0.6051, stage5_pos_acc: 58.6226, stage5_loss_bbox: 0.2505, stage5_loss_iou: 0.4190, stage5_loss_mask: 0.6179, loss: 14.2196\n",
      "2025-07-16 13:52:09,724 - mmdet - INFO - Epoch [14][400/750]\tlr: 2.500e-05, eta: 3:36:14, time: 0.329, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1241, stage0_pos_acc: 33.2321, stage0_loss_bbox: 0.6185, stage0_loss_iou: 1.0920, stage0_loss_mask: 1.0926, stage1_loss_cls: 0.8464, stage1_pos_acc: 45.2453, stage1_loss_bbox: 0.2913, stage1_loss_iou: 0.5782, stage1_loss_mask: 0.7101, stage2_loss_cls: 0.7508, stage2_pos_acc: 54.1180, stage2_loss_bbox: 0.2357, stage2_loss_iou: 0.4681, stage2_loss_mask: 0.6662, stage3_loss_cls: 0.6709, stage3_pos_acc: 53.9854, stage3_loss_bbox: 0.2282, stage3_loss_iou: 0.4387, stage3_loss_mask: 0.6430, stage4_loss_cls: 0.6407, stage4_pos_acc: 58.6706, stage4_loss_bbox: 0.2214, stage4_loss_iou: 0.4213, stage4_loss_mask: 0.6414, stage5_loss_cls: 0.6348, stage5_pos_acc: 59.0509, stage5_loss_bbox: 0.2190, stage5_loss_iou: 0.4197, stage5_loss_mask: 0.6374, loss: 14.2906\n",
      "2025-07-16 13:52:26,367 - mmdet - INFO - Epoch [14][450/750]\tlr: 2.500e-05, eta: 3:35:57, time: 0.333, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.1063, stage0_pos_acc: 36.4403, stage0_loss_bbox: 0.6061, stage0_loss_iou: 1.1128, stage0_loss_mask: 1.0559, stage1_loss_cls: 0.7973, stage1_pos_acc: 49.9696, stage1_loss_bbox: 0.2800, stage1_loss_iou: 0.5202, stage1_loss_mask: 0.5686, stage2_loss_cls: 0.7404, stage2_pos_acc: 50.6506, stage2_loss_bbox: 0.2142, stage2_loss_iou: 0.3993, stage2_loss_mask: 0.4571, stage3_loss_cls: 0.6381, stage3_pos_acc: 53.7139, stage3_loss_bbox: 0.2097, stage3_loss_iou: 0.3892, stage3_loss_mask: 0.4618, stage4_loss_cls: 0.6170, stage4_pos_acc: 56.8022, stage4_loss_bbox: 0.1985, stage4_loss_iou: 0.3651, stage4_loss_mask: 0.4489, stage5_loss_cls: 0.6096, stage5_pos_acc: 58.4059, stage5_loss_bbox: 0.2051, stage5_loss_iou: 0.3639, stage5_loss_mask: 0.4595, loss: 12.8245\n",
      "2025-07-16 13:52:42,689 - mmdet - INFO - Epoch [14][500/750]\tlr: 2.500e-05, eta: 3:35:38, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0988, stage0_pos_acc: 29.6977, stage0_loss_bbox: 0.6272, stage0_loss_iou: 0.9848, stage0_loss_mask: 0.9194, stage1_loss_cls: 0.8305, stage1_pos_acc: 43.1022, stage1_loss_bbox: 0.3238, stage1_loss_iou: 0.4877, stage1_loss_mask: 0.6082, stage2_loss_cls: 0.7585, stage2_pos_acc: 46.5945, stage2_loss_bbox: 0.2718, stage2_loss_iou: 0.4157, stage2_loss_mask: 0.5595, stage3_loss_cls: 0.6722, stage3_pos_acc: 48.9136, stage3_loss_bbox: 0.2607, stage3_loss_iou: 0.4006, stage3_loss_mask: 0.5575, stage4_loss_cls: 0.6485, stage4_pos_acc: 50.4540, stage4_loss_bbox: 0.2616, stage4_loss_iou: 0.3906, stage4_loss_mask: 0.5624, stage5_loss_cls: 0.6449, stage5_pos_acc: 49.8001, stage5_loss_bbox: 0.2635, stage5_loss_iou: 0.3887, stage5_loss_mask: 0.5746, loss: 13.5116\n",
      "2025-07-16 13:52:59,106 - mmdet - INFO - Epoch [14][550/750]\tlr: 2.500e-05, eta: 3:35:20, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0920, stage0_pos_acc: 37.1715, stage0_loss_bbox: 0.5952, stage0_loss_iou: 1.0310, stage0_loss_mask: 1.0065, stage1_loss_cls: 0.8327, stage1_pos_acc: 45.5968, stage1_loss_bbox: 0.2994, stage1_loss_iou: 0.5348, stage1_loss_mask: 0.6266, stage2_loss_cls: 0.7514, stage2_pos_acc: 52.1069, stage2_loss_bbox: 0.2509, stage2_loss_iou: 0.4430, stage2_loss_mask: 0.5979, stage3_loss_cls: 0.6764, stage3_pos_acc: 52.2033, stage3_loss_bbox: 0.2395, stage3_loss_iou: 0.4207, stage3_loss_mask: 0.5746, stage4_loss_cls: 0.6401, stage4_pos_acc: 56.6883, stage4_loss_bbox: 0.2317, stage4_loss_iou: 0.4184, stage4_loss_mask: 0.5619, stage5_loss_cls: 0.6406, stage5_pos_acc: 57.9009, stage5_loss_bbox: 0.2256, stage5_loss_iou: 0.4156, stage5_loss_mask: 0.5847, loss: 13.6912\n",
      "2025-07-16 13:53:15,420 - mmdet - INFO - Epoch [14][600/750]\tlr: 2.500e-05, eta: 3:35:01, time: 0.326, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1032, stage0_pos_acc: 35.9892, stage0_loss_bbox: 0.5672, stage0_loss_iou: 1.0674, stage0_loss_mask: 1.0611, stage1_loss_cls: 0.8262, stage1_pos_acc: 43.3103, stage1_loss_bbox: 0.2935, stage1_loss_iou: 0.5428, stage1_loss_mask: 0.6995, stage2_loss_cls: 0.7358, stage2_pos_acc: 52.1142, stage2_loss_bbox: 0.2553, stage2_loss_iou: 0.4631, stage2_loss_mask: 0.6460, stage3_loss_cls: 0.6734, stage3_pos_acc: 54.2332, stage3_loss_bbox: 0.2496, stage3_loss_iou: 0.4500, stage3_loss_mask: 0.6346, stage4_loss_cls: 0.6427, stage4_pos_acc: 53.0174, stage4_loss_bbox: 0.2321, stage4_loss_iou: 0.4367, stage4_loss_mask: 0.6492, stage5_loss_cls: 0.6311, stage5_pos_acc: 57.7166, stage5_loss_bbox: 0.2407, stage5_loss_iou: 0.4351, stage5_loss_mask: 0.6266, loss: 14.1630\n",
      "2025-07-16 13:53:31,837 - mmdet - INFO - Epoch [14][650/750]\tlr: 2.500e-05, eta: 3:34:43, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0902, stage0_pos_acc: 35.6411, stage0_loss_bbox: 0.6145, stage0_loss_iou: 1.0578, stage0_loss_mask: 1.2057, stage1_loss_cls: 0.8217, stage1_pos_acc: 46.2371, stage1_loss_bbox: 0.3276, stage1_loss_iou: 0.5915, stage1_loss_mask: 0.8136, stage2_loss_cls: 0.7534, stage2_pos_acc: 52.1313, stage2_loss_bbox: 0.2813, stage2_loss_iou: 0.5093, stage2_loss_mask: 0.7118, stage3_loss_cls: 0.6905, stage3_pos_acc: 51.8082, stage3_loss_bbox: 0.2631, stage3_loss_iou: 0.4793, stage3_loss_mask: 0.6824, stage4_loss_cls: 0.6632, stage4_pos_acc: 53.1415, stage4_loss_bbox: 0.2486, stage4_loss_iou: 0.4639, stage4_loss_mask: 0.6991, stage5_loss_cls: 0.6634, stage5_pos_acc: 51.6029, stage5_loss_bbox: 0.2450, stage5_loss_iou: 0.4586, stage5_loss_mask: 0.6945, loss: 15.0300\n",
      "2025-07-16 13:53:48,518 - mmdet - INFO - Epoch [14][700/750]\tlr: 2.500e-05, eta: 3:34:26, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1070, stage0_pos_acc: 32.3985, stage0_loss_bbox: 0.5887, stage0_loss_iou: 1.0528, stage0_loss_mask: 0.9911, stage1_loss_cls: 0.7975, stage1_pos_acc: 43.5317, stage1_loss_bbox: 0.3000, stage1_loss_iou: 0.5110, stage1_loss_mask: 0.6207, stage2_loss_cls: 0.7148, stage2_pos_acc: 53.2178, stage2_loss_bbox: 0.2564, stage2_loss_iou: 0.4493, stage2_loss_mask: 0.6079, stage3_loss_cls: 0.6646, stage3_pos_acc: 54.2831, stage3_loss_bbox: 0.2367, stage3_loss_iou: 0.4212, stage3_loss_mask: 0.5731, stage4_loss_cls: 0.6070, stage4_pos_acc: 56.6093, stage4_loss_bbox: 0.2371, stage4_loss_iou: 0.4178, stage4_loss_mask: 0.5644, stage5_loss_cls: 0.5947, stage5_pos_acc: 60.4584, stage5_loss_bbox: 0.2323, stage5_loss_iou: 0.4200, stage5_loss_mask: 0.5600, loss: 13.5262\n",
      "2025-07-16 13:54:05,055 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:54:05,055 - mmdet - INFO - Epoch [14][750/750]\tlr: 2.500e-05, eta: 3:34:08, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1205, stage0_pos_acc: 33.1929, stage0_loss_bbox: 0.5399, stage0_loss_iou: 0.9965, stage0_loss_mask: 0.9400, stage1_loss_cls: 0.8049, stage1_pos_acc: 44.0393, stage1_loss_bbox: 0.2734, stage1_loss_iou: 0.4989, stage1_loss_mask: 0.5449, stage2_loss_cls: 0.7470, stage2_pos_acc: 44.3232, stage2_loss_bbox: 0.2215, stage2_loss_iou: 0.4102, stage2_loss_mask: 0.5356, stage3_loss_cls: 0.6730, stage3_pos_acc: 47.7125, stage3_loss_bbox: 0.2226, stage3_loss_iou: 0.4087, stage3_loss_mask: 0.5180, stage4_loss_cls: 0.6418, stage4_pos_acc: 52.7958, stage4_loss_bbox: 0.2035, stage4_loss_iou: 0.3896, stage4_loss_mask: 0.5067, stage5_loss_cls: 0.6374, stage5_pos_acc: 49.5661, stage5_loss_bbox: 0.2055, stage5_loss_iou: 0.3858, stage5_loss_mask: 0.5017, loss: 12.9279\n",
      "2025-07-16 13:54:05,166 - mmdet - INFO - Saving checkpoint at 14 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 13:57:09,303 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.311\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.510\n",
      "2025-07-16 13:57:11,091 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.80s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.489\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.489\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.338\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.304\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.529\n",
      "2025-07-16 13:57:14,173 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 13:57:14,173 - mmdet - INFO - Epoch(val) [14][750]\tbbox_mAP: 0.0340, bbox_mAP_50: 0.0650, bbox_mAP_75: 0.0320, bbox_mAP_s: 0.0490, bbox_mAP_m: 0.0210, bbox_mAP_l: 0.0410, bbox_mAP_copypaste: 0.034 0.065 0.032 0.049 0.021 0.041, segm_mAP: 0.0350, segm_mAP_50: 0.0650, segm_mAP_75: 0.0340, segm_mAP_s: 0.0510, segm_mAP_m: 0.0210, segm_mAP_l: 0.0420, segm_mAP_copypaste: 0.035 0.065 0.034 0.051 0.021 0.042\n",
      "2025-07-16 13:57:33,400 - mmdet - INFO - Epoch [15][50/750]\tlr: 2.500e-05, eta: 3:34:00, time: 0.384, data_time: 0.056, memory: 11264, stage0_loss_cls: 1.0739, stage0_pos_acc: 37.5991, stage0_loss_bbox: 0.5776, stage0_loss_iou: 1.0249, stage0_loss_mask: 0.9409, stage1_loss_cls: 0.8196, stage1_pos_acc: 44.5326, stage1_loss_bbox: 0.2892, stage1_loss_iou: 0.5434, stage1_loss_mask: 0.5991, stage2_loss_cls: 0.7377, stage2_pos_acc: 49.6377, stage2_loss_bbox: 0.2613, stage2_loss_iou: 0.4344, stage2_loss_mask: 0.6081, stage3_loss_cls: 0.6878, stage3_pos_acc: 50.3814, stage3_loss_bbox: 0.2336, stage3_loss_iou: 0.4035, stage3_loss_mask: 0.5650, stage4_loss_cls: 0.6430, stage4_pos_acc: 55.1722, stage4_loss_bbox: 0.2424, stage4_loss_iou: 0.4007, stage4_loss_mask: 0.5469, stage5_loss_cls: 0.6465, stage5_pos_acc: 55.8166, stage5_loss_bbox: 0.2405, stage5_loss_iou: 0.3940, stage5_loss_mask: 0.5390, loss: 13.4530\n",
      "2025-07-16 13:57:50,055 - mmdet - INFO - Epoch [15][100/750]\tlr: 2.500e-05, eta: 3:33:43, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1004, stage0_pos_acc: 34.2333, stage0_loss_bbox: 0.5567, stage0_loss_iou: 1.0848, stage0_loss_mask: 1.0004, stage1_loss_cls: 0.7812, stage1_pos_acc: 51.3484, stage1_loss_bbox: 0.2754, stage1_loss_iou: 0.5546, stage1_loss_mask: 0.7064, stage2_loss_cls: 0.7013, stage2_pos_acc: 50.9214, stage2_loss_bbox: 0.2394, stage2_loss_iou: 0.4525, stage2_loss_mask: 0.6643, stage3_loss_cls: 0.6140, stage3_pos_acc: 57.8714, stage3_loss_bbox: 0.2427, stage3_loss_iou: 0.4302, stage3_loss_mask: 0.6590, stage4_loss_cls: 0.5552, stage4_pos_acc: 61.4048, stage4_loss_bbox: 0.2289, stage4_loss_iou: 0.4282, stage4_loss_mask: 0.6544, stage5_loss_cls: 0.5540, stage5_pos_acc: 58.2119, stage5_loss_bbox: 0.2274, stage5_loss_iou: 0.4259, stage5_loss_mask: 0.6378, loss: 13.7751\n",
      "2025-07-16 13:58:06,733 - mmdet - INFO - Epoch [15][150/750]\tlr: 2.500e-05, eta: 3:33:26, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1380, stage0_pos_acc: 35.1175, stage0_loss_bbox: 0.5685, stage0_loss_iou: 1.1103, stage0_loss_mask: 0.9794, stage1_loss_cls: 0.7878, stage1_pos_acc: 49.6788, stage1_loss_bbox: 0.2566, stage1_loss_iou: 0.5501, stage1_loss_mask: 0.6000, stage2_loss_cls: 0.7154, stage2_pos_acc: 54.7185, stage2_loss_bbox: 0.1981, stage2_loss_iou: 0.4147, stage2_loss_mask: 0.5530, stage3_loss_cls: 0.6423, stage3_pos_acc: 57.0250, stage3_loss_bbox: 0.1812, stage3_loss_iou: 0.3848, stage3_loss_mask: 0.5142, stage4_loss_cls: 0.5708, stage4_pos_acc: 64.9243, stage4_loss_bbox: 0.1760, stage4_loss_iou: 0.3760, stage4_loss_mask: 0.5155, stage5_loss_cls: 0.5710, stage5_pos_acc: 62.6309, stage5_loss_bbox: 0.1757, stage5_loss_iou: 0.3688, stage5_loss_mask: 0.5276, loss: 12.8759\n",
      "2025-07-16 13:58:24,146 - mmdet - INFO - Epoch [15][200/750]\tlr: 2.500e-05, eta: 3:33:11, time: 0.348, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1047, stage0_pos_acc: 29.9148, stage0_loss_bbox: 0.5610, stage0_loss_iou: 1.0071, stage0_loss_mask: 1.0246, stage1_loss_cls: 0.7921, stage1_pos_acc: 45.1183, stage1_loss_bbox: 0.3028, stage1_loss_iou: 0.5323, stage1_loss_mask: 0.7070, stage2_loss_cls: 0.7328, stage2_pos_acc: 54.4829, stage2_loss_bbox: 0.2378, stage2_loss_iou: 0.4475, stage2_loss_mask: 0.6523, stage3_loss_cls: 0.6410, stage3_pos_acc: 52.5078, stage3_loss_bbox: 0.2173, stage3_loss_iou: 0.4127, stage3_loss_mask: 0.6318, stage4_loss_cls: 0.6218, stage4_pos_acc: 58.7800, stage4_loss_bbox: 0.2050, stage4_loss_iou: 0.3922, stage4_loss_mask: 0.5759, stage5_loss_cls: 0.6076, stage5_pos_acc: 60.6092, stage5_loss_bbox: 0.2039, stage5_loss_iou: 0.3920, stage5_loss_mask: 0.5872, loss: 13.5903\n",
      "2025-07-16 13:58:41,405 - mmdet - INFO - Epoch [15][250/750]\tlr: 2.500e-05, eta: 3:32:56, time: 0.345, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1112, stage0_pos_acc: 28.7311, stage0_loss_bbox: 0.5652, stage0_loss_iou: 1.0682, stage0_loss_mask: 1.0025, stage1_loss_cls: 0.7895, stage1_pos_acc: 49.4287, stage1_loss_bbox: 0.2736, stage1_loss_iou: 0.5385, stage1_loss_mask: 0.6490, stage2_loss_cls: 0.7350, stage2_pos_acc: 51.9172, stage2_loss_bbox: 0.2262, stage2_loss_iou: 0.4362, stage2_loss_mask: 0.5945, stage3_loss_cls: 0.6494, stage3_pos_acc: 53.4783, stage3_loss_bbox: 0.2096, stage3_loss_iou: 0.4195, stage3_loss_mask: 0.5668, stage4_loss_cls: 0.6025, stage4_pos_acc: 55.9196, stage4_loss_bbox: 0.2077, stage4_loss_iou: 0.4069, stage4_loss_mask: 0.5478, stage5_loss_cls: 0.5918, stage5_pos_acc: 58.0928, stage5_loss_bbox: 0.1967, stage5_loss_iou: 0.3980, stage5_loss_mask: 0.5427, loss: 13.3290\n",
      "2025-07-16 13:58:58,169 - mmdet - INFO - Epoch [15][300/750]\tlr: 2.500e-05, eta: 3:32:39, time: 0.335, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1162, stage0_pos_acc: 38.4563, stage0_loss_bbox: 0.5439, stage0_loss_iou: 1.0769, stage0_loss_mask: 1.0564, stage1_loss_cls: 0.8156, stage1_pos_acc: 45.1865, stage1_loss_bbox: 0.2603, stage1_loss_iou: 0.5405, stage1_loss_mask: 0.6197, stage2_loss_cls: 0.7127, stage2_pos_acc: 48.6698, stage2_loss_bbox: 0.2202, stage2_loss_iou: 0.4375, stage2_loss_mask: 0.5588, stage3_loss_cls: 0.6297, stage3_pos_acc: 55.6944, stage3_loss_bbox: 0.2094, stage3_loss_iou: 0.4084, stage3_loss_mask: 0.5132, stage4_loss_cls: 0.5933, stage4_pos_acc: 60.4579, stage4_loss_bbox: 0.2035, stage4_loss_iou: 0.4000, stage4_loss_mask: 0.5094, stage5_loss_cls: 0.5844, stage5_pos_acc: 64.1468, stage5_loss_bbox: 0.2074, stage5_loss_iou: 0.3959, stage5_loss_mask: 0.5021, loss: 13.1155\n",
      "2025-07-16 13:59:14,909 - mmdet - INFO - Epoch [15][350/750]\tlr: 2.500e-05, eta: 3:32:22, time: 0.335, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1223, stage0_pos_acc: 31.3505, stage0_loss_bbox: 0.5194, stage0_loss_iou: 1.0257, stage0_loss_mask: 0.7534, stage1_loss_cls: 0.7563, stage1_pos_acc: 50.0101, stage1_loss_bbox: 0.2243, stage1_loss_iou: 0.4485, stage1_loss_mask: 0.3911, stage2_loss_cls: 0.6698, stage2_pos_acc: 54.1854, stage2_loss_bbox: 0.1946, stage2_loss_iou: 0.3534, stage2_loss_mask: 0.3373, stage3_loss_cls: 0.5709, stage3_pos_acc: 55.6783, stage3_loss_bbox: 0.1972, stage3_loss_iou: 0.3500, stage3_loss_mask: 0.3345, stage4_loss_cls: 0.5387, stage4_pos_acc: 63.7949, stage4_loss_bbox: 0.1830, stage4_loss_iou: 0.3266, stage4_loss_mask: 0.2957, stage5_loss_cls: 0.5145, stage5_pos_acc: 65.7465, stage5_loss_bbox: 0.1893, stage5_loss_iou: 0.3290, stage5_loss_mask: 0.3108, loss: 10.9364\n",
      "2025-07-16 13:59:31,927 - mmdet - INFO - Epoch [15][400/750]\tlr: 2.500e-05, eta: 3:32:06, time: 0.340, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1270, stage0_pos_acc: 32.8569, stage0_loss_bbox: 0.5870, stage0_loss_iou: 1.0867, stage0_loss_mask: 1.1483, stage1_loss_cls: 0.7976, stage1_pos_acc: 49.3217, stage1_loss_bbox: 0.2782, stage1_loss_iou: 0.5684, stage1_loss_mask: 0.7699, stage2_loss_cls: 0.7260, stage2_pos_acc: 55.0562, stage2_loss_bbox: 0.2277, stage2_loss_iou: 0.4574, stage2_loss_mask: 0.7274, stage3_loss_cls: 0.6393, stage3_pos_acc: 57.5945, stage3_loss_bbox: 0.2239, stage3_loss_iou: 0.4535, stage3_loss_mask: 0.7228, stage4_loss_cls: 0.6070, stage4_pos_acc: 59.6167, stage4_loss_bbox: 0.2132, stage4_loss_iou: 0.4423, stage4_loss_mask: 0.7123, stage5_loss_cls: 0.5921, stage5_pos_acc: 59.4618, stage5_loss_bbox: 0.2119, stage5_loss_iou: 0.4393, stage5_loss_mask: 0.7070, loss: 14.4664\n",
      "2025-07-16 13:59:48,793 - mmdet - INFO - Epoch [15][450/750]\tlr: 2.500e-05, eta: 3:31:49, time: 0.337, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0835, stage0_pos_acc: 34.1206, stage0_loss_bbox: 0.5672, stage0_loss_iou: 0.9873, stage0_loss_mask: 0.8782, stage1_loss_cls: 0.8001, stage1_pos_acc: 51.0574, stage1_loss_bbox: 0.3000, stage1_loss_iou: 0.5041, stage1_loss_mask: 0.5003, stage2_loss_cls: 0.7323, stage2_pos_acc: 52.3208, stage2_loss_bbox: 0.2444, stage2_loss_iou: 0.4036, stage2_loss_mask: 0.4673, stage3_loss_cls: 0.6690, stage3_pos_acc: 55.0881, stage3_loss_bbox: 0.2281, stage3_loss_iou: 0.3812, stage3_loss_mask: 0.4476, stage4_loss_cls: 0.6388, stage4_pos_acc: 58.5667, stage4_loss_bbox: 0.2254, stage4_loss_iou: 0.3696, stage4_loss_mask: 0.4445, stage5_loss_cls: 0.6301, stage5_pos_acc: 57.3177, stage5_loss_bbox: 0.2209, stage5_loss_iou: 0.3660, stage5_loss_mask: 0.4563, loss: 12.5458\n",
      "2025-07-16 14:00:05,632 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:00:05,632 - mmdet - INFO - Epoch [15][500/750]\tlr: 2.500e-05, eta: 3:31:32, time: 0.337, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1071, stage0_pos_acc: 29.8921, stage0_loss_bbox: 0.6198, stage0_loss_iou: 1.1775, stage0_loss_mask: 1.2258, stage1_loss_cls: 0.7804, stage1_pos_acc: 48.5992, stage1_loss_bbox: 0.3084, stage1_loss_iou: 0.6245, stage1_loss_mask: 0.7771, stage2_loss_cls: 0.6943, stage2_pos_acc: 48.9365, stage2_loss_bbox: 0.2448, stage2_loss_iou: 0.5159, stage2_loss_mask: 0.7492, stage3_loss_cls: 0.6554, stage3_pos_acc: 52.5869, stage3_loss_bbox: 0.2236, stage3_loss_iou: 0.4703, stage3_loss_mask: 0.7122, stage4_loss_cls: 0.6008, stage4_pos_acc: 55.8325, stage4_loss_bbox: 0.2181, stage4_loss_iou: 0.4533, stage4_loss_mask: 0.6897, stage5_loss_cls: 0.5970, stage5_pos_acc: 59.0885, stage5_loss_bbox: 0.2099, stage5_loss_iou: 0.4392, stage5_loss_mask: 0.6632, loss: 14.7576\n",
      "2025-07-16 14:00:22,814 - mmdet - INFO - Epoch [15][550/750]\tlr: 2.500e-05, eta: 3:31:17, time: 0.344, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0804, stage0_pos_acc: 37.2294, stage0_loss_bbox: 0.5262, stage0_loss_iou: 1.0274, stage0_loss_mask: 0.9740, stage1_loss_cls: 0.7968, stage1_pos_acc: 48.3160, stage1_loss_bbox: 0.2602, stage1_loss_iou: 0.5196, stage1_loss_mask: 0.6075, stage2_loss_cls: 0.7193, stage2_pos_acc: 55.6004, stage2_loss_bbox: 0.2217, stage2_loss_iou: 0.4211, stage2_loss_mask: 0.5134, stage3_loss_cls: 0.6520, stage3_pos_acc: 60.6720, stage3_loss_bbox: 0.2067, stage3_loss_iou: 0.3828, stage3_loss_mask: 0.4729, stage4_loss_cls: 0.6320, stage4_pos_acc: 58.8428, stage4_loss_bbox: 0.1975, stage4_loss_iou: 0.3612, stage4_loss_mask: 0.4696, stage5_loss_cls: 0.6181, stage5_pos_acc: 58.9229, stage5_loss_bbox: 0.1950, stage5_loss_iou: 0.3569, stage5_loss_mask: 0.4618, loss: 12.6743\n",
      "2025-07-16 14:00:39,646 - mmdet - INFO - Epoch [15][600/750]\tlr: 2.500e-05, eta: 3:31:00, time: 0.337, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1300, stage0_pos_acc: 30.3569, stage0_loss_bbox: 0.5621, stage0_loss_iou: 1.0628, stage0_loss_mask: 0.9202, stage1_loss_cls: 0.7888, stage1_pos_acc: 48.0090, stage1_loss_bbox: 0.2493, stage1_loss_iou: 0.5147, stage1_loss_mask: 0.6337, stage2_loss_cls: 0.7122, stage2_pos_acc: 51.2196, stage2_loss_bbox: 0.2113, stage2_loss_iou: 0.4098, stage2_loss_mask: 0.5836, stage3_loss_cls: 0.6399, stage3_pos_acc: 55.4885, stage3_loss_bbox: 0.1927, stage3_loss_iou: 0.3877, stage3_loss_mask: 0.5569, stage4_loss_cls: 0.6173, stage4_pos_acc: 54.4584, stage4_loss_bbox: 0.1921, stage4_loss_iou: 0.3769, stage4_loss_mask: 0.5682, stage5_loss_cls: 0.6021, stage5_pos_acc: 60.9590, stage5_loss_bbox: 0.1967, stage5_loss_iou: 0.3784, stage5_loss_mask: 0.5610, loss: 13.0485\n",
      "2025-07-16 14:00:56,853 - mmdet - INFO - Epoch [15][650/750]\tlr: 2.500e-05, eta: 3:30:44, time: 0.344, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0936, stage0_pos_acc: 34.4500, stage0_loss_bbox: 0.5946, stage0_loss_iou: 0.9668, stage0_loss_mask: 0.8102, stage1_loss_cls: 0.8056, stage1_pos_acc: 42.0488, stage1_loss_bbox: 0.3042, stage1_loss_iou: 0.4809, stage1_loss_mask: 0.5909, stage2_loss_cls: 0.7237, stage2_pos_acc: 47.0119, stage2_loss_bbox: 0.2576, stage2_loss_iou: 0.3865, stage2_loss_mask: 0.5520, stage3_loss_cls: 0.6436, stage3_pos_acc: 48.3750, stage3_loss_bbox: 0.2479, stage3_loss_iou: 0.3713, stage3_loss_mask: 0.5541, stage4_loss_cls: 0.6147, stage4_pos_acc: 53.8202, stage4_loss_bbox: 0.2421, stage4_loss_iou: 0.3595, stage4_loss_mask: 0.5438, stage5_loss_cls: 0.5958, stage5_pos_acc: 54.3440, stage5_loss_bbox: 0.2482, stage5_loss_iou: 0.3644, stage5_loss_mask: 0.5529, loss: 12.9048\n",
      "2025-07-16 14:01:14,267 - mmdet - INFO - Epoch [15][700/750]\tlr: 2.500e-05, eta: 3:30:30, time: 0.348, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1050, stage0_pos_acc: 30.9942, stage0_loss_bbox: 0.5977, stage0_loss_iou: 1.0812, stage0_loss_mask: 1.3236, stage1_loss_cls: 0.7846, stage1_pos_acc: 49.6567, stage1_loss_bbox: 0.3406, stage1_loss_iou: 0.6379, stage1_loss_mask: 0.9259, stage2_loss_cls: 0.7182, stage2_pos_acc: 51.2153, stage2_loss_bbox: 0.2820, stage2_loss_iou: 0.5344, stage2_loss_mask: 0.8341, stage3_loss_cls: 0.6401, stage3_pos_acc: 53.6123, stage3_loss_bbox: 0.2787, stage3_loss_iou: 0.5079, stage3_loss_mask: 0.8441, stage4_loss_cls: 0.6064, stage4_pos_acc: 54.9325, stage4_loss_bbox: 0.2668, stage4_loss_iou: 0.4828, stage4_loss_mask: 0.8038, stage5_loss_cls: 0.5978, stage5_pos_acc: 61.1550, stage5_loss_bbox: 0.2627, stage5_loss_iou: 0.4770, stage5_loss_mask: 0.7971, loss: 15.7304\n",
      "2025-07-16 14:01:31,570 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:01:31,570 - mmdet - INFO - Epoch [15][750/750]\tlr: 2.500e-05, eta: 3:30:14, time: 0.346, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0696, stage0_pos_acc: 31.1912, stage0_loss_bbox: 0.5288, stage0_loss_iou: 0.9902, stage0_loss_mask: 0.8322, stage1_loss_cls: 0.7516, stage1_pos_acc: 45.6832, stage1_loss_bbox: 0.2679, stage1_loss_iou: 0.4733, stage1_loss_mask: 0.4617, stage2_loss_cls: 0.6861, stage2_pos_acc: 46.6227, stage2_loss_bbox: 0.2082, stage2_loss_iou: 0.3586, stage2_loss_mask: 0.4144, stage3_loss_cls: 0.6322, stage3_pos_acc: 47.4251, stage3_loss_bbox: 0.1970, stage3_loss_iou: 0.3279, stage3_loss_mask: 0.3983, stage4_loss_cls: 0.5838, stage4_pos_acc: 54.0522, stage4_loss_bbox: 0.1922, stage4_loss_iou: 0.3241, stage4_loss_mask: 0.3892, stage5_loss_cls: 0.5743, stage5_pos_acc: 53.6608, stage5_loss_bbox: 0.1953, stage5_loss_iou: 0.3207, stage5_loss_mask: 0.3772, loss: 11.5548\n",
      "2025-07-16 14:01:31,698 - mmdet - INFO - Saving checkpoint at 15 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 87s, ETA:     0s2025-07-16 14:04:36,664 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.24s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.042\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.073\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.048\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.443\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.201\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.504\n",
      "2025-07-16 14:04:38,560 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.42s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.93s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.074\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.049\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.458\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.461\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.461\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.210\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.520\n",
      "2025-07-16 14:04:41,638 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:04:41,638 - mmdet - INFO - Epoch(val) [15][750]\tbbox_mAP: 0.0420, bbox_mAP_50: 0.0730, bbox_mAP_75: 0.0410, bbox_mAP_s: 0.0060, bbox_mAP_m: 0.0280, bbox_mAP_l: 0.0480, bbox_mAP_copypaste: 0.042 0.073 0.041 0.006 0.028 0.048, segm_mAP: 0.0430, segm_mAP_50: 0.0740, segm_mAP_75: 0.0450, segm_mAP_s: 0.0120, segm_mAP_m: 0.0280, segm_mAP_l: 0.0490, segm_mAP_copypaste: 0.043 0.074 0.045 0.012 0.028 0.049\n",
      "2025-07-16 14:05:01,122 - mmdet - INFO - Epoch [16][50/750]\tlr: 2.500e-05, eta: 3:30:06, time: 0.389, data_time: 0.053, memory: 11264, stage0_loss_cls: 1.0902, stage0_pos_acc: 35.3593, stage0_loss_bbox: 0.5836, stage0_loss_iou: 1.0971, stage0_loss_mask: 1.2162, stage1_loss_cls: 0.7959, stage1_pos_acc: 43.9070, stage1_loss_bbox: 0.2856, stage1_loss_iou: 0.5804, stage1_loss_mask: 0.8421, stage2_loss_cls: 0.7197, stage2_pos_acc: 51.0456, stage2_loss_bbox: 0.2421, stage2_loss_iou: 0.4816, stage2_loss_mask: 0.7610, stage3_loss_cls: 0.6467, stage3_pos_acc: 55.2504, stage3_loss_bbox: 0.2303, stage3_loss_iou: 0.4543, stage3_loss_mask: 0.7558, stage4_loss_cls: 0.6006, stage4_pos_acc: 58.8694, stage4_loss_bbox: 0.2232, stage4_loss_iou: 0.4450, stage4_loss_mask: 0.7577, stage5_loss_cls: 0.5717, stage5_pos_acc: 62.6694, stage5_loss_bbox: 0.2305, stage5_loss_iou: 0.4438, stage5_loss_mask: 0.7603, loss: 14.8153\n",
      "2025-07-16 14:05:18,550 - mmdet - INFO - Epoch [16][100/750]\tlr: 2.500e-05, eta: 3:29:52, time: 0.349, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0877, stage0_pos_acc: 32.1123, stage0_loss_bbox: 0.5613, stage0_loss_iou: 1.0780, stage0_loss_mask: 1.0638, stage1_loss_cls: 0.7572, stage1_pos_acc: 55.6508, stage1_loss_bbox: 0.2902, stage1_loss_iou: 0.5323, stage1_loss_mask: 0.6119, stage2_loss_cls: 0.6557, stage2_pos_acc: 63.2240, stage2_loss_bbox: 0.2337, stage2_loss_iou: 0.4193, stage2_loss_mask: 0.5965, stage3_loss_cls: 0.5937, stage3_pos_acc: 61.8933, stage3_loss_bbox: 0.2293, stage3_loss_iou: 0.3996, stage3_loss_mask: 0.5881, stage4_loss_cls: 0.5513, stage4_pos_acc: 63.7142, stage4_loss_bbox: 0.2152, stage4_loss_iou: 0.3837, stage4_loss_mask: 0.5762, stage5_loss_cls: 0.5414, stage5_pos_acc: 66.7365, stage5_loss_bbox: 0.2195, stage5_loss_iou: 0.3787, stage5_loss_mask: 0.5679, loss: 13.1319\n",
      "2025-07-16 14:05:35,970 - mmdet - INFO - Epoch [16][150/750]\tlr: 2.500e-05, eta: 3:29:37, time: 0.348, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0744, stage0_pos_acc: 35.8437, stage0_loss_bbox: 0.5348, stage0_loss_iou: 0.9872, stage0_loss_mask: 0.9742, stage1_loss_cls: 0.7904, stage1_pos_acc: 47.9063, stage1_loss_bbox: 0.2678, stage1_loss_iou: 0.5073, stage1_loss_mask: 0.6005, stage2_loss_cls: 0.7062, stage2_pos_acc: 55.3322, stage2_loss_bbox: 0.2226, stage2_loss_iou: 0.4051, stage2_loss_mask: 0.5139, stage3_loss_cls: 0.6370, stage3_pos_acc: 56.0936, stage3_loss_bbox: 0.2076, stage3_loss_iou: 0.3823, stage3_loss_mask: 0.4979, stage4_loss_cls: 0.5913, stage4_pos_acc: 59.3244, stage4_loss_bbox: 0.2044, stage4_loss_iou: 0.3667, stage4_loss_mask: 0.4948, stage5_loss_cls: 0.5726, stage5_pos_acc: 66.2642, stage5_loss_bbox: 0.2007, stage5_loss_iou: 0.3622, stage5_loss_mask: 0.4837, loss: 12.5855\n",
      "2025-07-16 14:05:53,027 - mmdet - INFO - Epoch [16][200/750]\tlr: 2.500e-05, eta: 3:29:21, time: 0.341, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0803, stage0_pos_acc: 31.6736, stage0_loss_bbox: 0.6401, stage0_loss_iou: 1.0903, stage0_loss_mask: 1.0287, stage1_loss_cls: 0.7667, stage1_pos_acc: 44.4677, stage1_loss_bbox: 0.2776, stage1_loss_iou: 0.5461, stage1_loss_mask: 0.6670, stage2_loss_cls: 0.6977, stage2_pos_acc: 51.9634, stage2_loss_bbox: 0.2119, stage2_loss_iou: 0.4159, stage2_loss_mask: 0.5609, stage3_loss_cls: 0.6275, stage3_pos_acc: 56.6788, stage3_loss_bbox: 0.1891, stage3_loss_iou: 0.3875, stage3_loss_mask: 0.5149, stage4_loss_cls: 0.5923, stage4_pos_acc: 58.1192, stage4_loss_bbox: 0.1821, stage4_loss_iou: 0.3761, stage4_loss_mask: 0.4862, stage5_loss_cls: 0.5820, stage5_pos_acc: 58.5344, stage5_loss_bbox: 0.1823, stage5_loss_iou: 0.3674, stage5_loss_mask: 0.4911, loss: 12.9616\n",
      "2025-07-16 14:06:10,762 - mmdet - INFO - Epoch [16][250/750]\tlr: 2.500e-05, eta: 3:29:07, time: 0.355, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0623, stage0_pos_acc: 33.7916, stage0_loss_bbox: 0.6418, stage0_loss_iou: 1.1279, stage0_loss_mask: 1.2934, stage1_loss_cls: 0.7845, stage1_pos_acc: 50.7993, stage1_loss_bbox: 0.3449, stage1_loss_iou: 0.6363, stage1_loss_mask: 0.9797, stage2_loss_cls: 0.7162, stage2_pos_acc: 55.7410, stage2_loss_bbox: 0.3169, stage2_loss_iou: 0.5067, stage2_loss_mask: 0.7918, stage3_loss_cls: 0.6406, stage3_pos_acc: 57.4382, stage3_loss_bbox: 0.2810, stage3_loss_iou: 0.4779, stage3_loss_mask: 0.7676, stage4_loss_cls: 0.5865, stage4_pos_acc: 59.4607, stage4_loss_bbox: 0.2704, stage4_loss_iou: 0.4660, stage4_loss_mask: 0.7576, stage5_loss_cls: 0.5736, stage5_pos_acc: 62.3996, stage5_loss_bbox: 0.2667, stage5_loss_iou: 0.4619, stage5_loss_mask: 0.7036, loss: 15.4559\n",
      "2025-07-16 14:06:27,972 - mmdet - INFO - Epoch [16][300/750]\tlr: 2.500e-05, eta: 3:28:51, time: 0.344, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0821, stage0_pos_acc: 29.1108, stage0_loss_bbox: 0.5604, stage0_loss_iou: 1.0056, stage0_loss_mask: 0.8784, stage1_loss_cls: 0.7897, stage1_pos_acc: 47.4981, stage1_loss_bbox: 0.3019, stage1_loss_iou: 0.5189, stage1_loss_mask: 0.6128, stage2_loss_cls: 0.7121, stage2_pos_acc: 51.7957, stage2_loss_bbox: 0.2410, stage2_loss_iou: 0.4211, stage2_loss_mask: 0.6175, stage3_loss_cls: 0.6382, stage3_pos_acc: 52.7452, stage3_loss_bbox: 0.2356, stage3_loss_iou: 0.4053, stage3_loss_mask: 0.5794, stage4_loss_cls: 0.6036, stage4_pos_acc: 56.5771, stage4_loss_bbox: 0.2253, stage4_loss_iou: 0.3918, stage4_loss_mask: 0.5697, stage5_loss_cls: 0.5883, stage5_pos_acc: 60.1823, stage5_loss_bbox: 0.2243, stage5_loss_iou: 0.3898, stage5_loss_mask: 0.5682, loss: 13.1612\n",
      "2025-07-16 14:06:45,095 - mmdet - INFO - Epoch [16][350/750]\tlr: 2.500e-05, eta: 3:28:35, time: 0.342, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0921, stage0_pos_acc: 28.7136, stage0_loss_bbox: 0.5827, stage0_loss_iou: 1.0759, stage0_loss_mask: 0.9630, stage1_loss_cls: 0.7461, stage1_pos_acc: 48.2385, stage1_loss_bbox: 0.2588, stage1_loss_iou: 0.5048, stage1_loss_mask: 0.5279, stage2_loss_cls: 0.6531, stage2_pos_acc: 55.4465, stage2_loss_bbox: 0.2114, stage2_loss_iou: 0.3942, stage2_loss_mask: 0.5053, stage3_loss_cls: 0.5675, stage3_pos_acc: 58.5108, stage3_loss_bbox: 0.2021, stage3_loss_iou: 0.3774, stage3_loss_mask: 0.4932, stage4_loss_cls: 0.5194, stage4_pos_acc: 63.8041, stage4_loss_bbox: 0.1908, stage4_loss_iou: 0.3673, stage4_loss_mask: 0.4792, stage5_loss_cls: 0.5230, stage5_pos_acc: 63.1541, stage5_loss_bbox: 0.1860, stage5_loss_iou: 0.3621, stage5_loss_mask: 0.4780, loss: 12.2612\n",
      "2025-07-16 14:07:02,191 - mmdet - INFO - Epoch [16][400/750]\tlr: 2.500e-05, eta: 3:28:19, time: 0.342, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0648, stage0_pos_acc: 36.3231, stage0_loss_bbox: 0.6011, stage0_loss_iou: 1.1234, stage0_loss_mask: 1.1225, stage1_loss_cls: 0.7482, stage1_pos_acc: 50.2423, stage1_loss_bbox: 0.3216, stage1_loss_iou: 0.6038, stage1_loss_mask: 0.7355, stage2_loss_cls: 0.6831, stage2_pos_acc: 58.9811, stage2_loss_bbox: 0.2504, stage2_loss_iou: 0.4781, stage2_loss_mask: 0.6817, stage3_loss_cls: 0.6510, stage3_pos_acc: 60.5668, stage3_loss_bbox: 0.2239, stage3_loss_iou: 0.4443, stage3_loss_mask: 0.6874, stage4_loss_cls: 0.6030, stage4_pos_acc: 61.4857, stage4_loss_bbox: 0.2245, stage4_loss_iou: 0.4425, stage4_loss_mask: 0.6870, stage5_loss_cls: 0.5901, stage5_pos_acc: 65.4914, stage5_loss_bbox: 0.2263, stage5_loss_iou: 0.4356, stage5_loss_mask: 0.6495, loss: 14.2793\n",
      "2025-07-16 14:07:19,252 - mmdet - INFO - Epoch [16][450/750]\tlr: 2.500e-05, eta: 3:28:03, time: 0.341, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0744, stage0_pos_acc: 34.9952, stage0_loss_bbox: 0.5522, stage0_loss_iou: 1.0711, stage0_loss_mask: 1.0839, stage1_loss_cls: 0.7747, stage1_pos_acc: 44.6266, stage1_loss_bbox: 0.2818, stage1_loss_iou: 0.5474, stage1_loss_mask: 0.7358, stage2_loss_cls: 0.6828, stage2_pos_acc: 53.4540, stage2_loss_bbox: 0.2437, stage2_loss_iou: 0.4519, stage2_loss_mask: 0.7246, stage3_loss_cls: 0.6209, stage3_pos_acc: 52.4022, stage3_loss_bbox: 0.2257, stage3_loss_iou: 0.4239, stage3_loss_mask: 0.7126, stage4_loss_cls: 0.5875, stage4_pos_acc: 56.5449, stage4_loss_bbox: 0.2076, stage4_loss_iou: 0.4123, stage4_loss_mask: 0.6847, stage5_loss_cls: 0.5758, stage5_pos_acc: 57.5235, stage5_loss_bbox: 0.2109, stage5_loss_iou: 0.4118, stage5_loss_mask: 0.6996, loss: 13.9978\n",
      "2025-07-16 14:07:36,020 - mmdet - INFO - Epoch [16][500/750]\tlr: 2.500e-05, eta: 3:27:46, time: 0.335, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0841, stage0_pos_acc: 32.6595, stage0_loss_bbox: 0.5200, stage0_loss_iou: 1.0246, stage0_loss_mask: 0.8069, stage1_loss_cls: 0.7759, stage1_pos_acc: 48.0651, stage1_loss_bbox: 0.2598, stage1_loss_iou: 0.4782, stage1_loss_mask: 0.4180, stage2_loss_cls: 0.6841, stage2_pos_acc: 53.6421, stage2_loss_bbox: 0.2037, stage2_loss_iou: 0.3663, stage2_loss_mask: 0.3850, stage3_loss_cls: 0.6261, stage3_pos_acc: 56.0254, stage3_loss_bbox: 0.1927, stage3_loss_iou: 0.3475, stage3_loss_mask: 0.3723, stage4_loss_cls: 0.5849, stage4_pos_acc: 62.2619, stage4_loss_bbox: 0.1850, stage4_loss_iou: 0.3300, stage4_loss_mask: 0.3452, stage5_loss_cls: 0.5692, stage5_pos_acc: 63.3238, stage5_loss_bbox: 0.1830, stage5_loss_iou: 0.3317, stage5_loss_mask: 0.3714, loss: 11.4456\n",
      "2025-07-16 14:07:52,738 - mmdet - INFO - Epoch [16][550/750]\tlr: 2.500e-05, eta: 3:27:28, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0856, stage0_pos_acc: 35.3685, stage0_loss_bbox: 0.5769, stage0_loss_iou: 1.0443, stage0_loss_mask: 0.8614, stage1_loss_cls: 0.7817, stage1_pos_acc: 44.8702, stage1_loss_bbox: 0.2703, stage1_loss_iou: 0.5206, stage1_loss_mask: 0.4244, stage2_loss_cls: 0.7052, stage2_pos_acc: 54.6011, stage2_loss_bbox: 0.2162, stage2_loss_iou: 0.3868, stage2_loss_mask: 0.3768, stage3_loss_cls: 0.6286, stage3_pos_acc: 52.2055, stage3_loss_bbox: 0.2036, stage3_loss_iou: 0.3711, stage3_loss_mask: 0.3845, stage4_loss_cls: 0.5978, stage4_pos_acc: 52.6677, stage4_loss_bbox: 0.1987, stage4_loss_iou: 0.3605, stage4_loss_mask: 0.3507, stage5_loss_cls: 0.5850, stage5_pos_acc: 54.2987, stage5_loss_bbox: 0.1881, stage5_loss_iou: 0.3540, stage5_loss_mask: 0.3538, loss: 11.8266\n",
      "2025-07-16 14:08:09,489 - mmdet - INFO - Epoch [16][600/750]\tlr: 2.500e-05, eta: 3:27:11, time: 0.335, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0971, stage0_pos_acc: 30.8139, stage0_loss_bbox: 0.5651, stage0_loss_iou: 1.0186, stage0_loss_mask: 1.0331, stage1_loss_cls: 0.7752, stage1_pos_acc: 49.6896, stage1_loss_bbox: 0.2881, stage1_loss_iou: 0.5274, stage1_loss_mask: 0.6558, stage2_loss_cls: 0.6974, stage2_pos_acc: 50.9686, stage2_loss_bbox: 0.2255, stage2_loss_iou: 0.4183, stage2_loss_mask: 0.6434, stage3_loss_cls: 0.5795, stage3_pos_acc: 58.1419, stage3_loss_bbox: 0.2112, stage3_loss_iou: 0.3933, stage3_loss_mask: 0.6555, stage4_loss_cls: 0.5602, stage4_pos_acc: 59.3158, stage4_loss_bbox: 0.2127, stage4_loss_iou: 0.3772, stage4_loss_mask: 0.5965, stage5_loss_cls: 0.5449, stage5_pos_acc: 62.9126, stage5_loss_bbox: 0.1959, stage5_loss_iou: 0.3714, stage5_loss_mask: 0.6130, loss: 13.2565\n",
      "2025-07-16 14:08:26,065 - mmdet - INFO - Epoch [16][650/750]\tlr: 2.500e-05, eta: 3:26:54, time: 0.332, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0899, stage0_pos_acc: 33.2213, stage0_loss_bbox: 0.5882, stage0_loss_iou: 1.0470, stage0_loss_mask: 1.0176, stage1_loss_cls: 0.8120, stage1_pos_acc: 43.8828, stage1_loss_bbox: 0.2897, stage1_loss_iou: 0.5334, stage1_loss_mask: 0.6962, stage2_loss_cls: 0.7351, stage2_pos_acc: 54.0668, stage2_loss_bbox: 0.2444, stage2_loss_iou: 0.4373, stage2_loss_mask: 0.6391, stage3_loss_cls: 0.6499, stage3_pos_acc: 54.6498, stage3_loss_bbox: 0.2306, stage3_loss_iou: 0.4129, stage3_loss_mask: 0.6000, stage4_loss_cls: 0.6187, stage4_pos_acc: 56.8257, stage4_loss_bbox: 0.2218, stage4_loss_iou: 0.3982, stage4_loss_mask: 0.5494, stage5_loss_cls: 0.6001, stage5_pos_acc: 60.0873, stage5_loss_bbox: 0.2178, stage5_loss_iou: 0.3970, stage5_loss_mask: 0.5713, loss: 13.5975\n",
      "2025-07-16 14:08:42,762 - mmdet - INFO - Epoch [16][700/750]\tlr: 2.500e-05, eta: 3:26:36, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0713, stage0_pos_acc: 36.8087, stage0_loss_bbox: 0.5348, stage0_loss_iou: 1.0393, stage0_loss_mask: 0.9555, stage1_loss_cls: 0.7666, stage1_pos_acc: 43.5026, stage1_loss_bbox: 0.2639, stage1_loss_iou: 0.5310, stage1_loss_mask: 0.5579, stage2_loss_cls: 0.7042, stage2_pos_acc: 51.5029, stage2_loss_bbox: 0.2062, stage2_loss_iou: 0.4115, stage2_loss_mask: 0.4772, stage3_loss_cls: 0.6360, stage3_pos_acc: 52.7793, stage3_loss_bbox: 0.1896, stage3_loss_iou: 0.3832, stage3_loss_mask: 0.4605, stage4_loss_cls: 0.5797, stage4_pos_acc: 58.8559, stage4_loss_bbox: 0.1843, stage4_loss_iou: 0.3711, stage4_loss_mask: 0.4731, stage5_loss_cls: 0.5714, stage5_pos_acc: 58.1640, stage5_loss_bbox: 0.1818, stage5_loss_iou: 0.3665, stage5_loss_mask: 0.4639, loss: 12.3802\n",
      "2025-07-16 14:09:00,103 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:09:00,104 - mmdet - INFO - Epoch [16][750/750]\tlr: 2.500e-05, eta: 3:26:21, time: 0.347, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1112, stage0_pos_acc: 36.4992, stage0_loss_bbox: 0.5720, stage0_loss_iou: 1.0912, stage0_loss_mask: 0.9803, stage1_loss_cls: 0.7522, stage1_pos_acc: 51.9668, stage1_loss_bbox: 0.2472, stage1_loss_iou: 0.5179, stage1_loss_mask: 0.5851, stage2_loss_cls: 0.6787, stage2_pos_acc: 55.9310, stage2_loss_bbox: 0.1946, stage2_loss_iou: 0.4026, stage2_loss_mask: 0.5322, stage3_loss_cls: 0.6116, stage3_pos_acc: 56.7695, stage3_loss_bbox: 0.1827, stage3_loss_iou: 0.3774, stage3_loss_mask: 0.4993, stage4_loss_cls: 0.5604, stage4_pos_acc: 59.5311, stage4_loss_bbox: 0.1805, stage4_loss_iou: 0.3700, stage4_loss_mask: 0.4773, stage5_loss_cls: 0.5480, stage5_pos_acc: 62.0637, stage5_loss_bbox: 0.1770, stage5_loss_iou: 0.3662, stage5_loss_mask: 0.4849, loss: 12.5005\n",
      "2025-07-16 14:09:00,232 - mmdet - INFO - Saving checkpoint at 16 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 96s, ETA:     0s2025-07-16 14:12:13,640 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.26s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.444\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.447\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.447\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.291\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.484\n",
      "2025-07-16 14:12:15,422 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.75s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.147\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.299\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.474\n",
      "2025-07-16 14:12:18,336 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:12:18,336 - mmdet - INFO - Epoch(val) [16][750]\tbbox_mAP: 0.0290, bbox_mAP_50: 0.0530, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.0130, bbox_mAP_m: 0.0120, bbox_mAP_l: 0.0410, bbox_mAP_copypaste: 0.029 0.053 0.028 0.013 0.012 0.041, segm_mAP: 0.0290, segm_mAP_50: 0.0520, segm_mAP_75: 0.0270, segm_mAP_s: 0.0130, segm_mAP_m: 0.0110, segm_mAP_l: 0.0410, segm_mAP_copypaste: 0.029 0.052 0.027 0.013 0.011 0.041\n",
      "2025-07-16 14:12:38,208 - mmdet - INFO - Epoch [17][50/750]\tlr: 2.500e-05, eta: 3:26:13, time: 0.397, data_time: 0.057, memory: 11264, stage0_loss_cls: 1.0453, stage0_pos_acc: 34.8613, stage0_loss_bbox: 0.6017, stage0_loss_iou: 1.0573, stage0_loss_mask: 1.0333, stage1_loss_cls: 0.7437, stage1_pos_acc: 48.4882, stage1_loss_bbox: 0.3025, stage1_loss_iou: 0.5228, stage1_loss_mask: 0.6490, stage2_loss_cls: 0.6677, stage2_pos_acc: 55.9248, stage2_loss_bbox: 0.2584, stage2_loss_iou: 0.4194, stage2_loss_mask: 0.5469, stage3_loss_cls: 0.6009, stage3_pos_acc: 61.5074, stage3_loss_bbox: 0.2260, stage3_loss_iou: 0.3851, stage3_loss_mask: 0.5285, stage4_loss_cls: 0.5589, stage4_pos_acc: 63.4265, stage4_loss_bbox: 0.2312, stage4_loss_iou: 0.3806, stage4_loss_mask: 0.5253, stage5_loss_cls: 0.5491, stage5_pos_acc: 62.5984, stage5_loss_bbox: 0.2323, stage5_loss_iou: 0.3702, stage5_loss_mask: 0.5127, loss: 12.9490\n",
      "2025-07-16 14:12:55,653 - mmdet - INFO - Epoch [17][100/750]\tlr: 2.500e-05, eta: 3:25:58, time: 0.349, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1092, stage0_pos_acc: 32.0987, stage0_loss_bbox: 0.5301, stage0_loss_iou: 1.0880, stage0_loss_mask: 0.9153, stage1_loss_cls: 0.7554, stage1_pos_acc: 51.8145, stage1_loss_bbox: 0.2448, stage1_loss_iou: 0.4971, stage1_loss_mask: 0.4799, stage2_loss_cls: 0.6406, stage2_pos_acc: 60.6844, stage2_loss_bbox: 0.1979, stage2_loss_iou: 0.3788, stage2_loss_mask: 0.4446, stage3_loss_cls: 0.5435, stage3_pos_acc: 59.8001, stage3_loss_bbox: 0.1804, stage3_loss_iou: 0.3538, stage3_loss_mask: 0.4223, stage4_loss_cls: 0.5000, stage4_pos_acc: 68.3136, stage4_loss_bbox: 0.1751, stage4_loss_iou: 0.3456, stage4_loss_mask: 0.4137, stage5_loss_cls: 0.4790, stage5_pos_acc: 69.5120, stage5_loss_bbox: 0.1832, stage5_loss_iou: 0.3514, stage5_loss_mask: 0.4258, loss: 11.6556\n",
      "2025-07-16 14:13:12,892 - mmdet - INFO - Epoch [17][150/750]\tlr: 2.500e-05, eta: 3:25:43, time: 0.345, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1078, stage0_pos_acc: 32.6651, stage0_loss_bbox: 0.5840, stage0_loss_iou: 1.0817, stage0_loss_mask: 0.9443, stage1_loss_cls: 0.7870, stage1_pos_acc: 45.2791, stage1_loss_bbox: 0.2822, stage1_loss_iou: 0.5245, stage1_loss_mask: 0.5428, stage2_loss_cls: 0.6865, stage2_pos_acc: 50.5157, stage2_loss_bbox: 0.2259, stage2_loss_iou: 0.4274, stage2_loss_mask: 0.5072, stage3_loss_cls: 0.6241, stage3_pos_acc: 54.2293, stage3_loss_bbox: 0.2074, stage3_loss_iou: 0.3867, stage3_loss_mask: 0.4841, stage4_loss_cls: 0.5710, stage4_pos_acc: 53.2616, stage4_loss_bbox: 0.2024, stage4_loss_iou: 0.3814, stage4_loss_mask: 0.4698, stage5_loss_cls: 0.5581, stage5_pos_acc: 59.0806, stage5_loss_bbox: 0.2003, stage5_loss_iou: 0.3784, stage5_loss_mask: 0.4665, loss: 12.6316\n",
      "2025-07-16 14:13:30,098 - mmdet - INFO - Epoch [17][200/750]\tlr: 2.500e-05, eta: 3:25:27, time: 0.344, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0767, stage0_pos_acc: 34.4241, stage0_loss_bbox: 0.5492, stage0_loss_iou: 1.0388, stage0_loss_mask: 0.9089, stage1_loss_cls: 0.7306, stage1_pos_acc: 53.1547, stage1_loss_bbox: 0.2564, stage1_loss_iou: 0.5130, stage1_loss_mask: 0.5487, stage2_loss_cls: 0.6611, stage2_pos_acc: 58.3824, stage2_loss_bbox: 0.2024, stage2_loss_iou: 0.4109, stage2_loss_mask: 0.4608, stage3_loss_cls: 0.5728, stage3_pos_acc: 60.9492, stage3_loss_bbox: 0.1858, stage3_loss_iou: 0.3783, stage3_loss_mask: 0.4270, stage4_loss_cls: 0.5179, stage4_pos_acc: 63.5150, stage4_loss_bbox: 0.1840, stage4_loss_iou: 0.3770, stage4_loss_mask: 0.4620, stage5_loss_cls: 0.5086, stage5_pos_acc: 62.6991, stage5_loss_bbox: 0.1853, stage5_loss_iou: 0.3756, stage5_loss_mask: 0.4596, loss: 11.9913\n",
      "2025-07-16 14:13:47,614 - mmdet - INFO - Epoch [17][250/750]\tlr: 2.500e-05, eta: 3:25:12, time: 0.350, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0702, stage0_pos_acc: 33.1333, stage0_loss_bbox: 0.6057, stage0_loss_iou: 1.0432, stage0_loss_mask: 1.1843, stage1_loss_cls: 0.7719, stage1_pos_acc: 50.8042, stage1_loss_bbox: 0.2897, stage1_loss_iou: 0.5368, stage1_loss_mask: 0.7607, stage2_loss_cls: 0.6949, stage2_pos_acc: 55.6952, stage2_loss_bbox: 0.2265, stage2_loss_iou: 0.4275, stage2_loss_mask: 0.6765, stage3_loss_cls: 0.6210, stage3_pos_acc: 61.5864, stage3_loss_bbox: 0.2204, stage3_loss_iou: 0.4052, stage3_loss_mask: 0.6435, stage4_loss_cls: 0.5774, stage4_pos_acc: 63.2838, stage4_loss_bbox: 0.2103, stage4_loss_iou: 0.3882, stage4_loss_mask: 0.6329, stage5_loss_cls: 0.5633, stage5_pos_acc: 63.8156, stage5_loss_bbox: 0.2061, stage5_loss_iou: 0.3825, stage5_loss_mask: 0.6146, loss: 13.7533\n",
      "2025-07-16 14:14:04,330 - mmdet - INFO - Epoch [17][300/750]\tlr: 2.500e-05, eta: 3:24:54, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0650, stage0_pos_acc: 34.7786, stage0_loss_bbox: 0.5938, stage0_loss_iou: 1.1561, stage0_loss_mask: 1.2012, stage1_loss_cls: 0.7560, stage1_pos_acc: 49.9869, stage1_loss_bbox: 0.2973, stage1_loss_iou: 0.5900, stage1_loss_mask: 0.8191, stage2_loss_cls: 0.6727, stage2_pos_acc: 54.6202, stage2_loss_bbox: 0.2383, stage2_loss_iou: 0.4631, stage2_loss_mask: 0.7287, stage3_loss_cls: 0.5671, stage3_pos_acc: 61.7714, stage3_loss_bbox: 0.2253, stage3_loss_iou: 0.4469, stage3_loss_mask: 0.7353, stage4_loss_cls: 0.5202, stage4_pos_acc: 65.8476, stage4_loss_bbox: 0.2178, stage4_loss_iou: 0.4353, stage4_loss_mask: 0.7345, stage5_loss_cls: 0.5032, stage5_pos_acc: 67.9143, stage5_loss_bbox: 0.2198, stage5_loss_iou: 0.4312, stage5_loss_mask: 0.7394, loss: 14.3573\n",
      "2025-07-16 14:14:21,021 - mmdet - INFO - Epoch [17][350/750]\tlr: 2.500e-05, eta: 3:24:37, time: 0.334, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0517, stage0_pos_acc: 39.9056, stage0_loss_bbox: 0.5666, stage0_loss_iou: 1.0877, stage0_loss_mask: 1.0703, stage1_loss_cls: 0.7857, stage1_pos_acc: 49.6506, stage1_loss_bbox: 0.2761, stage1_loss_iou: 0.5597, stage1_loss_mask: 0.6351, stage2_loss_cls: 0.7139, stage2_pos_acc: 53.7440, stage2_loss_bbox: 0.2135, stage2_loss_iou: 0.4578, stage2_loss_mask: 0.5930, stage3_loss_cls: 0.6314, stage3_pos_acc: 55.1719, stage3_loss_bbox: 0.2079, stage3_loss_iou: 0.4435, stage3_loss_mask: 0.6062, stage4_loss_cls: 0.5883, stage4_pos_acc: 60.0863, stage4_loss_bbox: 0.1984, stage4_loss_iou: 0.4256, stage4_loss_mask: 0.6004, stage5_loss_cls: 0.5783, stage5_pos_acc: 62.1104, stage5_loss_bbox: 0.1994, stage5_loss_iou: 0.4174, stage5_loss_mask: 0.6074, loss: 13.5155\n",
      "2025-07-16 14:14:37,959 - mmdet - INFO - Epoch [17][400/750]\tlr: 2.500e-05, eta: 3:24:20, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0741, stage0_pos_acc: 33.9095, stage0_loss_bbox: 0.5493, stage0_loss_iou: 1.0039, stage0_loss_mask: 0.7952, stage1_loss_cls: 0.7671, stage1_pos_acc: 47.5119, stage1_loss_bbox: 0.2559, stage1_loss_iou: 0.4492, stage1_loss_mask: 0.4089, stage2_loss_cls: 0.6720, stage2_pos_acc: 52.9746, stage2_loss_bbox: 0.2024, stage2_loss_iou: 0.3502, stage2_loss_mask: 0.3629, stage3_loss_cls: 0.5999, stage3_pos_acc: 56.3024, stage3_loss_bbox: 0.1942, stage3_loss_iou: 0.3269, stage3_loss_mask: 0.3379, stage4_loss_cls: 0.5698, stage4_pos_acc: 58.8579, stage4_loss_bbox: 0.1755, stage4_loss_iou: 0.3085, stage4_loss_mask: 0.3472, stage5_loss_cls: 0.5525, stage5_pos_acc: 62.2294, stage5_loss_bbox: 0.1807, stage5_loss_iou: 0.3064, stage5_loss_mask: 0.3375, loss: 11.1282\n",
      "2025-07-16 14:14:54,710 - mmdet - INFO - Epoch [17][450/750]\tlr: 2.500e-05, eta: 3:24:03, time: 0.335, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0930, stage0_pos_acc: 29.2326, stage0_loss_bbox: 0.5164, stage0_loss_iou: 1.0349, stage0_loss_mask: 1.0350, stage1_loss_cls: 0.7495, stage1_pos_acc: 49.5674, stage1_loss_bbox: 0.2582, stage1_loss_iou: 0.5251, stage1_loss_mask: 0.6567, stage2_loss_cls: 0.6723, stage2_pos_acc: 55.7961, stage2_loss_bbox: 0.2158, stage2_loss_iou: 0.4263, stage2_loss_mask: 0.6262, stage3_loss_cls: 0.5933, stage3_pos_acc: 60.4684, stage3_loss_bbox: 0.2043, stage3_loss_iou: 0.3906, stage3_loss_mask: 0.5689, stage4_loss_cls: 0.5551, stage4_pos_acc: 66.5961, stage4_loss_bbox: 0.1971, stage4_loss_iou: 0.3799, stage4_loss_mask: 0.5660, stage5_loss_cls: 0.5425, stage5_pos_acc: 67.5073, stage5_loss_bbox: 0.1939, stage5_loss_iou: 0.3715, stage5_loss_mask: 0.5590, loss: 12.9318\n",
      "2025-07-16 14:15:11,296 - mmdet - INFO - Epoch [17][500/750]\tlr: 2.500e-05, eta: 3:23:46, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0920, stage0_pos_acc: 32.8944, stage0_loss_bbox: 0.5732, stage0_loss_iou: 1.0889, stage0_loss_mask: 0.8669, stage1_loss_cls: 0.7426, stage1_pos_acc: 50.0000, stage1_loss_bbox: 0.2209, stage1_loss_iou: 0.4746, stage1_loss_mask: 0.4623, stage2_loss_cls: 0.6698, stage2_pos_acc: 53.6333, stage2_loss_bbox: 0.1742, stage2_loss_iou: 0.3676, stage2_loss_mask: 0.4426, stage3_loss_cls: 0.5830, stage3_pos_acc: 54.4532, stage3_loss_bbox: 0.1659, stage3_loss_iou: 0.3426, stage3_loss_mask: 0.4179, stage4_loss_cls: 0.5435, stage4_pos_acc: 55.9532, stage4_loss_bbox: 0.1618, stage4_loss_iou: 0.3330, stage4_loss_mask: 0.4124, stage5_loss_cls: 0.5282, stage5_pos_acc: 61.4365, stage5_loss_bbox: 0.1551, stage5_loss_iou: 0.3255, stage5_loss_mask: 0.4066, loss: 11.5510\n",
      "2025-07-16 14:15:27,805 - mmdet - INFO - Epoch [17][550/750]\tlr: 2.500e-05, eta: 3:23:28, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1085, stage0_pos_acc: 37.7456, stage0_loss_bbox: 0.5656, stage0_loss_iou: 1.0638, stage0_loss_mask: 1.0246, stage1_loss_cls: 0.7489, stage1_pos_acc: 50.6512, stage1_loss_bbox: 0.2779, stage1_loss_iou: 0.5682, stage1_loss_mask: 0.6654, stage2_loss_cls: 0.6727, stage2_pos_acc: 55.4478, stage2_loss_bbox: 0.2281, stage2_loss_iou: 0.4518, stage2_loss_mask: 0.6336, stage3_loss_cls: 0.5777, stage3_pos_acc: 60.3901, stage3_loss_bbox: 0.2301, stage3_loss_iou: 0.4312, stage3_loss_mask: 0.6010, stage4_loss_cls: 0.5355, stage4_pos_acc: 66.4294, stage4_loss_bbox: 0.2249, stage4_loss_iou: 0.4224, stage4_loss_mask: 0.6025, stage5_loss_cls: 0.5246, stage5_pos_acc: 66.5738, stage5_loss_bbox: 0.2244, stage5_loss_iou: 0.4195, stage5_loss_mask: 0.6089, loss: 13.4120\n",
      "2025-07-16 14:15:44,489 - mmdet - INFO - Epoch [17][600/750]\tlr: 2.500e-05, eta: 3:23:10, time: 0.334, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0751, stage0_pos_acc: 31.1866, stage0_loss_bbox: 0.5164, stage0_loss_iou: 0.9750, stage0_loss_mask: 0.8130, stage1_loss_cls: 0.7590, stage1_pos_acc: 48.1436, stage1_loss_bbox: 0.2698, stage1_loss_iou: 0.4650, stage1_loss_mask: 0.5019, stage2_loss_cls: 0.6911, stage2_pos_acc: 53.6524, stage2_loss_bbox: 0.2193, stage2_loss_iou: 0.3613, stage2_loss_mask: 0.4606, stage3_loss_cls: 0.6193, stage3_pos_acc: 60.0080, stage3_loss_bbox: 0.2113, stage3_loss_iou: 0.3349, stage3_loss_mask: 0.4140, stage4_loss_cls: 0.5792, stage4_pos_acc: 63.3313, stage4_loss_bbox: 0.2050, stage4_loss_iou: 0.3287, stage4_loss_mask: 0.4099, stage5_loss_cls: 0.5674, stage5_pos_acc: 65.5471, stage5_loss_bbox: 0.2131, stage5_loss_iou: 0.3259, stage5_loss_mask: 0.3931, loss: 11.7092\n",
      "2025-07-16 14:16:01,561 - mmdet - INFO - Epoch [17][650/750]\tlr: 2.500e-05, eta: 3:22:54, time: 0.341, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1087, stage0_pos_acc: 35.9084, stage0_loss_bbox: 0.5001, stage0_loss_iou: 0.9794, stage0_loss_mask: 0.9328, stage1_loss_cls: 0.7743, stage1_pos_acc: 51.8180, stage1_loss_bbox: 0.2518, stage1_loss_iou: 0.4978, stage1_loss_mask: 0.6120, stage2_loss_cls: 0.7059, stage2_pos_acc: 53.5180, stage2_loss_bbox: 0.2033, stage2_loss_iou: 0.4036, stage2_loss_mask: 0.5780, stage3_loss_cls: 0.6296, stage3_pos_acc: 59.3736, stage3_loss_bbox: 0.1900, stage3_loss_iou: 0.3846, stage3_loss_mask: 0.5457, stage4_loss_cls: 0.5664, stage4_pos_acc: 63.7914, stage4_loss_bbox: 0.1930, stage4_loss_iou: 0.3709, stage4_loss_mask: 0.5469, stage5_loss_cls: 0.5585, stage5_pos_acc: 61.7399, stage5_loss_bbox: 0.1850, stage5_loss_iou: 0.3653, stage5_loss_mask: 0.5416, loss: 12.6251\n",
      "2025-07-16 14:16:18,566 - mmdet - INFO - Epoch [17][700/750]\tlr: 2.500e-05, eta: 3:22:38, time: 0.340, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0448, stage0_pos_acc: 35.8425, stage0_loss_bbox: 0.5280, stage0_loss_iou: 0.9702, stage0_loss_mask: 1.0437, stage1_loss_cls: 0.7829, stage1_pos_acc: 49.6466, stage1_loss_bbox: 0.2829, stage1_loss_iou: 0.5415, stage1_loss_mask: 0.7138, stage2_loss_cls: 0.7176, stage2_pos_acc: 56.8748, stage2_loss_bbox: 0.2176, stage2_loss_iou: 0.4229, stage2_loss_mask: 0.6147, stage3_loss_cls: 0.6422, stage3_pos_acc: 58.2458, stage3_loss_bbox: 0.2057, stage3_loss_iou: 0.4030, stage3_loss_mask: 0.5941, stage4_loss_cls: 0.6026, stage4_pos_acc: 59.6855, stage4_loss_bbox: 0.2003, stage4_loss_iou: 0.3986, stage4_loss_mask: 0.5992, stage5_loss_cls: 0.5798, stage5_pos_acc: 60.4534, stage5_loss_bbox: 0.2051, stage5_loss_iou: 0.3981, stage5_loss_mask: 0.5953, loss: 13.3046\n",
      "2025-07-16 14:16:35,105 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:16:35,106 - mmdet - INFO - Epoch [17][750/750]\tlr: 2.500e-05, eta: 3:22:20, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0707, stage0_pos_acc: 33.5435, stage0_loss_bbox: 0.5404, stage0_loss_iou: 0.9922, stage0_loss_mask: 0.8155, stage1_loss_cls: 0.7157, stage1_pos_acc: 48.1928, stage1_loss_bbox: 0.2563, stage1_loss_iou: 0.4720, stage1_loss_mask: 0.6168, stage2_loss_cls: 0.6233, stage2_pos_acc: 61.2314, stage2_loss_bbox: 0.2048, stage2_loss_iou: 0.3735, stage2_loss_mask: 0.5582, stage3_loss_cls: 0.5593, stage3_pos_acc: 57.2092, stage3_loss_bbox: 0.1966, stage3_loss_iou: 0.3485, stage3_loss_mask: 0.5311, stage4_loss_cls: 0.5241, stage4_pos_acc: 63.1235, stage4_loss_bbox: 0.1901, stage4_loss_iou: 0.3393, stage4_loss_mask: 0.5261, stage5_loss_cls: 0.5037, stage5_pos_acc: 64.6182, stage5_loss_bbox: 0.1940, stage5_loss_iou: 0.3371, stage5_loss_mask: 0.5198, loss: 12.0092\n",
      "2025-07-16 14:16:35,210 - mmdet - INFO - Saving checkpoint at 17 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 87s, ETA:     0s2025-07-16 14:19:40,503 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.118\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.465\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.207\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.509\n",
      "2025-07-16 14:19:42,291 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.80s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.145\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.492\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.492\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.267\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.326\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.530\n",
      "2025-07-16 14:19:45,380 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:19:45,381 - mmdet - INFO - Epoch(val) [17][750]\tbbox_mAP: 0.0320, bbox_mAP_50: 0.0590, bbox_mAP_75: 0.0290, bbox_mAP_s: 0.1180, bbox_mAP_m: 0.0140, bbox_mAP_l: 0.0400, bbox_mAP_copypaste: 0.032 0.059 0.029 0.118 0.014 0.040, segm_mAP: 0.0330, segm_mAP_50: 0.0590, segm_mAP_75: 0.0340, segm_mAP_s: 0.1450, segm_mAP_m: 0.0140, segm_mAP_l: 0.0420, segm_mAP_copypaste: 0.033 0.059 0.034 0.145 0.014 0.042\n",
      "2025-07-16 14:20:04,184 - mmdet - INFO - Epoch [18][50/750]\tlr: 2.500e-05, eta: 3:22:08, time: 0.376, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.0839, stage0_pos_acc: 34.5770, stage0_loss_bbox: 0.5272, stage0_loss_iou: 0.9268, stage0_loss_mask: 0.7842, stage1_loss_cls: 0.7484, stage1_pos_acc: 51.6177, stage1_loss_bbox: 0.2731, stage1_loss_iou: 0.4355, stage1_loss_mask: 0.4780, stage2_loss_cls: 0.6663, stage2_pos_acc: 56.8772, stage2_loss_bbox: 0.2268, stage2_loss_iou: 0.3589, stage2_loss_mask: 0.4192, stage3_loss_cls: 0.5745, stage3_pos_acc: 58.3970, stage3_loss_bbox: 0.2121, stage3_loss_iou: 0.3342, stage3_loss_mask: 0.4054, stage4_loss_cls: 0.5452, stage4_pos_acc: 63.9240, stage4_loss_bbox: 0.2011, stage4_loss_iou: 0.3291, stage4_loss_mask: 0.4049, stage5_loss_cls: 0.5342, stage5_pos_acc: 65.3008, stage5_loss_bbox: 0.2026, stage5_loss_iou: 0.3274, stage5_loss_mask: 0.4137, loss: 11.4126\n",
      "2025-07-16 14:20:21,306 - mmdet - INFO - Epoch [18][100/750]\tlr: 2.500e-05, eta: 3:21:52, time: 0.342, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0695, stage0_pos_acc: 36.3480, stage0_loss_bbox: 0.5036, stage0_loss_iou: 1.0268, stage0_loss_mask: 0.7704, stage1_loss_cls: 0.7390, stage1_pos_acc: 49.7421, stage1_loss_bbox: 0.2505, stage1_loss_iou: 0.4584, stage1_loss_mask: 0.4348, stage2_loss_cls: 0.6515, stage2_pos_acc: 58.0000, stage2_loss_bbox: 0.2025, stage2_loss_iou: 0.3640, stage2_loss_mask: 0.3920, stage3_loss_cls: 0.5627, stage3_pos_acc: 64.4595, stage3_loss_bbox: 0.1856, stage3_loss_iou: 0.3368, stage3_loss_mask: 0.3608, stage4_loss_cls: 0.5211, stage4_pos_acc: 65.4234, stage4_loss_bbox: 0.1879, stage4_loss_iou: 0.3343, stage4_loss_mask: 0.3671, stage5_loss_cls: 0.4946, stage5_pos_acc: 72.1444, stage5_loss_bbox: 0.1817, stage5_loss_iou: 0.3229, stage5_loss_mask: 0.3648, loss: 11.0834\n",
      "2025-07-16 14:20:38,204 - mmdet - INFO - Epoch [18][150/750]\tlr: 2.500e-05, eta: 3:21:35, time: 0.338, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0586, stage0_pos_acc: 32.3340, stage0_loss_bbox: 0.5110, stage0_loss_iou: 1.0887, stage0_loss_mask: 0.9716, stage1_loss_cls: 0.7241, stage1_pos_acc: 52.3351, stage1_loss_bbox: 0.2443, stage1_loss_iou: 0.5321, stage1_loss_mask: 0.6218, stage2_loss_cls: 0.6247, stage2_pos_acc: 59.5661, stage2_loss_bbox: 0.1873, stage2_loss_iou: 0.4200, stage2_loss_mask: 0.5774, stage3_loss_cls: 0.5279, stage3_pos_acc: 64.8270, stage3_loss_bbox: 0.1736, stage3_loss_iou: 0.3843, stage3_loss_mask: 0.5431, stage4_loss_cls: 0.4688, stage4_pos_acc: 68.0235, stage4_loss_bbox: 0.1814, stage4_loss_iou: 0.3819, stage4_loss_mask: 0.5465, stage5_loss_cls: 0.4374, stage5_pos_acc: 72.7494, stage5_loss_bbox: 0.1741, stage5_loss_iou: 0.3822, stage5_loss_mask: 0.5404, loss: 12.3033\n",
      "2025-07-16 14:20:54,870 - mmdet - INFO - Epoch [18][200/750]\tlr: 2.500e-05, eta: 3:21:18, time: 0.333, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0844, stage0_pos_acc: 33.7558, stage0_loss_bbox: 0.5006, stage0_loss_iou: 0.9284, stage0_loss_mask: 0.8893, stage1_loss_cls: 0.7194, stage1_pos_acc: 49.2299, stage1_loss_bbox: 0.2644, stage1_loss_iou: 0.4732, stage1_loss_mask: 0.6201, stage2_loss_cls: 0.6196, stage2_pos_acc: 57.1985, stage2_loss_bbox: 0.2175, stage2_loss_iou: 0.3891, stage2_loss_mask: 0.5854, stage3_loss_cls: 0.5401, stage3_pos_acc: 64.2017, stage3_loss_bbox: 0.2101, stage3_loss_iou: 0.3591, stage3_loss_mask: 0.5756, stage4_loss_cls: 0.4896, stage4_pos_acc: 65.4723, stage4_loss_bbox: 0.2052, stage4_loss_iou: 0.3536, stage4_loss_mask: 0.5802, stage5_loss_cls: 0.4701, stage5_pos_acc: 64.1970, stage5_loss_bbox: 0.1948, stage5_loss_iou: 0.3460, stage5_loss_mask: 0.5689, loss: 12.1847\n",
      "2025-07-16 14:21:11,947 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:21:11,947 - mmdet - INFO - Epoch [18][250/750]\tlr: 2.500e-05, eta: 3:21:02, time: 0.342, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0715, stage0_pos_acc: 38.3708, stage0_loss_bbox: 0.6025, stage0_loss_iou: 0.9981, stage0_loss_mask: 0.7840, stage1_loss_cls: 0.7184, stage1_pos_acc: 54.8835, stage1_loss_bbox: 0.3052, stage1_loss_iou: 0.4900, stage1_loss_mask: 0.4658, stage2_loss_cls: 0.6487, stage2_pos_acc: 61.7692, stage2_loss_bbox: 0.2188, stage2_loss_iou: 0.3681, stage2_loss_mask: 0.3848, stage3_loss_cls: 0.5363, stage3_pos_acc: 63.7329, stage3_loss_bbox: 0.2063, stage3_loss_iou: 0.3449, stage3_loss_mask: 0.3855, stage4_loss_cls: 0.5077, stage4_pos_acc: 69.3797, stage4_loss_bbox: 0.1982, stage4_loss_iou: 0.3307, stage4_loss_mask: 0.3740, stage5_loss_cls: 0.4960, stage5_pos_acc: 70.0178, stage5_loss_bbox: 0.1956, stage5_loss_iou: 0.3260, stage5_loss_mask: 0.3710, loss: 11.3279\n",
      "2025-07-16 14:21:29,002 - mmdet - INFO - Epoch [18][300/750]\tlr: 2.500e-05, eta: 3:20:45, time: 0.341, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0665, stage0_pos_acc: 35.4038, stage0_loss_bbox: 0.5752, stage0_loss_iou: 1.0871, stage0_loss_mask: 1.0855, stage1_loss_cls: 0.7281, stage1_pos_acc: 50.9947, stage1_loss_bbox: 0.2490, stage1_loss_iou: 0.5225, stage1_loss_mask: 0.5547, stage2_loss_cls: 0.6181, stage2_pos_acc: 56.3170, stage2_loss_bbox: 0.2072, stage2_loss_iou: 0.4035, stage2_loss_mask: 0.4883, stage3_loss_cls: 0.5365, stage3_pos_acc: 59.1107, stage3_loss_bbox: 0.1957, stage3_loss_iou: 0.3680, stage3_loss_mask: 0.4752, stage4_loss_cls: 0.4861, stage4_pos_acc: 62.9790, stage4_loss_bbox: 0.1819, stage4_loss_iou: 0.3558, stage4_loss_mask: 0.4593, stage5_loss_cls: 0.4629, stage5_pos_acc: 66.3108, stage5_loss_bbox: 0.1752, stage5_loss_iou: 0.3553, stage5_loss_mask: 0.4795, loss: 12.1170\n",
      "2025-07-16 14:21:46,212 - mmdet - INFO - Epoch [18][350/750]\tlr: 2.500e-05, eta: 3:20:29, time: 0.344, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0861, stage0_pos_acc: 38.4481, stage0_loss_bbox: 0.5746, stage0_loss_iou: 1.0954, stage0_loss_mask: 1.1830, stage1_loss_cls: 0.7212, stage1_pos_acc: 52.3523, stage1_loss_bbox: 0.2822, stage1_loss_iou: 0.5781, stage1_loss_mask: 0.7209, stage2_loss_cls: 0.6564, stage2_pos_acc: 57.9959, stage2_loss_bbox: 0.2381, stage2_loss_iou: 0.4689, stage2_loss_mask: 0.6602, stage3_loss_cls: 0.5759, stage3_pos_acc: 63.2766, stage3_loss_bbox: 0.2160, stage3_loss_iou: 0.4174, stage3_loss_mask: 0.6219, stage4_loss_cls: 0.5201, stage4_pos_acc: 65.8949, stage4_loss_bbox: 0.2085, stage4_loss_iou: 0.4122, stage4_loss_mask: 0.6697, stage5_loss_cls: 0.5003, stage5_pos_acc: 70.5686, stage5_loss_bbox: 0.2087, stage5_loss_iou: 0.4081, stage5_loss_mask: 0.6423, loss: 13.6660\n",
      "2025-07-16 14:22:02,734 - mmdet - INFO - Epoch [18][400/750]\tlr: 2.500e-05, eta: 3:20:12, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0890, stage0_pos_acc: 38.9626, stage0_loss_bbox: 0.5422, stage0_loss_iou: 1.1355, stage0_loss_mask: 1.2583, stage1_loss_cls: 0.7335, stage1_pos_acc: 55.3388, stage1_loss_bbox: 0.2748, stage1_loss_iou: 0.5972, stage1_loss_mask: 0.8427, stage2_loss_cls: 0.6734, stage2_pos_acc: 60.0855, stage2_loss_bbox: 0.2196, stage2_loss_iou: 0.4746, stage2_loss_mask: 0.7170, stage3_loss_cls: 0.6192, stage3_pos_acc: 61.3484, stage3_loss_bbox: 0.2154, stage3_loss_iou: 0.4403, stage3_loss_mask: 0.7288, stage4_loss_cls: 0.5724, stage4_pos_acc: 63.7317, stage4_loss_bbox: 0.2043, stage4_loss_iou: 0.4230, stage4_loss_mask: 0.7242, stage5_loss_cls: 0.5570, stage5_pos_acc: 67.4817, stage5_loss_bbox: 0.2051, stage5_loss_iou: 0.4186, stage5_loss_mask: 0.7182, loss: 14.3845\n",
      "2025-07-16 14:22:19,381 - mmdet - INFO - Epoch [18][450/750]\tlr: 2.500e-05, eta: 3:19:54, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0499, stage0_pos_acc: 36.3908, stage0_loss_bbox: 0.5555, stage0_loss_iou: 1.0126, stage0_loss_mask: 1.0858, stage1_loss_cls: 0.7246, stage1_pos_acc: 51.4703, stage1_loss_bbox: 0.3138, stage1_loss_iou: 0.5444, stage1_loss_mask: 0.7499, stage2_loss_cls: 0.6704, stage2_pos_acc: 51.6014, stage2_loss_bbox: 0.2682, stage2_loss_iou: 0.4746, stage2_loss_mask: 0.7014, stage3_loss_cls: 0.5851, stage3_pos_acc: 59.1086, stage3_loss_bbox: 0.2583, stage3_loss_iou: 0.4586, stage3_loss_mask: 0.7368, stage4_loss_cls: 0.5427, stage4_pos_acc: 62.8536, stage4_loss_bbox: 0.2427, stage4_loss_iou: 0.4451, stage4_loss_mask: 0.7166, stage5_loss_cls: 0.5315, stage5_pos_acc: 66.4863, stage5_loss_bbox: 0.2407, stage5_loss_iou: 0.4400, stage5_loss_mask: 0.7025, loss: 14.0520\n",
      "2025-07-16 14:22:35,855 - mmdet - INFO - Epoch [18][500/750]\tlr: 2.500e-05, eta: 3:19:36, time: 0.329, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0920, stage0_pos_acc: 31.4019, stage0_loss_bbox: 0.5710, stage0_loss_iou: 1.0090, stage0_loss_mask: 0.9495, stage1_loss_cls: 0.7190, stage1_pos_acc: 56.5198, stage1_loss_bbox: 0.2917, stage1_loss_iou: 0.5227, stage1_loss_mask: 0.6605, stage2_loss_cls: 0.6450, stage2_pos_acc: 56.5805, stage2_loss_bbox: 0.2511, stage2_loss_iou: 0.4387, stage2_loss_mask: 0.6227, stage3_loss_cls: 0.5572, stage3_pos_acc: 64.5147, stage3_loss_bbox: 0.2301, stage3_loss_iou: 0.3979, stage3_loss_mask: 0.5319, stage4_loss_cls: 0.5045, stage4_pos_acc: 69.6341, stage4_loss_bbox: 0.2267, stage4_loss_iou: 0.3819, stage4_loss_mask: 0.5085, stage5_loss_cls: 0.4892, stage5_pos_acc: 69.9210, stage5_loss_bbox: 0.2120, stage5_loss_iou: 0.3670, stage5_loss_mask: 0.5029, loss: 12.6828\n",
      "2025-07-16 14:22:52,466 - mmdet - INFO - Epoch [18][550/750]\tlr: 2.500e-05, eta: 3:19:19, time: 0.332, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1023, stage0_pos_acc: 32.1076, stage0_loss_bbox: 0.5139, stage0_loss_iou: 0.9989, stage0_loss_mask: 0.8008, stage1_loss_cls: 0.7506, stage1_pos_acc: 49.0380, stage1_loss_bbox: 0.2476, stage1_loss_iou: 0.4646, stage1_loss_mask: 0.3903, stage2_loss_cls: 0.6722, stage2_pos_acc: 57.1833, stage2_loss_bbox: 0.1938, stage2_loss_iou: 0.3469, stage2_loss_mask: 0.3429, stage3_loss_cls: 0.5769, stage3_pos_acc: 63.1677, stage3_loss_bbox: 0.1720, stage3_loss_iou: 0.3032, stage3_loss_mask: 0.3191, stage4_loss_cls: 0.5275, stage4_pos_acc: 65.8938, stage4_loss_bbox: 0.1676, stage4_loss_iou: 0.2886, stage4_loss_mask: 0.2904, stage5_loss_cls: 0.5070, stage5_pos_acc: 67.0795, stage5_loss_bbox: 0.1659, stage5_loss_iou: 0.2855, stage5_loss_mask: 0.2859, loss: 10.7144\n",
      "2025-07-16 14:23:09,124 - mmdet - INFO - Epoch [18][600/750]\tlr: 2.500e-05, eta: 3:19:01, time: 0.333, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0971, stage0_pos_acc: 35.0430, stage0_loss_bbox: 0.5962, stage0_loss_iou: 1.1223, stage0_loss_mask: 1.1152, stage1_loss_cls: 0.7405, stage1_pos_acc: 52.1706, stage1_loss_bbox: 0.3096, stage1_loss_iou: 0.5880, stage1_loss_mask: 0.8889, stage2_loss_cls: 0.6751, stage2_pos_acc: 56.9158, stage2_loss_bbox: 0.2455, stage2_loss_iou: 0.4612, stage2_loss_mask: 0.8022, stage3_loss_cls: 0.5919, stage3_pos_acc: 63.6969, stage3_loss_bbox: 0.2416, stage3_loss_iou: 0.4401, stage3_loss_mask: 0.7923, stage4_loss_cls: 0.5567, stage4_pos_acc: 66.9338, stage4_loss_bbox: 0.2389, stage4_loss_iou: 0.4284, stage4_loss_mask: 0.7822, stage5_loss_cls: 0.5551, stage5_pos_acc: 65.4122, stage5_loss_bbox: 0.2381, stage5_loss_iou: 0.4323, stage5_loss_mask: 0.7844, loss: 14.7236\n",
      "2025-07-16 14:23:25,740 - mmdet - INFO - Epoch [18][650/750]\tlr: 2.500e-05, eta: 3:18:44, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0998, stage0_pos_acc: 29.6726, stage0_loss_bbox: 0.6570, stage0_loss_iou: 1.2100, stage0_loss_mask: 1.2611, stage1_loss_cls: 0.7845, stage1_pos_acc: 47.6020, stage1_loss_bbox: 0.3126, stage1_loss_iou: 0.6012, stage1_loss_mask: 0.7634, stage2_loss_cls: 0.7021, stage2_pos_acc: 56.6679, stage2_loss_bbox: 0.2489, stage2_loss_iou: 0.4648, stage2_loss_mask: 0.6357, stage3_loss_cls: 0.6082, stage3_pos_acc: 60.7313, stage3_loss_bbox: 0.2114, stage3_loss_iou: 0.4175, stage3_loss_mask: 0.5987, stage4_loss_cls: 0.5680, stage4_pos_acc: 59.8643, stage4_loss_bbox: 0.1920, stage4_loss_iou: 0.3881, stage4_loss_mask: 0.5780, stage5_loss_cls: 0.5637, stage5_pos_acc: 59.0254, stage5_loss_bbox: 0.1956, stage5_loss_iou: 0.3887, stage5_loss_mask: 0.5926, loss: 14.0436\n",
      "2025-07-16 14:23:42,063 - mmdet - INFO - Epoch [18][700/750]\tlr: 2.500e-05, eta: 3:18:25, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0922, stage0_pos_acc: 28.8857, stage0_loss_bbox: 0.5761, stage0_loss_iou: 1.0397, stage0_loss_mask: 1.0551, stage1_loss_cls: 0.8166, stage1_pos_acc: 46.3540, stage1_loss_bbox: 0.2896, stage1_loss_iou: 0.5308, stage1_loss_mask: 0.5835, stage2_loss_cls: 0.7240, stage2_pos_acc: 51.7540, stage2_loss_bbox: 0.2374, stage2_loss_iou: 0.4161, stage2_loss_mask: 0.5716, stage3_loss_cls: 0.6330, stage3_pos_acc: 56.1635, stage3_loss_bbox: 0.2182, stage3_loss_iou: 0.3869, stage3_loss_mask: 0.5428, stage4_loss_cls: 0.5840, stage4_pos_acc: 61.3302, stage4_loss_bbox: 0.2133, stage4_loss_iou: 0.3718, stage4_loss_mask: 0.5400, stage5_loss_cls: 0.5650, stage5_pos_acc: 65.4444, stage5_loss_bbox: 0.2087, stage5_loss_iou: 0.3642, stage5_loss_mask: 0.5400, loss: 13.1005\n",
      "2025-07-16 14:23:58,541 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:23:58,542 - mmdet - INFO - Epoch [18][750/750]\tlr: 2.500e-05, eta: 3:18:07, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0997, stage0_pos_acc: 36.0938, stage0_loss_bbox: 0.5279, stage0_loss_iou: 0.9977, stage0_loss_mask: 0.8288, stage1_loss_cls: 0.7641, stage1_pos_acc: 51.9470, stage1_loss_bbox: 0.2418, stage1_loss_iou: 0.4772, stage1_loss_mask: 0.5204, stage2_loss_cls: 0.6940, stage2_pos_acc: 54.9003, stage2_loss_bbox: 0.2009, stage2_loss_iou: 0.3773, stage2_loss_mask: 0.4583, stage3_loss_cls: 0.5997, stage3_pos_acc: 59.2604, stage3_loss_bbox: 0.1868, stage3_loss_iou: 0.3601, stage3_loss_mask: 0.4770, stage4_loss_cls: 0.5562, stage4_pos_acc: 61.1882, stage4_loss_bbox: 0.1852, stage4_loss_iou: 0.3534, stage4_loss_mask: 0.4660, stage5_loss_cls: 0.5435, stage5_pos_acc: 62.2540, stage5_loss_bbox: 0.1855, stage5_loss_iou: 0.3517, stage5_loss_mask: 0.4660, loss: 11.9191\n",
      "2025-07-16 14:23:58,651 - mmdet - INFO - Saving checkpoint at 18 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 88s, ETA:     0s2025-07-16 14:27:04,601 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.066\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.071\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.469\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.251\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.518\n",
      "2025-07-16 14:27:06,505 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.066\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.084\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.472\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.474\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.474\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.227\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.248\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.519\n",
      "2025-07-16 14:27:09,584 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:27:09,584 - mmdet - INFO - Epoch(val) [18][750]\tbbox_mAP: 0.0340, bbox_mAP_50: 0.0660, bbox_mAP_75: 0.0320, bbox_mAP_s: 0.0710, bbox_mAP_m: 0.0310, bbox_mAP_l: 0.0390, bbox_mAP_copypaste: 0.034 0.066 0.032 0.071 0.031 0.039, segm_mAP: 0.0340, segm_mAP_50: 0.0660, segm_mAP_75: 0.0310, segm_mAP_s: 0.0840, segm_mAP_m: 0.0270, segm_mAP_l: 0.0390, segm_mAP_copypaste: 0.034 0.066 0.031 0.084 0.027 0.039\n",
      "2025-07-16 14:27:28,575 - mmdet - INFO - Epoch [19][50/750]\tlr: 2.500e-05, eta: 3:17:56, time: 0.380, data_time: 0.051, memory: 11264, stage0_loss_cls: 1.0795, stage0_pos_acc: 40.3701, stage0_loss_bbox: 0.5261, stage0_loss_iou: 0.9669, stage0_loss_mask: 0.7933, stage1_loss_cls: 0.7212, stage1_pos_acc: 57.6545, stage1_loss_bbox: 0.2355, stage1_loss_iou: 0.4429, stage1_loss_mask: 0.5256, stage2_loss_cls: 0.5923, stage2_pos_acc: 62.7768, stage2_loss_bbox: 0.1965, stage2_loss_iou: 0.3645, stage2_loss_mask: 0.4944, stage3_loss_cls: 0.5223, stage3_pos_acc: 63.9823, stage3_loss_bbox: 0.1774, stage3_loss_iou: 0.3472, stage3_loss_mask: 0.4821, stage4_loss_cls: 0.4423, stage4_pos_acc: 76.2949, stage4_loss_bbox: 0.1778, stage4_loss_iou: 0.3423, stage4_loss_mask: 0.5011, stage5_loss_cls: 0.4240, stage5_pos_acc: 80.0196, stage5_loss_bbox: 0.1767, stage5_loss_iou: 0.3338, stage5_loss_mask: 0.4819, loss: 11.3475\n",
      "2025-07-16 14:27:45,291 - mmdet - INFO - Epoch [19][100/750]\tlr: 2.500e-05, eta: 3:17:39, time: 0.334, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1107, stage0_pos_acc: 29.6262, stage0_loss_bbox: 0.5146, stage0_loss_iou: 1.0141, stage0_loss_mask: 0.8370, stage1_loss_cls: 0.6941, stage1_pos_acc: 53.8095, stage1_loss_bbox: 0.2238, stage1_loss_iou: 0.4615, stage1_loss_mask: 0.4915, stage2_loss_cls: 0.6080, stage2_pos_acc: 64.3929, stage2_loss_bbox: 0.1685, stage2_loss_iou: 0.3537, stage2_loss_mask: 0.4303, stage3_loss_cls: 0.5122, stage3_pos_acc: 65.3595, stage3_loss_bbox: 0.1525, stage3_loss_iou: 0.3170, stage3_loss_mask: 0.3745, stage4_loss_cls: 0.4417, stage4_pos_acc: 68.4286, stage4_loss_bbox: 0.1515, stage4_loss_iou: 0.3111, stage4_loss_mask: 0.3866, stage5_loss_cls: 0.4137, stage5_pos_acc: 74.8952, stage5_loss_bbox: 0.1509, stage5_loss_iou: 0.3063, stage5_loss_mask: 0.3832, loss: 10.8091\n",
      "2025-07-16 14:28:01,759 - mmdet - INFO - Epoch [19][150/750]\tlr: 2.500e-05, eta: 3:17:21, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1122, stage0_pos_acc: 29.8771, stage0_loss_bbox: 0.5143, stage0_loss_iou: 1.0744, stage0_loss_mask: 1.1103, stage1_loss_cls: 0.7491, stage1_pos_acc: 50.5630, stage1_loss_bbox: 0.2366, stage1_loss_iou: 0.5275, stage1_loss_mask: 0.6406, stage2_loss_cls: 0.6269, stage2_pos_acc: 58.8892, stage2_loss_bbox: 0.1980, stage2_loss_iou: 0.4284, stage2_loss_mask: 0.5836, stage3_loss_cls: 0.5490, stage3_pos_acc: 62.8412, stage3_loss_bbox: 0.1885, stage3_loss_iou: 0.4018, stage3_loss_mask: 0.5869, stage4_loss_cls: 0.5098, stage4_pos_acc: 66.9697, stage4_loss_bbox: 0.1850, stage4_loss_iou: 0.3938, stage4_loss_mask: 0.5696, stage5_loss_cls: 0.5052, stage5_pos_acc: 70.9641, stage5_loss_bbox: 0.1796, stage5_loss_iou: 0.3873, stage5_loss_mask: 0.5756, loss: 12.8341\n",
      "2025-07-16 14:28:18,776 - mmdet - INFO - Epoch [19][200/750]\tlr: 2.500e-05, eta: 3:17:04, time: 0.340, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1063, stage0_pos_acc: 34.3595, stage0_loss_bbox: 0.6261, stage0_loss_iou: 1.0112, stage0_loss_mask: 0.8510, stage1_loss_cls: 0.7272, stage1_pos_acc: 52.1429, stage1_loss_bbox: 0.3061, stage1_loss_iou: 0.4943, stage1_loss_mask: 0.4539, stage2_loss_cls: 0.6477, stage2_pos_acc: 56.3516, stage2_loss_bbox: 0.2569, stage2_loss_iou: 0.3821, stage2_loss_mask: 0.4185, stage3_loss_cls: 0.5656, stage3_pos_acc: 62.1706, stage3_loss_bbox: 0.2419, stage3_loss_iou: 0.3601, stage3_loss_mask: 0.4093, stage4_loss_cls: 0.4952, stage4_pos_acc: 66.9040, stage4_loss_bbox: 0.2426, stage4_loss_iou: 0.3555, stage4_loss_mask: 0.4004, stage5_loss_cls: 0.4782, stage5_pos_acc: 68.5984, stage5_loss_bbox: 0.2429, stage5_loss_iou: 0.3489, stage5_loss_mask: 0.4140, loss: 11.8359\n",
      "2025-07-16 14:28:36,208 - mmdet - INFO - Epoch [19][250/750]\tlr: 2.500e-05, eta: 3:16:49, time: 0.349, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1060, stage0_pos_acc: 32.1300, stage0_loss_bbox: 0.5470, stage0_loss_iou: 0.9830, stage0_loss_mask: 0.7984, stage1_loss_cls: 0.7676, stage1_pos_acc: 51.6618, stage1_loss_bbox: 0.2905, stage1_loss_iou: 0.4836, stage1_loss_mask: 0.5206, stage2_loss_cls: 0.6837, stage2_pos_acc: 52.4800, stage2_loss_bbox: 0.2159, stage2_loss_iou: 0.3734, stage2_loss_mask: 0.4852, stage3_loss_cls: 0.5951, stage3_pos_acc: 61.7603, stage3_loss_bbox: 0.1961, stage3_loss_iou: 0.3521, stage3_loss_mask: 0.4728, stage4_loss_cls: 0.5488, stage4_pos_acc: 62.7284, stage4_loss_bbox: 0.1886, stage4_loss_iou: 0.3370, stage4_loss_mask: 0.4575, stage5_loss_cls: 0.5252, stage5_pos_acc: 68.1966, stage5_loss_bbox: 0.1919, stage5_loss_iou: 0.3383, stage5_loss_mask: 0.4508, loss: 11.9092\n",
      "2025-07-16 14:28:53,701 - mmdet - INFO - Epoch [19][300/750]\tlr: 2.500e-05, eta: 3:16:34, time: 0.350, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0966, stage0_pos_acc: 38.5750, stage0_loss_bbox: 0.5896, stage0_loss_iou: 1.0828, stage0_loss_mask: 1.2256, stage1_loss_cls: 0.7811, stage1_pos_acc: 52.9792, stage1_loss_bbox: 0.2667, stage1_loss_iou: 0.5281, stage1_loss_mask: 0.6946, stage2_loss_cls: 0.6750, stage2_pos_acc: 60.8894, stage2_loss_bbox: 0.2249, stage2_loss_iou: 0.4278, stage2_loss_mask: 0.6629, stage3_loss_cls: 0.5875, stage3_pos_acc: 64.6833, stage3_loss_bbox: 0.2076, stage3_loss_iou: 0.4017, stage3_loss_mask: 0.6103, stage4_loss_cls: 0.5344, stage4_pos_acc: 69.9808, stage4_loss_bbox: 0.2083, stage4_loss_iou: 0.3918, stage4_loss_mask: 0.6031, stage5_loss_cls: 0.5236, stage5_pos_acc: 69.3998, stage5_loss_bbox: 0.2038, stage5_loss_iou: 0.3848, stage5_loss_mask: 0.6172, loss: 13.5294\n",
      "2025-07-16 14:29:10,688 - mmdet - INFO - Epoch [19][350/750]\tlr: 2.500e-05, eta: 3:16:17, time: 0.340, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1075, stage0_pos_acc: 34.2177, stage0_loss_bbox: 0.5266, stage0_loss_iou: 1.0058, stage0_loss_mask: 0.8800, stage1_loss_cls: 0.7560, stage1_pos_acc: 47.2293, stage1_loss_bbox: 0.2479, stage1_loss_iou: 0.4993, stage1_loss_mask: 0.5343, stage2_loss_cls: 0.6751, stage2_pos_acc: 53.0579, stage2_loss_bbox: 0.1933, stage2_loss_iou: 0.3972, stage2_loss_mask: 0.4475, stage3_loss_cls: 0.5913, stage3_pos_acc: 55.9695, stage3_loss_bbox: 0.1825, stage3_loss_iou: 0.3633, stage3_loss_mask: 0.4245, stage4_loss_cls: 0.5248, stage4_pos_acc: 65.7831, stage4_loss_bbox: 0.1769, stage4_loss_iou: 0.3544, stage4_loss_mask: 0.4322, stage5_loss_cls: 0.5070, stage5_pos_acc: 68.6042, stage5_loss_bbox: 0.1709, stage5_loss_iou: 0.3455, stage5_loss_mask: 0.4087, loss: 11.7526\n",
      "2025-07-16 14:29:28,126 - mmdet - INFO - Epoch [19][400/750]\tlr: 2.500e-05, eta: 3:16:02, time: 0.349, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1086, stage0_pos_acc: 33.2214, stage0_loss_bbox: 0.5037, stage0_loss_iou: 1.0520, stage0_loss_mask: 0.8081, stage1_loss_cls: 0.7212, stage1_pos_acc: 55.4405, stage1_loss_bbox: 0.2221, stage1_loss_iou: 0.4701, stage1_loss_mask: 0.5120, stage2_loss_cls: 0.6243, stage2_pos_acc: 64.6952, stage2_loss_bbox: 0.1792, stage2_loss_iou: 0.3588, stage2_loss_mask: 0.4976, stage3_loss_cls: 0.5215, stage3_pos_acc: 62.9643, stage3_loss_bbox: 0.1695, stage3_loss_iou: 0.3320, stage3_loss_mask: 0.4692, stage4_loss_cls: 0.4842, stage4_pos_acc: 65.0119, stage4_loss_bbox: 0.1641, stage4_loss_iou: 0.3262, stage4_loss_mask: 0.4468, stage5_loss_cls: 0.4560, stage5_pos_acc: 70.6786, stage5_loss_bbox: 0.1744, stage5_loss_iou: 0.3298, stage5_loss_mask: 0.4584, loss: 11.3895\n",
      "2025-07-16 14:29:45,052 - mmdet - INFO - Epoch [19][450/750]\tlr: 2.500e-05, eta: 3:15:45, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1182, stage0_pos_acc: 36.7898, stage0_loss_bbox: 0.5447, stage0_loss_iou: 1.0945, stage0_loss_mask: 0.8397, stage1_loss_cls: 0.7293, stage1_pos_acc: 57.4017, stage1_loss_bbox: 0.2416, stage1_loss_iou: 0.5112, stage1_loss_mask: 0.4080, stage2_loss_cls: 0.6134, stage2_pos_acc: 63.4677, stage2_loss_bbox: 0.1781, stage2_loss_iou: 0.3625, stage2_loss_mask: 0.3612, stage3_loss_cls: 0.5092, stage3_pos_acc: 68.4494, stage3_loss_bbox: 0.1639, stage3_loss_iou: 0.3445, stage3_loss_mask: 0.3749, stage4_loss_cls: 0.4583, stage4_pos_acc: 69.1565, stage4_loss_bbox: 0.1605, stage4_loss_iou: 0.3356, stage4_loss_mask: 0.3754, stage5_loss_cls: 0.4186, stage5_pos_acc: 71.1629, stage5_loss_bbox: 0.1699, stage5_loss_iou: 0.3274, stage5_loss_mask: 0.3835, loss: 11.0240\n",
      "2025-07-16 14:30:02,106 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:30:02,106 - mmdet - INFO - Epoch [19][500/750]\tlr: 2.500e-05, eta: 3:15:28, time: 0.341, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1357, stage0_pos_acc: 35.6730, stage0_loss_bbox: 0.5331, stage0_loss_iou: 1.0580, stage0_loss_mask: 1.0681, stage1_loss_cls: 0.7463, stage1_pos_acc: 53.2602, stage1_loss_bbox: 0.2619, stage1_loss_iou: 0.5144, stage1_loss_mask: 0.6689, stage2_loss_cls: 0.6815, stage2_pos_acc: 55.8221, stage2_loss_bbox: 0.2064, stage2_loss_iou: 0.4170, stage2_loss_mask: 0.6219, stage3_loss_cls: 0.5744, stage3_pos_acc: 58.8501, stage3_loss_bbox: 0.1840, stage3_loss_iou: 0.3861, stage3_loss_mask: 0.5736, stage4_loss_cls: 0.5581, stage4_pos_acc: 61.8352, stage4_loss_bbox: 0.1737, stage4_loss_iou: 0.3777, stage4_loss_mask: 0.5783, stage5_loss_cls: 0.5440, stage5_pos_acc: 62.2417, stage5_loss_bbox: 0.1723, stage5_loss_iou: 0.3728, stage5_loss_mask: 0.5664, loss: 12.9746\n",
      "2025-07-16 14:30:19,090 - mmdet - INFO - Epoch [19][550/750]\tlr: 2.500e-05, eta: 3:15:12, time: 0.340, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0870, stage0_pos_acc: 41.1236, stage0_loss_bbox: 0.5612, stage0_loss_iou: 0.9851, stage0_loss_mask: 0.7904, stage1_loss_cls: 0.7942, stage1_pos_acc: 46.2964, stage1_loss_bbox: 0.2854, stage1_loss_iou: 0.4790, stage1_loss_mask: 0.5480, stage2_loss_cls: 0.7106, stage2_pos_acc: 52.3117, stage2_loss_bbox: 0.2395, stage2_loss_iou: 0.4040, stage2_loss_mask: 0.5012, stage3_loss_cls: 0.6373, stage3_pos_acc: 59.6736, stage3_loss_bbox: 0.2222, stage3_loss_iou: 0.3800, stage3_loss_mask: 0.4995, stage4_loss_cls: 0.5823, stage4_pos_acc: 63.4007, stage4_loss_bbox: 0.2177, stage4_loss_iou: 0.3743, stage4_loss_mask: 0.4793, stage5_loss_cls: 0.5628, stage5_pos_acc: 65.0864, stage5_loss_bbox: 0.2194, stage5_loss_iou: 0.3707, stage5_loss_mask: 0.4687, loss: 12.4000\n",
      "2025-07-16 14:30:35,768 - mmdet - INFO - Epoch [19][600/750]\tlr: 2.500e-05, eta: 3:14:54, time: 0.334, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0901, stage0_pos_acc: 41.4790, stage0_loss_bbox: 0.5371, stage0_loss_iou: 1.0717, stage0_loss_mask: 1.2481, stage1_loss_cls: 0.7388, stage1_pos_acc: 55.6582, stage1_loss_bbox: 0.2814, stage1_loss_iou: 0.5734, stage1_loss_mask: 0.8353, stage2_loss_cls: 0.6668, stage2_pos_acc: 59.2646, stage2_loss_bbox: 0.2325, stage2_loss_iou: 0.4679, stage2_loss_mask: 0.7664, stage3_loss_cls: 0.5741, stage3_pos_acc: 64.6901, stage3_loss_bbox: 0.2199, stage3_loss_iou: 0.4466, stage3_loss_mask: 0.7445, stage4_loss_cls: 0.5261, stage4_pos_acc: 69.8656, stage4_loss_bbox: 0.2068, stage4_loss_iou: 0.4366, stage4_loss_mask: 0.7493, stage5_loss_cls: 0.5274, stage5_pos_acc: 72.2695, stage5_loss_bbox: 0.2028, stage5_loss_iou: 0.4248, stage5_loss_mask: 0.7465, loss: 14.3151\n",
      "2025-07-16 14:30:52,498 - mmdet - INFO - Epoch [19][650/750]\tlr: 2.500e-05, eta: 3:14:37, time: 0.335, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0724, stage0_pos_acc: 38.2371, stage0_loss_bbox: 0.4962, stage0_loss_iou: 0.9943, stage0_loss_mask: 0.9216, stage1_loss_cls: 0.7715, stage1_pos_acc: 51.6959, stage1_loss_bbox: 0.2557, stage1_loss_iou: 0.4982, stage1_loss_mask: 0.5680, stage2_loss_cls: 0.6797, stage2_pos_acc: 56.3265, stage2_loss_bbox: 0.2160, stage2_loss_iou: 0.4024, stage2_loss_mask: 0.5288, stage3_loss_cls: 0.5993, stage3_pos_acc: 59.0237, stage3_loss_bbox: 0.1925, stage3_loss_iou: 0.3589, stage3_loss_mask: 0.4959, stage4_loss_cls: 0.5649, stage4_pos_acc: 60.9701, stage4_loss_bbox: 0.1865, stage4_loss_iou: 0.3485, stage4_loss_mask: 0.4943, stage5_loss_cls: 0.5436, stage5_pos_acc: 66.7922, stage5_loss_bbox: 0.2007, stage5_loss_iou: 0.3486, stage5_loss_mask: 0.4916, loss: 12.2301\n",
      "2025-07-16 14:31:09,133 - mmdet - INFO - Epoch [19][700/750]\tlr: 2.500e-05, eta: 3:14:20, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1102, stage0_pos_acc: 32.4905, stage0_loss_bbox: 0.5244, stage0_loss_iou: 0.9522, stage0_loss_mask: 0.8734, stage1_loss_cls: 0.7623, stage1_pos_acc: 52.7992, stage1_loss_bbox: 0.2406, stage1_loss_iou: 0.4581, stage1_loss_mask: 0.4406, stage2_loss_cls: 0.6549, stage2_pos_acc: 60.4683, stage2_loss_bbox: 0.1990, stage2_loss_iou: 0.3576, stage2_loss_mask: 0.3825, stage3_loss_cls: 0.5705, stage3_pos_acc: 59.9079, stage3_loss_bbox: 0.1820, stage3_loss_iou: 0.3283, stage3_loss_mask: 0.3672, stage4_loss_cls: 0.5280, stage4_pos_acc: 66.7595, stage4_loss_bbox: 0.1677, stage4_loss_iou: 0.3136, stage4_loss_mask: 0.3584, stage5_loss_cls: 0.4921, stage5_pos_acc: 70.5397, stage5_loss_bbox: 0.1792, stage5_loss_iou: 0.3134, stage5_loss_mask: 0.3284, loss: 11.0844\n",
      "2025-07-16 14:31:26,211 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:31:26,211 - mmdet - INFO - Epoch [19][750/750]\tlr: 2.500e-05, eta: 3:14:03, time: 0.342, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1417, stage0_pos_acc: 31.0861, stage0_loss_bbox: 0.5510, stage0_loss_iou: 1.0138, stage0_loss_mask: 0.9721, stage1_loss_cls: 0.7522, stage1_pos_acc: 53.3922, stage1_loss_bbox: 0.2511, stage1_loss_iou: 0.5089, stage1_loss_mask: 0.6130, stage2_loss_cls: 0.6699, stage2_pos_acc: 54.7316, stage2_loss_bbox: 0.2016, stage2_loss_iou: 0.3974, stage2_loss_mask: 0.5545, stage3_loss_cls: 0.5874, stage3_pos_acc: 58.0514, stage3_loss_bbox: 0.1894, stage3_loss_iou: 0.3782, stage3_loss_mask: 0.5707, stage4_loss_cls: 0.5361, stage4_pos_acc: 64.5041, stage4_loss_bbox: 0.1804, stage4_loss_iou: 0.3636, stage4_loss_mask: 0.5614, stage5_loss_cls: 0.5222, stage5_pos_acc: 69.5418, stage5_loss_bbox: 0.1915, stage5_loss_iou: 0.3658, stage5_loss_mask: 0.5631, loss: 12.6370\n",
      "2025-07-16 14:31:26,323 - mmdet - INFO - Saving checkpoint at 19 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 14:34:30,854 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.21s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.398\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.398\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.186\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.445\n",
      "2025-07-16 14:34:32,587 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.75s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.401\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.060\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.445\n",
      "2025-07-16 14:34:35,621 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:34:35,621 - mmdet - INFO - Epoch(val) [19][750]\tbbox_mAP: 0.0240, bbox_mAP_50: 0.0440, bbox_mAP_75: 0.0220, bbox_mAP_s: 0.0050, bbox_mAP_m: 0.0100, bbox_mAP_l: 0.0310, bbox_mAP_copypaste: 0.024 0.044 0.022 0.005 0.010 0.031, segm_mAP: 0.0240, segm_mAP_50: 0.0440, segm_mAP_75: 0.0240, segm_mAP_s: 0.0090, segm_mAP_m: 0.0110, segm_mAP_l: 0.0320, segm_mAP_copypaste: 0.024 0.044 0.024 0.009 0.011 0.032\n",
      "2025-07-16 14:34:54,615 - mmdet - INFO - Epoch [20][50/750]\tlr: 2.500e-05, eta: 3:13:52, time: 0.380, data_time: 0.053, memory: 11264, stage0_loss_cls: 1.1472, stage0_pos_acc: 32.1958, stage0_loss_bbox: 0.5493, stage0_loss_iou: 1.0604, stage0_loss_mask: 1.1236, stage1_loss_cls: 0.7325, stage1_pos_acc: 58.0016, stage1_loss_bbox: 0.2546, stage1_loss_iou: 0.5621, stage1_loss_mask: 0.7921, stage2_loss_cls: 0.6539, stage2_pos_acc: 65.2992, stage2_loss_bbox: 0.2097, stage2_loss_iou: 0.4483, stage2_loss_mask: 0.7332, stage3_loss_cls: 0.5618, stage3_pos_acc: 66.9614, stage3_loss_bbox: 0.2014, stage3_loss_iou: 0.4259, stage3_loss_mask: 0.7197, stage4_loss_cls: 0.5217, stage4_pos_acc: 69.1237, stage4_loss_bbox: 0.1971, stage4_loss_iou: 0.4210, stage4_loss_mask: 0.7154, stage5_loss_cls: 0.5073, stage5_pos_acc: 70.2338, stage5_loss_bbox: 0.1925, stage5_loss_iou: 0.4175, stage5_loss_mask: 0.7047, loss: 13.8530\n",
      "2025-07-16 14:35:11,897 - mmdet - INFO - Epoch [20][100/750]\tlr: 2.500e-05, eta: 3:13:36, time: 0.346, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1059, stage0_pos_acc: 38.5881, stage0_loss_bbox: 0.5233, stage0_loss_iou: 1.1216, stage0_loss_mask: 1.1038, stage1_loss_cls: 0.7023, stage1_pos_acc: 57.2345, stage1_loss_bbox: 0.2352, stage1_loss_iou: 0.5692, stage1_loss_mask: 0.7185, stage2_loss_cls: 0.6487, stage2_pos_acc: 61.5801, stage2_loss_bbox: 0.1917, stage2_loss_iou: 0.4478, stage2_loss_mask: 0.6889, stage3_loss_cls: 0.5348, stage3_pos_acc: 68.5151, stage3_loss_bbox: 0.1793, stage3_loss_iou: 0.4231, stage3_loss_mask: 0.6234, stage4_loss_cls: 0.4996, stage4_pos_acc: 72.0788, stage4_loss_bbox: 0.1686, stage4_loss_iou: 0.3968, stage4_loss_mask: 0.6104, stage5_loss_cls: 0.4758, stage5_pos_acc: 76.2663, stage5_loss_bbox: 0.1617, stage5_loss_iou: 0.3862, stage5_loss_mask: 0.6121, loss: 13.1288\n",
      "2025-07-16 14:35:28,651 - mmdet - INFO - Epoch [20][150/750]\tlr: 2.500e-05, eta: 3:13:18, time: 0.335, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1115, stage0_pos_acc: 29.2548, stage0_loss_bbox: 0.5184, stage0_loss_iou: 0.9536, stage0_loss_mask: 0.7959, stage1_loss_cls: 0.7485, stage1_pos_acc: 53.5278, stage1_loss_bbox: 0.2515, stage1_loss_iou: 0.4501, stage1_loss_mask: 0.4386, stage2_loss_cls: 0.6445, stage2_pos_acc: 59.2881, stage2_loss_bbox: 0.1963, stage2_loss_iou: 0.3560, stage2_loss_mask: 0.3778, stage3_loss_cls: 0.5518, stage3_pos_acc: 61.7421, stage3_loss_bbox: 0.1794, stage3_loss_iou: 0.3262, stage3_loss_mask: 0.3544, stage4_loss_cls: 0.5038, stage4_pos_acc: 66.8706, stage4_loss_bbox: 0.1748, stage4_loss_iou: 0.3221, stage4_loss_mask: 0.3572, stage5_loss_cls: 0.4892, stage5_pos_acc: 66.9706, stage5_loss_bbox: 0.1734, stage5_loss_iou: 0.3208, stage5_loss_mask: 0.3455, loss: 10.9414\n",
      "2025-07-16 14:35:45,560 - mmdet - INFO - Epoch [20][200/750]\tlr: 2.500e-05, eta: 3:13:02, time: 0.338, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1007, stage0_pos_acc: 37.9415, stage0_loss_bbox: 0.5378, stage0_loss_iou: 1.0149, stage0_loss_mask: 0.8220, stage1_loss_cls: 0.7392, stage1_pos_acc: 52.6757, stage1_loss_bbox: 0.2496, stage1_loss_iou: 0.4590, stage1_loss_mask: 0.3737, stage2_loss_cls: 0.6721, stage2_pos_acc: 60.4540, stage2_loss_bbox: 0.1948, stage2_loss_iou: 0.3520, stage2_loss_mask: 0.3288, stage3_loss_cls: 0.5705, stage3_pos_acc: 61.4084, stage3_loss_bbox: 0.1786, stage3_loss_iou: 0.3275, stage3_loss_mask: 0.3099, stage4_loss_cls: 0.5304, stage4_pos_acc: 66.2609, stage4_loss_bbox: 0.1667, stage4_loss_iou: 0.3155, stage4_loss_mask: 0.2864, stage5_loss_cls: 0.5165, stage5_pos_acc: 69.0685, stage5_loss_bbox: 0.1668, stage5_loss_iou: 0.3101, stage5_loss_mask: 0.2864, loss: 10.8097\n",
      "2025-07-16 14:36:02,564 - mmdet - INFO - Epoch [20][250/750]\tlr: 2.500e-05, eta: 3:12:45, time: 0.340, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1017, stage0_pos_acc: 34.7123, stage0_loss_bbox: 0.5491, stage0_loss_iou: 0.9700, stage0_loss_mask: 0.8085, stage1_loss_cls: 0.7195, stage1_pos_acc: 52.5094, stage1_loss_bbox: 0.2564, stage1_loss_iou: 0.4477, stage1_loss_mask: 0.3931, stage2_loss_cls: 0.6437, stage2_pos_acc: 61.0113, stage2_loss_bbox: 0.1956, stage2_loss_iou: 0.3276, stage2_loss_mask: 0.3670, stage3_loss_cls: 0.5689, stage3_pos_acc: 63.2650, stage3_loss_bbox: 0.1675, stage3_loss_iou: 0.2868, stage3_loss_mask: 0.3438, stage4_loss_cls: 0.5044, stage4_pos_acc: 63.5360, stage4_loss_bbox: 0.1755, stage4_loss_iou: 0.2870, stage4_loss_mask: 0.3561, stage5_loss_cls: 0.4866, stage5_pos_acc: 65.2650, stage5_loss_bbox: 0.1608, stage5_loss_iou: 0.2756, stage5_loss_mask: 0.3467, loss: 10.7396\n",
      "2025-07-16 14:36:19,499 - mmdet - INFO - Epoch [20][300/750]\tlr: 2.500e-05, eta: 3:12:28, time: 0.339, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1372, stage0_pos_acc: 31.1379, stage0_loss_bbox: 0.5931, stage0_loss_iou: 1.1118, stage0_loss_mask: 1.1107, stage1_loss_cls: 0.7699, stage1_pos_acc: 53.8442, stage1_loss_bbox: 0.2575, stage1_loss_iou: 0.5272, stage1_loss_mask: 0.5806, stage2_loss_cls: 0.6565, stage2_pos_acc: 59.7823, stage2_loss_bbox: 0.1923, stage2_loss_iou: 0.3929, stage2_loss_mask: 0.5519, stage3_loss_cls: 0.5663, stage3_pos_acc: 60.2846, stage3_loss_bbox: 0.1903, stage3_loss_iou: 0.3571, stage3_loss_mask: 0.4975, stage4_loss_cls: 0.4984, stage4_pos_acc: 68.4325, stage4_loss_bbox: 0.1836, stage4_loss_iou: 0.3410, stage4_loss_mask: 0.4811, stage5_loss_cls: 0.4701, stage5_pos_acc: 70.8673, stage5_loss_bbox: 0.1792, stage5_loss_iou: 0.3324, stage5_loss_mask: 0.4996, loss: 12.4782\n",
      "2025-07-16 14:36:36,436 - mmdet - INFO - Epoch [20][350/750]\tlr: 2.500e-05, eta: 3:12:12, time: 0.339, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0865, stage0_pos_acc: 40.1130, stage0_loss_bbox: 0.5596, stage0_loss_iou: 1.0717, stage0_loss_mask: 1.0075, stage1_loss_cls: 0.7475, stage1_pos_acc: 53.3504, stage1_loss_bbox: 0.2809, stage1_loss_iou: 0.5663, stage1_loss_mask: 0.7633, stage2_loss_cls: 0.6523, stage2_pos_acc: 59.5513, stage2_loss_bbox: 0.2301, stage2_loss_iou: 0.4417, stage2_loss_mask: 0.6950, stage3_loss_cls: 0.5885, stage3_pos_acc: 63.2902, stage3_loss_bbox: 0.2069, stage3_loss_iou: 0.4156, stage3_loss_mask: 0.6659, stage4_loss_cls: 0.5178, stage4_pos_acc: 69.8181, stage4_loss_bbox: 0.2029, stage4_loss_iou: 0.4036, stage4_loss_mask: 0.6483, stage5_loss_cls: 0.4937, stage5_pos_acc: 74.8651, stage5_loss_bbox: 0.1993, stage5_loss_iou: 0.4016, stage5_loss_mask: 0.6541, loss: 13.5005\n",
      "2025-07-16 14:36:53,522 - mmdet - INFO - Epoch [20][400/750]\tlr: 2.500e-05, eta: 3:11:55, time: 0.342, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0678, stage0_pos_acc: 36.0352, stage0_loss_bbox: 0.5170, stage0_loss_iou: 0.9084, stage0_loss_mask: 0.6505, stage1_loss_cls: 0.7306, stage1_pos_acc: 53.1260, stage1_loss_bbox: 0.2364, stage1_loss_iou: 0.4266, stage1_loss_mask: 0.4025, stage2_loss_cls: 0.6375, stage2_pos_acc: 57.6894, stage2_loss_bbox: 0.1927, stage2_loss_iou: 0.3485, stage2_loss_mask: 0.3623, stage3_loss_cls: 0.5329, stage3_pos_acc: 63.4971, stage3_loss_bbox: 0.1920, stage3_loss_iou: 0.3349, stage3_loss_mask: 0.3616, stage4_loss_cls: 0.4772, stage4_pos_acc: 64.2947, stage4_loss_bbox: 0.1817, stage4_loss_iou: 0.3315, stage4_loss_mask: 0.3697, stage5_loss_cls: 0.4688, stage5_pos_acc: 66.9527, stage5_loss_bbox: 0.1769, stage5_loss_iou: 0.3276, stage5_loss_mask: 0.3661, loss: 10.6016\n",
      "2025-07-16 14:37:10,862 - mmdet - INFO - Epoch [20][450/750]\tlr: 2.500e-05, eta: 3:11:39, time: 0.347, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0885, stage0_pos_acc: 37.8688, stage0_loss_bbox: 0.5141, stage0_loss_iou: 0.9785, stage0_loss_mask: 0.7606, stage1_loss_cls: 0.7166, stage1_pos_acc: 56.9985, stage1_loss_bbox: 0.2431, stage1_loss_iou: 0.4725, stage1_loss_mask: 0.4614, stage2_loss_cls: 0.6264, stage2_pos_acc: 58.7522, stage2_loss_bbox: 0.1905, stage2_loss_iou: 0.3761, stage2_loss_mask: 0.4182, stage3_loss_cls: 0.5254, stage3_pos_acc: 65.6866, stage3_loss_bbox: 0.1734, stage3_loss_iou: 0.3422, stage3_loss_mask: 0.3945, stage4_loss_cls: 0.4699, stage4_pos_acc: 69.1001, stage4_loss_bbox: 0.1807, stage4_loss_iou: 0.3296, stage4_loss_mask: 0.3984, stage5_loss_cls: 0.4637, stage5_pos_acc: 69.2937, stage5_loss_bbox: 0.1663, stage5_loss_iou: 0.3175, stage5_loss_mask: 0.3780, loss: 10.9861\n",
      "2025-07-16 14:37:27,351 - mmdet - INFO - Epoch [20][500/750]\tlr: 2.500e-05, eta: 3:11:21, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0701, stage0_pos_acc: 33.2092, stage0_loss_bbox: 0.5293, stage0_loss_iou: 1.0040, stage0_loss_mask: 0.9666, stage1_loss_cls: 0.7066, stage1_pos_acc: 54.6079, stage1_loss_bbox: 0.2734, stage1_loss_iou: 0.5199, stage1_loss_mask: 0.6472, stage2_loss_cls: 0.6210, stage2_pos_acc: 59.9665, stage2_loss_bbox: 0.2267, stage2_loss_iou: 0.4325, stage2_loss_mask: 0.5990, stage3_loss_cls: 0.5696, stage3_pos_acc: 60.2065, stage3_loss_bbox: 0.2050, stage3_loss_iou: 0.4025, stage3_loss_mask: 0.5869, stage4_loss_cls: 0.5169, stage4_pos_acc: 66.2247, stage4_loss_bbox: 0.2034, stage4_loss_iou: 0.3906, stage4_loss_mask: 0.5781, stage5_loss_cls: 0.4917, stage5_pos_acc: 69.5802, stage5_loss_bbox: 0.1977, stage5_loss_iou: 0.3856, stage5_loss_mask: 0.5609, loss: 12.6855\n",
      "2025-07-16 14:37:43,857 - mmdet - INFO - Epoch [20][550/750]\tlr: 2.500e-05, eta: 3:11:04, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1302, stage0_pos_acc: 29.2167, stage0_loss_bbox: 0.5447, stage0_loss_iou: 1.0471, stage0_loss_mask: 0.6153, stage1_loss_cls: 0.6955, stage1_pos_acc: 54.1833, stage1_loss_bbox: 0.2275, stage1_loss_iou: 0.4552, stage1_loss_mask: 0.4207, stage2_loss_cls: 0.6345, stage2_pos_acc: 54.8833, stage2_loss_bbox: 0.1798, stage2_loss_iou: 0.3403, stage2_loss_mask: 0.3814, stage3_loss_cls: 0.5235, stage3_pos_acc: 56.4167, stage3_loss_bbox: 0.1661, stage3_loss_iou: 0.3145, stage3_loss_mask: 0.3509, stage4_loss_cls: 0.4951, stage4_pos_acc: 64.1333, stage4_loss_bbox: 0.1571, stage4_loss_iou: 0.2912, stage4_loss_mask: 0.3058, stage5_loss_cls: 0.4693, stage5_pos_acc: 69.2833, stage5_loss_bbox: 0.1529, stage5_loss_iou: 0.2833, stage5_loss_mask: 0.3289, loss: 10.5110\n",
      "2025-07-16 14:38:00,356 - mmdet - INFO - Epoch [20][600/750]\tlr: 2.500e-05, eta: 3:10:46, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0802, stage0_pos_acc: 33.7139, stage0_loss_bbox: 0.5489, stage0_loss_iou: 1.0402, stage0_loss_mask: 0.9283, stage1_loss_cls: 0.7229, stage1_pos_acc: 55.8044, stage1_loss_bbox: 0.2686, stage1_loss_iou: 0.5044, stage1_loss_mask: 0.6026, stage2_loss_cls: 0.6122, stage2_pos_acc: 64.3737, stage2_loss_bbox: 0.2199, stage2_loss_iou: 0.4032, stage2_loss_mask: 0.5511, stage3_loss_cls: 0.5281, stage3_pos_acc: 68.1644, stage3_loss_bbox: 0.2030, stage3_loss_iou: 0.3830, stage3_loss_mask: 0.5175, stage4_loss_cls: 0.4893, stage4_pos_acc: 70.0983, stage4_loss_bbox: 0.2002, stage4_loss_iou: 0.3716, stage4_loss_mask: 0.5208, stage5_loss_cls: 0.4564, stage5_pos_acc: 74.3567, stage5_loss_bbox: 0.2039, stage5_loss_iou: 0.3757, stage5_loss_mask: 0.5305, loss: 12.2624\n",
      "2025-07-16 14:38:16,940 - mmdet - INFO - Epoch [20][650/750]\tlr: 2.500e-05, eta: 3:10:28, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0547, stage0_pos_acc: 38.3346, stage0_loss_bbox: 0.5822, stage0_loss_iou: 1.0347, stage0_loss_mask: 0.8849, stage1_loss_cls: 0.7151, stage1_pos_acc: 55.2974, stage1_loss_bbox: 0.2928, stage1_loss_iou: 0.5233, stage1_loss_mask: 0.5837, stage2_loss_cls: 0.6057, stage2_pos_acc: 59.6196, stage2_loss_bbox: 0.2274, stage2_loss_iou: 0.4140, stage2_loss_mask: 0.5249, stage3_loss_cls: 0.5463, stage3_pos_acc: 64.0153, stage3_loss_bbox: 0.2165, stage3_loss_iou: 0.3864, stage3_loss_mask: 0.4835, stage4_loss_cls: 0.5052, stage4_pos_acc: 69.9335, stage4_loss_bbox: 0.2031, stage4_loss_iou: 0.3675, stage4_loss_mask: 0.4948, stage5_loss_cls: 0.4837, stage5_pos_acc: 69.9177, stage5_loss_bbox: 0.1951, stage5_loss_iou: 0.3598, stage5_loss_mask: 0.4800, loss: 12.1653\n",
      "2025-07-16 14:38:34,017 - mmdet - INFO - Epoch [20][700/750]\tlr: 2.500e-05, eta: 3:10:12, time: 0.342, data_time: 0.019, memory: 11264, stage0_loss_cls: 1.0638, stage0_pos_acc: 31.3539, stage0_loss_bbox: 0.6373, stage0_loss_iou: 1.0660, stage0_loss_mask: 0.8654, stage1_loss_cls: 0.7260, stage1_pos_acc: 47.4501, stage1_loss_bbox: 0.2775, stage1_loss_iou: 0.5008, stage1_loss_mask: 0.4871, stage2_loss_cls: 0.6560, stage2_pos_acc: 55.0363, stage2_loss_bbox: 0.2083, stage2_loss_iou: 0.3678, stage2_loss_mask: 0.4456, stage3_loss_cls: 0.5712, stage3_pos_acc: 59.3912, stage3_loss_bbox: 0.1937, stage3_loss_iou: 0.3430, stage3_loss_mask: 0.4163, stage4_loss_cls: 0.5250, stage4_pos_acc: 62.0339, stage4_loss_bbox: 0.1819, stage4_loss_iou: 0.3203, stage4_loss_mask: 0.4267, stage5_loss_cls: 0.4945, stage5_pos_acc: 69.1029, stage5_loss_bbox: 0.1798, stage5_loss_iou: 0.3156, stage5_loss_mask: 0.3929, loss: 11.6623\n",
      "2025-07-16 14:38:50,617 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:38:50,617 - mmdet - INFO - Epoch [20][750/750]\tlr: 2.500e-05, eta: 3:09:54, time: 0.332, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0549, stage0_pos_acc: 35.6795, stage0_loss_bbox: 0.6306, stage0_loss_iou: 1.0962, stage0_loss_mask: 1.1539, stage1_loss_cls: 0.7234, stage1_pos_acc: 57.1805, stage1_loss_bbox: 0.3240, stage1_loss_iou: 0.5477, stage1_loss_mask: 0.8020, stage2_loss_cls: 0.6323, stage2_pos_acc: 63.6833, stage2_loss_bbox: 0.2710, stage2_loss_iou: 0.4416, stage2_loss_mask: 0.7580, stage3_loss_cls: 0.5198, stage3_pos_acc: 68.4967, stage3_loss_bbox: 0.2643, stage3_loss_iou: 0.4142, stage3_loss_mask: 0.7484, stage4_loss_cls: 0.4870, stage4_pos_acc: 74.4626, stage4_loss_bbox: 0.2508, stage4_loss_iou: 0.3977, stage4_loss_mask: 0.7451, stage5_loss_cls: 0.4677, stage5_pos_acc: 74.5041, stage5_loss_bbox: 0.2519, stage5_loss_iou: 0.3937, stage5_loss_mask: 0.7389, loss: 14.1150\n",
      "2025-07-16 14:38:50,742 - mmdet - INFO - Saving checkpoint at 20 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 87s, ETA:     0s2025-07-16 14:41:54,912 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.26s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.097\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.375\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.377\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.377\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.147\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.217\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.413\n",
      "2025-07-16 14:41:56,685 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.82s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.114\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.394\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.396\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.396\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.235\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.432\n",
      "2025-07-16 14:41:59,783 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:41:59,783 - mmdet - INFO - Epoch(val) [20][750]\tbbox_mAP: 0.0290, bbox_mAP_50: 0.0520, bbox_mAP_75: 0.0270, bbox_mAP_s: 0.0970, bbox_mAP_m: 0.0100, bbox_mAP_l: 0.0350, bbox_mAP_copypaste: 0.029 0.052 0.027 0.097 0.010 0.035, segm_mAP: 0.0300, segm_mAP_50: 0.0510, segm_mAP_75: 0.0300, segm_mAP_s: 0.1140, segm_mAP_m: 0.0100, segm_mAP_l: 0.0370, segm_mAP_copypaste: 0.030 0.051 0.030 0.114 0.010 0.037\n",
      "2025-07-16 14:42:18,627 - mmdet - INFO - Epoch [21][50/750]\tlr: 2.500e-05, eta: 3:09:42, time: 0.377, data_time: 0.053, memory: 11264, stage0_loss_cls: 1.0980, stage0_pos_acc: 30.0248, stage0_loss_bbox: 0.5755, stage0_loss_iou: 1.0943, stage0_loss_mask: 1.0123, stage1_loss_cls: 0.7159, stage1_pos_acc: 53.0049, stage1_loss_bbox: 0.2557, stage1_loss_iou: 0.5199, stage1_loss_mask: 0.4844, stage2_loss_cls: 0.6047, stage2_pos_acc: 60.9269, stage2_loss_bbox: 0.1874, stage2_loss_iou: 0.3802, stage2_loss_mask: 0.4125, stage3_loss_cls: 0.5102, stage3_pos_acc: 70.5624, stage3_loss_bbox: 0.1718, stage3_loss_iou: 0.3461, stage3_loss_mask: 0.4038, stage4_loss_cls: 0.4248, stage4_pos_acc: 76.0176, stage4_loss_bbox: 0.1675, stage4_loss_iou: 0.3374, stage4_loss_mask: 0.4026, stage5_loss_cls: 0.4062, stage5_pos_acc: 76.3994, stage5_loss_bbox: 0.1719, stage5_loss_iou: 0.3417, stage5_loss_mask: 0.4118, loss: 11.4365\n",
      "2025-07-16 14:42:35,066 - mmdet - INFO - Epoch [21][100/750]\tlr: 2.500e-05, eta: 3:09:24, time: 0.329, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0703, stage0_pos_acc: 43.4071, stage0_loss_bbox: 0.5437, stage0_loss_iou: 1.1170, stage0_loss_mask: 1.0731, stage1_loss_cls: 0.6837, stage1_pos_acc: 55.7547, stage1_loss_bbox: 0.2699, stage1_loss_iou: 0.5260, stage1_loss_mask: 0.7682, stage2_loss_cls: 0.5838, stage2_pos_acc: 66.9612, stage2_loss_bbox: 0.2044, stage2_loss_iou: 0.4288, stage2_loss_mask: 0.7134, stage3_loss_cls: 0.4885, stage3_pos_acc: 72.3365, stage3_loss_bbox: 0.1905, stage3_loss_iou: 0.3868, stage3_loss_mask: 0.6621, stage4_loss_cls: 0.4067, stage4_pos_acc: 76.5460, stage4_loss_bbox: 0.1761, stage4_loss_iou: 0.3746, stage4_loss_mask: 0.6462, stage5_loss_cls: 0.3795, stage5_pos_acc: 81.0399, stage5_loss_bbox: 0.1811, stage5_loss_iou: 0.3712, stage5_loss_mask: 0.6412, loss: 12.8866\n",
      "2025-07-16 14:42:51,498 - mmdet - INFO - Epoch [21][150/750]\tlr: 2.500e-05, eta: 3:09:06, time: 0.329, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1158, stage0_pos_acc: 31.4078, stage0_loss_bbox: 0.5330, stage0_loss_iou: 1.0288, stage0_loss_mask: 0.8927, stage1_loss_cls: 0.7320, stage1_pos_acc: 55.9864, stage1_loss_bbox: 0.2518, stage1_loss_iou: 0.5035, stage1_loss_mask: 0.5668, stage2_loss_cls: 0.6311, stage2_pos_acc: 61.6920, stage2_loss_bbox: 0.2016, stage2_loss_iou: 0.3757, stage2_loss_mask: 0.5283, stage3_loss_cls: 0.5108, stage3_pos_acc: 64.0131, stage3_loss_bbox: 0.1908, stage3_loss_iou: 0.3540, stage3_loss_mask: 0.5164, stage4_loss_cls: 0.4567, stage4_pos_acc: 69.5023, stage4_loss_bbox: 0.1781, stage4_loss_iou: 0.3438, stage4_loss_mask: 0.5267, stage5_loss_cls: 0.4326, stage5_pos_acc: 72.0571, stage5_loss_bbox: 0.1703, stage5_loss_iou: 0.3382, stage5_loss_mask: 0.5208, loss: 11.9003\n",
      "2025-07-16 14:43:08,019 - mmdet - INFO - Epoch [21][200/750]\tlr: 2.500e-05, eta: 3:08:48, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0667, stage0_pos_acc: 39.6690, stage0_loss_bbox: 0.5354, stage0_loss_iou: 1.0211, stage0_loss_mask: 0.8976, stage1_loss_cls: 0.7070, stage1_pos_acc: 56.6540, stage1_loss_bbox: 0.2599, stage1_loss_iou: 0.4853, stage1_loss_mask: 0.6425, stage2_loss_cls: 0.6088, stage2_pos_acc: 60.7325, stage2_loss_bbox: 0.2102, stage2_loss_iou: 0.3939, stage2_loss_mask: 0.6157, stage3_loss_cls: 0.4949, stage3_pos_acc: 73.9873, stage3_loss_bbox: 0.1916, stage3_loss_iou: 0.3781, stage3_loss_mask: 0.6145, stage4_loss_cls: 0.4386, stage4_pos_acc: 72.7016, stage4_loss_bbox: 0.1893, stage4_loss_iou: 0.3719, stage4_loss_mask: 0.6073, stage5_loss_cls: 0.4198, stage5_pos_acc: 75.5976, stage5_loss_bbox: 0.1910, stage5_loss_iou: 0.3708, stage5_loss_mask: 0.6142, loss: 12.3260\n",
      "2025-07-16 14:43:24,688 - mmdet - INFO - Epoch [21][250/750]\tlr: 2.500e-05, eta: 3:08:31, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0918, stage0_pos_acc: 33.7513, stage0_loss_bbox: 0.4918, stage0_loss_iou: 0.9514, stage0_loss_mask: 0.6928, stage1_loss_cls: 0.7113, stage1_pos_acc: 53.5846, stage1_loss_bbox: 0.2261, stage1_loss_iou: 0.4285, stage1_loss_mask: 0.4718, stage2_loss_cls: 0.5898, stage2_pos_acc: 63.2679, stage2_loss_bbox: 0.1901, stage2_loss_iou: 0.3488, stage2_loss_mask: 0.4376, stage3_loss_cls: 0.4941, stage3_pos_acc: 67.4013, stage3_loss_bbox: 0.1861, stage3_loss_iou: 0.3401, stage3_loss_mask: 0.4553, stage4_loss_cls: 0.4454, stage4_pos_acc: 74.2385, stage4_loss_bbox: 0.1770, stage4_loss_iou: 0.3300, stage4_loss_mask: 0.4302, stage5_loss_cls: 0.4239, stage5_pos_acc: 76.3423, stage5_loss_bbox: 0.1731, stage5_loss_iou: 0.3216, stage5_loss_mask: 0.4254, loss: 10.8341\n",
      "2025-07-16 14:43:41,501 - mmdet - INFO - Epoch [21][300/750]\tlr: 2.500e-05, eta: 3:08:14, time: 0.336, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0740, stage0_pos_acc: 38.1747, stage0_loss_bbox: 0.5560, stage0_loss_iou: 1.0213, stage0_loss_mask: 0.8826, stage1_loss_cls: 0.7199, stage1_pos_acc: 56.5213, stage1_loss_bbox: 0.2854, stage1_loss_iou: 0.5038, stage1_loss_mask: 0.5930, stage2_loss_cls: 0.6311, stage2_pos_acc: 62.9102, stage2_loss_bbox: 0.2543, stage2_loss_iou: 0.4134, stage2_loss_mask: 0.5815, stage3_loss_cls: 0.5448, stage3_pos_acc: 67.7450, stage3_loss_bbox: 0.2271, stage3_loss_iou: 0.3945, stage3_loss_mask: 0.5322, stage4_loss_cls: 0.4989, stage4_pos_acc: 71.7424, stage4_loss_bbox: 0.2141, stage4_loss_iou: 0.3889, stage4_loss_mask: 0.5331, stage5_loss_cls: 0.4839, stage5_pos_acc: 73.0188, stage5_loss_bbox: 0.2233, stage5_loss_iou: 0.3914, stage5_loss_mask: 0.5437, loss: 12.4923\n",
      "2025-07-16 14:43:57,904 - mmdet - INFO - Epoch [21][350/750]\tlr: 2.500e-05, eta: 3:07:56, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0747, stage0_pos_acc: 31.4584, stage0_loss_bbox: 0.5232, stage0_loss_iou: 0.9577, stage0_loss_mask: 0.7354, stage1_loss_cls: 0.6927, stage1_pos_acc: 49.7887, stage1_loss_bbox: 0.2647, stage1_loss_iou: 0.4942, stage1_loss_mask: 0.5237, stage2_loss_cls: 0.6143, stage2_pos_acc: 61.9692, stage2_loss_bbox: 0.2154, stage2_loss_iou: 0.3925, stage2_loss_mask: 0.4788, stage3_loss_cls: 0.4994, stage3_pos_acc: 65.9525, stage3_loss_bbox: 0.2130, stage3_loss_iou: 0.3768, stage3_loss_mask: 0.4591, stage4_loss_cls: 0.4742, stage4_pos_acc: 69.1462, stage4_loss_bbox: 0.2069, stage4_loss_iou: 0.3588, stage4_loss_mask: 0.4301, stage5_loss_cls: 0.4658, stage5_pos_acc: 70.0192, stage5_loss_bbox: 0.2070, stage5_loss_iou: 0.3592, stage5_loss_mask: 0.4378, loss: 11.4554\n",
      "2025-07-16 14:44:14,727 - mmdet - INFO - Epoch [21][400/750]\tlr: 2.500e-05, eta: 3:07:39, time: 0.336, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0666, stage0_pos_acc: 35.7441, stage0_loss_bbox: 0.5447, stage0_loss_iou: 1.0162, stage0_loss_mask: 0.8681, stage1_loss_cls: 0.7321, stage1_pos_acc: 60.9308, stage1_loss_bbox: 0.2540, stage1_loss_iou: 0.4929, stage1_loss_mask: 0.5831, stage2_loss_cls: 0.6074, stage2_pos_acc: 65.8810, stage2_loss_bbox: 0.2124, stage2_loss_iou: 0.3834, stage2_loss_mask: 0.5152, stage3_loss_cls: 0.5265, stage3_pos_acc: 65.4507, stage3_loss_bbox: 0.1951, stage3_loss_iou: 0.3510, stage3_loss_mask: 0.4979, stage4_loss_cls: 0.4745, stage4_pos_acc: 69.1072, stage4_loss_bbox: 0.1848, stage4_loss_iou: 0.3385, stage4_loss_mask: 0.4927, stage5_loss_cls: 0.4517, stage5_pos_acc: 74.9149, stage5_loss_bbox: 0.1832, stage5_loss_iou: 0.3364, stage5_loss_mask: 0.4993, loss: 11.8078\n",
      "2025-07-16 14:44:31,057 - mmdet - INFO - Epoch [21][450/750]\tlr: 2.500e-05, eta: 3:07:21, time: 0.327, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0706, stage0_pos_acc: 32.7778, stage0_loss_bbox: 0.4912, stage0_loss_iou: 0.9949, stage0_loss_mask: 0.9027, stage1_loss_cls: 0.6841, stage1_pos_acc: 58.5328, stage1_loss_bbox: 0.2163, stage1_loss_iou: 0.4994, stage1_loss_mask: 0.6452, stage2_loss_cls: 0.5900, stage2_pos_acc: 63.2455, stage2_loss_bbox: 0.1800, stage2_loss_iou: 0.4173, stage2_loss_mask: 0.6176, stage3_loss_cls: 0.5033, stage3_pos_acc: 64.7282, stage3_loss_bbox: 0.1666, stage3_loss_iou: 0.3855, stage3_loss_mask: 0.5137, stage4_loss_cls: 0.4538, stage4_pos_acc: 69.6051, stage4_loss_bbox: 0.1579, stage4_loss_iou: 0.3697, stage4_loss_mask: 0.5372, stage5_loss_cls: 0.4518, stage5_pos_acc: 73.5575, stage5_loss_bbox: 0.1541, stage5_loss_iou: 0.3600, stage5_loss_mask: 0.5130, loss: 11.8761\n",
      "2025-07-16 14:44:47,604 - mmdet - INFO - Epoch [21][500/750]\tlr: 2.500e-05, eta: 3:07:03, time: 0.331, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0858, stage0_pos_acc: 35.2293, stage0_loss_bbox: 0.5774, stage0_loss_iou: 1.0586, stage0_loss_mask: 0.9013, stage1_loss_cls: 0.7138, stage1_pos_acc: 53.6231, stage1_loss_bbox: 0.2929, stage1_loss_iou: 0.5353, stage1_loss_mask: 0.5800, stage2_loss_cls: 0.6429, stage2_pos_acc: 59.1508, stage2_loss_bbox: 0.2296, stage2_loss_iou: 0.4198, stage2_loss_mask: 0.5453, stage3_loss_cls: 0.5500, stage3_pos_acc: 65.3654, stage3_loss_bbox: 0.2206, stage3_loss_iou: 0.3929, stage3_loss_mask: 0.5441, stage4_loss_cls: 0.4874, stage4_pos_acc: 71.3779, stage4_loss_bbox: 0.2125, stage4_loss_iou: 0.3893, stage4_loss_mask: 0.5393, stage5_loss_cls: 0.4639, stage5_pos_acc: 74.6789, stage5_loss_bbox: 0.2128, stage5_loss_iou: 0.3884, stage5_loss_mask: 0.5412, loss: 12.5250\n",
      "2025-07-16 14:45:04,842 - mmdet - INFO - Epoch [21][550/750]\tlr: 2.500e-05, eta: 3:06:47, time: 0.345, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0904, stage0_pos_acc: 36.4383, stage0_loss_bbox: 0.5152, stage0_loss_iou: 0.9714, stage0_loss_mask: 0.6924, stage1_loss_cls: 0.7082, stage1_pos_acc: 57.1229, stage1_loss_bbox: 0.2324, stage1_loss_iou: 0.4295, stage1_loss_mask: 0.4553, stage2_loss_cls: 0.6141, stage2_pos_acc: 61.4225, stage2_loss_bbox: 0.1994, stage2_loss_iou: 0.3345, stage2_loss_mask: 0.4102, stage3_loss_cls: 0.5135, stage3_pos_acc: 64.7038, stage3_loss_bbox: 0.1761, stage3_loss_iou: 0.3154, stage3_loss_mask: 0.4007, stage4_loss_cls: 0.4508, stage4_pos_acc: 73.1723, stage4_loss_bbox: 0.1723, stage4_loss_iou: 0.3038, stage4_loss_mask: 0.3663, stage5_loss_cls: 0.4244, stage5_pos_acc: 76.5503, stage5_loss_bbox: 0.1727, stage5_loss_iou: 0.3007, stage5_loss_mask: 0.3798, loss: 10.6295\n",
      "2025-07-16 14:45:22,240 - mmdet - INFO - Epoch [21][600/750]\tlr: 2.500e-05, eta: 3:06:32, time: 0.348, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0702, stage0_pos_acc: 36.3354, stage0_loss_bbox: 0.4912, stage0_loss_iou: 0.8745, stage0_loss_mask: 0.6202, stage1_loss_cls: 0.6830, stage1_pos_acc: 59.1758, stage1_loss_bbox: 0.2526, stage1_loss_iou: 0.4214, stage1_loss_mask: 0.4427, stage2_loss_cls: 0.6054, stage2_pos_acc: 62.2659, stage2_loss_bbox: 0.2148, stage2_loss_iou: 0.3298, stage2_loss_mask: 0.3823, stage3_loss_cls: 0.4932, stage3_pos_acc: 67.0654, stage3_loss_bbox: 0.1988, stage3_loss_iou: 0.3236, stage3_loss_mask: 0.3713, stage4_loss_cls: 0.4438, stage4_pos_acc: 71.2690, stage4_loss_bbox: 0.2012, stage4_loss_iou: 0.3178, stage4_loss_mask: 0.3636, stage5_loss_cls: 0.4166, stage5_pos_acc: 74.6671, stage5_loss_bbox: 0.1987, stage5_loss_iou: 0.3121, stage5_loss_mask: 0.3589, loss: 10.3875\n",
      "2025-07-16 14:45:39,401 - mmdet - INFO - Epoch [21][650/750]\tlr: 2.500e-05, eta: 3:06:15, time: 0.343, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1100, stage0_pos_acc: 33.3393, stage0_loss_bbox: 0.5563, stage0_loss_iou: 1.0178, stage0_loss_mask: 0.9002, stage1_loss_cls: 0.6772, stage1_pos_acc: 61.3125, stage1_loss_bbox: 0.2382, stage1_loss_iou: 0.4722, stage1_loss_mask: 0.5152, stage2_loss_cls: 0.5552, stage2_pos_acc: 65.9041, stage2_loss_bbox: 0.1994, stage2_loss_iou: 0.3913, stage2_loss_mask: 0.4874, stage3_loss_cls: 0.4726, stage3_pos_acc: 70.3429, stage3_loss_bbox: 0.1849, stage3_loss_iou: 0.3661, stage3_loss_mask: 0.4525, stage4_loss_cls: 0.4046, stage4_pos_acc: 77.7541, stage4_loss_bbox: 0.1799, stage4_loss_iou: 0.3490, stage4_loss_mask: 0.3916, stage5_loss_cls: 0.3743, stage5_pos_acc: 79.1874, stage5_loss_bbox: 0.1868, stage5_loss_iou: 0.3488, stage5_loss_mask: 0.3921, loss: 11.2238\n",
      "2025-07-16 14:45:56,593 - mmdet - INFO - Epoch [21][700/750]\tlr: 2.500e-05, eta: 3:05:59, time: 0.344, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0991, stage0_pos_acc: 32.2719, stage0_loss_bbox: 0.5692, stage0_loss_iou: 1.0303, stage0_loss_mask: 0.8611, stage1_loss_cls: 0.7319, stage1_pos_acc: 54.6156, stage1_loss_bbox: 0.2487, stage1_loss_iou: 0.4919, stage1_loss_mask: 0.4606, stage2_loss_cls: 0.6444, stage2_pos_acc: 56.1045, stage2_loss_bbox: 0.1928, stage2_loss_iou: 0.3650, stage2_loss_mask: 0.3849, stage3_loss_cls: 0.5749, stage3_pos_acc: 61.5418, stage3_loss_bbox: 0.1728, stage3_loss_iou: 0.3258, stage3_loss_mask: 0.3648, stage4_loss_cls: 0.5285, stage4_pos_acc: 62.7530, stage4_loss_bbox: 0.1623, stage4_loss_iou: 0.3174, stage4_loss_mask: 0.3106, stage5_loss_cls: 0.5120, stage5_pos_acc: 67.8037, stage5_loss_bbox: 0.1561, stage5_loss_iou: 0.3038, stage5_loss_mask: 0.3135, loss: 11.1226\n",
      "2025-07-16 14:46:13,433 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:46:13,433 - mmdet - INFO - Epoch [21][750/750]\tlr: 2.500e-05, eta: 3:05:42, time: 0.337, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0912, stage0_pos_acc: 32.6190, stage0_loss_bbox: 0.5632, stage0_loss_iou: 1.0697, stage0_loss_mask: 0.9265, stage1_loss_cls: 0.7070, stage1_pos_acc: 58.6500, stage1_loss_bbox: 0.2507, stage1_loss_iou: 0.5074, stage1_loss_mask: 0.5392, stage2_loss_cls: 0.6049, stage2_pos_acc: 64.3714, stage2_loss_bbox: 0.1883, stage2_loss_iou: 0.3926, stage2_loss_mask: 0.4997, stage3_loss_cls: 0.5195, stage3_pos_acc: 68.6548, stage3_loss_bbox: 0.1839, stage3_loss_iou: 0.3757, stage3_loss_mask: 0.4904, stage4_loss_cls: 0.4627, stage4_pos_acc: 72.2405, stage4_loss_bbox: 0.1756, stage4_loss_iou: 0.3602, stage4_loss_mask: 0.4852, stage5_loss_cls: 0.4363, stage5_pos_acc: 74.0405, stage5_loss_bbox: 0.1878, stage5_loss_iou: 0.3528, stage5_loss_mask: 0.4857, loss: 11.8562\n",
      "2025-07-16 14:46:13,530 - mmdet - INFO - Saving checkpoint at 21 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 14:49:16,772 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.23s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.045\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.449\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.250\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.497\n",
      "2025-07-16 14:49:18,657 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.76s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.066\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.034\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.046\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.452\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.452\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.265\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.493\n",
      "2025-07-16 14:49:21,559 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:49:21,560 - mmdet - INFO - Epoch(val) [21][750]\tbbox_mAP: 0.0360, bbox_mAP_50: 0.0640, bbox_mAP_75: 0.0360, bbox_mAP_s: 0.0350, bbox_mAP_m: 0.0160, bbox_mAP_l: 0.0450, bbox_mAP_copypaste: 0.036 0.064 0.036 0.035 0.016 0.045, segm_mAP: 0.0360, segm_mAP_50: 0.0660, segm_mAP_75: 0.0340, segm_mAP_s: 0.0370, segm_mAP_m: 0.0150, segm_mAP_l: 0.0460, segm_mAP_copypaste: 0.036 0.066 0.034 0.037 0.015 0.046\n",
      "2025-07-16 14:49:40,487 - mmdet - INFO - Epoch [22][50/750]\tlr: 2.500e-05, eta: 3:05:29, time: 0.378, data_time: 0.053, memory: 11264, stage0_loss_cls: 1.0595, stage0_pos_acc: 37.9619, stage0_loss_bbox: 0.5477, stage0_loss_iou: 1.0377, stage0_loss_mask: 0.9224, stage1_loss_cls: 0.6880, stage1_pos_acc: 54.5437, stage1_loss_bbox: 0.2384, stage1_loss_iou: 0.4610, stage1_loss_mask: 0.5125, stage2_loss_cls: 0.5729, stage2_pos_acc: 65.7087, stage2_loss_bbox: 0.1840, stage2_loss_iou: 0.3687, stage2_loss_mask: 0.4934, stage3_loss_cls: 0.4519, stage3_pos_acc: 70.8849, stage3_loss_bbox: 0.1736, stage3_loss_iou: 0.3450, stage3_loss_mask: 0.4825, stage4_loss_cls: 0.3935, stage4_pos_acc: 76.0929, stage4_loss_bbox: 0.1672, stage4_loss_iou: 0.3362, stage4_loss_mask: 0.4900, stage5_loss_cls: 0.3644, stage5_pos_acc: 79.4206, stage5_loss_bbox: 0.1633, stage5_loss_iou: 0.3279, stage5_loss_mask: 0.4822, loss: 11.2640\n",
      "2025-07-16 14:49:56,872 - mmdet - INFO - Epoch [22][100/750]\tlr: 2.500e-05, eta: 3:05:12, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0898, stage0_pos_acc: 31.9196, stage0_loss_bbox: 0.5052, stage0_loss_iou: 0.9395, stage0_loss_mask: 0.6389, stage1_loss_cls: 0.6786, stage1_pos_acc: 57.9165, stage1_loss_bbox: 0.2444, stage1_loss_iou: 0.4138, stage1_loss_mask: 0.4356, stage2_loss_cls: 0.5664, stage2_pos_acc: 68.8424, stage2_loss_bbox: 0.1910, stage2_loss_iou: 0.3304, stage2_loss_mask: 0.3641, stage3_loss_cls: 0.4615, stage3_pos_acc: 74.0221, stage3_loss_bbox: 0.1786, stage3_loss_iou: 0.3153, stage3_loss_mask: 0.3483, stage4_loss_cls: 0.4062, stage4_pos_acc: 74.6843, stage4_loss_bbox: 0.1716, stage4_loss_iou: 0.3047, stage4_loss_mask: 0.3328, stage5_loss_cls: 0.3829, stage5_pos_acc: 76.8677, stage5_loss_bbox: 0.1710, stage5_loss_iou: 0.2997, stage5_loss_mask: 0.3268, loss: 10.0972\n",
      "2025-07-16 14:50:13,182 - mmdet - INFO - Epoch [22][150/750]\tlr: 2.500e-05, eta: 3:04:53, time: 0.326, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1141, stage0_pos_acc: 32.5716, stage0_loss_bbox: 0.5412, stage0_loss_iou: 1.0863, stage0_loss_mask: 0.8913, stage1_loss_cls: 0.6848, stage1_pos_acc: 56.2760, stage1_loss_bbox: 0.2271, stage1_loss_iou: 0.4692, stage1_loss_mask: 0.5035, stage2_loss_cls: 0.5957, stage2_pos_acc: 65.4712, stage2_loss_bbox: 0.1823, stage2_loss_iou: 0.3699, stage2_loss_mask: 0.4861, stage3_loss_cls: 0.4991, stage3_pos_acc: 64.2391, stage3_loss_bbox: 0.1705, stage3_loss_iou: 0.3479, stage3_loss_mask: 0.4693, stage4_loss_cls: 0.4379, stage4_pos_acc: 73.0034, stage4_loss_bbox: 0.1690, stage4_loss_iou: 0.3352, stage4_loss_mask: 0.4869, stage5_loss_cls: 0.4149, stage5_pos_acc: 76.3201, stage5_loss_bbox: 0.1699, stage5_loss_iou: 0.3343, stage5_loss_mask: 0.4868, loss: 11.4732\n",
      "2025-07-16 14:50:29,546 - mmdet - INFO - Epoch [22][200/750]\tlr: 2.500e-05, eta: 3:04:35, time: 0.327, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1498, stage0_pos_acc: 35.3567, stage0_loss_bbox: 0.5258, stage0_loss_iou: 1.1246, stage0_loss_mask: 0.7950, stage1_loss_cls: 0.7112, stage1_pos_acc: 53.1114, stage1_loss_bbox: 0.1995, stage1_loss_iou: 0.4912, stage1_loss_mask: 0.4452, stage2_loss_cls: 0.5865, stage2_pos_acc: 64.2081, stage2_loss_bbox: 0.1567, stage2_loss_iou: 0.3641, stage2_loss_mask: 0.3798, stage3_loss_cls: 0.4857, stage3_pos_acc: 69.9686, stage3_loss_bbox: 0.1413, stage3_loss_iou: 0.3333, stage3_loss_mask: 0.3399, stage4_loss_cls: 0.4185, stage4_pos_acc: 78.5908, stage4_loss_bbox: 0.1382, stage4_loss_iou: 0.3232, stage4_loss_mask: 0.3365, stage5_loss_cls: 0.4025, stage5_pos_acc: 78.1089, stage5_loss_bbox: 0.1376, stage5_loss_iou: 0.3145, stage5_loss_mask: 0.3218, loss: 10.6226\n",
      "2025-07-16 14:50:45,958 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:50:45,958 - mmdet - INFO - Epoch [22][250/750]\tlr: 2.500e-05, eta: 3:04:18, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0716, stage0_pos_acc: 35.0774, stage0_loss_bbox: 0.5132, stage0_loss_iou: 0.9991, stage0_loss_mask: 0.8455, stage1_loss_cls: 0.7051, stage1_pos_acc: 54.9712, stage1_loss_bbox: 0.2361, stage1_loss_iou: 0.4651, stage1_loss_mask: 0.5147, stage2_loss_cls: 0.5939, stage2_pos_acc: 60.8045, stage2_loss_bbox: 0.1860, stage2_loss_iou: 0.3595, stage2_loss_mask: 0.4134, stage3_loss_cls: 0.4940, stage3_pos_acc: 68.3986, stage3_loss_bbox: 0.1769, stage3_loss_iou: 0.3427, stage3_loss_mask: 0.4333, stage4_loss_cls: 0.4248, stage4_pos_acc: 74.1888, stage4_loss_bbox: 0.1729, stage4_loss_iou: 0.3326, stage4_loss_mask: 0.4248, stage5_loss_cls: 0.4120, stage5_pos_acc: 75.4403, stage5_loss_bbox: 0.1606, stage5_loss_iou: 0.3213, stage5_loss_mask: 0.4112, loss: 11.0103\n",
      "2025-07-16 14:51:02,366 - mmdet - INFO - Epoch [22][300/750]\tlr: 2.500e-05, eta: 3:04:00, time: 0.328, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0584, stage0_pos_acc: 38.5095, stage0_loss_bbox: 0.4935, stage0_loss_iou: 0.9098, stage0_loss_mask: 0.6250, stage1_loss_cls: 0.6801, stage1_pos_acc: 53.6754, stage1_loss_bbox: 0.2270, stage1_loss_iou: 0.4014, stage1_loss_mask: 0.4333, stage2_loss_cls: 0.5909, stage2_pos_acc: 65.7499, stage2_loss_bbox: 0.1878, stage2_loss_iou: 0.3193, stage2_loss_mask: 0.3953, stage3_loss_cls: 0.4874, stage3_pos_acc: 67.6375, stage3_loss_bbox: 0.1713, stage3_loss_iou: 0.3009, stage3_loss_mask: 0.3796, stage4_loss_cls: 0.4321, stage4_pos_acc: 73.3597, stage4_loss_bbox: 0.1710, stage4_loss_iou: 0.2975, stage4_loss_mask: 0.3756, stage5_loss_cls: 0.4002, stage5_pos_acc: 75.5606, stage5_loss_bbox: 0.1696, stage5_loss_iou: 0.2928, stage5_loss_mask: 0.3824, loss: 10.1823\n",
      "2025-07-16 14:51:18,941 - mmdet - INFO - Epoch [22][350/750]\tlr: 2.500e-05, eta: 3:03:42, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0950, stage0_pos_acc: 34.4858, stage0_loss_bbox: 0.4955, stage0_loss_iou: 0.9937, stage0_loss_mask: 1.0413, stage1_loss_cls: 0.7184, stage1_pos_acc: 59.8672, stage1_loss_bbox: 0.2328, stage1_loss_iou: 0.4881, stage1_loss_mask: 0.5543, stage2_loss_cls: 0.6133, stage2_pos_acc: 61.4693, stage2_loss_bbox: 0.1868, stage2_loss_iou: 0.4006, stage2_loss_mask: 0.5127, stage3_loss_cls: 0.5191, stage3_pos_acc: 68.3074, stage3_loss_bbox: 0.1710, stage3_loss_iou: 0.3794, stage3_loss_mask: 0.5044, stage4_loss_cls: 0.4699, stage4_pos_acc: 72.5747, stage4_loss_bbox: 0.1719, stage4_loss_iou: 0.3723, stage4_loss_mask: 0.5083, stage5_loss_cls: 0.4392, stage5_pos_acc: 74.9944, stage5_loss_bbox: 0.1780, stage5_loss_iou: 0.3824, stage5_loss_mask: 0.5423, loss: 11.9706\n",
      "2025-07-16 14:51:35,443 - mmdet - INFO - Epoch [22][400/750]\tlr: 2.500e-05, eta: 3:03:25, time: 0.330, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0534, stage0_pos_acc: 37.4768, stage0_loss_bbox: 0.5541, stage0_loss_iou: 0.9990, stage0_loss_mask: 1.2126, stage1_loss_cls: 0.6973, stage1_pos_acc: 57.6009, stage1_loss_bbox: 0.3336, stage1_loss_iou: 0.5487, stage1_loss_mask: 0.7880, stage2_loss_cls: 0.5911, stage2_pos_acc: 63.3075, stage2_loss_bbox: 0.2778, stage2_loss_iou: 0.4493, stage2_loss_mask: 0.7646, stage3_loss_cls: 0.5127, stage3_pos_acc: 67.3912, stage3_loss_bbox: 0.2537, stage3_loss_iou: 0.4207, stage3_loss_mask: 0.7383, stage4_loss_cls: 0.4726, stage4_pos_acc: 72.1800, stage4_loss_bbox: 0.2448, stage4_loss_iou: 0.4162, stage4_loss_mask: 0.7252, stage5_loss_cls: 0.4470, stage5_pos_acc: 74.3102, stage5_loss_bbox: 0.2378, stage5_loss_iou: 0.4076, stage5_loss_mask: 0.7276, loss: 13.8737\n",
      "2025-07-16 14:51:52,008 - mmdet - INFO - Epoch [22][450/750]\tlr: 2.500e-05, eta: 3:03:07, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1013, stage0_pos_acc: 35.0377, stage0_loss_bbox: 0.4616, stage0_loss_iou: 1.0225, stage0_loss_mask: 0.6071, stage1_loss_cls: 0.6381, stage1_pos_acc: 57.7886, stage1_loss_bbox: 0.1831, stage1_loss_iou: 0.3947, stage1_loss_mask: 0.3495, stage2_loss_cls: 0.5300, stage2_pos_acc: 65.6856, stage2_loss_bbox: 0.1383, stage2_loss_iou: 0.2927, stage2_loss_mask: 0.3148, stage3_loss_cls: 0.4405, stage3_pos_acc: 70.2000, stage3_loss_bbox: 0.1196, stage3_loss_iou: 0.2626, stage3_loss_mask: 0.2746, stage4_loss_cls: 0.3870, stage4_pos_acc: 74.9026, stage4_loss_bbox: 0.1176, stage4_loss_iou: 0.2604, stage4_loss_mask: 0.2850, stage5_loss_cls: 0.3878, stage5_pos_acc: 75.7155, stage5_loss_bbox: 0.1183, stage5_loss_iou: 0.2543, stage5_loss_mask: 0.2803, loss: 9.2218\n",
      "2025-07-16 14:52:08,461 - mmdet - INFO - Epoch [22][500/750]\tlr: 2.500e-05, eta: 3:02:49, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1157, stage0_pos_acc: 31.1706, stage0_loss_bbox: 0.5515, stage0_loss_iou: 1.0557, stage0_loss_mask: 0.9359, stage1_loss_cls: 0.7530, stage1_pos_acc: 53.8913, stage1_loss_bbox: 0.2786, stage1_loss_iou: 0.5389, stage1_loss_mask: 0.7186, stage2_loss_cls: 0.6595, stage2_pos_acc: 63.7681, stage2_loss_bbox: 0.2370, stage2_loss_iou: 0.4364, stage2_loss_mask: 0.6224, stage3_loss_cls: 0.5620, stage3_pos_acc: 65.0475, stage3_loss_bbox: 0.2286, stage3_loss_iou: 0.4096, stage3_loss_mask: 0.6066, stage4_loss_cls: 0.5005, stage4_pos_acc: 71.4535, stage4_loss_bbox: 0.2151, stage4_loss_iou: 0.4024, stage4_loss_mask: 0.6130, stage5_loss_cls: 0.4816, stage5_pos_acc: 72.7732, stage5_loss_bbox: 0.2237, stage5_loss_iou: 0.3944, stage5_loss_mask: 0.5933, loss: 13.1338\n",
      "2025-07-16 14:52:25,095 - mmdet - INFO - Epoch [22][550/750]\tlr: 2.500e-05, eta: 3:02:32, time: 0.333, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1092, stage0_pos_acc: 33.9333, stage0_loss_bbox: 0.5950, stage0_loss_iou: 1.1555, stage0_loss_mask: 1.1422, stage1_loss_cls: 0.7232, stage1_pos_acc: 56.3206, stage1_loss_bbox: 0.2579, stage1_loss_iou: 0.5336, stage1_loss_mask: 0.6858, stage2_loss_cls: 0.6399, stage2_pos_acc: 61.7473, stage2_loss_bbox: 0.1975, stage2_loss_iou: 0.4060, stage2_loss_mask: 0.6301, stage3_loss_cls: 0.5188, stage3_pos_acc: 65.4998, stage3_loss_bbox: 0.1826, stage3_loss_iou: 0.3869, stage3_loss_mask: 0.6039, stage4_loss_cls: 0.4652, stage4_pos_acc: 66.9407, stage4_loss_bbox: 0.1785, stage4_loss_iou: 0.3716, stage4_loss_mask: 0.6024, stage5_loss_cls: 0.4612, stage5_pos_acc: 72.7626, stage5_loss_bbox: 0.1756, stage5_loss_iou: 0.3654, stage5_loss_mask: 0.5883, loss: 12.9763\n",
      "2025-07-16 14:52:41,634 - mmdet - INFO - Epoch [22][600/750]\tlr: 2.500e-05, eta: 3:02:14, time: 0.331, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0990, stage0_pos_acc: 37.5755, stage0_loss_bbox: 0.5715, stage0_loss_iou: 0.9748, stage0_loss_mask: 0.8137, stage1_loss_cls: 0.7215, stage1_pos_acc: 54.4790, stage1_loss_bbox: 0.2640, stage1_loss_iou: 0.4498, stage1_loss_mask: 0.4401, stage2_loss_cls: 0.6103, stage2_pos_acc: 59.9109, stage2_loss_bbox: 0.2143, stage2_loss_iou: 0.3660, stage2_loss_mask: 0.4057, stage3_loss_cls: 0.5180, stage3_pos_acc: 66.7332, stage3_loss_bbox: 0.2044, stage3_loss_iou: 0.3469, stage3_loss_mask: 0.3947, stage4_loss_cls: 0.4528, stage4_pos_acc: 70.1189, stage4_loss_bbox: 0.1969, stage4_loss_iou: 0.3430, stage4_loss_mask: 0.3888, stage5_loss_cls: 0.4236, stage5_pos_acc: 74.8046, stage5_loss_bbox: 0.1987, stage5_loss_iou: 0.3417, stage5_loss_mask: 0.4040, loss: 11.1440\n",
      "2025-07-16 14:52:58,071 - mmdet - INFO - Epoch [22][650/750]\tlr: 2.500e-05, eta: 3:01:57, time: 0.329, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0564, stage0_pos_acc: 32.5062, stage0_loss_bbox: 0.5575, stage0_loss_iou: 1.0382, stage0_loss_mask: 0.9835, stage1_loss_cls: 0.6692, stage1_pos_acc: 57.2299, stage1_loss_bbox: 0.2819, stage1_loss_iou: 0.5283, stage1_loss_mask: 0.5793, stage2_loss_cls: 0.5983, stage2_pos_acc: 62.4157, stage2_loss_bbox: 0.2111, stage2_loss_iou: 0.4215, stage2_loss_mask: 0.5329, stage3_loss_cls: 0.4863, stage3_pos_acc: 69.3840, stage3_loss_bbox: 0.2037, stage3_loss_iou: 0.3945, stage3_loss_mask: 0.5455, stage4_loss_cls: 0.4411, stage4_pos_acc: 75.4879, stage4_loss_bbox: 0.1969, stage4_loss_iou: 0.3803, stage4_loss_mask: 0.5298, stage5_loss_cls: 0.4208, stage5_pos_acc: 77.2490, stage5_loss_bbox: 0.1996, stage5_loss_iou: 0.3764, stage5_loss_mask: 0.5324, loss: 12.1654\n",
      "2025-07-16 14:53:14,568 - mmdet - INFO - Epoch [22][700/750]\tlr: 2.500e-05, eta: 3:01:39, time: 0.330, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0555, stage0_pos_acc: 35.1664, stage0_loss_bbox: 0.5677, stage0_loss_iou: 1.0443, stage0_loss_mask: 1.0508, stage1_loss_cls: 0.7533, stage1_pos_acc: 49.1657, stage1_loss_bbox: 0.2591, stage1_loss_iou: 0.5028, stage1_loss_mask: 0.6819, stage2_loss_cls: 0.6514, stage2_pos_acc: 58.7677, stage2_loss_bbox: 0.2214, stage2_loss_iou: 0.4110, stage2_loss_mask: 0.6396, stage3_loss_cls: 0.5719, stage3_pos_acc: 60.4545, stage3_loss_bbox: 0.2093, stage3_loss_iou: 0.4019, stage3_loss_mask: 0.6491, stage4_loss_cls: 0.5228, stage4_pos_acc: 67.1002, stage4_loss_bbox: 0.1999, stage4_loss_iou: 0.3892, stage4_loss_mask: 0.6460, stage5_loss_cls: 0.4935, stage5_pos_acc: 70.4891, stage5_loss_bbox: 0.2018, stage5_loss_iou: 0.3922, stage5_loss_mask: 0.6515, loss: 13.1676\n",
      "2025-07-16 14:53:31,185 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 14:53:31,185 - mmdet - INFO - Epoch [22][750/750]\tlr: 2.500e-05, eta: 3:01:22, time: 0.332, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0747, stage0_pos_acc: 36.6074, stage0_loss_bbox: 0.4998, stage0_loss_iou: 0.9858, stage0_loss_mask: 0.6505, stage1_loss_cls: 0.7197, stage1_pos_acc: 53.6047, stage1_loss_bbox: 0.2341, stage1_loss_iou: 0.4304, stage1_loss_mask: 0.3882, stage2_loss_cls: 0.6110, stage2_pos_acc: 65.1753, stage2_loss_bbox: 0.1984, stage2_loss_iou: 0.3445, stage2_loss_mask: 0.3473, stage3_loss_cls: 0.5400, stage3_pos_acc: 65.9511, stage3_loss_bbox: 0.1835, stage3_loss_iou: 0.3250, stage3_loss_mask: 0.3310, stage4_loss_cls: 0.4867, stage4_pos_acc: 68.8825, stage4_loss_bbox: 0.1733, stage4_loss_iou: 0.3140, stage4_loss_mask: 0.3082, stage5_loss_cls: 0.4766, stage5_pos_acc: 71.4812, stage5_loss_bbox: 0.1758, stage5_loss_iou: 0.3059, stage5_loss_mask: 0.3262, loss: 10.4306\n",
      "2025-07-16 14:53:31,307 - mmdet - INFO - Saving checkpoint at 22 epochs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================== 5. 启动训练 ==========================\n",
    "# 如果你有自定义数据目录，记得修改 config 文件中的数据路径\n",
    "!python tools/train.py configs/ocor/ocor_swin_large_patch4_window7_fpn_300_proposals.py \\\n",
    "    --cfg-options load_from=checkpoint/model.pth\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "继续训练："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "apex is not installed\n",
      "/environment/miniconda3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "apex is not installed\n",
      "apex is not installed\n",
      "apex is not installed\n",
      ">>> RUNTIME PYTHON: /environment/miniconda3/bin/python\n",
      ">>> TORCH: 2.0.1+cu118\n",
      ">>> NUMPY: 1.25.2\n",
      "2025-07-16 15:24:15,457 - mmdet - INFO - Environment info:\n",
      "------------------------------------------------------------\n",
      "sys.platform: linux\n",
      "Python: 3.11.8 (main, Feb 26 2024, 21:39:34) [GCC 11.2.0]\n",
      "CUDA available: True\n",
      "GPU 0: NVIDIA GeForce RTX 4090\n",
      "CUDA_HOME: /usr/local/cuda\n",
      "NVCC: Cuda compilation tools, release 12.3, V12.3.107\n",
      "GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "PyTorch: 2.0.1+cu118\n",
      "PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "TorchVision: 0.15.2+cu118\n",
      "OpenCV: 4.12.0\n",
      "MMCV: 1.7.2\n",
      "MMCV Compiler: GCC 9.3\n",
      "MMCV CUDA Compiler: 11.8\n",
      "MMDetection: 2.12.0+0c8ce2c\n",
      "------------------------------------------------------------\n",
      "\n",
      "2025-07-16 15:24:16,023 - mmdet - INFO - Distributed training: False\n",
      "2025-07-16 15:24:16,706 - mmdet - INFO - Config:\n",
      "dataset_type = 'CocoDataset'\n",
      "data_root = './Datasets/ASSR/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='AutoAugment',\n",
      "        policies=[[{\n",
      "            'type': 'Resize',\n",
      "            'img_scale': [(800, 800)],\n",
      "            'multiscale_mode': 'range',\n",
      "            'keep_ratio': True\n",
      "        }]]),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[123.675, 116.28, 103.53],\n",
      "        std=[58.395, 57.12, 57.375],\n",
      "        to_rgb=True),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=1,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='./Datasets/ASSR/instances_train.json',\n",
      "        img_prefix='./Datasets/ASSR/images/train/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='AutoAugment',\n",
      "                policies=[[{\n",
      "                    'type': 'Resize',\n",
      "                    'img_scale': [(800, 800)],\n",
      "                    'multiscale_mode': 'range',\n",
      "                    'keep_ratio': True\n",
      "                }]]),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[123.675, 116.28, 103.53],\n",
      "                std=[58.395, 57.12, 57.375],\n",
      "                to_rgb=True),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='./Datasets/ASSR/instances_test.json',\n",
      "        img_prefix='./Datasets/ASSR/images/test/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file='./Datasets/ASSR/instances_test.json',\n",
      "        img_prefix='./Datasets/ASSR/images/test/',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[123.675, 116.28, 103.53],\n",
      "                        std=[58.395, 57.12, 57.375],\n",
      "                        to_rgb=True),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric=['bbox', 'segm'])\n",
      "optimizer = dict(\n",
      "    type='AdamW',\n",
      "    lr=2.5e-05,\n",
      "    weight_decay=0.0001,\n",
      "    paramwise_cfg=dict(\n",
      "        custom_keys=dict(\n",
      "            absolute_pos_embed=dict(decay_mult=0.0),\n",
      "            relative_position_bias_table=dict(decay_mult=0.0),\n",
      "            norm=dict(decay_mult=0.0))))\n",
      "optimizer_config = dict(\n",
      "    grad_clip=dict(max_norm=1, norm_type=2),\n",
      "    type='DistOptimizerHook',\n",
      "    update_interval=1,\n",
      "    coalesce=True,\n",
      "    bucket_size_mb=-1,\n",
      "    use_fp16=False)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup='linear',\n",
      "    warmup_iters=1000,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[40])\n",
      "runner = dict(type='EpochBasedRunnerAmp', max_epochs=65)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = 'work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals/latest.pth'\n",
      "workflow = [('train', 1)]\n",
      "num_stages = 6\n",
      "num_proposals = 300\n",
      "model = dict(\n",
      "    type='QueryInst',\n",
      "    pretrained=None,\n",
      "    backbone=dict(\n",
      "        type='SwinTransformer',\n",
      "        embed_dim=192,\n",
      "        depths=[2, 2, 18, 2],\n",
      "        num_heads=[6, 12, 24, 48],\n",
      "        window_size=7,\n",
      "        mlp_ratio=4.0,\n",
      "        qkv_bias=True,\n",
      "        qk_scale=None,\n",
      "        drop_rate=0.0,\n",
      "        attn_drop_rate=0.0,\n",
      "        drop_path_rate=0.3,\n",
      "        ape=False,\n",
      "        patch_norm=True,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        use_checkpoint=False),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[192, 384, 768, 1536],\n",
      "        out_channels=256,\n",
      "        start_level=0,\n",
      "        add_extra_convs='on_input',\n",
      "        num_outs=4),\n",
      "    rpn_head=dict(\n",
      "        type='EmbeddingRPNHead',\n",
      "        num_proposals=300,\n",
      "        proposal_feature_channel=256),\n",
      "    roi_head=dict(\n",
      "        type='QueryRoIHead',\n",
      "        num_stages=6,\n",
      "        stage_loss_weights=[1, 1, 1, 1, 1, 1],\n",
      "        proposal_feature_channel=256,\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=2),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        mask_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=2),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=[\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0])),\n",
      "            dict(\n",
      "                type='DIIHead',\n",
      "                num_classes=5,\n",
      "                num_ffn_fcs=2,\n",
      "                num_heads=8,\n",
      "                num_cls_fcs=1,\n",
      "                num_reg_fcs=3,\n",
      "                feedforward_channels=2048,\n",
      "                in_channels=256,\n",
      "                dropout=0.0,\n",
      "                ffn_act_cfg=dict(type='ReLU', inplace=True),\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=7,\n",
      "                    with_proj=True,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
      "                loss_iou=dict(type='GIoULoss', loss_weight=2.0),\n",
      "                loss_cls=dict(\n",
      "                    type='FocalLoss',\n",
      "                    use_sigmoid=True,\n",
      "                    gamma=2.0,\n",
      "                    alpha=0.25,\n",
      "                    loss_weight=2.0),\n",
      "                bbox_coder=dict(\n",
      "                    type='DeltaXYWHBBoxCoder',\n",
      "                    clip_border=False,\n",
      "                    target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                    target_stds=[0.5, 0.5, 1.0, 1.0]))\n",
      "        ],\n",
      "        mask_head=[\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0)),\n",
      "            dict(\n",
      "                type='DynamicMaskHead',\n",
      "                dynamic_conv_cfg=dict(\n",
      "                    type='DynamicConv',\n",
      "                    in_channels=256,\n",
      "                    feat_channels=64,\n",
      "                    out_channels=256,\n",
      "                    input_feat_shape=14,\n",
      "                    with_proj=False,\n",
      "                    act_cfg=dict(type='ReLU', inplace=True),\n",
      "                    norm_cfg=dict(type='LN')),\n",
      "                dropout=0.0,\n",
      "                num_convs=4,\n",
      "                roi_feat_size=14,\n",
      "                in_channels=256,\n",
      "                conv_kernel_size=3,\n",
      "                conv_out_channels=256,\n",
      "                class_agnostic=False,\n",
      "                norm_cfg=dict(type='BN'),\n",
      "                upsample_cfg=dict(type='deconv', scale_factor=2),\n",
      "                loss_dice=dict(type='DiceLoss', loss_weight=8.0))\n",
      "        ]),\n",
      "    train_cfg=dict(\n",
      "        rpn=None,\n",
      "        rcnn=[\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False),\n",
      "            dict(\n",
      "                assigner=dict(\n",
      "                    type='HungarianAssigner',\n",
      "                    cls_cost=dict(type='FocalLossCost', weight=2.0),\n",
      "                    reg_cost=dict(type='BBoxL1Cost', weight=5.0),\n",
      "                    iou_cost=dict(type='IoUCost', iou_mode='giou',\n",
      "                                  weight=2.0)),\n",
      "                sampler=dict(type='PseudoSampler'),\n",
      "                pos_weight=1,\n",
      "                mask_size=28,\n",
      "                debug=False)\n",
      "        ]),\n",
      "    test_cfg=dict(rpn=None, rcnn=dict(max_per_img=300, mask_thr_binary=0.5)))\n",
      "total_epochs = 65\n",
      "min_values = (480, 512, 544, 576, 608, 640)\n",
      "fp16 = None\n",
      "work_dir = './work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals'\n",
      "gpu_ids = range(0, 1)\n",
      "\n",
      "/environment/miniconda3/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/environment/miniconda3/lib/python3.11/site-packages/mmcv/cnn/bricks/conv_module.py:153: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n",
      "2025-07-16 15:24:21,154 - mmcv - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}\n",
      "2025-07-16 15:24:21,175 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 15:24:21,361 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 15:24:21,540 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 15:24:21,718 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 15:24:21,900 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 15:24:22,080 - mmcv - INFO - initialize DIIHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}]\n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.patch_embed.proj.weight - torch.Size([192, 3, 4, 4]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.patch_embed.proj.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.patch_embed.norm.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.patch_embed.norm.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm1.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm1.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.relative_position_bias_table - torch.Size([169, 6]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.qkv.weight - torch.Size([576, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,281 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.qkv.bias - torch.Size([576]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.proj.weight - torch.Size([192, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.attn.proj.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm2.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.norm2.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc1.weight - torch.Size([768, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc1.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc2.weight - torch.Size([192, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.0.mlp.fc2.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm1.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm1.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.relative_position_bias_table - torch.Size([169, 6]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.qkv.weight - torch.Size([576, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.qkv.bias - torch.Size([576]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.proj.weight - torch.Size([192, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.attn.proj.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm2.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.norm2.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc1.weight - torch.Size([768, 192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc1.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc2.weight - torch.Size([192, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.blocks.1.mlp.fc2.bias - torch.Size([192]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.downsample.reduction.weight - torch.Size([384, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.downsample.norm.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.0.downsample.norm.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm1.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm1.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.relative_position_bias_table - torch.Size([169, 12]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.qkv.weight - torch.Size([1152, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.qkv.bias - torch.Size([1152]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.proj.weight - torch.Size([384, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.attn.proj.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm2.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.norm2.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc1.weight - torch.Size([1536, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc1.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc2.weight - torch.Size([384, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.0.mlp.fc2.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm1.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm1.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.relative_position_bias_table - torch.Size([169, 12]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.qkv.weight - torch.Size([1152, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,282 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.qkv.bias - torch.Size([1152]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.proj.weight - torch.Size([384, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.attn.proj.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm2.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.norm2.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc1.weight - torch.Size([1536, 384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc1.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc2.weight - torch.Size([384, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.blocks.1.mlp.fc2.bias - torch.Size([384]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.downsample.reduction.weight - torch.Size([768, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.downsample.norm.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.1.downsample.norm.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.0.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.1.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,283 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.2.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.3.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.4.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.5.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,284 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.6.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.7.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.8.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,285 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.9.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.10.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.11.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,286 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.12.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.13.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.14.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.15.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,287 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.16.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm1.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm1.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.relative_position_bias_table - torch.Size([169, 24]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.qkv.weight - torch.Size([2304, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.qkv.bias - torch.Size([2304]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.proj.weight - torch.Size([768, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.attn.proj.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc1.weight - torch.Size([3072, 768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc1.bias - torch.Size([3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc2.weight - torch.Size([768, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.blocks.17.mlp.fc2.bias - torch.Size([768]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.downsample.reduction.weight - torch.Size([1536, 3072]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.downsample.norm.weight - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.2.downsample.norm.bias - torch.Size([3072]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm1.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm1.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.relative_position_bias_table - torch.Size([169, 48]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.qkv.weight - torch.Size([4608, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.qkv.bias - torch.Size([4608]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.proj.weight - torch.Size([1536, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.attn.proj.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm2.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.norm2.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc1.weight - torch.Size([6144, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc1.bias - torch.Size([6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc2.weight - torch.Size([1536, 6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.0.mlp.fc2.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm1.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm1.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.relative_position_bias_table - torch.Size([169, 48]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.qkv.weight - torch.Size([4608, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.qkv.bias - torch.Size([4608]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.proj.weight - torch.Size([1536, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.attn.proj.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm2.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.norm2.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc1.weight - torch.Size([6144, 1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,288 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc1.bias - torch.Size([6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc2.weight - torch.Size([1536, 6144]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.layers.3.blocks.1.mlp.fc2.bias - torch.Size([1536]): \n",
      "Initialized by user-defined `init_weights` in SwinTransformer  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm0.weight - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm0.bias - torch.Size([192]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm1.weight - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm1.bias - torch.Size([384]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm2.weight - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm2.bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm3.weight - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "backbone.norm3.bias - torch.Size([1536]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.weight - torch.Size([256, 192, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.weight - torch.Size([256, 384, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.weight - torch.Size([256, 768, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.3.conv.weight - torch.Size([256, 1536, 1, 1]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.lateral_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "XavierInit: gain=1, distribution=uniform, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "neck.fpn_convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "rpn_head.init_proposal_bboxes.weight - torch.Size([300, 4]): \n",
      "Initialized by user-defined `init_weights` in EmbeddingRPNHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "rpn_head.init_proposal_features.weight - torch.Size([300, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,289 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.0.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,290 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.1.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,291 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,292 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.2.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,293 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.3.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,294 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.4.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_cls.weight - torch.Size([5, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_cls.bias - torch.Size([5]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_reg.weight - torch.Size([4, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.fc_reg.bias - torch.Size([4]): \n",
      "NormalInit: mean=0, std=0.001, bias=0 \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.0.weight - torch.Size([64, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.0.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.2.weight - torch.Size([1024, 64]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sos.fc.2.bias - torch.Size([1024]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv_q.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv_k.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv_v.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.conv.weight - torch.Size([512, 512, 3, 3]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.norm.weight - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.sel_att.norm.bias - torch.Size([512]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.c_down.weight - torch.Size([256, 512, 1, 1]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.in_proj_weight - torch.Size([768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.in_proj_bias - torch.Size([768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.out_proj.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention.attn.out_proj.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.attention_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_layer.weight - torch.Size([256, 12544]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_layer.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv.fc_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.instance_interactive_conv_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,295 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.0.0.bias - torch.Size([2048]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.1.weight - torch.Size([256, 2048]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn.layers.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn_norm.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.ffn_norm.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.cls_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.cls_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.cls_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.0.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.1.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.1.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.3.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.4.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.4.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.6.weight - torch.Size([256, 256]): \n",
      "Initialized by user-defined `init_weights` in DIIHead  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.7.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.bbox_head.5.reg_fcs.7.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.0.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,296 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.1.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,297 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.2.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.3.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,298 - mmcv - INFO - \n",
      "roi_head.mask_head.4.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.4.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.0.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.1.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.2.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.conv.weight - torch.Size([256, 256, 3, 3]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.conv.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.bn.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.convs.3.bn.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.upsample.weight - torch.Size([256, 256, 2, 2]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.upsample.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.conv_logits.weight - torch.Size([5, 256, 1, 1]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.conv_logits.bias - torch.Size([5]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.dynamic_layer.weight - torch.Size([32768, 256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.dynamic_layer.bias - torch.Size([32768]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_in.weight - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_in.bias - torch.Size([64]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_out.weight - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "2025-07-16 15:24:22,299 - mmcv - INFO - \n",
      "roi_head.mask_head.5.instance_interactive_conv.norm_out.bias - torch.Size([256]): \n",
      "The value is the same before and after calling `init_weights` of QueryInst  \n",
      " \n",
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "2025-07-16 15:24:22,759 - mmdet - INFO - load checkpoint from work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals/latest.pth\n",
      "2025-07-16 15:24:22,759 - mmdet - INFO - load checkpoint from local path: work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals/latest.pth\n",
      "2025-07-16 15:24:40,050 - mmdet - INFO - resumed epoch 21, iter 15750\n",
      "2025-07-16 15:24:40,054 - mmdet - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/OCOR/work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals\n",
      "2025-07-16 15:24:40,054 - mmdet - INFO - workflow: [('train', 1)], max: 65 epochs\n",
      "2025-07-16 15:24:40,055 - mmdet - INFO - Checkpoints will be saved to /home/featurize/work/OCOR/work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals by HardDiskBackend.\n",
      "2025-07-16 15:25:01,214 - mmdet - INFO - Epoch [22][50/750]\tlr: 2.500e-05, eta: 3:52:19, time: 0.423, data_time: 0.052, memory: 10310, stage0_loss_cls: 1.0823, stage0_pos_acc: 32.3326, stage0_loss_bbox: 0.4951, stage0_loss_iou: 0.9450, stage0_loss_mask: 0.8222, stage1_loss_cls: 0.7051, stage1_pos_acc: 58.1772, stage1_loss_bbox: 0.2355, stage1_loss_iou: 0.4600, stage1_loss_mask: 0.5347, stage2_loss_cls: 0.5944, stage2_pos_acc: 64.4423, stage2_loss_bbox: 0.2005, stage2_loss_iou: 0.3724, stage2_loss_mask: 0.4697, stage3_loss_cls: 0.5064, stage3_pos_acc: 70.2836, stage3_loss_bbox: 0.1940, stage3_loss_iou: 0.3599, stage3_loss_mask: 0.4569, stage4_loss_cls: 0.4447, stage4_pos_acc: 72.9340, stage4_loss_bbox: 0.1966, stage4_loss_iou: 0.3540, stage4_loss_mask: 0.4541, stage5_loss_cls: 0.4149, stage5_pos_acc: 74.7724, stage5_loss_bbox: 0.2001, stage5_loss_iou: 0.3571, stage5_loss_mask: 0.4632, loss: 11.3190\n",
      "2025-07-16 15:25:19,446 - mmdet - INFO - Epoch [22][100/750]\tlr: 2.500e-05, eta: 3:35:57, time: 0.365, data_time: 0.006, memory: 10310, stage0_loss_cls: 1.0841, stage0_pos_acc: 34.1100, stage0_loss_bbox: 0.5206, stage0_loss_iou: 0.9606, stage0_loss_mask: 0.6911, stage1_loss_cls: 0.6975, stage1_pos_acc: 53.5532, stage1_loss_bbox: 0.2202, stage1_loss_iou: 0.4110, stage1_loss_mask: 0.4153, stage2_loss_cls: 0.5864, stage2_pos_acc: 65.4200, stage2_loss_bbox: 0.1744, stage2_loss_iou: 0.3334, stage2_loss_mask: 0.3828, stage3_loss_cls: 0.4894, stage3_pos_acc: 67.9867, stage3_loss_bbox: 0.1597, stage3_loss_iou: 0.3067, stage3_loss_mask: 0.3505, stage4_loss_cls: 0.4361, stage4_pos_acc: 71.6081, stage4_loss_bbox: 0.1502, stage4_loss_iou: 0.2915, stage4_loss_mask: 0.3297, stage5_loss_cls: 0.4049, stage5_pos_acc: 73.5248, stage5_loss_bbox: 0.1518, stage5_loss_iou: 0.2894, stage5_loss_mask: 0.3287, loss: 10.1659\n",
      "2025-07-16 15:25:38,160 - mmdet - INFO - Epoch [22][150/750]\tlr: 2.500e-05, eta: 3:32:03, time: 0.374, data_time: 0.018, memory: 10310, stage0_loss_cls: 1.0815, stage0_pos_acc: 37.6794, stage0_loss_bbox: 0.5843, stage0_loss_iou: 1.0712, stage0_loss_mask: 1.1234, stage1_loss_cls: 0.7079, stage1_pos_acc: 55.6587, stage1_loss_bbox: 0.2624, stage1_loss_iou: 0.5633, stage1_loss_mask: 0.7401, stage2_loss_cls: 0.6170, stage2_pos_acc: 63.1500, stage2_loss_bbox: 0.2062, stage2_loss_iou: 0.4498, stage2_loss_mask: 0.6606, stage3_loss_cls: 0.5230, stage3_pos_acc: 64.7306, stage3_loss_bbox: 0.1969, stage3_loss_iou: 0.4203, stage3_loss_mask: 0.6046, stage4_loss_cls: 0.4635, stage4_pos_acc: 69.1980, stage4_loss_bbox: 0.1870, stage4_loss_iou: 0.4016, stage4_loss_mask: 0.5711, stage5_loss_cls: 0.4465, stage5_pos_acc: 72.3702, stage5_loss_bbox: 0.1863, stage5_loss_iou: 0.3900, stage5_loss_mask: 0.5503, loss: 13.0088\n",
      "2025-07-16 15:25:56,600 - mmdet - INFO - Epoch [22][200/750]\tlr: 2.500e-05, eta: 3:29:11, time: 0.369, data_time: 0.008, memory: 10408, stage0_loss_cls: 1.0956, stage0_pos_acc: 33.4304, stage0_loss_bbox: 0.5931, stage0_loss_iou: 1.1519, stage0_loss_mask: 1.2504, stage1_loss_cls: 0.7249, stage1_pos_acc: 58.6738, stage1_loss_bbox: 0.2734, stage1_loss_iou: 0.5928, stage1_loss_mask: 0.9054, stage2_loss_cls: 0.6257, stage2_pos_acc: 62.8491, stage2_loss_bbox: 0.2245, stage2_loss_iou: 0.4773, stage2_loss_mask: 0.8414, stage3_loss_cls: 0.5233, stage3_pos_acc: 71.6445, stage3_loss_bbox: 0.2068, stage3_loss_iou: 0.4389, stage3_loss_mask: 0.7817, stage4_loss_cls: 0.4593, stage4_pos_acc: 74.2715, stage4_loss_bbox: 0.1950, stage4_loss_iou: 0.4262, stage4_loss_mask: 0.7741, stage5_loss_cls: 0.4380, stage5_pos_acc: 77.9295, stage5_loss_bbox: 0.1955, stage5_loss_iou: 0.4237, stage5_loss_mask: 0.7736, loss: 14.3924\n",
      "2025-07-16 15:26:15,217 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:26:15,217 - mmdet - INFO - Epoch [22][250/750]\tlr: 2.500e-05, eta: 3:27:44, time: 0.372, data_time: 0.007, memory: 10481, stage0_loss_cls: 1.0918, stage0_pos_acc: 31.4439, stage0_loss_bbox: 0.5220, stage0_loss_iou: 0.9751, stage0_loss_mask: 0.7913, stage1_loss_cls: 0.7355, stage1_pos_acc: 51.1577, stage1_loss_bbox: 0.2453, stage1_loss_iou: 0.4739, stage1_loss_mask: 0.4733, stage2_loss_cls: 0.6297, stage2_pos_acc: 61.8881, stage2_loss_bbox: 0.1858, stage2_loss_iou: 0.3720, stage2_loss_mask: 0.4144, stage3_loss_cls: 0.5162, stage3_pos_acc: 66.8541, stage3_loss_bbox: 0.1765, stage3_loss_iou: 0.3477, stage3_loss_mask: 0.4170, stage4_loss_cls: 0.4784, stage4_pos_acc: 69.3103, stage4_loss_bbox: 0.1712, stage4_loss_iou: 0.3333, stage4_loss_mask: 0.4267, stage5_loss_cls: 0.4480, stage5_pos_acc: 73.4377, stage5_loss_bbox: 0.1754, stage5_loss_iou: 0.3304, stage5_loss_mask: 0.4307, loss: 11.1616\n",
      "2025-07-16 15:26:33,338 - mmdet - INFO - Epoch [22][300/750]\tlr: 2.500e-05, eta: 3:25:46, time: 0.362, data_time: 0.007, memory: 10499, stage0_loss_cls: 1.0721, stage0_pos_acc: 39.4369, stage0_loss_bbox: 0.5508, stage0_loss_iou: 1.0930, stage0_loss_mask: 1.0802, stage1_loss_cls: 0.7347, stage1_pos_acc: 57.6069, stage1_loss_bbox: 0.2832, stage1_loss_iou: 0.5497, stage1_loss_mask: 0.7226, stage2_loss_cls: 0.6261, stage2_pos_acc: 62.6389, stage2_loss_bbox: 0.2070, stage2_loss_iou: 0.4161, stage2_loss_mask: 0.6338, stage3_loss_cls: 0.5403, stage3_pos_acc: 65.3524, stage3_loss_bbox: 0.1866, stage3_loss_iou: 0.3685, stage3_loss_mask: 0.5729, stage4_loss_cls: 0.4771, stage4_pos_acc: 74.9966, stage4_loss_bbox: 0.1735, stage4_loss_iou: 0.3513, stage4_loss_mask: 0.5555, stage5_loss_cls: 0.4583, stage5_pos_acc: 76.0259, stage5_loss_bbox: 0.1680, stage5_loss_iou: 0.3454, stage5_loss_mask: 0.5489, loss: 12.7156\n",
      "2025-07-16 15:26:51,834 - mmdet - INFO - Epoch [22][350/750]\tlr: 2.500e-05, eta: 3:24:52, time: 0.370, data_time: 0.009, memory: 11258, stage0_loss_cls: 1.0801, stage0_pos_acc: 41.0118, stage0_loss_bbox: 0.5285, stage0_loss_iou: 0.9340, stage0_loss_mask: 0.7385, stage1_loss_cls: 0.6832, stage1_pos_acc: 59.2165, stage1_loss_bbox: 0.2534, stage1_loss_iou: 0.4472, stage1_loss_mask: 0.5056, stage2_loss_cls: 0.5867, stage2_pos_acc: 65.8490, stage2_loss_bbox: 0.2054, stage2_loss_iou: 0.3687, stage2_loss_mask: 0.4749, stage3_loss_cls: 0.4644, stage3_pos_acc: 70.9165, stage3_loss_bbox: 0.1919, stage3_loss_iou: 0.3530, stage3_loss_mask: 0.4602, stage4_loss_cls: 0.3993, stage4_pos_acc: 75.5665, stage4_loss_bbox: 0.1850, stage4_loss_iou: 0.3391, stage4_loss_mask: 0.4612, stage5_loss_cls: 0.3769, stage5_pos_acc: 77.7808, stage5_loss_bbox: 0.1810, stage5_loss_iou: 0.3346, stage5_loss_mask: 0.4467, loss: 10.9996\n",
      "2025-07-16 15:27:10,200 - mmdet - INFO - Epoch [22][400/750]\tlr: 2.500e-05, eta: 3:23:55, time: 0.367, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.1071, stage0_pos_acc: 36.7087, stage0_loss_bbox: 0.5446, stage0_loss_iou: 1.0838, stage0_loss_mask: 1.0067, stage1_loss_cls: 0.7040, stage1_pos_acc: 57.8135, stage1_loss_bbox: 0.2500, stage1_loss_iou: 0.4924, stage1_loss_mask: 0.5850, stage2_loss_cls: 0.5888, stage2_pos_acc: 63.1595, stage2_loss_bbox: 0.2118, stage2_loss_iou: 0.4071, stage2_loss_mask: 0.5136, stage3_loss_cls: 0.4851, stage3_pos_acc: 68.1587, stage3_loss_bbox: 0.1937, stage3_loss_iou: 0.3780, stage3_loss_mask: 0.4925, stage4_loss_cls: 0.4400, stage4_pos_acc: 73.7484, stage4_loss_bbox: 0.1906, stage4_loss_iou: 0.3748, stage4_loss_mask: 0.4973, stage5_loss_cls: 0.4214, stage5_pos_acc: 77.6524, stage5_loss_bbox: 0.1863, stage5_loss_iou: 0.3691, stage5_loss_mask: 0.5042, loss: 12.0278\n",
      "2025-07-16 15:27:28,529 - mmdet - INFO - Epoch [22][450/750]\tlr: 2.500e-05, eta: 3:23:05, time: 0.367, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.0619, stage0_pos_acc: 33.2937, stage0_loss_bbox: 0.5513, stage0_loss_iou: 0.9286, stage0_loss_mask: 0.7736, stage1_loss_cls: 0.7113, stage1_pos_acc: 55.1937, stage1_loss_bbox: 0.2799, stage1_loss_iou: 0.4508, stage1_loss_mask: 0.4716, stage2_loss_cls: 0.6390, stage2_pos_acc: 56.8825, stage2_loss_bbox: 0.2478, stage2_loss_iou: 0.3730, stage2_loss_mask: 0.4502, stage3_loss_cls: 0.5440, stage3_pos_acc: 64.4349, stage3_loss_bbox: 0.2399, stage3_loss_iou: 0.3566, stage3_loss_mask: 0.4536, stage4_loss_cls: 0.4774, stage4_pos_acc: 69.9016, stage4_loss_bbox: 0.2433, stage4_loss_iou: 0.3664, stage4_loss_mask: 0.5160, stage5_loss_cls: 0.4546, stage5_pos_acc: 76.2429, stage5_loss_bbox: 0.2423, stage5_loss_iou: 0.3651, stage5_loss_mask: 0.5183, loss: 11.7166\n",
      "2025-07-16 15:27:46,886 - mmdet - INFO - Epoch [22][500/750]\tlr: 2.500e-05, eta: 3:22:23, time: 0.367, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.0885, stage0_pos_acc: 35.4179, stage0_loss_bbox: 0.4946, stage0_loss_iou: 0.9902, stage0_loss_mask: 0.6494, stage1_loss_cls: 0.6971, stage1_pos_acc: 53.5679, stage1_loss_bbox: 0.2186, stage1_loss_iou: 0.4254, stage1_loss_mask: 0.3665, stage2_loss_cls: 0.5882, stage2_pos_acc: 64.4391, stage2_loss_bbox: 0.1637, stage2_loss_iou: 0.3152, stage2_loss_mask: 0.3232, stage3_loss_cls: 0.4913, stage3_pos_acc: 69.6470, stage3_loss_bbox: 0.1515, stage3_loss_iou: 0.2886, stage3_loss_mask: 0.3153, stage4_loss_cls: 0.4257, stage4_pos_acc: 74.6573, stage4_loss_bbox: 0.1448, stage4_loss_iou: 0.2770, stage4_loss_mask: 0.3044, stage5_loss_cls: 0.3983, stage5_pos_acc: 77.2629, stage5_loss_bbox: 0.1422, stage5_loss_iou: 0.2767, stage5_loss_mask: 0.3143, loss: 9.8505\n",
      "2025-07-16 15:28:05,059 - mmdet - INFO - Epoch [22][550/750]\tlr: 2.500e-05, eta: 3:21:34, time: 0.363, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.0998, stage0_pos_acc: 35.1443, stage0_loss_bbox: 0.5253, stage0_loss_iou: 1.0071, stage0_loss_mask: 0.8472, stage1_loss_cls: 0.7189, stage1_pos_acc: 53.6517, stage1_loss_bbox: 0.2129, stage1_loss_iou: 0.4534, stage1_loss_mask: 0.5016, stage2_loss_cls: 0.5862, stage2_pos_acc: 61.6850, stage2_loss_bbox: 0.1757, stage2_loss_iou: 0.3558, stage2_loss_mask: 0.4354, stage3_loss_cls: 0.4881, stage3_pos_acc: 68.7427, stage3_loss_bbox: 0.1503, stage3_loss_iou: 0.3262, stage3_loss_mask: 0.3961, stage4_loss_cls: 0.4375, stage4_pos_acc: 73.7910, stage4_loss_bbox: 0.1563, stage4_loss_iou: 0.3144, stage4_loss_mask: 0.3641, stage5_loss_cls: 0.4126, stage5_pos_acc: 76.7743, stage5_loss_bbox: 0.1514, stage5_loss_iou: 0.3122, stage5_loss_mask: 0.3546, loss: 10.7829\n",
      "2025-07-16 15:28:23,218 - mmdet - INFO - Epoch [22][600/750]\tlr: 2.500e-05, eta: 3:20:50, time: 0.363, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.1000, stage0_pos_acc: 35.0827, stage0_loss_bbox: 0.5218, stage0_loss_iou: 0.9586, stage0_loss_mask: 0.8027, stage1_loss_cls: 0.7166, stage1_pos_acc: 59.1103, stage1_loss_bbox: 0.2373, stage1_loss_iou: 0.4488, stage1_loss_mask: 0.4685, stage2_loss_cls: 0.5979, stage2_pos_acc: 69.1274, stage2_loss_bbox: 0.1881, stage2_loss_iou: 0.3609, stage2_loss_mask: 0.3820, stage3_loss_cls: 0.4773, stage3_pos_acc: 72.1588, stage3_loss_bbox: 0.1819, stage3_loss_iou: 0.3364, stage3_loss_mask: 0.3564, stage4_loss_cls: 0.4281, stage4_pos_acc: 74.5767, stage4_loss_bbox: 0.1723, stage4_loss_iou: 0.3173, stage4_loss_mask: 0.3419, stage5_loss_cls: 0.3995, stage5_pos_acc: 77.9956, stage5_loss_bbox: 0.1721, stage5_loss_iou: 0.3120, stage5_loss_mask: 0.3334, loss: 10.6117\n",
      "2025-07-16 15:28:41,537 - mmdet - INFO - Epoch [22][650/750]\tlr: 2.500e-05, eta: 3:20:17, time: 0.366, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.0712, stage0_pos_acc: 34.0627, stage0_loss_bbox: 0.5053, stage0_loss_iou: 1.0083, stage0_loss_mask: 0.8840, stage1_loss_cls: 0.6991, stage1_pos_acc: 56.3812, stage1_loss_bbox: 0.2475, stage1_loss_iou: 0.4906, stage1_loss_mask: 0.5526, stage2_loss_cls: 0.6100, stage2_pos_acc: 64.2104, stage2_loss_bbox: 0.2012, stage2_loss_iou: 0.3853, stage2_loss_mask: 0.4888, stage3_loss_cls: 0.5052, stage3_pos_acc: 67.2746, stage3_loss_bbox: 0.2006, stage3_loss_iou: 0.3625, stage3_loss_mask: 0.4479, stage4_loss_cls: 0.4446, stage4_pos_acc: 69.8131, stage4_loss_bbox: 0.1804, stage4_loss_iou: 0.3487, stage4_loss_mask: 0.4325, stage5_loss_cls: 0.4188, stage5_pos_acc: 75.7469, stage5_loss_bbox: 0.1731, stage5_loss_iou: 0.3469, stage5_loss_mask: 0.4617, loss: 11.4668\n",
      "2025-07-16 15:28:59,649 - mmdet - INFO - Epoch [22][700/750]\tlr: 2.500e-05, eta: 3:19:37, time: 0.362, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.0806, stage0_pos_acc: 31.7856, stage0_loss_bbox: 0.4988, stage0_loss_iou: 0.9574, stage0_loss_mask: 0.8504, stage1_loss_cls: 0.7331, stage1_pos_acc: 51.8094, stage1_loss_bbox: 0.2354, stage1_loss_iou: 0.4684, stage1_loss_mask: 0.5040, stage2_loss_cls: 0.6323, stage2_pos_acc: 62.0720, stage2_loss_bbox: 0.1878, stage2_loss_iou: 0.3600, stage2_loss_mask: 0.3967, stage3_loss_cls: 0.5240, stage3_pos_acc: 64.6285, stage3_loss_bbox: 0.1865, stage3_loss_iou: 0.3407, stage3_loss_mask: 0.3904, stage4_loss_cls: 0.4577, stage4_pos_acc: 70.8102, stage4_loss_bbox: 0.1758, stage4_loss_iou: 0.3279, stage4_loss_mask: 0.3917, stage5_loss_cls: 0.4299, stage5_pos_acc: 71.4119, stage5_loss_bbox: 0.1841, stage5_loss_iou: 0.3299, stage5_loss_mask: 0.3878, loss: 11.0314\n",
      "2025-07-16 15:29:17,730 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:29:17,730 - mmdet - INFO - Epoch [22][750/750]\tlr: 2.500e-05, eta: 3:18:59, time: 0.362, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.1099, stage0_pos_acc: 40.1806, stage0_loss_bbox: 0.5281, stage0_loss_iou: 1.0091, stage0_loss_mask: 0.8131, stage1_loss_cls: 0.6929, stage1_pos_acc: 61.5977, stage1_loss_bbox: 0.2302, stage1_loss_iou: 0.4628, stage1_loss_mask: 0.4899, stage2_loss_cls: 0.5707, stage2_pos_acc: 67.3077, stage2_loss_bbox: 0.1830, stage2_loss_iou: 0.3564, stage2_loss_mask: 0.4296, stage3_loss_cls: 0.4570, stage3_pos_acc: 75.3865, stage3_loss_bbox: 0.1647, stage3_loss_iou: 0.3337, stage3_loss_mask: 0.3969, stage4_loss_cls: 0.4005, stage4_pos_acc: 77.8366, stage4_loss_bbox: 0.1610, stage4_loss_iou: 0.3192, stage4_loss_mask: 0.3799, stage5_loss_cls: 0.3877, stage5_pos_acc: 77.8631, stage5_loss_bbox: 0.1611, stage5_loss_iou: 0.3158, stage5_loss_mask: 0.3788, loss: 10.7320\n",
      "2025-07-16 15:29:17,838 - mmdet - INFO - Saving checkpoint at 22 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 87s, ETA:     0s2025-07-16 15:32:23,030 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.065\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.459\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.462\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.462\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.345\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.287\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.497\n",
      "2025-07-16 15:32:24,953 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.82s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.036\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.454\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.454\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.347\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.299\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.484\n",
      "2025-07-16 15:32:28,038 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:32:28,038 - mmdet - INFO - Epoch(val) [22][750]\tbbox_mAP: 0.0360, bbox_mAP_50: 0.0650, bbox_mAP_75: 0.0360, bbox_mAP_s: 0.1060, bbox_mAP_m: 0.0250, bbox_mAP_l: 0.0400, bbox_mAP_copypaste: 0.036 0.065 0.036 0.106 0.025 0.040, segm_mAP: 0.0360, segm_mAP_50: 0.0640, segm_mAP_75: 0.0350, segm_mAP_s: 0.1060, segm_mAP_m: 0.0250, segm_mAP_l: 0.0410, segm_mAP_copypaste: 0.036 0.064 0.035 0.106 0.025 0.041\n",
      "2025-07-16 15:32:48,489 - mmdet - INFO - Epoch [23][50/750]\tlr: 2.500e-05, eta: 3:19:58, time: 0.409, data_time: 0.052, memory: 11258, stage0_loss_cls: 1.0645, stage0_pos_acc: 35.3325, stage0_loss_bbox: 0.5345, stage0_loss_iou: 0.9935, stage0_loss_mask: 0.9179, stage1_loss_cls: 0.6876, stage1_pos_acc: 59.1754, stage1_loss_bbox: 0.2595, stage1_loss_iou: 0.4799, stage1_loss_mask: 0.6104, stage2_loss_cls: 0.5700, stage2_pos_acc: 66.1016, stage2_loss_bbox: 0.2214, stage2_loss_iou: 0.3897, stage2_loss_mask: 0.5633, stage3_loss_cls: 0.4922, stage3_pos_acc: 71.5540, stage3_loss_bbox: 0.2063, stage3_loss_iou: 0.3666, stage3_loss_mask: 0.5163, stage4_loss_cls: 0.4271, stage4_pos_acc: 75.7937, stage4_loss_bbox: 0.1986, stage4_loss_iou: 0.3493, stage4_loss_mask: 0.5144, stage5_loss_cls: 0.3946, stage5_pos_acc: 82.7556, stage5_loss_bbox: 0.1985, stage5_loss_iou: 0.3448, stage5_loss_mask: 0.5182, loss: 11.8192\n",
      "2025-07-16 15:33:06,497 - mmdet - INFO - Epoch [23][100/750]\tlr: 2.500e-05, eta: 3:19:16, time: 0.360, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.0998, stage0_pos_acc: 34.0045, stage0_loss_bbox: 0.5012, stage0_loss_iou: 0.9357, stage0_loss_mask: 0.5624, stage1_loss_cls: 0.6854, stage1_pos_acc: 55.7434, stage1_loss_bbox: 0.2223, stage1_loss_iou: 0.4018, stage1_loss_mask: 0.3206, stage2_loss_cls: 0.5589, stage2_pos_acc: 67.7566, stage2_loss_bbox: 0.1747, stage2_loss_iou: 0.2983, stage2_loss_mask: 0.2878, stage3_loss_cls: 0.4506, stage3_pos_acc: 71.3081, stage3_loss_bbox: 0.1661, stage3_loss_iou: 0.2747, stage3_loss_mask: 0.2872, stage4_loss_cls: 0.3751, stage4_pos_acc: 78.1475, stage4_loss_bbox: 0.1642, stage4_loss_iou: 0.2677, stage4_loss_mask: 0.2703, stage5_loss_cls: 0.3485, stage5_pos_acc: 80.0530, stage5_loss_bbox: 0.1642, stage5_loss_iou: 0.2694, stage5_loss_mask: 0.2774, loss: 9.3642\n",
      "2025-07-16 15:33:24,478 - mmdet - INFO - Epoch [23][150/750]\tlr: 2.500e-05, eta: 3:18:35, time: 0.360, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.1305, stage0_pos_acc: 36.0061, stage0_loss_bbox: 0.5725, stage0_loss_iou: 1.0986, stage0_loss_mask: 1.0680, stage1_loss_cls: 0.6635, stage1_pos_acc: 58.2712, stage1_loss_bbox: 0.2566, stage1_loss_iou: 0.5022, stage1_loss_mask: 0.5384, stage2_loss_cls: 0.5643, stage2_pos_acc: 64.6450, stage2_loss_bbox: 0.1944, stage2_loss_iou: 0.3873, stage2_loss_mask: 0.5045, stage3_loss_cls: 0.4758, stage3_pos_acc: 73.3602, stage3_loss_bbox: 0.1666, stage3_loss_iou: 0.3392, stage3_loss_mask: 0.4548, stage4_loss_cls: 0.4008, stage4_pos_acc: 80.5203, stage4_loss_bbox: 0.1551, stage4_loss_iou: 0.3158, stage4_loss_mask: 0.4608, stage5_loss_cls: 0.3690, stage5_pos_acc: 79.3188, stage5_loss_bbox: 0.1488, stage5_loss_iou: 0.3062, stage5_loss_mask: 0.4592, loss: 11.5329\n",
      "2025-07-16 15:33:42,788 - mmdet - INFO - Epoch [23][200/750]\tlr: 2.500e-05, eta: 3:18:08, time: 0.366, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.1193, stage0_pos_acc: 37.4185, stage0_loss_bbox: 0.5344, stage0_loss_iou: 1.1079, stage0_loss_mask: 1.0234, stage1_loss_cls: 0.7231, stage1_pos_acc: 57.6618, stage1_loss_bbox: 0.2317, stage1_loss_iou: 0.5324, stage1_loss_mask: 0.7234, stage2_loss_cls: 0.6272, stage2_pos_acc: 64.6543, stage2_loss_bbox: 0.1849, stage2_loss_iou: 0.4219, stage2_loss_mask: 0.7172, stage3_loss_cls: 0.5077, stage3_pos_acc: 70.7972, stage3_loss_bbox: 0.1704, stage3_loss_iou: 0.4020, stage3_loss_mask: 0.6835, stage4_loss_cls: 0.4440, stage4_pos_acc: 74.0155, stage4_loss_bbox: 0.1636, stage4_loss_iou: 0.3983, stage4_loss_mask: 0.6685, stage5_loss_cls: 0.4242, stage5_pos_acc: 74.9475, stage5_loss_bbox: 0.1571, stage5_loss_iou: 0.3852, stage5_loss_mask: 0.6817, loss: 13.0330\n",
      "2025-07-16 15:34:01,010 - mmdet - INFO - Epoch [23][250/750]\tlr: 2.500e-05, eta: 3:17:39, time: 0.364, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.0934, stage0_pos_acc: 43.5736, stage0_loss_bbox: 0.5206, stage0_loss_iou: 0.9820, stage0_loss_mask: 0.6090, stage1_loss_cls: 0.6464, stage1_pos_acc: 63.6631, stage1_loss_bbox: 0.2219, stage1_loss_iou: 0.4242, stage1_loss_mask: 0.3372, stage2_loss_cls: 0.5331, stage2_pos_acc: 70.4585, stage2_loss_bbox: 0.1523, stage2_loss_iou: 0.3009, stage2_loss_mask: 0.2889, stage3_loss_cls: 0.4183, stage3_pos_acc: 76.8595, stage3_loss_bbox: 0.1462, stage3_loss_iou: 0.2827, stage3_loss_mask: 0.2876, stage4_loss_cls: 0.3538, stage4_pos_acc: 80.1400, stage4_loss_bbox: 0.1458, stage4_loss_iou: 0.2740, stage4_loss_mask: 0.2868, stage5_loss_cls: 0.3286, stage5_pos_acc: 82.9182, stage5_loss_bbox: 0.1425, stage5_loss_iou: 0.2668, stage5_loss_mask: 0.2628, loss: 9.3059\n",
      "2025-07-16 15:34:19,043 - mmdet - INFO - Epoch [23][300/750]\tlr: 2.500e-05, eta: 3:17:06, time: 0.361, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.1035, stage0_pos_acc: 33.1333, stage0_loss_bbox: 0.4939, stage0_loss_iou: 0.9375, stage0_loss_mask: 0.5842, stage1_loss_cls: 0.6747, stage1_pos_acc: 58.3754, stage1_loss_bbox: 0.2069, stage1_loss_iou: 0.3829, stage1_loss_mask: 0.2820, stage2_loss_cls: 0.5751, stage2_pos_acc: 63.4286, stage2_loss_bbox: 0.1551, stage2_loss_iou: 0.2833, stage2_loss_mask: 0.2171, stage3_loss_cls: 0.4770, stage3_pos_acc: 69.5429, stage3_loss_bbox: 0.1412, stage3_loss_iou: 0.2621, stage3_loss_mask: 0.2147, stage4_loss_cls: 0.4115, stage4_pos_acc: 73.7095, stage4_loss_bbox: 0.1363, stage4_loss_iou: 0.2500, stage4_loss_mask: 0.2200, stage5_loss_cls: 0.3839, stage5_pos_acc: 78.1365, stage5_loss_bbox: 0.1389, stage5_loss_iou: 0.2449, stage5_loss_mask: 0.2301, loss: 9.0069\n",
      "2025-07-16 15:34:37,126 - mmdet - INFO - Epoch [23][350/750]\tlr: 2.500e-05, eta: 3:16:35, time: 0.362, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.1077, stage0_pos_acc: 37.1660, stage0_loss_bbox: 0.5846, stage0_loss_iou: 1.0710, stage0_loss_mask: 0.8668, stage1_loss_cls: 0.7438, stage1_pos_acc: 53.5749, stage1_loss_bbox: 0.2432, stage1_loss_iou: 0.4939, stage1_loss_mask: 0.4881, stage2_loss_cls: 0.6167, stage2_pos_acc: 63.1918, stage2_loss_bbox: 0.1786, stage2_loss_iou: 0.3614, stage2_loss_mask: 0.4476, stage3_loss_cls: 0.5033, stage3_pos_acc: 69.5409, stage3_loss_bbox: 0.1599, stage3_loss_iou: 0.3287, stage3_loss_mask: 0.4252, stage4_loss_cls: 0.4418, stage4_pos_acc: 74.1838, stage4_loss_bbox: 0.1415, stage4_loss_iou: 0.3036, stage4_loss_mask: 0.3970, stage5_loss_cls: 0.4016, stage5_pos_acc: 78.2355, stage5_loss_bbox: 0.1439, stage5_loss_iou: 0.3016, stage5_loss_mask: 0.3981, loss: 11.1497\n",
      "2025-07-16 15:34:55,161 - mmdet - INFO - Epoch [23][400/750]\tlr: 2.500e-05, eta: 3:16:04, time: 0.361, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.1048, stage0_pos_acc: 35.9815, stage0_loss_bbox: 0.5021, stage0_loss_iou: 1.0184, stage0_loss_mask: 0.9749, stage1_loss_cls: 0.7011, stage1_pos_acc: 56.1553, stage1_loss_bbox: 0.2693, stage1_loss_iou: 0.4838, stage1_loss_mask: 0.5201, stage2_loss_cls: 0.5986, stage2_pos_acc: 64.1085, stage2_loss_bbox: 0.2160, stage2_loss_iou: 0.3814, stage2_loss_mask: 0.4955, stage3_loss_cls: 0.4977, stage3_pos_acc: 69.9149, stage3_loss_bbox: 0.1947, stage3_loss_iou: 0.3508, stage3_loss_mask: 0.4724, stage4_loss_cls: 0.4561, stage4_pos_acc: 75.6681, stage4_loss_bbox: 0.1800, stage4_loss_iou: 0.3378, stage4_loss_mask: 0.4656, stage5_loss_cls: 0.4143, stage5_pos_acc: 79.3570, stage5_loss_bbox: 0.1994, stage5_loss_iou: 0.3410, stage5_loss_mask: 0.4847, loss: 11.6607\n",
      "2025-07-16 15:35:13,227 - mmdet - INFO - Epoch [23][450/750]\tlr: 2.500e-05, eta: 3:15:35, time: 0.361, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.1051, stage0_pos_acc: 33.5026, stage0_loss_bbox: 0.5189, stage0_loss_iou: 1.0163, stage0_loss_mask: 0.9965, stage1_loss_cls: 0.7331, stage1_pos_acc: 58.6909, stage1_loss_bbox: 0.2645, stage1_loss_iou: 0.5314, stage1_loss_mask: 0.6792, stage2_loss_cls: 0.6192, stage2_pos_acc: 61.7310, stage2_loss_bbox: 0.2104, stage2_loss_iou: 0.4379, stage2_loss_mask: 0.6281, stage3_loss_cls: 0.5281, stage3_pos_acc: 66.5186, stage3_loss_bbox: 0.1963, stage3_loss_iou: 0.4096, stage3_loss_mask: 0.6152, stage4_loss_cls: 0.4946, stage4_pos_acc: 70.4219, stage4_loss_bbox: 0.1881, stage4_loss_iou: 0.3983, stage4_loss_mask: 0.6159, stage5_loss_cls: 0.4599, stage5_pos_acc: 75.6294, stage5_loss_bbox: 0.1873, stage5_loss_iou: 0.4011, stage5_loss_mask: 0.6172, loss: 12.8522\n",
      "2025-07-16 15:35:31,371 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:35:31,371 - mmdet - INFO - Epoch [23][500/750]\tlr: 2.500e-05, eta: 3:15:08, time: 0.363, data_time: 0.007, memory: 11258, stage0_loss_cls: 1.0586, stage0_pos_acc: 39.5784, stage0_loss_bbox: 0.5471, stage0_loss_iou: 0.9408, stage0_loss_mask: 0.7680, stage1_loss_cls: 0.6990, stage1_pos_acc: 58.1623, stage1_loss_bbox: 0.2778, stage1_loss_iou: 0.4636, stage1_loss_mask: 0.5730, stage2_loss_cls: 0.6351, stage2_pos_acc: 65.8394, stage2_loss_bbox: 0.2297, stage2_loss_iou: 0.3832, stage2_loss_mask: 0.5228, stage3_loss_cls: 0.5474, stage3_pos_acc: 67.6506, stage3_loss_bbox: 0.2220, stage3_loss_iou: 0.3660, stage3_loss_mask: 0.5136, stage4_loss_cls: 0.4977, stage4_pos_acc: 71.9768, stage4_loss_bbox: 0.2090, stage4_loss_iou: 0.3529, stage4_loss_mask: 0.5064, stage5_loss_cls: 0.4795, stage5_pos_acc: 72.6259, stage5_loss_bbox: 0.2118, stage5_loss_iou: 0.3490, stage5_loss_mask: 0.5045, loss: 11.8585\n",
      "2025-07-16 15:35:49,592 - mmdet - INFO - Epoch [23][550/750]\tlr: 2.500e-05, eta: 3:14:45, time: 0.364, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.0628, stage0_pos_acc: 37.0873, stage0_loss_bbox: 0.4518, stage0_loss_iou: 0.9650, stage0_loss_mask: 0.7554, stage1_loss_cls: 0.6866, stage1_pos_acc: 60.1897, stage1_loss_bbox: 0.2104, stage1_loss_iou: 0.4201, stage1_loss_mask: 0.4539, stage2_loss_cls: 0.5848, stage2_pos_acc: 68.6460, stage2_loss_bbox: 0.1649, stage2_loss_iou: 0.3318, stage2_loss_mask: 0.3963, stage3_loss_cls: 0.4711, stage3_pos_acc: 68.0238, stage3_loss_bbox: 0.1587, stage3_loss_iou: 0.3135, stage3_loss_mask: 0.3765, stage4_loss_cls: 0.4115, stage4_pos_acc: 76.6762, stage4_loss_bbox: 0.1459, stage4_loss_iou: 0.2965, stage4_loss_mask: 0.3682, stage5_loss_cls: 0.3837, stage5_pos_acc: 78.3778, stage5_loss_bbox: 0.1467, stage5_loss_iou: 0.2946, stage5_loss_mask: 0.3767, loss: 10.2274\n",
      "2025-07-16 15:36:07,632 - mmdet - INFO - Epoch [23][600/750]\tlr: 2.500e-05, eta: 3:14:17, time: 0.361, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.0918, stage0_pos_acc: 37.6540, stage0_loss_bbox: 0.5411, stage0_loss_iou: 1.0206, stage0_loss_mask: 0.7569, stage1_loss_cls: 0.7146, stage1_pos_acc: 53.5253, stage1_loss_bbox: 0.2356, stage1_loss_iou: 0.4343, stage1_loss_mask: 0.4248, stage2_loss_cls: 0.5872, stage2_pos_acc: 62.9026, stage2_loss_bbox: 0.1923, stage2_loss_iou: 0.3325, stage2_loss_mask: 0.3620, stage3_loss_cls: 0.4799, stage3_pos_acc: 67.3502, stage3_loss_bbox: 0.1800, stage3_loss_iou: 0.3112, stage3_loss_mask: 0.3528, stage4_loss_cls: 0.4207, stage4_pos_acc: 72.5392, stage4_loss_bbox: 0.1696, stage4_loss_iou: 0.2978, stage4_loss_mask: 0.3487, stage5_loss_cls: 0.3990, stage5_pos_acc: 76.7230, stage5_loss_bbox: 0.1695, stage5_loss_iou: 0.2938, stage5_loss_mask: 0.3385, loss: 10.4553\n",
      "2025-07-16 15:36:25,708 - mmdet - INFO - Epoch [23][650/750]\tlr: 2.500e-05, eta: 3:13:51, time: 0.362, data_time: 0.006, memory: 11258, stage0_loss_cls: 1.0792, stage0_pos_acc: 34.6083, stage0_loss_bbox: 0.5189, stage0_loss_iou: 0.9794, stage0_loss_mask: 0.8816, stage1_loss_cls: 0.7191, stage1_pos_acc: 52.5107, stage1_loss_bbox: 0.2490, stage1_loss_iou: 0.4629, stage1_loss_mask: 0.5736, stage2_loss_cls: 0.6387, stage2_pos_acc: 59.9417, stage2_loss_bbox: 0.1920, stage2_loss_iou: 0.3638, stage2_loss_mask: 0.4901, stage3_loss_cls: 0.5250, stage3_pos_acc: 64.9750, stage3_loss_bbox: 0.1757, stage3_loss_iou: 0.3416, stage3_loss_mask: 0.4925, stage4_loss_cls: 0.4647, stage4_pos_acc: 67.5512, stage4_loss_bbox: 0.1764, stage4_loss_iou: 0.3414, stage4_loss_mask: 0.5081, stage5_loss_cls: 0.4468, stage5_pos_acc: 71.4917, stage5_loss_bbox: 0.1712, stage5_loss_iou: 0.3380, stage5_loss_mask: 0.4998, loss: 11.6294\n",
      "2025-07-16 15:36:44,027 - mmdet - INFO - Epoch [23][700/750]\tlr: 2.500e-05, eta: 3:13:31, time: 0.366, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0959, stage0_pos_acc: 34.6739, stage0_loss_bbox: 0.5533, stage0_loss_iou: 1.0154, stage0_loss_mask: 1.0262, stage1_loss_cls: 0.7431, stage1_pos_acc: 51.4516, stage1_loss_bbox: 0.2858, stage1_loss_iou: 0.5271, stage1_loss_mask: 0.7048, stage2_loss_cls: 0.6723, stage2_pos_acc: 56.8543, stage2_loss_bbox: 0.2319, stage2_loss_iou: 0.4336, stage2_loss_mask: 0.6813, stage3_loss_cls: 0.5710, stage3_pos_acc: 60.5420, stage3_loss_bbox: 0.2192, stage3_loss_iou: 0.4133, stage3_loss_mask: 0.6533, stage4_loss_cls: 0.5215, stage4_pos_acc: 66.9278, stage4_loss_bbox: 0.2136, stage4_loss_iou: 0.4065, stage4_loss_mask: 0.6499, stage5_loss_cls: 0.5142, stage5_pos_acc: 71.1477, stage5_loss_bbox: 0.2134, stage5_loss_iou: 0.4056, stage5_loss_mask: 0.6424, loss: 13.3947\n",
      "2025-07-16 15:37:04,003 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:37:04,004 - mmdet - INFO - Epoch [23][750/750]\tlr: 2.500e-05, eta: 3:13:45, time: 0.400, data_time: 0.043, memory: 11264, stage0_loss_cls: 1.0934, stage0_pos_acc: 36.3547, stage0_loss_bbox: 0.4997, stage0_loss_iou: 0.9936, stage0_loss_mask: 0.8200, stage1_loss_cls: 0.6713, stage1_pos_acc: 58.9525, stage1_loss_bbox: 0.2278, stage1_loss_iou: 0.4481, stage1_loss_mask: 0.4753, stage2_loss_cls: 0.5743, stage2_pos_acc: 64.5222, stage2_loss_bbox: 0.1873, stage2_loss_iou: 0.3657, stage2_loss_mask: 0.4148, stage3_loss_cls: 0.4781, stage3_pos_acc: 69.3226, stage3_loss_bbox: 0.1722, stage3_loss_iou: 0.3412, stage3_loss_mask: 0.3750, stage4_loss_cls: 0.4265, stage4_pos_acc: 75.0805, stage4_loss_bbox: 0.1664, stage4_loss_iou: 0.3278, stage4_loss_mask: 0.3687, stage5_loss_cls: 0.4146, stage5_pos_acc: 76.5785, stage5_loss_bbox: 0.1652, stage5_loss_iou: 0.3203, stage5_loss_mask: 0.3648, loss: 10.6922\n",
      "2025-07-16 15:37:04,102 - mmdet - INFO - Saving checkpoint at 23 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 85s, ETA:     0s2025-07-16 15:40:05,873 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.037\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.440\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.160\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.246\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.484\n",
      "2025-07-16 15:40:07,799 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.040\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.451\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.259\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.493\n",
      "2025-07-16 15:40:10,867 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:40:10,867 - mmdet - INFO - Epoch(val) [23][750]\tbbox_mAP: 0.0280, bbox_mAP_50: 0.0540, bbox_mAP_75: 0.0220, bbox_mAP_s: 0.0370, bbox_mAP_m: 0.0140, bbox_mAP_l: 0.0350, bbox_mAP_copypaste: 0.028 0.054 0.022 0.037 0.014 0.035, segm_mAP: 0.0280, segm_mAP_50: 0.0540, segm_mAP_75: 0.0270, segm_mAP_s: 0.0400, segm_mAP_m: 0.0130, segm_mAP_l: 0.0360, segm_mAP_copypaste: 0.028 0.054 0.027 0.040 0.013 0.036\n",
      "2025-07-16 15:40:31,499 - mmdet - INFO - Epoch [24][50/750]\tlr: 2.500e-05, eta: 3:14:11, time: 0.412, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.0736, stage0_pos_acc: 39.9599, stage0_loss_bbox: 0.5753, stage0_loss_iou: 1.0348, stage0_loss_mask: 0.8575, stage1_loss_cls: 0.6967, stage1_pos_acc: 58.1264, stage1_loss_bbox: 0.2793, stage1_loss_iou: 0.4977, stage1_loss_mask: 0.6093, stage2_loss_cls: 0.5868, stage2_pos_acc: 67.1409, stage2_loss_bbox: 0.2255, stage2_loss_iou: 0.3982, stage2_loss_mask: 0.5403, stage3_loss_cls: 0.4746, stage3_pos_acc: 75.0139, stage3_loss_bbox: 0.2049, stage3_loss_iou: 0.3545, stage3_loss_mask: 0.4959, stage4_loss_cls: 0.4331, stage4_pos_acc: 79.3226, stage4_loss_bbox: 0.1928, stage4_loss_iou: 0.3354, stage4_loss_mask: 0.4735, stage5_loss_cls: 0.3942, stage5_pos_acc: 82.8258, stage5_loss_bbox: 0.1900, stage5_loss_iou: 0.3349, stage5_loss_mask: 0.4800, loss: 11.7389\n",
      "2025-07-16 15:40:49,856 - mmdet - INFO - Epoch [24][100/750]\tlr: 2.500e-05, eta: 3:13:49, time: 0.367, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0568, stage0_pos_acc: 35.3652, stage0_loss_bbox: 0.5069, stage0_loss_iou: 0.9842, stage0_loss_mask: 0.7835, stage1_loss_cls: 0.6702, stage1_pos_acc: 64.4121, stage1_loss_bbox: 0.2139, stage1_loss_iou: 0.4431, stage1_loss_mask: 0.3970, stage2_loss_cls: 0.5623, stage2_pos_acc: 68.6146, stage2_loss_bbox: 0.1716, stage2_loss_iou: 0.3455, stage2_loss_mask: 0.3808, stage3_loss_cls: 0.4545, stage3_pos_acc: 74.5328, stage3_loss_bbox: 0.1556, stage3_loss_iou: 0.3195, stage3_loss_mask: 0.3696, stage4_loss_cls: 0.3896, stage4_pos_acc: 81.5662, stage4_loss_bbox: 0.1514, stage4_loss_iou: 0.3030, stage4_loss_mask: 0.3198, stage5_loss_cls: 0.3517, stage5_pos_acc: 83.1343, stage5_loss_bbox: 0.1473, stage5_loss_iou: 0.2960, stage5_loss_mask: 0.3148, loss: 10.0886\n",
      "2025-07-16 15:41:08,217 - mmdet - INFO - Epoch [24][150/750]\tlr: 2.500e-05, eta: 3:13:28, time: 0.367, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0536, stage0_pos_acc: 31.3945, stage0_loss_bbox: 0.5133, stage0_loss_iou: 0.8687, stage0_loss_mask: 0.6763, stage1_loss_cls: 0.6750, stage1_pos_acc: 58.2213, stage1_loss_bbox: 0.2593, stage1_loss_iou: 0.4046, stage1_loss_mask: 0.3769, stage2_loss_cls: 0.5986, stage2_pos_acc: 61.7297, stage2_loss_bbox: 0.1883, stage2_loss_iou: 0.3073, stage2_loss_mask: 0.3317, stage3_loss_cls: 0.5083, stage3_pos_acc: 66.9792, stage3_loss_bbox: 0.1790, stage3_loss_iou: 0.2853, stage3_loss_mask: 0.2883, stage4_loss_cls: 0.4498, stage4_pos_acc: 73.3787, stage4_loss_bbox: 0.1690, stage4_loss_iou: 0.2766, stage4_loss_mask: 0.2819, stage5_loss_cls: 0.4217, stage5_pos_acc: 76.0459, stage5_loss_bbox: 0.1638, stage5_loss_iou: 0.2717, stage5_loss_mask: 0.2884, loss: 9.8374\n",
      "2025-07-16 15:41:26,597 - mmdet - INFO - Epoch [24][200/750]\tlr: 2.500e-05, eta: 3:13:07, time: 0.368, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0949, stage0_pos_acc: 38.2089, stage0_loss_bbox: 0.5636, stage0_loss_iou: 1.0103, stage0_loss_mask: 0.7548, stage1_loss_cls: 0.7045, stage1_pos_acc: 54.3288, stage1_loss_bbox: 0.2398, stage1_loss_iou: 0.4555, stage1_loss_mask: 0.4535, stage2_loss_cls: 0.5685, stage2_pos_acc: 71.2597, stage2_loss_bbox: 0.1719, stage2_loss_iou: 0.3482, stage2_loss_mask: 0.4321, stage3_loss_cls: 0.4556, stage3_pos_acc: 75.1264, stage3_loss_bbox: 0.1638, stage3_loss_iou: 0.3187, stage3_loss_mask: 0.4238, stage4_loss_cls: 0.3981, stage4_pos_acc: 79.9955, stage4_loss_bbox: 0.1650, stage4_loss_iou: 0.3075, stage4_loss_mask: 0.4132, stage5_loss_cls: 0.3725, stage5_pos_acc: 80.6379, stage5_loss_bbox: 0.1698, stage5_loss_iou: 0.3094, stage5_loss_mask: 0.4160, loss: 10.7110\n",
      "2025-07-16 15:41:44,891 - mmdet - INFO - Epoch [24][250/750]\tlr: 2.500e-05, eta: 3:12:44, time: 0.366, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0824, stage0_pos_acc: 35.9026, stage0_loss_bbox: 0.5260, stage0_loss_iou: 0.9566, stage0_loss_mask: 0.7772, stage1_loss_cls: 0.6960, stage1_pos_acc: 60.1295, stage1_loss_bbox: 0.2258, stage1_loss_iou: 0.4413, stage1_loss_mask: 0.5108, stage2_loss_cls: 0.5823, stage2_pos_acc: 65.9110, stage2_loss_bbox: 0.1910, stage2_loss_iou: 0.3550, stage2_loss_mask: 0.4592, stage3_loss_cls: 0.4848, stage3_pos_acc: 70.9372, stage3_loss_bbox: 0.1687, stage3_loss_iou: 0.3181, stage3_loss_mask: 0.4083, stage4_loss_cls: 0.4154, stage4_pos_acc: 80.1984, stage4_loss_bbox: 0.1671, stage4_loss_iou: 0.3105, stage4_loss_mask: 0.4068, stage5_loss_cls: 0.3750, stage5_pos_acc: 82.8443, stage5_loss_bbox: 0.1691, stage5_loss_iou: 0.3112, stage5_loss_mask: 0.4133, loss: 10.7518\n",
      "2025-07-16 15:42:03,462 - mmdet - INFO - Epoch [24][300/750]\tlr: 2.500e-05, eta: 3:12:27, time: 0.371, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1104, stage0_pos_acc: 36.4600, stage0_loss_bbox: 0.4956, stage0_loss_iou: 0.9741, stage0_loss_mask: 0.6502, stage1_loss_cls: 0.6945, stage1_pos_acc: 57.3481, stage1_loss_bbox: 0.2030, stage1_loss_iou: 0.4284, stage1_loss_mask: 0.3547, stage2_loss_cls: 0.6307, stage2_pos_acc: 60.3352, stage2_loss_bbox: 0.1543, stage2_loss_iou: 0.3016, stage2_loss_mask: 0.3416, stage3_loss_cls: 0.5210, stage3_pos_acc: 62.9237, stage3_loss_bbox: 0.1395, stage3_loss_iou: 0.2751, stage3_loss_mask: 0.3070, stage4_loss_cls: 0.4582, stage4_pos_acc: 70.5208, stage4_loss_bbox: 0.1326, stage4_loss_iou: 0.2613, stage4_loss_mask: 0.3026, stage5_loss_cls: 0.4338, stage5_pos_acc: 76.9653, stage5_loss_bbox: 0.1286, stage5_loss_iou: 0.2537, stage5_loss_mask: 0.3066, loss: 9.8592\n",
      "2025-07-16 15:42:21,925 - mmdet - INFO - Epoch [24][350/750]\tlr: 2.500e-05, eta: 3:12:08, time: 0.369, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0851, stage0_pos_acc: 32.6222, stage0_loss_bbox: 0.5072, stage0_loss_iou: 0.9633, stage0_loss_mask: 0.8394, stage1_loss_cls: 0.6931, stage1_pos_acc: 58.4683, stage1_loss_bbox: 0.2313, stage1_loss_iou: 0.4488, stage1_loss_mask: 0.5387, stage2_loss_cls: 0.5730, stage2_pos_acc: 61.7895, stage2_loss_bbox: 0.1816, stage2_loss_iou: 0.3701, stage2_loss_mask: 0.4998, stage3_loss_cls: 0.4752, stage3_pos_acc: 70.5212, stage3_loss_bbox: 0.1664, stage3_loss_iou: 0.3560, stage3_loss_mask: 0.4784, stage4_loss_cls: 0.4204, stage4_pos_acc: 75.8070, stage4_loss_bbox: 0.1601, stage4_loss_iou: 0.3267, stage4_loss_mask: 0.4655, stage5_loss_cls: 0.3866, stage5_pos_acc: 79.8591, stage5_loss_bbox: 0.1672, stage5_loss_iou: 0.3326, stage5_loss_mask: 0.4794, loss: 11.1459\n",
      "2025-07-16 15:42:40,266 - mmdet - INFO - Epoch [24][400/750]\tlr: 2.500e-05, eta: 3:11:46, time: 0.367, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0893, stage0_pos_acc: 38.4226, stage0_loss_bbox: 0.5449, stage0_loss_iou: 1.0629, stage0_loss_mask: 0.9993, stage1_loss_cls: 0.6895, stage1_pos_acc: 58.0274, stage1_loss_bbox: 0.2702, stage1_loss_iou: 0.5139, stage1_loss_mask: 0.6349, stage2_loss_cls: 0.6084, stage2_pos_acc: 63.1234, stage2_loss_bbox: 0.2106, stage2_loss_iou: 0.4056, stage2_loss_mask: 0.5987, stage3_loss_cls: 0.5213, stage3_pos_acc: 65.9111, stage3_loss_bbox: 0.1957, stage3_loss_iou: 0.3720, stage3_loss_mask: 0.5726, stage4_loss_cls: 0.4469, stage4_pos_acc: 70.8504, stage4_loss_bbox: 0.1833, stage4_loss_iou: 0.3529, stage4_loss_mask: 0.5363, stage5_loss_cls: 0.4255, stage5_pos_acc: 73.8897, stage5_loss_bbox: 0.1791, stage5_loss_iou: 0.3517, stage5_loss_mask: 0.5495, loss: 12.3151\n",
      "2025-07-16 15:42:58,416 - mmdet - INFO - Epoch [24][450/750]\tlr: 2.500e-05, eta: 3:11:22, time: 0.363, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1119, stage0_pos_acc: 34.2548, stage0_loss_bbox: 0.5140, stage0_loss_iou: 1.0050, stage0_loss_mask: 0.8497, stage1_loss_cls: 0.6898, stage1_pos_acc: 57.0905, stage1_loss_bbox: 0.2334, stage1_loss_iou: 0.4739, stage1_loss_mask: 0.5516, stage2_loss_cls: 0.5766, stage2_pos_acc: 64.1500, stage2_loss_bbox: 0.2039, stage2_loss_iou: 0.3797, stage2_loss_mask: 0.5173, stage3_loss_cls: 0.4860, stage3_pos_acc: 72.8452, stage3_loss_bbox: 0.1938, stage3_loss_iou: 0.3544, stage3_loss_mask: 0.4928, stage4_loss_cls: 0.4119, stage4_pos_acc: 77.9952, stage4_loss_bbox: 0.1901, stage4_loss_iou: 0.3515, stage4_loss_mask: 0.5082, stage5_loss_cls: 0.3776, stage5_pos_acc: 81.8071, stage5_loss_bbox: 0.1886, stage5_loss_iou: 0.3512, stage5_loss_mask: 0.5083, loss: 11.5212\n",
      "2025-07-16 15:43:16,485 - mmdet - INFO - Epoch [24][500/750]\tlr: 2.500e-05, eta: 3:10:57, time: 0.361, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0856, stage0_pos_acc: 34.8878, stage0_loss_bbox: 0.5266, stage0_loss_iou: 1.0622, stage0_loss_mask: 1.2451, stage1_loss_cls: 0.7126, stage1_pos_acc: 52.9654, stage1_loss_bbox: 0.2664, stage1_loss_iou: 0.5489, stage1_loss_mask: 0.8218, stage2_loss_cls: 0.5937, stage2_pos_acc: 59.9359, stage2_loss_bbox: 0.2102, stage2_loss_iou: 0.4480, stage2_loss_mask: 0.6999, stage3_loss_cls: 0.5214, stage3_pos_acc: 67.2964, stage3_loss_bbox: 0.2044, stage3_loss_iou: 0.4167, stage3_loss_mask: 0.6757, stage4_loss_cls: 0.4708, stage4_pos_acc: 70.0116, stage4_loss_bbox: 0.1984, stage4_loss_iou: 0.4014, stage4_loss_mask: 0.6594, stage5_loss_cls: 0.4401, stage5_pos_acc: 76.0921, stage5_loss_bbox: 0.1902, stage5_loss_iou: 0.3936, stage5_loss_mask: 0.6613, loss: 13.4543\n",
      "2025-07-16 15:43:34,477 - mmdet - INFO - Epoch [24][550/750]\tlr: 2.500e-05, eta: 3:10:31, time: 0.360, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0984, stage0_pos_acc: 37.2962, stage0_loss_bbox: 0.5109, stage0_loss_iou: 1.0555, stage0_loss_mask: 0.9652, stage1_loss_cls: 0.7147, stage1_pos_acc: 54.1442, stage1_loss_bbox: 0.2328, stage1_loss_iou: 0.5309, stage1_loss_mask: 0.7005, stage2_loss_cls: 0.6237, stage2_pos_acc: 61.4382, stage2_loss_bbox: 0.1871, stage2_loss_iou: 0.4276, stage2_loss_mask: 0.5851, stage3_loss_cls: 0.5223, stage3_pos_acc: 64.5883, stage3_loss_bbox: 0.1776, stage3_loss_iou: 0.3922, stage3_loss_mask: 0.5686, stage4_loss_cls: 0.4654, stage4_pos_acc: 71.9331, stage4_loss_bbox: 0.1632, stage4_loss_iou: 0.3722, stage4_loss_mask: 0.5461, stage5_loss_cls: 0.4379, stage5_pos_acc: 74.3573, stage5_loss_bbox: 0.1611, stage5_loss_iou: 0.3667, stage5_loss_mask: 0.5482, loss: 12.3538\n",
      "2025-07-16 15:43:52,705 - mmdet - INFO - Epoch [24][600/750]\tlr: 2.500e-05, eta: 3:10:09, time: 0.365, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0704, stage0_pos_acc: 39.9515, stage0_loss_bbox: 0.5113, stage0_loss_iou: 0.9596, stage0_loss_mask: 0.8074, stage1_loss_cls: 0.6824, stage1_pos_acc: 60.6247, stage1_loss_bbox: 0.2393, stage1_loss_iou: 0.4581, stage1_loss_mask: 0.5393, stage2_loss_cls: 0.5998, stage2_pos_acc: 64.8736, stage2_loss_bbox: 0.1975, stage2_loss_iou: 0.3776, stage2_loss_mask: 0.5122, stage3_loss_cls: 0.4962, stage3_pos_acc: 67.0310, stage3_loss_bbox: 0.1857, stage3_loss_iou: 0.3664, stage3_loss_mask: 0.5223, stage4_loss_cls: 0.4347, stage4_pos_acc: 73.5053, stage4_loss_bbox: 0.1835, stage4_loss_iou: 0.3505, stage4_loss_mask: 0.4956, stage5_loss_cls: 0.4107, stage5_pos_acc: 75.6495, stage5_loss_bbox: 0.1861, stage5_loss_iou: 0.3489, stage5_loss_mask: 0.5054, loss: 11.4409\n",
      "2025-07-16 15:44:11,131 - mmdet - INFO - Epoch [24][650/750]\tlr: 2.500e-05, eta: 3:09:50, time: 0.369, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.1215, stage0_pos_acc: 35.2592, stage0_loss_bbox: 0.5353, stage0_loss_iou: 1.0360, stage0_loss_mask: 0.8927, stage1_loss_cls: 0.7102, stage1_pos_acc: 57.2412, stage1_loss_bbox: 0.2401, stage1_loss_iou: 0.5002, stage1_loss_mask: 0.5475, stage2_loss_cls: 0.6146, stage2_pos_acc: 65.1559, stage2_loss_bbox: 0.1850, stage2_loss_iou: 0.3709, stage2_loss_mask: 0.4842, stage3_loss_cls: 0.4923, stage3_pos_acc: 70.2970, stage3_loss_bbox: 0.1799, stage3_loss_iou: 0.3559, stage3_loss_mask: 0.4712, stage4_loss_cls: 0.4293, stage4_pos_acc: 76.7290, stage4_loss_bbox: 0.1751, stage4_loss_iou: 0.3507, stage4_loss_mask: 0.4720, stage5_loss_cls: 0.4055, stage5_pos_acc: 80.5127, stage5_loss_bbox: 0.1700, stage5_loss_iou: 0.3418, stage5_loss_mask: 0.4783, loss: 11.5603\n",
      "2025-07-16 15:44:29,404 - mmdet - INFO - Epoch [24][700/750]\tlr: 2.500e-05, eta: 3:09:29, time: 0.365, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0937, stage0_pos_acc: 35.5937, stage0_loss_bbox: 0.5071, stage0_loss_iou: 0.9897, stage0_loss_mask: 0.8042, stage1_loss_cls: 0.6932, stage1_pos_acc: 58.2487, stage1_loss_bbox: 0.2096, stage1_loss_iou: 0.4116, stage1_loss_mask: 0.3039, stage2_loss_cls: 0.5737, stage2_pos_acc: 59.9463, stage2_loss_bbox: 0.1513, stage2_loss_iou: 0.2893, stage2_loss_mask: 0.2633, stage3_loss_cls: 0.4856, stage3_pos_acc: 69.7299, stage3_loss_bbox: 0.1418, stage3_loss_iou: 0.2675, stage3_loss_mask: 0.2625, stage4_loss_cls: 0.4383, stage4_pos_acc: 71.0249, stage4_loss_bbox: 0.1351, stage4_loss_iou: 0.2562, stage4_loss_mask: 0.2516, stage5_loss_cls: 0.4249, stage5_pos_acc: 74.2861, stage5_loss_bbox: 0.1353, stage5_loss_iou: 0.2512, stage5_loss_mask: 0.2561, loss: 9.5966\n",
      "2025-07-16 15:44:47,773 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:44:47,773 - mmdet - INFO - Epoch [24][750/750]\tlr: 2.500e-05, eta: 3:09:10, time: 0.367, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1033, stage0_pos_acc: 33.0154, stage0_loss_bbox: 0.5660, stage0_loss_iou: 1.0212, stage0_loss_mask: 0.9371, stage1_loss_cls: 0.7782, stage1_pos_acc: 57.9897, stage1_loss_bbox: 0.2670, stage1_loss_iou: 0.5177, stage1_loss_mask: 0.5253, stage2_loss_cls: 0.6583, stage2_pos_acc: 61.9888, stage2_loss_bbox: 0.2296, stage2_loss_iou: 0.4146, stage2_loss_mask: 0.4777, stage3_loss_cls: 0.5591, stage3_pos_acc: 67.6309, stage3_loss_bbox: 0.2018, stage3_loss_iou: 0.3665, stage3_loss_mask: 0.4681, stage4_loss_cls: 0.4877, stage4_pos_acc: 72.2833, stage4_loss_bbox: 0.1982, stage4_loss_iou: 0.3506, stage4_loss_mask: 0.4714, stage5_loss_cls: 0.4623, stage5_pos_acc: 74.9643, stage5_loss_bbox: 0.1947, stage5_loss_iou: 0.3468, stage5_loss_mask: 0.4704, loss: 12.0735\n",
      "2025-07-16 15:44:47,897 - mmdet - INFO - Saving checkpoint at 24 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 85s, ETA:     0s2025-07-16 15:47:50,962 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.22s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.046\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.162\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.437\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.491\n",
      "2025-07-16 15:47:52,828 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.76s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.046\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.183\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.441\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.260\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.223\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.488\n",
      "2025-07-16 15:47:55,821 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:47:55,821 - mmdet - INFO - Epoch(val) [24][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0460, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.1620, bbox_mAP_m: 0.0070, bbox_mAP_l: 0.0310, bbox_mAP_copypaste: 0.025 0.046 0.023 0.162 0.007 0.031, segm_mAP: 0.0250, segm_mAP_50: 0.0460, segm_mAP_75: 0.0240, segm_mAP_s: 0.1830, segm_mAP_m: 0.0070, segm_mAP_l: 0.0310, segm_mAP_copypaste: 0.025 0.046 0.024 0.183 0.007 0.031\n",
      "2025-07-16 15:48:16,387 - mmdet - INFO - Epoch [25][50/750]\tlr: 2.500e-05, eta: 3:09:19, time: 0.411, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.0834, stage0_pos_acc: 36.5680, stage0_loss_bbox: 0.5284, stage0_loss_iou: 0.9937, stage0_loss_mask: 0.8369, stage1_loss_cls: 0.6840, stage1_pos_acc: 62.6498, stage1_loss_bbox: 0.2232, stage1_loss_iou: 0.4172, stage1_loss_mask: 0.3741, stage2_loss_cls: 0.5684, stage2_pos_acc: 67.7983, stage2_loss_bbox: 0.1822, stage2_loss_iou: 0.3207, stage2_loss_mask: 0.3401, stage3_loss_cls: 0.4615, stage3_pos_acc: 73.1403, stage3_loss_bbox: 0.1612, stage3_loss_iou: 0.2874, stage3_loss_mask: 0.3088, stage4_loss_cls: 0.3932, stage4_pos_acc: 82.7078, stage4_loss_bbox: 0.1564, stage4_loss_iou: 0.2765, stage4_loss_mask: 0.3060, stage5_loss_cls: 0.3762, stage5_pos_acc: 79.6459, stage5_loss_bbox: 0.1481, stage5_loss_iou: 0.2648, stage5_loss_mask: 0.2789, loss: 9.9714\n",
      "2025-07-16 15:48:34,845 - mmdet - INFO - Epoch [25][100/750]\tlr: 2.500e-05, eta: 3:09:00, time: 0.369, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0783, stage0_pos_acc: 38.3033, stage0_loss_bbox: 0.5128, stage0_loss_iou: 1.0189, stage0_loss_mask: 0.9165, stage1_loss_cls: 0.6728, stage1_pos_acc: 56.4599, stage1_loss_bbox: 0.2362, stage1_loss_iou: 0.4788, stage1_loss_mask: 0.4879, stage2_loss_cls: 0.5291, stage2_pos_acc: 69.5261, stage2_loss_bbox: 0.1879, stage2_loss_iou: 0.3637, stage2_loss_mask: 0.4270, stage3_loss_cls: 0.4243, stage3_pos_acc: 75.7013, stage3_loss_bbox: 0.1785, stage3_loss_iou: 0.3343, stage3_loss_mask: 0.4133, stage4_loss_cls: 0.3550, stage4_pos_acc: 81.4817, stage4_loss_bbox: 0.1676, stage4_loss_iou: 0.3241, stage4_loss_mask: 0.4136, stage5_loss_cls: 0.3303, stage5_pos_acc: 84.2723, stage5_loss_bbox: 0.1671, stage5_loss_iou: 0.3268, stage5_loss_mask: 0.4127, loss: 10.7577\n",
      "2025-07-16 15:48:52,900 - mmdet - INFO - Epoch [25][150/750]\tlr: 2.500e-05, eta: 3:08:36, time: 0.361, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0869, stage0_pos_acc: 41.2667, stage0_loss_bbox: 0.5085, stage0_loss_iou: 0.9983, stage0_loss_mask: 0.8745, stage1_loss_cls: 0.6827, stage1_pos_acc: 55.8667, stage1_loss_bbox: 0.2170, stage1_loss_iou: 0.4426, stage1_loss_mask: 0.4625, stage2_loss_cls: 0.5536, stage2_pos_acc: 68.4000, stage2_loss_bbox: 0.1727, stage2_loss_iou: 0.3500, stage2_loss_mask: 0.4480, stage3_loss_cls: 0.4390, stage3_pos_acc: 76.1667, stage3_loss_bbox: 0.1620, stage3_loss_iou: 0.3253, stage3_loss_mask: 0.4257, stage4_loss_cls: 0.3872, stage4_pos_acc: 79.0000, stage4_loss_bbox: 0.1553, stage4_loss_iou: 0.3068, stage4_loss_mask: 0.3969, stage5_loss_cls: 0.3442, stage5_pos_acc: 84.9000, stage5_loss_bbox: 0.1441, stage5_loss_iou: 0.3061, stage5_loss_mask: 0.4042, loss: 10.5940\n",
      "2025-07-16 15:49:11,160 - mmdet - INFO - Epoch [25][200/750]\tlr: 2.500e-05, eta: 3:08:15, time: 0.365, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0825, stage0_pos_acc: 38.4649, stage0_loss_bbox: 0.5264, stage0_loss_iou: 0.9521, stage0_loss_mask: 0.9215, stage1_loss_cls: 0.6833, stage1_pos_acc: 56.7038, stage1_loss_bbox: 0.2614, stage1_loss_iou: 0.4838, stage1_loss_mask: 0.5885, stage2_loss_cls: 0.6132, stage2_pos_acc: 63.9681, stage2_loss_bbox: 0.2081, stage2_loss_iou: 0.3836, stage2_loss_mask: 0.5050, stage3_loss_cls: 0.5115, stage3_pos_acc: 67.8106, stage3_loss_bbox: 0.1979, stage3_loss_iou: 0.3608, stage3_loss_mask: 0.4621, stage4_loss_cls: 0.4433, stage4_pos_acc: 72.4552, stage4_loss_bbox: 0.1894, stage4_loss_iou: 0.3511, stage4_loss_mask: 0.4673, stage5_loss_cls: 0.4240, stage5_pos_acc: 74.7243, stage5_loss_bbox: 0.1922, stage5_loss_iou: 0.3485, stage5_loss_mask: 0.4622, loss: 11.6195\n",
      "2025-07-16 15:49:29,400 - mmdet - INFO - Epoch [25][250/750]\tlr: 2.500e-05, eta: 3:07:53, time: 0.365, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0514, stage0_pos_acc: 42.9531, stage0_loss_bbox: 0.5681, stage0_loss_iou: 1.0332, stage0_loss_mask: 0.8791, stage1_loss_cls: 0.6846, stage1_pos_acc: 61.0941, stage1_loss_bbox: 0.2414, stage1_loss_iou: 0.4908, stage1_loss_mask: 0.5972, stage2_loss_cls: 0.5914, stage2_pos_acc: 68.4907, stage2_loss_bbox: 0.1897, stage2_loss_iou: 0.3800, stage2_loss_mask: 0.5521, stage3_loss_cls: 0.5001, stage3_pos_acc: 70.3216, stage3_loss_bbox: 0.1831, stage3_loss_iou: 0.3602, stage3_loss_mask: 0.5532, stage4_loss_cls: 0.4217, stage4_pos_acc: 78.6343, stage4_loss_bbox: 0.1861, stage4_loss_iou: 0.3620, stage4_loss_mask: 0.5533, stage5_loss_cls: 0.3878, stage5_pos_acc: 82.0114, stage5_loss_bbox: 0.1854, stage5_loss_iou: 0.3642, stage5_loss_mask: 0.5664, loss: 11.8827\n",
      "2025-07-16 15:49:47,562 - mmdet - INFO - Epoch [25][300/750]\tlr: 2.500e-05, eta: 3:07:31, time: 0.363, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0838, stage0_pos_acc: 36.7607, stage0_loss_bbox: 0.4909, stage0_loss_iou: 0.9561, stage0_loss_mask: 0.8354, stage1_loss_cls: 0.6705, stage1_pos_acc: 60.7685, stage1_loss_bbox: 0.2293, stage1_loss_iou: 0.4485, stage1_loss_mask: 0.5754, stage2_loss_cls: 0.5733, stage2_pos_acc: 69.1290, stage2_loss_bbox: 0.1740, stage2_loss_iou: 0.3462, stage2_loss_mask: 0.4357, stage3_loss_cls: 0.4751, stage3_pos_acc: 71.0107, stage3_loss_bbox: 0.1632, stage3_loss_iou: 0.3222, stage3_loss_mask: 0.4341, stage4_loss_cls: 0.3978, stage4_pos_acc: 78.7896, stage4_loss_bbox: 0.1641, stage4_loss_iou: 0.3130, stage4_loss_mask: 0.4076, stage5_loss_cls: 0.3627, stage5_pos_acc: 81.6377, stage5_loss_bbox: 0.1688, stage5_loss_iou: 0.3070, stage5_loss_mask: 0.4233, loss: 10.7580\n",
      "2025-07-16 15:50:05,876 - mmdet - INFO - Epoch [25][350/750]\tlr: 2.500e-05, eta: 3:07:10, time: 0.366, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1127, stage0_pos_acc: 33.0323, stage0_loss_bbox: 0.5115, stage0_loss_iou: 0.9829, stage0_loss_mask: 0.7454, stage1_loss_cls: 0.7074, stage1_pos_acc: 57.2101, stage1_loss_bbox: 0.2337, stage1_loss_iou: 0.4473, stage1_loss_mask: 0.4536, stage2_loss_cls: 0.6059, stage2_pos_acc: 64.2641, stage2_loss_bbox: 0.1771, stage2_loss_iou: 0.3416, stage2_loss_mask: 0.4242, stage3_loss_cls: 0.5045, stage3_pos_acc: 69.8736, stage3_loss_bbox: 0.1664, stage3_loss_iou: 0.3240, stage3_loss_mask: 0.3994, stage4_loss_cls: 0.4382, stage4_pos_acc: 73.5239, stage4_loss_bbox: 0.1539, stage4_loss_iou: 0.3027, stage4_loss_mask: 0.3826, stage5_loss_cls: 0.4160, stage5_pos_acc: 76.4152, stage5_loss_bbox: 0.1548, stage5_loss_iou: 0.2945, stage5_loss_mask: 0.3774, loss: 10.6577\n",
      "2025-07-16 15:50:24,185 - mmdet - INFO - Epoch [25][400/750]\tlr: 2.500e-05, eta: 3:06:50, time: 0.366, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1047, stage0_pos_acc: 33.5881, stage0_loss_bbox: 0.5527, stage0_loss_iou: 1.0078, stage0_loss_mask: 0.9335, stage1_loss_cls: 0.7284, stage1_pos_acc: 59.7165, stage1_loss_bbox: 0.2512, stage1_loss_iou: 0.4919, stage1_loss_mask: 0.5946, stage2_loss_cls: 0.6226, stage2_pos_acc: 64.1790, stage2_loss_bbox: 0.2093, stage2_loss_iou: 0.3849, stage2_loss_mask: 0.5411, stage3_loss_cls: 0.5098, stage3_pos_acc: 66.9919, stage3_loss_bbox: 0.1981, stage3_loss_iou: 0.3522, stage3_loss_mask: 0.5128, stage4_loss_cls: 0.4379, stage4_pos_acc: 77.1033, stage4_loss_bbox: 0.1883, stage4_loss_iou: 0.3418, stage4_loss_mask: 0.5083, stage5_loss_cls: 0.4119, stage5_pos_acc: 79.3662, stage5_loss_bbox: 0.1853, stage5_loss_iou: 0.3416, stage5_loss_mask: 0.5209, loss: 11.9317\n",
      "2025-07-16 15:50:42,496 - mmdet - INFO - Epoch [25][450/750]\tlr: 2.500e-05, eta: 3:06:30, time: 0.366, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0831, stage0_pos_acc: 35.2758, stage0_loss_bbox: 0.5449, stage0_loss_iou: 0.9964, stage0_loss_mask: 0.8680, stage1_loss_cls: 0.7042, stage1_pos_acc: 59.3895, stage1_loss_bbox: 0.2276, stage1_loss_iou: 0.4620, stage1_loss_mask: 0.5119, stage2_loss_cls: 0.5834, stage2_pos_acc: 69.0102, stage2_loss_bbox: 0.1820, stage2_loss_iou: 0.3693, stage2_loss_mask: 0.4747, stage3_loss_cls: 0.4899, stage3_pos_acc: 74.2657, stage3_loss_bbox: 0.1663, stage3_loss_iou: 0.3408, stage3_loss_mask: 0.4381, stage4_loss_cls: 0.4485, stage4_pos_acc: 76.4372, stage4_loss_bbox: 0.1660, stage4_loss_iou: 0.3302, stage4_loss_mask: 0.4387, stage5_loss_cls: 0.4075, stage5_pos_acc: 80.4832, stage5_loss_bbox: 0.1731, stage5_loss_iou: 0.3348, stage5_loss_mask: 0.4529, loss: 11.1945\n",
      "2025-07-16 15:51:00,571 - mmdet - INFO - Epoch [25][500/750]\tlr: 2.500e-05, eta: 3:06:07, time: 0.361, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1272, stage0_pos_acc: 34.6295, stage0_loss_bbox: 0.5183, stage0_loss_iou: 0.9785, stage0_loss_mask: 0.7619, stage1_loss_cls: 0.6700, stage1_pos_acc: 62.9793, stage1_loss_bbox: 0.2175, stage1_loss_iou: 0.4370, stage1_loss_mask: 0.4630, stage2_loss_cls: 0.5422, stage2_pos_acc: 68.8736, stage2_loss_bbox: 0.1806, stage2_loss_iou: 0.3350, stage2_loss_mask: 0.4085, stage3_loss_cls: 0.4212, stage3_pos_acc: 70.4293, stage3_loss_bbox: 0.1657, stage3_loss_iou: 0.3103, stage3_loss_mask: 0.4139, stage4_loss_cls: 0.3641, stage4_pos_acc: 75.4192, stage4_loss_bbox: 0.1598, stage4_loss_iou: 0.2900, stage4_loss_mask: 0.3933, stage5_loss_cls: 0.3313, stage5_pos_acc: 81.1159, stage5_loss_bbox: 0.1664, stage5_loss_iou: 0.2937, stage5_loss_mask: 0.3943, loss: 10.3437\n",
      "2025-07-16 15:51:18,849 - mmdet - INFO - Epoch [25][550/750]\tlr: 2.500e-05, eta: 3:05:47, time: 0.366, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0656, stage0_pos_acc: 36.0990, stage0_loss_bbox: 0.5010, stage0_loss_iou: 1.0337, stage0_loss_mask: 0.8912, stage1_loss_cls: 0.6730, stage1_pos_acc: 59.9052, stage1_loss_bbox: 0.2068, stage1_loss_iou: 0.4655, stage1_loss_mask: 0.5551, stage2_loss_cls: 0.5456, stage2_pos_acc: 70.1366, stage2_loss_bbox: 0.1662, stage2_loss_iou: 0.3698, stage2_loss_mask: 0.4797, stage3_loss_cls: 0.4299, stage3_pos_acc: 74.3223, stage3_loss_bbox: 0.1717, stage3_loss_iou: 0.3639, stage3_loss_mask: 0.4926, stage4_loss_cls: 0.3723, stage4_pos_acc: 77.2940, stage4_loss_bbox: 0.1520, stage4_loss_iou: 0.3453, stage4_loss_mask: 0.4644, stage5_loss_cls: 0.3262, stage5_pos_acc: 83.5421, stage5_loss_bbox: 0.1632, stage5_loss_iou: 0.3507, stage5_loss_mask: 0.4606, loss: 11.0463\n",
      "2025-07-16 15:51:37,020 - mmdet - INFO - Epoch [25][600/750]\tlr: 2.500e-05, eta: 3:05:25, time: 0.363, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0607, stage0_pos_acc: 35.7847, stage0_loss_bbox: 0.5073, stage0_loss_iou: 0.9576, stage0_loss_mask: 0.7698, stage1_loss_cls: 0.6544, stage1_pos_acc: 59.6294, stage1_loss_bbox: 0.2334, stage1_loss_iou: 0.4561, stage1_loss_mask: 0.5005, stage2_loss_cls: 0.5536, stage2_pos_acc: 68.5887, stage2_loss_bbox: 0.1940, stage2_loss_iou: 0.3671, stage2_loss_mask: 0.4507, stage3_loss_cls: 0.4519, stage3_pos_acc: 74.6265, stage3_loss_bbox: 0.1978, stage3_loss_iou: 0.3541, stage3_loss_mask: 0.4482, stage4_loss_cls: 0.4017, stage4_pos_acc: 78.3948, stage4_loss_bbox: 0.1935, stage4_loss_iou: 0.3465, stage4_loss_mask: 0.4367, stage5_loss_cls: 0.3751, stage5_pos_acc: 84.1254, stage5_loss_bbox: 0.1903, stage5_loss_iou: 0.3415, stage5_loss_mask: 0.4440, loss: 10.8868\n",
      "2025-07-16 15:51:55,136 - mmdet - INFO - Epoch [25][650/750]\tlr: 2.500e-05, eta: 3:05:03, time: 0.362, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0937, stage0_pos_acc: 31.4213, stage0_loss_bbox: 0.5736, stage0_loss_iou: 0.9938, stage0_loss_mask: 0.9427, stage1_loss_cls: 0.7069, stage1_pos_acc: 62.5111, stage1_loss_bbox: 0.2961, stage1_loss_iou: 0.4990, stage1_loss_mask: 0.6478, stage2_loss_cls: 0.5814, stage2_pos_acc: 64.2145, stage2_loss_bbox: 0.2474, stage2_loss_iou: 0.3908, stage2_loss_mask: 0.5573, stage3_loss_cls: 0.4616, stage3_pos_acc: 69.3189, stage3_loss_bbox: 0.2425, stage3_loss_iou: 0.3778, stage3_loss_mask: 0.5868, stage4_loss_cls: 0.3963, stage4_pos_acc: 73.3416, stage4_loss_bbox: 0.2335, stage4_loss_iou: 0.3597, stage4_loss_mask: 0.5704, stage5_loss_cls: 0.3832, stage5_pos_acc: 77.9942, stage5_loss_bbox: 0.2256, stage5_loss_iou: 0.3543, stage5_loss_mask: 0.5592, loss: 12.2815\n",
      "2025-07-16 15:52:13,235 - mmdet - INFO - Epoch [25][700/750]\tlr: 2.500e-05, eta: 3:04:41, time: 0.362, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1019, stage0_pos_acc: 35.3618, stage0_loss_bbox: 0.4693, stage0_loss_iou: 1.0017, stage0_loss_mask: 0.7586, stage1_loss_cls: 0.6594, stage1_pos_acc: 58.9059, stage1_loss_bbox: 0.2080, stage1_loss_iou: 0.4435, stage1_loss_mask: 0.5161, stage2_loss_cls: 0.5695, stage2_pos_acc: 66.6647, stage2_loss_bbox: 0.1550, stage2_loss_iou: 0.3464, stage2_loss_mask: 0.5112, stage3_loss_cls: 0.4639, stage3_pos_acc: 72.4566, stage3_loss_bbox: 0.1466, stage3_loss_iou: 0.3235, stage3_loss_mask: 0.4729, stage4_loss_cls: 0.3866, stage4_pos_acc: 77.9527, stage4_loss_bbox: 0.1359, stage4_loss_iou: 0.3148, stage4_loss_mask: 0.4795, stage5_loss_cls: 0.3506, stage5_pos_acc: 82.4989, stage5_loss_bbox: 0.1342, stage5_loss_iou: 0.3101, stage5_loss_mask: 0.4577, loss: 10.7168\n",
      "2025-07-16 15:52:31,580 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:52:31,580 - mmdet - INFO - Epoch [25][750/750]\tlr: 2.500e-05, eta: 3:04:22, time: 0.367, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0769, stage0_pos_acc: 42.5352, stage0_loss_bbox: 0.4757, stage0_loss_iou: 1.0162, stage0_loss_mask: 0.7303, stage1_loss_cls: 0.6822, stage1_pos_acc: 54.7392, stage1_loss_bbox: 0.2135, stage1_loss_iou: 0.4487, stage1_loss_mask: 0.4586, stage2_loss_cls: 0.5934, stage2_pos_acc: 69.1453, stage2_loss_bbox: 0.1659, stage2_loss_iou: 0.3294, stage2_loss_mask: 0.4098, stage3_loss_cls: 0.4544, stage3_pos_acc: 71.9573, stage3_loss_bbox: 0.1609, stage3_loss_iou: 0.3109, stage3_loss_mask: 0.3757, stage4_loss_cls: 0.3918, stage4_pos_acc: 76.8379, stage4_loss_bbox: 0.1549, stage4_loss_iou: 0.3075, stage4_loss_mask: 0.3688, stage5_loss_cls: 0.3870, stage5_pos_acc: 77.2590, stage5_loss_bbox: 0.1431, stage5_loss_iou: 0.2968, stage5_loss_mask: 0.3632, loss: 10.3158\n",
      "2025-07-16 15:52:31,694 - mmdet - INFO - Saving checkpoint at 25 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 85s, ETA:     0s2025-07-16 15:55:34,334 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.046\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.084\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.429\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.348\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.454\n",
      "2025-07-16 15:55:36,140 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.80s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.442\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.442\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.233\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.353\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.462\n",
      "2025-07-16 15:55:39,143 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:55:39,143 - mmdet - INFO - Epoch(val) [25][750]\tbbox_mAP: 0.0240, bbox_mAP_50: 0.0460, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.0840, bbox_mAP_m: 0.0170, bbox_mAP_l: 0.0290, bbox_mAP_copypaste: 0.024 0.046 0.023 0.084 0.017 0.029, segm_mAP: 0.0240, segm_mAP_50: 0.0450, segm_mAP_75: 0.0230, segm_mAP_s: 0.0520, segm_mAP_m: 0.0160, segm_mAP_l: 0.0310, segm_mAP_copypaste: 0.024 0.045 0.023 0.052 0.016 0.031\n",
      "2025-07-16 15:56:00,326 - mmdet - INFO - Epoch [26][50/750]\tlr: 2.500e-05, eta: 3:04:30, time: 0.423, data_time: 0.054, memory: 11264, stage0_loss_cls: 1.1310, stage0_pos_acc: 38.5557, stage0_loss_bbox: 0.4953, stage0_loss_iou: 1.0552, stage0_loss_mask: 0.8786, stage1_loss_cls: 0.6948, stage1_pos_acc: 61.8596, stage1_loss_bbox: 0.2120, stage1_loss_iou: 0.4506, stage1_loss_mask: 0.4191, stage2_loss_cls: 0.5385, stage2_pos_acc: 69.2350, stage2_loss_bbox: 0.1630, stage2_loss_iou: 0.3489, stage2_loss_mask: 0.3842, stage3_loss_cls: 0.4262, stage3_pos_acc: 74.9235, stage3_loss_bbox: 0.1596, stage3_loss_iou: 0.3239, stage3_loss_mask: 0.3677, stage4_loss_cls: 0.3586, stage4_pos_acc: 78.5573, stage4_loss_bbox: 0.1517, stage4_loss_iou: 0.3057, stage4_loss_mask: 0.3179, stage5_loss_cls: 0.3241, stage5_pos_acc: 82.4962, stage5_loss_bbox: 0.1436, stage5_loss_iou: 0.3000, stage5_loss_mask: 0.3428, loss: 10.2928\n",
      "2025-07-16 15:56:19,451 - mmdet - INFO - Epoch [26][100/750]\tlr: 2.500e-05, eta: 3:04:18, time: 0.382, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1043, stage0_pos_acc: 32.6333, stage0_loss_bbox: 0.4705, stage0_loss_iou: 0.9711, stage0_loss_mask: 0.5355, stage1_loss_cls: 0.6245, stage1_pos_acc: 60.2333, stage1_loss_bbox: 0.1906, stage1_loss_iou: 0.3846, stage1_loss_mask: 0.3394, stage2_loss_cls: 0.5316, stage2_pos_acc: 69.2833, stage2_loss_bbox: 0.1510, stage2_loss_iou: 0.3095, stage2_loss_mask: 0.2991, stage3_loss_cls: 0.4410, stage3_pos_acc: 71.9833, stage3_loss_bbox: 0.1391, stage3_loss_iou: 0.2829, stage3_loss_mask: 0.2747, stage4_loss_cls: 0.3606, stage4_pos_acc: 78.1833, stage4_loss_bbox: 0.1385, stage4_loss_iou: 0.2703, stage4_loss_mask: 0.2786, stage5_loss_cls: 0.3090, stage5_pos_acc: 83.1333, stage5_loss_bbox: 0.1336, stage5_loss_iou: 0.2656, stage5_loss_mask: 0.2771, loss: 9.0828\n",
      "2025-07-16 15:56:38,417 - mmdet - INFO - Epoch [26][150/750]\tlr: 2.500e-05, eta: 3:04:04, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0843, stage0_pos_acc: 40.3018, stage0_loss_bbox: 0.5335, stage0_loss_iou: 1.0251, stage0_loss_mask: 1.0211, stage1_loss_cls: 0.6720, stage1_pos_acc: 58.2594, stage1_loss_bbox: 0.2326, stage1_loss_iou: 0.5336, stage1_loss_mask: 0.7472, stage2_loss_cls: 0.5480, stage2_pos_acc: 73.4772, stage2_loss_bbox: 0.1921, stage2_loss_iou: 0.4351, stage2_loss_mask: 0.7004, stage3_loss_cls: 0.4155, stage3_pos_acc: 75.9127, stage3_loss_bbox: 0.1743, stage3_loss_iou: 0.3966, stage3_loss_mask: 0.6605, stage4_loss_cls: 0.3557, stage4_pos_acc: 81.1449, stage4_loss_bbox: 0.1541, stage4_loss_iou: 0.3728, stage4_loss_mask: 0.6633, stage5_loss_cls: 0.3273, stage5_pos_acc: 83.6651, stage5_loss_bbox: 0.1550, stage5_loss_iou: 0.3762, stage5_loss_mask: 0.6815, loss: 12.4576\n",
      "2025-07-16 15:56:57,206 - mmdet - INFO - Epoch [26][200/750]\tlr: 2.500e-05, eta: 3:03:48, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.1194, stage0_pos_acc: 30.4317, stage0_loss_bbox: 0.4990, stage0_loss_iou: 0.9814, stage0_loss_mask: 0.6864, stage1_loss_cls: 0.6593, stage1_pos_acc: 63.3413, stage1_loss_bbox: 0.2122, stage1_loss_iou: 0.4202, stage1_loss_mask: 0.4279, stage2_loss_cls: 0.5255, stage2_pos_acc: 70.5222, stage2_loss_bbox: 0.1728, stage2_loss_iou: 0.3343, stage2_loss_mask: 0.4082, stage3_loss_cls: 0.4329, stage3_pos_acc: 73.6048, stage3_loss_bbox: 0.1767, stage3_loss_iou: 0.3191, stage3_loss_mask: 0.4051, stage4_loss_cls: 0.3430, stage4_pos_acc: 82.4460, stage4_loss_bbox: 0.1619, stage4_loss_iou: 0.2989, stage4_loss_mask: 0.4049, stage5_loss_cls: 0.3086, stage5_pos_acc: 85.8714, stage5_loss_bbox: 0.1649, stage5_loss_iou: 0.2974, stage5_loss_mask: 0.4089, loss: 10.1688\n",
      "2025-07-16 15:57:15,686 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 15:57:15,686 - mmdet - INFO - Epoch [26][250/750]\tlr: 2.500e-05, eta: 3:03:29, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0559, stage0_pos_acc: 35.8981, stage0_loss_bbox: 0.5063, stage0_loss_iou: 0.9776, stage0_loss_mask: 0.9918, stage1_loss_cls: 0.6625, stage1_pos_acc: 57.8450, stage1_loss_bbox: 0.2670, stage1_loss_iou: 0.5175, stage1_loss_mask: 0.6809, stage2_loss_cls: 0.5508, stage2_pos_acc: 69.9617, stage2_loss_bbox: 0.2104, stage2_loss_iou: 0.4026, stage2_loss_mask: 0.6066, stage3_loss_cls: 0.4591, stage3_pos_acc: 76.5403, stage3_loss_bbox: 0.1982, stage3_loss_iou: 0.3856, stage3_loss_mask: 0.5823, stage4_loss_cls: 0.3759, stage4_pos_acc: 82.1632, stage4_loss_bbox: 0.1889, stage4_loss_iou: 0.3796, stage4_loss_mask: 0.5607, stage5_loss_cls: 0.3544, stage5_pos_acc: 83.0374, stage5_loss_bbox: 0.1863, stage5_loss_iou: 0.3719, stage5_loss_mask: 0.5752, loss: 12.0479\n",
      "2025-07-16 15:57:33,909 - mmdet - INFO - Epoch [26][300/750]\tlr: 2.500e-05, eta: 3:03:08, time: 0.364, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0847, stage0_pos_acc: 38.9873, stage0_loss_bbox: 0.5134, stage0_loss_iou: 0.9772, stage0_loss_mask: 0.8317, stage1_loss_cls: 0.6344, stage1_pos_acc: 64.0071, stage1_loss_bbox: 0.2279, stage1_loss_iou: 0.4291, stage1_loss_mask: 0.4656, stage2_loss_cls: 0.5482, stage2_pos_acc: 70.3765, stage2_loss_bbox: 0.1843, stage2_loss_iou: 0.3307, stage2_loss_mask: 0.3518, stage3_loss_cls: 0.4187, stage3_pos_acc: 74.5626, stage3_loss_bbox: 0.1725, stage3_loss_iou: 0.3109, stage3_loss_mask: 0.3483, stage4_loss_cls: 0.3223, stage4_pos_acc: 84.3214, stage4_loss_bbox: 0.1767, stage4_loss_iou: 0.3089, stage4_loss_mask: 0.3731, stage5_loss_cls: 0.2915, stage5_pos_acc: 86.6757, stage5_loss_bbox: 0.1726, stage5_loss_iou: 0.3033, stage5_loss_mask: 0.3612, loss: 10.1389\n",
      "2025-07-16 15:57:52,095 - mmdet - INFO - Epoch [26][350/750]\tlr: 2.500e-05, eta: 3:02:47, time: 0.364, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0561, stage0_pos_acc: 36.0543, stage0_loss_bbox: 0.4910, stage0_loss_iou: 1.0000, stage0_loss_mask: 0.7593, stage1_loss_cls: 0.6803, stage1_pos_acc: 58.8261, stage1_loss_bbox: 0.2150, stage1_loss_iou: 0.4235, stage1_loss_mask: 0.4877, stage2_loss_cls: 0.5617, stage2_pos_acc: 70.3603, stage2_loss_bbox: 0.1759, stage2_loss_iou: 0.3368, stage2_loss_mask: 0.4474, stage3_loss_cls: 0.4447, stage3_pos_acc: 70.7127, stage3_loss_bbox: 0.1652, stage3_loss_iou: 0.3240, stage3_loss_mask: 0.4499, stage4_loss_cls: 0.3961, stage4_pos_acc: 75.1952, stage4_loss_bbox: 0.1653, stage4_loss_iou: 0.3138, stage4_loss_mask: 0.4334, stage5_loss_cls: 0.3733, stage5_pos_acc: 78.6158, stage5_loss_bbox: 0.1557, stage5_loss_iou: 0.3019, stage5_loss_mask: 0.4366, loss: 10.5950\n",
      "2025-07-16 15:58:10,467 - mmdet - INFO - Epoch [26][400/750]\tlr: 2.500e-05, eta: 3:02:28, time: 0.367, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0383, stage0_pos_acc: 37.0395, stage0_loss_bbox: 0.5324, stage0_loss_iou: 0.9465, stage0_loss_mask: 0.7834, stage1_loss_cls: 0.6608, stage1_pos_acc: 60.4438, stage1_loss_bbox: 0.2628, stage1_loss_iou: 0.4501, stage1_loss_mask: 0.4961, stage2_loss_cls: 0.5587, stage2_pos_acc: 69.5177, stage2_loss_bbox: 0.2147, stage2_loss_iou: 0.3556, stage2_loss_mask: 0.4581, stage3_loss_cls: 0.4467, stage3_pos_acc: 75.1860, stage3_loss_bbox: 0.2160, stage3_loss_iou: 0.3487, stage3_loss_mask: 0.4617, stage4_loss_cls: 0.3678, stage4_pos_acc: 82.8970, stage4_loss_bbox: 0.1956, stage4_loss_iou: 0.3287, stage4_loss_mask: 0.4265, stage5_loss_cls: 0.3339, stage5_pos_acc: 84.7162, stage5_loss_bbox: 0.2006, stage5_loss_iou: 0.3250, stage5_loss_mask: 0.4389, loss: 10.8478\n",
      "2025-07-16 15:58:28,722 - mmdet - INFO - Epoch [26][450/750]\tlr: 2.500e-05, eta: 3:02:07, time: 0.365, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0491, stage0_pos_acc: 35.7444, stage0_loss_bbox: 0.5254, stage0_loss_iou: 0.9853, stage0_loss_mask: 0.7592, stage1_loss_cls: 0.6349, stage1_pos_acc: 64.3620, stage1_loss_bbox: 0.2271, stage1_loss_iou: 0.4255, stage1_loss_mask: 0.4182, stage2_loss_cls: 0.5367, stage2_pos_acc: 71.4127, stage2_loss_bbox: 0.1811, stage2_loss_iou: 0.3316, stage2_loss_mask: 0.3732, stage3_loss_cls: 0.4284, stage3_pos_acc: 78.8676, stage3_loss_bbox: 0.1677, stage3_loss_iou: 0.3052, stage3_loss_mask: 0.3698, stage4_loss_cls: 0.3556, stage4_pos_acc: 79.8834, stage4_loss_bbox: 0.1674, stage4_loss_iou: 0.2966, stage4_loss_mask: 0.3765, stage5_loss_cls: 0.3374, stage5_pos_acc: 82.9065, stage5_loss_bbox: 0.1669, stage5_loss_iou: 0.2917, stage5_loss_mask: 0.3674, loss: 10.0779\n",
      "2025-07-16 15:58:47,237 - mmdet - INFO - Epoch [26][500/750]\tlr: 2.500e-05, eta: 3:01:49, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0684, stage0_pos_acc: 32.6834, stage0_loss_bbox: 0.5737, stage0_loss_iou: 1.0464, stage0_loss_mask: 1.0092, stage1_loss_cls: 0.7076, stage1_pos_acc: 57.6808, stage1_loss_bbox: 0.2797, stage1_loss_iou: 0.5235, stage1_loss_mask: 0.6275, stage2_loss_cls: 0.6089, stage2_pos_acc: 65.5799, stage2_loss_bbox: 0.2052, stage2_loss_iou: 0.4154, stage2_loss_mask: 0.5846, stage3_loss_cls: 0.4890, stage3_pos_acc: 69.3085, stage3_loss_bbox: 0.1890, stage3_loss_iou: 0.3996, stage3_loss_mask: 0.5760, stage4_loss_cls: 0.4359, stage4_pos_acc: 76.6420, stage4_loss_bbox: 0.1755, stage4_loss_iou: 0.3812, stage4_loss_mask: 0.5775, stage5_loss_cls: 0.3996, stage5_pos_acc: 81.6044, stage5_loss_bbox: 0.1784, stage5_loss_iou: 0.3793, stage5_loss_mask: 0.5707, loss: 12.4020\n",
      "2025-07-16 15:59:06,041 - mmdet - INFO - Epoch [26][550/750]\tlr: 2.500e-05, eta: 3:01:33, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0166, stage0_pos_acc: 38.4188, stage0_loss_bbox: 0.5292, stage0_loss_iou: 1.0213, stage0_loss_mask: 0.7221, stage1_loss_cls: 0.6412, stage1_pos_acc: 63.1642, stage1_loss_bbox: 0.2154, stage1_loss_iou: 0.4398, stage1_loss_mask: 0.3930, stage2_loss_cls: 0.5559, stage2_pos_acc: 70.2623, stage2_loss_bbox: 0.1624, stage2_loss_iou: 0.3214, stage2_loss_mask: 0.3236, stage3_loss_cls: 0.4323, stage3_pos_acc: 76.1521, stage3_loss_bbox: 0.1716, stage3_loss_iou: 0.3032, stage3_loss_mask: 0.3081, stage4_loss_cls: 0.3499, stage4_pos_acc: 83.8543, stage4_loss_bbox: 0.1595, stage4_loss_iou: 0.2913, stage4_loss_mask: 0.2944, stage5_loss_cls: 0.3311, stage5_pos_acc: 82.9082, stage5_loss_bbox: 0.1456, stage5_loss_iou: 0.2824, stage5_loss_mask: 0.2923, loss: 9.7038\n",
      "2025-07-16 15:59:24,853 - mmdet - INFO - Epoch [26][600/750]\tlr: 2.500e-05, eta: 3:01:17, time: 0.376, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0700, stage0_pos_acc: 36.6789, stage0_loss_bbox: 0.5705, stage0_loss_iou: 1.1193, stage0_loss_mask: 1.1453, stage1_loss_cls: 0.6877, stage1_pos_acc: 59.3544, stage1_loss_bbox: 0.2565, stage1_loss_iou: 0.5386, stage1_loss_mask: 0.6159, stage2_loss_cls: 0.5858, stage2_pos_acc: 69.6803, stage2_loss_bbox: 0.1874, stage2_loss_iou: 0.3998, stage2_loss_mask: 0.4904, stage3_loss_cls: 0.4931, stage3_pos_acc: 74.2566, stage3_loss_bbox: 0.1871, stage3_loss_iou: 0.3680, stage3_loss_mask: 0.4391, stage4_loss_cls: 0.4438, stage4_pos_acc: 79.7639, stage4_loss_bbox: 0.1650, stage4_loss_iou: 0.3504, stage4_loss_mask: 0.4622, stage5_loss_cls: 0.4206, stage5_pos_acc: 82.4789, stage5_loss_bbox: 0.1590, stage5_loss_iou: 0.3371, stage5_loss_mask: 0.4405, loss: 11.9333\n",
      "2025-07-16 15:59:43,820 - mmdet - INFO - Epoch [26][650/750]\tlr: 2.500e-05, eta: 3:01:02, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0315, stage0_pos_acc: 41.1192, stage0_loss_bbox: 0.5566, stage0_loss_iou: 1.0815, stage0_loss_mask: 1.0271, stage1_loss_cls: 0.6897, stage1_pos_acc: 59.3502, stage1_loss_bbox: 0.2780, stage1_loss_iou: 0.5505, stage1_loss_mask: 0.6462, stage2_loss_cls: 0.5877, stage2_pos_acc: 68.7605, stage2_loss_bbox: 0.2241, stage2_loss_iou: 0.4402, stage2_loss_mask: 0.6041, stage3_loss_cls: 0.5000, stage3_pos_acc: 70.1835, stage3_loss_bbox: 0.1999, stage3_loss_iou: 0.4045, stage3_loss_mask: 0.5648, stage4_loss_cls: 0.4309, stage4_pos_acc: 77.3557, stage4_loss_bbox: 0.1975, stage4_loss_iou: 0.3963, stage4_loss_mask: 0.5582, stage5_loss_cls: 0.4105, stage5_pos_acc: 79.9783, stage5_loss_bbox: 0.1968, stage5_loss_iou: 0.3926, stage5_loss_mask: 0.5536, loss: 12.5228\n",
      "2025-07-16 16:00:02,531 - mmdet - INFO - Epoch [26][700/750]\tlr: 2.500e-05, eta: 3:00:45, time: 0.374, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0187, stage0_pos_acc: 41.5188, stage0_loss_bbox: 0.5649, stage0_loss_iou: 1.0261, stage0_loss_mask: 0.9295, stage1_loss_cls: 0.6657, stage1_pos_acc: 59.7055, stage1_loss_bbox: 0.2594, stage1_loss_iou: 0.4669, stage1_loss_mask: 0.5441, stage2_loss_cls: 0.5367, stage2_pos_acc: 68.2330, stage2_loss_bbox: 0.2026, stage2_loss_iou: 0.3738, stage2_loss_mask: 0.4667, stage3_loss_cls: 0.4353, stage3_pos_acc: 72.5831, stage3_loss_bbox: 0.1895, stage3_loss_iou: 0.3459, stage3_loss_mask: 0.4383, stage4_loss_cls: 0.3790, stage4_pos_acc: 75.8312, stage4_loss_bbox: 0.1762, stage4_loss_iou: 0.3293, stage4_loss_mask: 0.4600, stage5_loss_cls: 0.3538, stage5_pos_acc: 77.0838, stage5_loss_bbox: 0.1676, stage5_loss_iou: 0.3145, stage5_loss_mask: 0.4536, loss: 11.0981\n",
      "2025-07-16 16:00:20,965 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:00:20,965 - mmdet - INFO - Epoch [26][750/750]\tlr: 2.500e-05, eta: 3:00:26, time: 0.369, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0450, stage0_pos_acc: 36.5680, stage0_loss_bbox: 0.5277, stage0_loss_iou: 0.9560, stage0_loss_mask: 0.7141, stage1_loss_cls: 0.6715, stage1_pos_acc: 55.4505, stage1_loss_bbox: 0.2343, stage1_loss_iou: 0.4191, stage1_loss_mask: 0.4312, stage2_loss_cls: 0.5864, stage2_pos_acc: 64.3362, stage2_loss_bbox: 0.1775, stage2_loss_iou: 0.3241, stage2_loss_mask: 0.3984, stage3_loss_cls: 0.4806, stage3_pos_acc: 68.1314, stage3_loss_bbox: 0.1708, stage3_loss_iou: 0.3068, stage3_loss_mask: 0.3900, stage4_loss_cls: 0.4076, stage4_pos_acc: 74.3061, stage4_loss_bbox: 0.1601, stage4_loss_iou: 0.2935, stage4_loss_mask: 0.3725, stage5_loss_cls: 0.3776, stage5_pos_acc: 77.4354, stage5_loss_bbox: 0.1563, stage5_loss_iou: 0.2914, stage5_loss_mask: 0.3594, loss: 10.2521\n",
      "2025-07-16 16:00:21,081 - mmdet - INFO - Saving checkpoint at 26 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 85s, ETA:     0s2025-07-16 16:03:24,233 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.41s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.438\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.442\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.442\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.073\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.226\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.492\n",
      "2025-07-16 16:03:26,281 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.82s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.449\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.449\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.242\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.497\n",
      "2025-07-16 16:03:29,221 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:03:29,223 - mmdet - INFO - Epoch(val) [26][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0470, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.0240, bbox_mAP_m: 0.0050, bbox_mAP_l: 0.0330, bbox_mAP_copypaste: 0.025 0.047 0.023 0.024 0.005 0.033, segm_mAP: 0.0240, segm_mAP_50: 0.0470, segm_mAP_75: 0.0230, segm_mAP_s: 0.0270, segm_mAP_m: 0.0050, segm_mAP_l: 0.0320, segm_mAP_copypaste: 0.024 0.047 0.023 0.027 0.005 0.032\n",
      "2025-07-16 16:03:49,934 - mmdet - INFO - Epoch [27][50/750]\tlr: 2.500e-05, eta: 3:00:25, time: 0.414, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.0258, stage0_pos_acc: 39.8775, stage0_loss_bbox: 0.5086, stage0_loss_iou: 0.9736, stage0_loss_mask: 0.7975, stage1_loss_cls: 0.6340, stage1_pos_acc: 63.9572, stage1_loss_bbox: 0.2192, stage1_loss_iou: 0.4171, stage1_loss_mask: 0.4209, stage2_loss_cls: 0.5190, stage2_pos_acc: 67.4802, stage2_loss_bbox: 0.1714, stage2_loss_iou: 0.3144, stage2_loss_mask: 0.3823, stage3_loss_cls: 0.4037, stage3_pos_acc: 79.7890, stage3_loss_bbox: 0.1628, stage3_loss_iou: 0.2972, stage3_loss_mask: 0.3477, stage4_loss_cls: 0.3145, stage4_pos_acc: 83.5660, stage4_loss_bbox: 0.1543, stage4_loss_iou: 0.2847, stage4_loss_mask: 0.3789, stage5_loss_cls: 0.2737, stage5_pos_acc: 85.6256, stage5_loss_bbox: 0.1605, stage5_loss_iou: 0.2795, stage5_loss_mask: 0.3908, loss: 9.8321\n",
      "2025-07-16 16:04:07,982 - mmdet - INFO - Epoch [27][100/750]\tlr: 2.500e-05, eta: 3:00:02, time: 0.361, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0509, stage0_pos_acc: 33.8991, stage0_loss_bbox: 0.5347, stage0_loss_iou: 1.0271, stage0_loss_mask: 0.9507, stage1_loss_cls: 0.6292, stage1_pos_acc: 63.4753, stage1_loss_bbox: 0.2375, stage1_loss_iou: 0.4862, stage1_loss_mask: 0.5806, stage2_loss_cls: 0.4938, stage2_pos_acc: 72.5699, stage2_loss_bbox: 0.1795, stage2_loss_iou: 0.3695, stage2_loss_mask: 0.5157, stage3_loss_cls: 0.4040, stage3_pos_acc: 78.5117, stage3_loss_bbox: 0.1653, stage3_loss_iou: 0.3378, stage3_loss_mask: 0.4867, stage4_loss_cls: 0.3338, stage4_pos_acc: 84.2605, stage4_loss_bbox: 0.1688, stage4_loss_iou: 0.3214, stage4_loss_mask: 0.4430, stage5_loss_cls: 0.3000, stage5_pos_acc: 87.1870, stage5_loss_bbox: 0.1640, stage5_loss_iou: 0.3172, stage5_loss_mask: 0.4495, loss: 10.9469\n",
      "2025-07-16 16:04:26,107 - mmdet - INFO - Epoch [27][150/750]\tlr: 2.500e-05, eta: 2:59:41, time: 0.362, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0692, stage0_pos_acc: 37.4146, stage0_loss_bbox: 0.5126, stage0_loss_iou: 1.0169, stage0_loss_mask: 0.8691, stage1_loss_cls: 0.6451, stage1_pos_acc: 67.4632, stage1_loss_bbox: 0.1960, stage1_loss_iou: 0.4596, stage1_loss_mask: 0.4687, stage2_loss_cls: 0.5277, stage2_pos_acc: 73.2149, stage2_loss_bbox: 0.1630, stage2_loss_iou: 0.3643, stage2_loss_mask: 0.4035, stage3_loss_cls: 0.4048, stage3_pos_acc: 79.0817, stage3_loss_bbox: 0.1469, stage3_loss_iou: 0.3261, stage3_loss_mask: 0.3832, stage4_loss_cls: 0.3362, stage4_pos_acc: 84.3888, stage4_loss_bbox: 0.1372, stage4_loss_iou: 0.2999, stage4_loss_mask: 0.3701, stage5_loss_cls: 0.3102, stage5_pos_acc: 87.5960, stage5_loss_bbox: 0.1441, stage5_loss_iou: 0.2982, stage5_loss_mask: 0.3619, loss: 10.2144\n",
      "2025-07-16 16:04:44,363 - mmdet - INFO - Epoch [27][200/750]\tlr: 2.500e-05, eta: 2:59:20, time: 0.365, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0404, stage0_pos_acc: 35.3970, stage0_loss_bbox: 0.5742, stage0_loss_iou: 1.0350, stage0_loss_mask: 1.0664, stage1_loss_cls: 0.6517, stage1_pos_acc: 64.4381, stage1_loss_bbox: 0.2703, stage1_loss_iou: 0.4901, stage1_loss_mask: 0.5327, stage2_loss_cls: 0.5465, stage2_pos_acc: 70.5032, stage2_loss_bbox: 0.2196, stage2_loss_iou: 0.3962, stage2_loss_mask: 0.4783, stage3_loss_cls: 0.4375, stage3_pos_acc: 74.7601, stage3_loss_bbox: 0.2073, stage3_loss_iou: 0.3672, stage3_loss_mask: 0.4743, stage4_loss_cls: 0.3509, stage4_pos_acc: 81.5278, stage4_loss_bbox: 0.2042, stage4_loss_iou: 0.3571, stage4_loss_mask: 0.4667, stage5_loss_cls: 0.3007, stage5_pos_acc: 85.1848, stage5_loss_bbox: 0.2017, stage5_loss_iou: 0.3521, stage5_loss_mask: 0.4668, loss: 11.4880\n",
      "2025-07-16 16:05:02,440 - mmdet - INFO - Epoch [27][250/750]\tlr: 2.500e-05, eta: 2:58:59, time: 0.362, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0501, stage0_pos_acc: 40.0471, stage0_loss_bbox: 0.5698, stage0_loss_iou: 1.0784, stage0_loss_mask: 0.7678, stage1_loss_cls: 0.6684, stage1_pos_acc: 61.6846, stage1_loss_bbox: 0.2480, stage1_loss_iou: 0.4762, stage1_loss_mask: 0.5166, stage2_loss_cls: 0.5431, stage2_pos_acc: 68.9602, stage2_loss_bbox: 0.2061, stage2_loss_iou: 0.3776, stage2_loss_mask: 0.4573, stage3_loss_cls: 0.4286, stage3_pos_acc: 75.0906, stage3_loss_bbox: 0.1836, stage3_loss_iou: 0.3454, stage3_loss_mask: 0.4412, stage4_loss_cls: 0.3752, stage4_pos_acc: 80.7792, stage4_loss_bbox: 0.1804, stage4_loss_iou: 0.3288, stage4_loss_mask: 0.4278, stage5_loss_cls: 0.3435, stage5_pos_acc: 83.3816, stage5_loss_bbox: 0.1737, stage5_loss_iou: 0.3248, stage5_loss_mask: 0.4250, loss: 10.9370\n",
      "2025-07-16 16:05:21,046 - mmdet - INFO - Epoch [27][300/750]\tlr: 2.500e-05, eta: 2:58:41, time: 0.372, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0450, stage0_pos_acc: 38.6477, stage0_loss_bbox: 0.5385, stage0_loss_iou: 1.0663, stage0_loss_mask: 1.1525, stage1_loss_cls: 0.6755, stage1_pos_acc: 61.7820, stage1_loss_bbox: 0.2384, stage1_loss_iou: 0.5007, stage1_loss_mask: 0.6281, stage2_loss_cls: 0.5328, stage2_pos_acc: 71.5907, stage2_loss_bbox: 0.1761, stage2_loss_iou: 0.3771, stage2_loss_mask: 0.5880, stage3_loss_cls: 0.4142, stage3_pos_acc: 74.8884, stage3_loss_bbox: 0.1694, stage3_loss_iou: 0.3462, stage3_loss_mask: 0.5670, stage4_loss_cls: 0.3417, stage4_pos_acc: 81.7556, stage4_loss_bbox: 0.1551, stage4_loss_iou: 0.3371, stage4_loss_mask: 0.5476, stage5_loss_cls: 0.3014, stage5_pos_acc: 86.6841, stage5_loss_bbox: 0.1499, stage5_loss_iou: 0.3319, stage5_loss_mask: 0.5452, loss: 11.7259\n",
      "2025-07-16 16:05:39,156 - mmdet - INFO - Epoch [27][350/750]\tlr: 2.500e-05, eta: 2:58:19, time: 0.362, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0245, stage0_pos_acc: 32.8937, stage0_loss_bbox: 0.4896, stage0_loss_iou: 0.9579, stage0_loss_mask: 0.6361, stage1_loss_cls: 0.6644, stage1_pos_acc: 56.9206, stage1_loss_bbox: 0.2219, stage1_loss_iou: 0.4208, stage1_loss_mask: 0.3939, stage2_loss_cls: 0.5204, stage2_pos_acc: 70.1706, stage2_loss_bbox: 0.2003, stage2_loss_iou: 0.3475, stage2_loss_mask: 0.3745, stage3_loss_cls: 0.3844, stage3_pos_acc: 77.0460, stage3_loss_bbox: 0.1995, stage3_loss_iou: 0.3267, stage3_loss_mask: 0.3690, stage4_loss_cls: 0.3560, stage4_pos_acc: 80.7698, stage4_loss_bbox: 0.1730, stage4_loss_iou: 0.3082, stage4_loss_mask: 0.3481, stage5_loss_cls: 0.3194, stage5_pos_acc: 85.4841, stage5_loss_bbox: 0.1742, stage5_loss_iou: 0.3056, stage5_loss_mask: 0.3467, loss: 9.8626\n",
      "2025-07-16 16:05:57,309 - mmdet - INFO - Epoch [27][400/750]\tlr: 2.500e-05, eta: 2:57:58, time: 0.363, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0472, stage0_pos_acc: 32.4593, stage0_loss_bbox: 0.5453, stage0_loss_iou: 0.9814, stage0_loss_mask: 0.8858, stage1_loss_cls: 0.6810, stage1_pos_acc: 56.7423, stage1_loss_bbox: 0.2735, stage1_loss_iou: 0.4783, stage1_loss_mask: 0.5528, stage2_loss_cls: 0.5663, stage2_pos_acc: 66.4456, stage2_loss_bbox: 0.2257, stage2_loss_iou: 0.3858, stage2_loss_mask: 0.4816, stage3_loss_cls: 0.4455, stage3_pos_acc: 77.7471, stage3_loss_bbox: 0.2151, stage3_loss_iou: 0.3502, stage3_loss_mask: 0.4486, stage4_loss_cls: 0.3814, stage4_pos_acc: 78.4637, stage4_loss_bbox: 0.2029, stage4_loss_iou: 0.3345, stage4_loss_mask: 0.4594, stage5_loss_cls: 0.3685, stage5_pos_acc: 83.3328, stage5_loss_bbox: 0.1948, stage5_loss_iou: 0.3232, stage5_loss_mask: 0.4505, loss: 11.2794\n",
      "2025-07-16 16:06:15,412 - mmdet - INFO - Epoch [27][450/750]\tlr: 2.500e-05, eta: 2:57:37, time: 0.362, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0654, stage0_pos_acc: 36.1727, stage0_loss_bbox: 0.4816, stage0_loss_iou: 0.9338, stage0_loss_mask: 0.7251, stage1_loss_cls: 0.6825, stage1_pos_acc: 56.0298, stage1_loss_bbox: 0.2219, stage1_loss_iou: 0.4231, stage1_loss_mask: 0.4562, stage2_loss_cls: 0.5605, stage2_pos_acc: 68.0881, stage2_loss_bbox: 0.1772, stage2_loss_iou: 0.3381, stage2_loss_mask: 0.4543, stage3_loss_cls: 0.4698, stage3_pos_acc: 74.3239, stage3_loss_bbox: 0.1704, stage3_loss_iou: 0.3170, stage3_loss_mask: 0.4034, stage4_loss_cls: 0.3780, stage4_pos_acc: 80.0274, stage4_loss_bbox: 0.1673, stage4_loss_iou: 0.3169, stage4_loss_mask: 0.4289, stage5_loss_cls: 0.3527, stage5_pos_acc: 85.3558, stage5_loss_bbox: 0.1615, stage5_loss_iou: 0.3049, stage5_loss_mask: 0.4018, loss: 10.3924\n",
      "2025-07-16 16:06:33,648 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:06:33,648 - mmdet - INFO - Epoch [27][500/750]\tlr: 2.500e-05, eta: 2:57:17, time: 0.365, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0969, stage0_pos_acc: 36.0376, stage0_loss_bbox: 0.5058, stage0_loss_iou: 1.0285, stage0_loss_mask: 0.9003, stage1_loss_cls: 0.6888, stage1_pos_acc: 58.7232, stage1_loss_bbox: 0.2379, stage1_loss_iou: 0.4821, stage1_loss_mask: 0.6187, stage2_loss_cls: 0.5744, stage2_pos_acc: 68.1673, stage2_loss_bbox: 0.1919, stage2_loss_iou: 0.3965, stage2_loss_mask: 0.5637, stage3_loss_cls: 0.4623, stage3_pos_acc: 78.6119, stage3_loss_bbox: 0.1863, stage3_loss_iou: 0.3692, stage3_loss_mask: 0.5404, stage4_loss_cls: 0.3906, stage4_pos_acc: 83.6541, stage4_loss_bbox: 0.1763, stage4_loss_iou: 0.3565, stage4_loss_mask: 0.5271, stage5_loss_cls: 0.3625, stage5_pos_acc: 84.6930, stage5_loss_bbox: 0.1800, stage5_loss_iou: 0.3566, stage5_loss_mask: 0.5334, loss: 11.7265\n",
      "2025-07-16 16:06:51,836 - mmdet - INFO - Epoch [27][550/750]\tlr: 2.500e-05, eta: 2:56:56, time: 0.364, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0757, stage0_pos_acc: 38.3708, stage0_loss_bbox: 0.5812, stage0_loss_iou: 1.0384, stage0_loss_mask: 0.8866, stage1_loss_cls: 0.6872, stage1_pos_acc: 59.8491, stage1_loss_bbox: 0.2565, stage1_loss_iou: 0.4801, stage1_loss_mask: 0.4689, stage2_loss_cls: 0.5456, stage2_pos_acc: 67.0411, stage2_loss_bbox: 0.1963, stage2_loss_iou: 0.3673, stage2_loss_mask: 0.4401, stage3_loss_cls: 0.4547, stage3_pos_acc: 76.9385, stage3_loss_bbox: 0.1671, stage3_loss_iou: 0.3325, stage3_loss_mask: 0.4083, stage4_loss_cls: 0.3759, stage4_pos_acc: 79.8690, stage4_loss_bbox: 0.1623, stage4_loss_iou: 0.3263, stage4_loss_mask: 0.4213, stage5_loss_cls: 0.3445, stage5_pos_acc: 87.1936, stage5_loss_bbox: 0.1626, stage5_loss_iou: 0.3167, stage5_loss_mask: 0.3912, loss: 10.8875\n",
      "2025-07-16 16:07:09,914 - mmdet - INFO - Epoch [27][600/750]\tlr: 2.500e-05, eta: 2:56:35, time: 0.362, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0652, stage0_pos_acc: 32.6503, stage0_loss_bbox: 0.5551, stage0_loss_iou: 1.0776, stage0_loss_mask: 0.8841, stage1_loss_cls: 0.6573, stage1_pos_acc: 59.9122, stage1_loss_bbox: 0.2479, stage1_loss_iou: 0.4822, stage1_loss_mask: 0.5100, stage2_loss_cls: 0.5190, stage2_pos_acc: 70.2203, stage2_loss_bbox: 0.1853, stage2_loss_iou: 0.3588, stage2_loss_mask: 0.4506, stage3_loss_cls: 0.3984, stage3_pos_acc: 75.9637, stage3_loss_bbox: 0.1623, stage3_loss_iou: 0.3321, stage3_loss_mask: 0.4517, stage4_loss_cls: 0.3279, stage4_pos_acc: 83.5298, stage4_loss_bbox: 0.1539, stage4_loss_iou: 0.3221, stage4_loss_mask: 0.4403, stage5_loss_cls: 0.3035, stage5_pos_acc: 81.8598, stage5_loss_bbox: 0.1499, stage5_loss_iou: 0.3155, stage5_loss_mask: 0.4252, loss: 10.7757\n",
      "2025-07-16 16:07:27,947 - mmdet - INFO - Epoch [27][650/750]\tlr: 2.500e-05, eta: 2:56:13, time: 0.361, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0394, stage0_pos_acc: 32.9457, stage0_loss_bbox: 0.5767, stage0_loss_iou: 1.0856, stage0_loss_mask: 1.1682, stage1_loss_cls: 0.7012, stage1_pos_acc: 55.8536, stage1_loss_bbox: 0.2809, stage1_loss_iou: 0.5130, stage1_loss_mask: 0.6440, stage2_loss_cls: 0.5641, stage2_pos_acc: 68.9429, stage2_loss_bbox: 0.2174, stage2_loss_iou: 0.3734, stage2_loss_mask: 0.5184, stage3_loss_cls: 0.4540, stage3_pos_acc: 71.2468, stage3_loss_bbox: 0.1954, stage3_loss_iou: 0.3402, stage3_loss_mask: 0.5083, stage4_loss_cls: 0.3997, stage4_pos_acc: 77.8731, stage4_loss_bbox: 0.1821, stage4_loss_iou: 0.3174, stage4_loss_mask: 0.4590, stage5_loss_cls: 0.3715, stage5_pos_acc: 81.7032, stage5_loss_bbox: 0.1758, stage5_loss_iou: 0.3108, stage5_loss_mask: 0.4644, loss: 11.8607\n",
      "2025-07-16 16:07:46,705 - mmdet - INFO - Epoch [27][700/750]\tlr: 2.500e-05, eta: 2:55:57, time: 0.375, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0892, stage0_pos_acc: 33.1318, stage0_loss_bbox: 0.5092, stage0_loss_iou: 0.9652, stage0_loss_mask: 0.7477, stage1_loss_cls: 0.7078, stage1_pos_acc: 56.2071, stage1_loss_bbox: 0.2025, stage1_loss_iou: 0.4002, stage1_loss_mask: 0.4354, stage2_loss_cls: 0.6074, stage2_pos_acc: 62.9684, stage2_loss_bbox: 0.1705, stage2_loss_iou: 0.3073, stage2_loss_mask: 0.3783, stage3_loss_cls: 0.5055, stage3_pos_acc: 69.2082, stage3_loss_bbox: 0.1617, stage3_loss_iou: 0.2907, stage3_loss_mask: 0.3834, stage4_loss_cls: 0.4315, stage4_pos_acc: 74.6130, stage4_loss_bbox: 0.1499, stage4_loss_iou: 0.2707, stage4_loss_mask: 0.3728, stage5_loss_cls: 0.3977, stage5_pos_acc: 77.7374, stage5_loss_bbox: 0.1445, stage5_loss_iou: 0.2632, stage5_loss_mask: 0.3505, loss: 10.2427\n",
      "2025-07-16 16:08:05,167 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:08:05,167 - mmdet - INFO - Epoch [27][750/750]\tlr: 2.500e-05, eta: 2:55:38, time: 0.369, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.1009, stage0_pos_acc: 35.3322, stage0_loss_bbox: 0.5094, stage0_loss_iou: 1.0352, stage0_loss_mask: 0.7716, stage1_loss_cls: 0.6615, stage1_pos_acc: 62.4107, stage1_loss_bbox: 0.1934, stage1_loss_iou: 0.4157, stage1_loss_mask: 0.3679, stage2_loss_cls: 0.5336, stage2_pos_acc: 70.3812, stage2_loss_bbox: 0.1523, stage2_loss_iou: 0.3241, stage2_loss_mask: 0.3162, stage3_loss_cls: 0.4195, stage3_pos_acc: 77.2114, stage3_loss_bbox: 0.1337, stage3_loss_iou: 0.2978, stage3_loss_mask: 0.2729, stage4_loss_cls: 0.3483, stage4_pos_acc: 82.7464, stage4_loss_bbox: 0.1277, stage4_loss_iou: 0.2825, stage4_loss_mask: 0.2844, stage5_loss_cls: 0.3317, stage5_pos_acc: 83.9002, stage5_loss_bbox: 0.1267, stage5_loss_iou: 0.2765, stage5_loss_mask: 0.2719, loss: 9.5553\n",
      "2025-07-16 16:08:05,275 - mmdet - INFO - Saving checkpoint at 27 epochs\n",
      "[>>>                            ] 12/100, 1.1 task/s, elapsed: 11s, ETA:    83s2025-07-16 16:12:11,689 - mmdet - INFO - Epoch [28][150/750]\tlr: 2.500e-05, eta: 2:55:01, time: 0.367, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.1203, stage0_pos_acc: 33.1126, stage0_loss_bbox: 0.5503, stage0_loss_iou: 1.0631, stage0_loss_mask: 0.9487, stage1_loss_cls: 0.6712, stage1_pos_acc: 65.9522, stage1_loss_bbox: 0.2308, stage1_loss_iou: 0.4708, stage1_loss_mask: 0.5452, stage2_loss_cls: 0.5143, stage2_pos_acc: 73.9505, stage2_loss_bbox: 0.1742, stage2_loss_iou: 0.3587, stage2_loss_mask: 0.5071, stage3_loss_cls: 0.3876, stage3_pos_acc: 78.5363, stage3_loss_bbox: 0.1692, stage3_loss_iou: 0.3391, stage3_loss_mask: 0.4978, stage4_loss_cls: 0.3203, stage4_pos_acc: 82.8604, stage4_loss_bbox: 0.1647, stage4_loss_iou: 0.3304, stage4_loss_mask: 0.4951, stage5_loss_cls: 0.2929, stage5_pos_acc: 87.0642, stage5_loss_bbox: 0.1702, stage5_loss_iou: 0.3283, stage5_loss_mask: 0.4924, loss: 11.1426\n",
      "2025-07-16 16:12:30,113 - mmdet - INFO - Epoch [28][200/750]\tlr: 2.500e-05, eta: 2:54:42, time: 0.368, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0638, stage0_pos_acc: 39.5685, stage0_loss_bbox: 0.5493, stage0_loss_iou: 0.9595, stage0_loss_mask: 0.9346, stage1_loss_cls: 0.6552, stage1_pos_acc: 63.1592, stage1_loss_bbox: 0.2802, stage1_loss_iou: 0.4830, stage1_loss_mask: 0.6976, stage2_loss_cls: 0.5484, stage2_pos_acc: 73.2645, stage2_loss_bbox: 0.2332, stage2_loss_iou: 0.4019, stage2_loss_mask: 0.5936, stage3_loss_cls: 0.4191, stage3_pos_acc: 78.5010, stage3_loss_bbox: 0.2246, stage3_loss_iou: 0.3823, stage3_loss_mask: 0.5893, stage4_loss_cls: 0.3446, stage4_pos_acc: 82.9741, stage4_loss_bbox: 0.2119, stage4_loss_iou: 0.3711, stage4_loss_mask: 0.5951, stage5_loss_cls: 0.3113, stage5_pos_acc: 85.9796, stage5_loss_bbox: 0.2170, stage5_loss_iou: 0.3673, stage5_loss_mask: 0.5856, loss: 12.0195\n",
      "2025-07-16 16:12:48,463 - mmdet - INFO - Epoch [28][250/750]\tlr: 2.500e-05, eta: 2:54:22, time: 0.367, data_time: 0.005, memory: 11264, stage0_loss_cls: 1.1308, stage0_pos_acc: 35.0659, stage0_loss_bbox: 0.5207, stage0_loss_iou: 1.0680, stage0_loss_mask: 0.8188, stage1_loss_cls: 0.6872, stage1_pos_acc: 63.0579, stage1_loss_bbox: 0.2192, stage1_loss_iou: 0.4771, stage1_loss_mask: 0.4383, stage2_loss_cls: 0.5539, stage2_pos_acc: 70.7437, stage2_loss_bbox: 0.1732, stage2_loss_iou: 0.3608, stage2_loss_mask: 0.3702, stage3_loss_cls: 0.4225, stage3_pos_acc: 77.0317, stage3_loss_bbox: 0.1524, stage3_loss_iou: 0.3246, stage3_loss_mask: 0.3654, stage4_loss_cls: 0.3487, stage4_pos_acc: 83.8119, stage4_loss_bbox: 0.1387, stage4_loss_iou: 0.3068, stage4_loss_mask: 0.3501, stage5_loss_cls: 0.3175, stage5_pos_acc: 86.2667, stage5_loss_bbox: 0.1279, stage5_loss_iou: 0.2933, stage5_loss_mask: 0.3395, loss: 10.3055\n",
      "2025-07-16 16:13:06,707 - mmdet - INFO - Epoch [28][300/750]\tlr: 2.500e-05, eta: 2:54:02, time: 0.365, data_time: 0.005, memory: 11264, stage0_loss_cls: 1.0526, stage0_pos_acc: 38.9749, stage0_loss_bbox: 0.5414, stage0_loss_iou: 1.0233, stage0_loss_mask: 0.8084, stage1_loss_cls: 0.6244, stage1_pos_acc: 64.3145, stage1_loss_bbox: 0.2550, stage1_loss_iou: 0.4509, stage1_loss_mask: 0.5030, stage2_loss_cls: 0.5148, stage2_pos_acc: 72.3020, stage2_loss_bbox: 0.1953, stage2_loss_iou: 0.3535, stage2_loss_mask: 0.4644, stage3_loss_cls: 0.4087, stage3_pos_acc: 79.7918, stage3_loss_bbox: 0.1871, stage3_loss_iou: 0.3270, stage3_loss_mask: 0.4506, stage4_loss_cls: 0.3348, stage4_pos_acc: 83.8767, stage4_loss_bbox: 0.1791, stage4_loss_iou: 0.3140, stage4_loss_mask: 0.4393, stage5_loss_cls: 0.2974, stage5_pos_acc: 90.2195, stage5_loss_bbox: 0.1826, stage5_loss_iou: 0.3100, stage5_loss_mask: 0.4316, loss: 10.6492\n",
      "2025-07-16 16:13:25,069 - mmdet - INFO - Epoch [28][350/750]\tlr: 2.500e-05, eta: 2:53:43, time: 0.367, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0476, stage0_pos_acc: 43.2765, stage0_loss_bbox: 0.5018, stage0_loss_iou: 0.9611, stage0_loss_mask: 0.9307, stage1_loss_cls: 0.6678, stage1_pos_acc: 62.5351, stage1_loss_bbox: 0.2201, stage1_loss_iou: 0.4599, stage1_loss_mask: 0.6601, stage2_loss_cls: 0.5296, stage2_pos_acc: 74.2176, stage2_loss_bbox: 0.1914, stage2_loss_iou: 0.3821, stage2_loss_mask: 0.6565, stage3_loss_cls: 0.4098, stage3_pos_acc: 80.2583, stage3_loss_bbox: 0.1984, stage3_loss_iou: 0.3637, stage3_loss_mask: 0.6338, stage4_loss_cls: 0.3510, stage4_pos_acc: 82.3237, stage4_loss_bbox: 0.1922, stage4_loss_iou: 0.3536, stage4_loss_mask: 0.6111, stage5_loss_cls: 0.3295, stage5_pos_acc: 84.3300, stage5_loss_bbox: 0.1909, stage5_loss_iou: 0.3467, stage5_loss_mask: 0.6088, loss: 11.7982\n",
      "2025-07-16 16:13:43,472 - mmdet - INFO - Epoch [28][400/750]\tlr: 2.500e-05, eta: 2:53:23, time: 0.368, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0629, stage0_pos_acc: 29.0239, stage0_loss_bbox: 0.5216, stage0_loss_iou: 0.9378, stage0_loss_mask: 0.6768, stage1_loss_cls: 0.6346, stage1_pos_acc: 66.0681, stage1_loss_bbox: 0.2093, stage1_loss_iou: 0.4014, stage1_loss_mask: 0.3443, stage2_loss_cls: 0.5127, stage2_pos_acc: 73.7090, stage2_loss_bbox: 0.1571, stage2_loss_iou: 0.2876, stage2_loss_mask: 0.2828, stage3_loss_cls: 0.3896, stage3_pos_acc: 77.8149, stage3_loss_bbox: 0.1497, stage3_loss_iou: 0.2658, stage3_loss_mask: 0.2743, stage4_loss_cls: 0.3262, stage4_pos_acc: 82.8704, stage4_loss_bbox: 0.1314, stage4_loss_iou: 0.2467, stage4_loss_mask: 0.2626, stage5_loss_cls: 0.2868, stage5_pos_acc: 85.7146, stage5_loss_bbox: 0.1314, stage5_loss_iou: 0.2479, stage5_loss_mask: 0.2590, loss: 9.0004\n",
      "2025-07-16 16:14:01,717 - mmdet - INFO - Epoch [28][450/750]\tlr: 2.500e-05, eta: 2:53:03, time: 0.365, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0612, stage0_pos_acc: 35.9344, stage0_loss_bbox: 0.5408, stage0_loss_iou: 1.0227, stage0_loss_mask: 0.8611, stage1_loss_cls: 0.6270, stage1_pos_acc: 68.8186, stage1_loss_bbox: 0.2544, stage1_loss_iou: 0.4458, stage1_loss_mask: 0.4717, stage2_loss_cls: 0.5027, stage2_pos_acc: 69.4251, stage2_loss_bbox: 0.1991, stage2_loss_iou: 0.3400, stage2_loss_mask: 0.4043, stage3_loss_cls: 0.4214, stage3_pos_acc: 75.9600, stage3_loss_bbox: 0.1775, stage3_loss_iou: 0.3101, stage3_loss_mask: 0.3875, stage4_loss_cls: 0.3475, stage4_pos_acc: 80.4487, stage4_loss_bbox: 0.1702, stage4_loss_iou: 0.2911, stage4_loss_mask: 0.3971, stage5_loss_cls: 0.3182, stage5_pos_acc: 84.4496, stage5_loss_bbox: 0.1642, stage5_loss_iou: 0.2800, stage5_loss_mask: 0.3943, loss: 10.3900\n",
      "2025-07-16 16:14:20,109 - mmdet - INFO - Epoch [28][500/750]\tlr: 2.500e-05, eta: 2:52:44, time: 0.368, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0812, stage0_pos_acc: 35.5044, stage0_loss_bbox: 0.5113, stage0_loss_iou: 1.0293, stage0_loss_mask: 0.8728, stage1_loss_cls: 0.6755, stage1_pos_acc: 61.1109, stage1_loss_bbox: 0.2332, stage1_loss_iou: 0.4584, stage1_loss_mask: 0.4678, stage2_loss_cls: 0.5718, stage2_pos_acc: 72.0842, stage2_loss_bbox: 0.1757, stage2_loss_iou: 0.3469, stage2_loss_mask: 0.4124, stage3_loss_cls: 0.4454, stage3_pos_acc: 74.1127, stage3_loss_bbox: 0.1680, stage3_loss_iou: 0.3229, stage3_loss_mask: 0.3834, stage4_loss_cls: 0.3789, stage4_pos_acc: 80.8013, stage4_loss_bbox: 0.1652, stage4_loss_iou: 0.3034, stage4_loss_mask: 0.3633, stage5_loss_cls: 0.3422, stage5_pos_acc: 86.5855, stage5_loss_bbox: 0.1510, stage5_loss_iou: 0.2969, stage5_loss_mask: 0.3618, loss: 10.5187\n",
      "2025-07-16 16:14:38,552 - mmdet - INFO - Epoch [28][550/750]\tlr: 2.500e-05, eta: 2:52:25, time: 0.369, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0868, stage0_pos_acc: 35.4611, stage0_loss_bbox: 0.5399, stage0_loss_iou: 1.0355, stage0_loss_mask: 0.8681, stage1_loss_cls: 0.6685, stage1_pos_acc: 57.3246, stage1_loss_bbox: 0.2471, stage1_loss_iou: 0.4697, stage1_loss_mask: 0.4529, stage2_loss_cls: 0.5546, stage2_pos_acc: 66.0683, stage2_loss_bbox: 0.1886, stage2_loss_iou: 0.3597, stage2_loss_mask: 0.4397, stage3_loss_cls: 0.4613, stage3_pos_acc: 71.6310, stage3_loss_bbox: 0.1820, stage3_loss_iou: 0.3409, stage3_loss_mask: 0.4096, stage4_loss_cls: 0.3818, stage4_pos_acc: 80.0556, stage4_loss_bbox: 0.1862, stage4_loss_iou: 0.3343, stage4_loss_mask: 0.3892, stage5_loss_cls: 0.3594, stage5_pos_acc: 79.3294, stage5_loss_bbox: 0.1831, stage5_loss_iou: 0.3308, stage5_loss_mask: 0.3886, loss: 10.8582\n",
      "2025-07-16 16:14:57,696 - mmdet - INFO - Epoch [28][600/750]\tlr: 2.500e-05, eta: 2:52:10, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0627, stage0_pos_acc: 37.3442, stage0_loss_bbox: 0.5098, stage0_loss_iou: 0.9928, stage0_loss_mask: 0.9151, stage1_loss_cls: 0.6253, stage1_pos_acc: 60.6544, stage1_loss_bbox: 0.2296, stage1_loss_iou: 0.4772, stage1_loss_mask: 0.5302, stage2_loss_cls: 0.4842, stage2_pos_acc: 70.9903, stage2_loss_bbox: 0.1855, stage2_loss_iou: 0.3678, stage2_loss_mask: 0.4623, stage3_loss_cls: 0.3925, stage3_pos_acc: 76.7015, stage3_loss_bbox: 0.1760, stage3_loss_iou: 0.3366, stage3_loss_mask: 0.4509, stage4_loss_cls: 0.3201, stage4_pos_acc: 81.2543, stage4_loss_bbox: 0.1685, stage4_loss_iou: 0.3285, stage4_loss_mask: 0.4491, stage5_loss_cls: 0.2942, stage5_pos_acc: 83.9266, stage5_loss_bbox: 0.1711, stage5_loss_iou: 0.3302, stage5_loss_mask: 0.4605, loss: 10.7208\n",
      "2025-07-16 16:15:16,318 - mmdet - INFO - Epoch [28][650/750]\tlr: 2.500e-05, eta: 2:51:52, time: 0.372, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0805, stage0_pos_acc: 34.4753, stage0_loss_bbox: 0.5425, stage0_loss_iou: 0.9975, stage0_loss_mask: 0.7478, stage1_loss_cls: 0.6619, stage1_pos_acc: 59.5646, stage1_loss_bbox: 0.2367, stage1_loss_iou: 0.4439, stage1_loss_mask: 0.4421, stage2_loss_cls: 0.5558, stage2_pos_acc: 65.5574, stage2_loss_bbox: 0.1949, stage2_loss_iou: 0.3585, stage2_loss_mask: 0.4280, stage3_loss_cls: 0.4225, stage3_pos_acc: 71.6919, stage3_loss_bbox: 0.1871, stage3_loss_iou: 0.3340, stage3_loss_mask: 0.4039, stage4_loss_cls: 0.3510, stage4_pos_acc: 79.7609, stage4_loss_bbox: 0.1814, stage4_loss_iou: 0.3288, stage4_loss_mask: 0.4178, stage5_loss_cls: 0.3178, stage5_pos_acc: 83.2900, stage5_loss_bbox: 0.1806, stage5_loss_iou: 0.3258, stage5_loss_mask: 0.4147, loss: 10.5557\n",
      "2025-07-16 16:15:34,700 - mmdet - INFO - Epoch [28][700/750]\tlr: 2.500e-05, eta: 2:51:33, time: 0.368, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0462, stage0_pos_acc: 35.1991, stage0_loss_bbox: 0.5028, stage0_loss_iou: 0.9807, stage0_loss_mask: 0.8715, stage1_loss_cls: 0.6735, stage1_pos_acc: 53.7857, stage1_loss_bbox: 0.2382, stage1_loss_iou: 0.4852, stage1_loss_mask: 0.6320, stage2_loss_cls: 0.5761, stage2_pos_acc: 66.4452, stage2_loss_bbox: 0.1984, stage2_loss_iou: 0.3995, stage2_loss_mask: 0.5456, stage3_loss_cls: 0.4725, stage3_pos_acc: 76.0881, stage3_loss_bbox: 0.1941, stage3_loss_iou: 0.3717, stage3_loss_mask: 0.5169, stage4_loss_cls: 0.3793, stage4_pos_acc: 77.5392, stage4_loss_bbox: 0.1941, stage4_loss_iou: 0.3665, stage4_loss_mask: 0.5307, stage5_loss_cls: 0.3521, stage5_pos_acc: 82.0591, stage5_loss_bbox: 0.1963, stage5_loss_iou: 0.3661, stage5_loss_mask: 0.5205, loss: 11.6105\n",
      "2025-07-16 16:15:53,388 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:15:53,388 - mmdet - INFO - Epoch [28][750/750]\tlr: 2.500e-05, eta: 2:51:16, time: 0.374, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0459, stage0_pos_acc: 35.6740, stage0_loss_bbox: 0.5008, stage0_loss_iou: 0.9298, stage0_loss_mask: 0.6803, stage1_loss_cls: 0.6270, stage1_pos_acc: 61.7372, stage1_loss_bbox: 0.2431, stage1_loss_iou: 0.4280, stage1_loss_mask: 0.3861, stage2_loss_cls: 0.5264, stage2_pos_acc: 67.4209, stage2_loss_bbox: 0.2022, stage2_loss_iou: 0.3514, stage2_loss_mask: 0.3402, stage3_loss_cls: 0.4261, stage3_pos_acc: 72.8595, stage3_loss_bbox: 0.1915, stage3_loss_iou: 0.3324, stage3_loss_mask: 0.3378, stage4_loss_cls: 0.3911, stage4_pos_acc: 76.4237, stage4_loss_bbox: 0.1938, stage4_loss_iou: 0.3306, stage4_loss_mask: 0.3336, stage5_loss_cls: 0.3683, stage5_pos_acc: 80.6087, stage5_loss_bbox: 0.1875, stage5_loss_iou: 0.3236, stage5_loss_mask: 0.3346, loss: 10.0124\n",
      "2025-07-16 16:15:53,491 - mmdet - INFO - Saving checkpoint at 28 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 16:18:55,643 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.26s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.396\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.398\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.398\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.041\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.196\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.444\n",
      "2025-07-16 16:18:57,417 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.41s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.80s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.044\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.061\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.208\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.455\n",
      "2025-07-16 16:19:00,460 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:19:00,460 - mmdet - INFO - Epoch(val) [28][750]\tbbox_mAP: 0.0330, bbox_mAP_50: 0.0640, bbox_mAP_75: 0.0300, bbox_mAP_s: 0.0110, bbox_mAP_m: 0.0110, bbox_mAP_l: 0.0420, bbox_mAP_copypaste: 0.033 0.064 0.030 0.011 0.011 0.042, segm_mAP: 0.0350, segm_mAP_50: 0.0640, segm_mAP_75: 0.0310, segm_mAP_s: 0.0110, segm_mAP_m: 0.0130, segm_mAP_l: 0.0440, segm_mAP_copypaste: 0.035 0.064 0.031 0.011 0.013 0.044\n",
      "2025-07-16 16:19:21,180 - mmdet - INFO - Epoch [29][50/750]\tlr: 2.500e-05, eta: 2:51:09, time: 0.414, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.0284, stage0_pos_acc: 36.7406, stage0_loss_bbox: 0.5653, stage0_loss_iou: 1.0357, stage0_loss_mask: 1.0017, stage1_loss_cls: 0.6277, stage1_pos_acc: 64.4663, stage1_loss_bbox: 0.2569, stage1_loss_iou: 0.4985, stage1_loss_mask: 0.5814, stage2_loss_cls: 0.5103, stage2_pos_acc: 72.8274, stage2_loss_bbox: 0.2011, stage2_loss_iou: 0.3790, stage2_loss_mask: 0.5284, stage3_loss_cls: 0.3803, stage3_pos_acc: 78.2599, stage3_loss_bbox: 0.1851, stage3_loss_iou: 0.3426, stage3_loss_mask: 0.4968, stage4_loss_cls: 0.3119, stage4_pos_acc: 85.7360, stage4_loss_bbox: 0.1822, stage4_loss_iou: 0.3310, stage4_loss_mask: 0.4545, stage5_loss_cls: 0.2852, stage5_pos_acc: 87.9867, stage5_loss_bbox: 0.1743, stage5_loss_iou: 0.3254, stage5_loss_mask: 0.4501, loss: 11.1341\n",
      "2025-07-16 16:19:39,662 - mmdet - INFO - Epoch [29][100/750]\tlr: 2.500e-05, eta: 2:50:50, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0715, stage0_pos_acc: 38.6028, stage0_loss_bbox: 0.5407, stage0_loss_iou: 1.0675, stage0_loss_mask: 1.0257, stage1_loss_cls: 0.6646, stage1_pos_acc: 63.2876, stage1_loss_bbox: 0.2452, stage1_loss_iou: 0.5070, stage1_loss_mask: 0.5884, stage2_loss_cls: 0.5421, stage2_pos_acc: 70.6242, stage2_loss_bbox: 0.1916, stage2_loss_iou: 0.3970, stage2_loss_mask: 0.5507, stage3_loss_cls: 0.4413, stage3_pos_acc: 75.1780, stage3_loss_bbox: 0.1819, stage3_loss_iou: 0.3627, stage3_loss_mask: 0.5249, stage4_loss_cls: 0.3636, stage4_pos_acc: 81.9701, stage4_loss_bbox: 0.1850, stage4_loss_iou: 0.3535, stage4_loss_mask: 0.5065, stage5_loss_cls: 0.3392, stage5_pos_acc: 84.4590, stage5_loss_bbox: 0.1846, stage5_loss_iou: 0.3488, stage5_loss_mask: 0.5256, loss: 11.7096\n",
      "2025-07-16 16:19:58,121 - mmdet - INFO - Epoch [29][150/750]\tlr: 2.500e-05, eta: 2:50:31, time: 0.369, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0180, stage0_pos_acc: 34.2796, stage0_loss_bbox: 0.5247, stage0_loss_iou: 1.0005, stage0_loss_mask: 0.7280, stage1_loss_cls: 0.6736, stage1_pos_acc: 61.6525, stage1_loss_bbox: 0.2525, stage1_loss_iou: 0.4718, stage1_loss_mask: 0.4430, stage2_loss_cls: 0.5386, stage2_pos_acc: 71.6958, stage2_loss_bbox: 0.2035, stage2_loss_iou: 0.3676, stage2_loss_mask: 0.3538, stage3_loss_cls: 0.4180, stage3_pos_acc: 77.7121, stage3_loss_bbox: 0.1821, stage3_loss_iou: 0.3319, stage3_loss_mask: 0.3411, stage4_loss_cls: 0.3525, stage4_pos_acc: 85.1364, stage4_loss_bbox: 0.1818, stage4_loss_iou: 0.3133, stage4_loss_mask: 0.3385, stage5_loss_cls: 0.2972, stage5_pos_acc: 85.8333, stage5_loss_bbox: 0.1782, stage5_loss_iou: 0.3083, stage5_loss_mask: 0.3371, loss: 10.1555\n",
      "2025-07-16 16:20:16,565 - mmdet - INFO - Epoch [29][200/750]\tlr: 2.500e-05, eta: 2:50:12, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9802, stage0_pos_acc: 38.0847, stage0_loss_bbox: 0.5023, stage0_loss_iou: 0.9335, stage0_loss_mask: 0.8684, stage1_loss_cls: 0.6549, stage1_pos_acc: 58.8387, stage1_loss_bbox: 0.2405, stage1_loss_iou: 0.4655, stage1_loss_mask: 0.4658, stage2_loss_cls: 0.5404, stage2_pos_acc: 68.1382, stage2_loss_bbox: 0.2000, stage2_loss_iou: 0.3786, stage2_loss_mask: 0.4249, stage3_loss_cls: 0.4539, stage3_pos_acc: 76.6453, stage3_loss_bbox: 0.1816, stage3_loss_iou: 0.3498, stage3_loss_mask: 0.4056, stage4_loss_cls: 0.3669, stage4_pos_acc: 81.4781, stage4_loss_bbox: 0.1717, stage4_loss_iou: 0.3374, stage4_loss_mask: 0.4081, stage5_loss_cls: 0.3395, stage5_pos_acc: 83.6763, stage5_loss_bbox: 0.1731, stage5_loss_iou: 0.3381, stage5_loss_mask: 0.3901, loss: 10.5708\n",
      "2025-07-16 16:20:35,238 - mmdet - INFO - Epoch [29][250/750]\tlr: 2.500e-05, eta: 2:49:54, time: 0.373, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0454, stage0_pos_acc: 37.1925, stage0_loss_bbox: 0.4897, stage0_loss_iou: 0.9382, stage0_loss_mask: 0.8731, stage1_loss_cls: 0.6706, stage1_pos_acc: 60.1600, stage1_loss_bbox: 0.2296, stage1_loss_iou: 0.4566, stage1_loss_mask: 0.6388, stage2_loss_cls: 0.5520, stage2_pos_acc: 69.3427, stage2_loss_bbox: 0.1938, stage2_loss_iou: 0.3771, stage2_loss_mask: 0.5750, stage3_loss_cls: 0.4664, stage3_pos_acc: 73.1711, stage3_loss_bbox: 0.1769, stage3_loss_iou: 0.3468, stage3_loss_mask: 0.5573, stage4_loss_cls: 0.3940, stage4_pos_acc: 78.6328, stage4_loss_bbox: 0.1655, stage4_loss_iou: 0.3418, stage4_loss_mask: 0.5552, stage5_loss_cls: 0.3636, stage5_pos_acc: 82.3066, stage5_loss_bbox: 0.1695, stage5_loss_iou: 0.3396, stage5_loss_mask: 0.5583, loss: 11.4747\n",
      "2025-07-16 16:20:53,851 - mmdet - INFO - Epoch [29][300/750]\tlr: 2.500e-05, eta: 2:49:36, time: 0.372, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0508, stage0_pos_acc: 37.7818, stage0_loss_bbox: 0.4942, stage0_loss_iou: 1.0468, stage0_loss_mask: 0.9435, stage1_loss_cls: 0.6143, stage1_pos_acc: 67.5260, stage1_loss_bbox: 0.2144, stage1_loss_iou: 0.4662, stage1_loss_mask: 0.5226, stage2_loss_cls: 0.4885, stage2_pos_acc: 76.0134, stage2_loss_bbox: 0.1840, stage2_loss_iou: 0.3624, stage2_loss_mask: 0.4462, stage3_loss_cls: 0.3774, stage3_pos_acc: 80.2134, stage3_loss_bbox: 0.1619, stage3_loss_iou: 0.3373, stage3_loss_mask: 0.4249, stage4_loss_cls: 0.3050, stage4_pos_acc: 85.1687, stage4_loss_bbox: 0.1614, stage4_loss_iou: 0.3308, stage4_loss_mask: 0.4467, stage5_loss_cls: 0.2728, stage5_pos_acc: 89.1539, stage5_loss_bbox: 0.1583, stage5_loss_iou: 0.3254, stage5_loss_mask: 0.4435, loss: 10.5792\n",
      "2025-07-16 16:21:12,373 - mmdet - INFO - Epoch [29][350/750]\tlr: 2.500e-05, eta: 2:49:17, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0333, stage0_pos_acc: 37.5217, stage0_loss_bbox: 0.5007, stage0_loss_iou: 1.0604, stage0_loss_mask: 0.7396, stage1_loss_cls: 0.6514, stage1_pos_acc: 59.3929, stage1_loss_bbox: 0.2135, stage1_loss_iou: 0.4503, stage1_loss_mask: 0.3959, stage2_loss_cls: 0.5332, stage2_pos_acc: 65.2922, stage2_loss_bbox: 0.1644, stage2_loss_iou: 0.3427, stage2_loss_mask: 0.4028, stage3_loss_cls: 0.4016, stage3_pos_acc: 81.6670, stage3_loss_bbox: 0.1510, stage3_loss_iou: 0.3093, stage3_loss_mask: 0.3665, stage4_loss_cls: 0.3302, stage4_pos_acc: 82.8765, stage4_loss_bbox: 0.1465, stage4_loss_iou: 0.2980, stage4_loss_mask: 0.3465, stage5_loss_cls: 0.2971, stage5_pos_acc: 86.9796, stage5_loss_bbox: 0.1444, stage5_loss_iou: 0.2969, stage5_loss_mask: 0.3422, loss: 9.9185\n",
      "2025-07-16 16:21:30,917 - mmdet - INFO - Epoch [29][400/750]\tlr: 2.500e-05, eta: 2:48:59, time: 0.371, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0536, stage0_pos_acc: 36.1333, stage0_loss_bbox: 0.5496, stage0_loss_iou: 1.0195, stage0_loss_mask: 0.8543, stage1_loss_cls: 0.6752, stage1_pos_acc: 63.5190, stage1_loss_bbox: 0.2408, stage1_loss_iou: 0.4581, stage1_loss_mask: 0.5054, stage2_loss_cls: 0.5053, stage2_pos_acc: 73.2238, stage2_loss_bbox: 0.1971, stage2_loss_iou: 0.3737, stage2_loss_mask: 0.4986, stage3_loss_cls: 0.4010, stage3_pos_acc: 76.1571, stage3_loss_bbox: 0.1872, stage3_loss_iou: 0.3495, stage3_loss_mask: 0.4760, stage4_loss_cls: 0.3527, stage4_pos_acc: 79.9937, stage4_loss_bbox: 0.1723, stage4_loss_iou: 0.3308, stage4_loss_mask: 0.4633, stage5_loss_cls: 0.3159, stage5_pos_acc: 84.8794, stage5_loss_bbox: 0.1765, stage5_loss_iou: 0.3290, stage5_loss_mask: 0.4626, loss: 10.9480\n",
      "2025-07-16 16:21:49,422 - mmdet - INFO - Epoch [29][450/750]\tlr: 2.500e-05, eta: 2:48:40, time: 0.370, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0076, stage0_pos_acc: 39.0171, stage0_loss_bbox: 0.5264, stage0_loss_iou: 1.0143, stage0_loss_mask: 0.9167, stage1_loss_cls: 0.6332, stage1_pos_acc: 61.3992, stage1_loss_bbox: 0.2332, stage1_loss_iou: 0.4627, stage1_loss_mask: 0.5232, stage2_loss_cls: 0.5272, stage2_pos_acc: 68.1448, stage2_loss_bbox: 0.1827, stage2_loss_iou: 0.3571, stage2_loss_mask: 0.4817, stage3_loss_cls: 0.4158, stage3_pos_acc: 73.5048, stage3_loss_bbox: 0.1802, stage3_loss_iou: 0.3462, stage3_loss_mask: 0.4419, stage4_loss_cls: 0.3513, stage4_pos_acc: 81.3726, stage4_loss_bbox: 0.1678, stage4_loss_iou: 0.3263, stage4_loss_mask: 0.4212, stage5_loss_cls: 0.3218, stage5_pos_acc: 84.2889, stage5_loss_bbox: 0.1704, stage5_loss_iou: 0.3179, stage5_loss_mask: 0.4239, loss: 10.7508\n",
      "2025-07-16 16:22:08,085 - mmdet - INFO - Epoch [29][500/750]\tlr: 2.500e-05, eta: 2:48:22, time: 0.373, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9996, stage0_pos_acc: 38.5562, stage0_loss_bbox: 0.5215, stage0_loss_iou: 0.9951, stage0_loss_mask: 0.7313, stage1_loss_cls: 0.6322, stage1_pos_acc: 67.3269, stage1_loss_bbox: 0.2344, stage1_loss_iou: 0.4474, stage1_loss_mask: 0.4184, stage2_loss_cls: 0.5149, stage2_pos_acc: 72.6617, stage2_loss_bbox: 0.1876, stage2_loss_iou: 0.3394, stage2_loss_mask: 0.3896, stage3_loss_cls: 0.4248, stage3_pos_acc: 77.3840, stage3_loss_bbox: 0.1796, stage3_loss_iou: 0.3071, stage3_loss_mask: 0.3213, stage4_loss_cls: 0.3272, stage4_pos_acc: 82.3612, stage4_loss_bbox: 0.1762, stage4_loss_iou: 0.2970, stage4_loss_mask: 0.3059, stage5_loss_cls: 0.2976, stage5_pos_acc: 87.6960, stage5_loss_bbox: 0.1740, stage5_loss_iou: 0.2906, stage5_loss_mask: 0.3039, loss: 9.8165\n",
      "2025-07-16 16:22:27,246 - mmdet - INFO - Epoch [29][550/750]\tlr: 2.500e-05, eta: 2:48:06, time: 0.383, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0334, stage0_pos_acc: 35.1711, stage0_loss_bbox: 0.5228, stage0_loss_iou: 0.9556, stage0_loss_mask: 0.7882, stage1_loss_cls: 0.6236, stage1_pos_acc: 62.6092, stage1_loss_bbox: 0.2586, stage1_loss_iou: 0.4279, stage1_loss_mask: 0.3718, stage2_loss_cls: 0.4900, stage2_pos_acc: 71.3911, stage2_loss_bbox: 0.2064, stage2_loss_iou: 0.3126, stage2_loss_mask: 0.3093, stage3_loss_cls: 0.4034, stage3_pos_acc: 76.3299, stage3_loss_bbox: 0.1723, stage3_loss_iou: 0.2754, stage3_loss_mask: 0.2724, stage4_loss_cls: 0.3407, stage4_pos_acc: 80.4450, stage4_loss_bbox: 0.1651, stage4_loss_iou: 0.2615, stage4_loss_mask: 0.2690, stage5_loss_cls: 0.3092, stage5_pos_acc: 82.2323, stage5_loss_bbox: 0.1607, stage5_loss_iou: 0.2597, stage5_loss_mask: 0.2631, loss: 9.4524\n",
      "2025-07-16 16:22:45,626 - mmdet - INFO - Epoch [29][600/750]\tlr: 2.500e-05, eta: 2:47:47, time: 0.368, data_time: 0.006, memory: 11264, stage0_loss_cls: 1.0414, stage0_pos_acc: 33.2157, stage0_loss_bbox: 0.5273, stage0_loss_iou: 1.1204, stage0_loss_mask: 0.9705, stage1_loss_cls: 0.6500, stage1_pos_acc: 62.8569, stage1_loss_bbox: 0.2080, stage1_loss_iou: 0.4723, stage1_loss_mask: 0.5008, stage2_loss_cls: 0.5008, stage2_pos_acc: 70.3613, stage2_loss_bbox: 0.1593, stage2_loss_iou: 0.3478, stage2_loss_mask: 0.4248, stage3_loss_cls: 0.3913, stage3_pos_acc: 76.7998, stage3_loss_bbox: 0.1379, stage3_loss_iou: 0.3078, stage3_loss_mask: 0.3794, stage4_loss_cls: 0.3225, stage4_pos_acc: 81.3510, stage4_loss_bbox: 0.1240, stage4_loss_iou: 0.2900, stage4_loss_mask: 0.3621, stage5_loss_cls: 0.2827, stage5_pos_acc: 87.3657, stage5_loss_bbox: 0.1218, stage5_loss_iou: 0.2859, stage5_loss_mask: 0.3726, loss: 10.3015\n",
      "2025-07-16 16:23:04,247 - mmdet - INFO - Epoch [29][650/750]\tlr: 2.500e-05, eta: 2:47:29, time: 0.372, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0414, stage0_pos_acc: 33.5367, stage0_loss_bbox: 0.5534, stage0_loss_iou: 1.0698, stage0_loss_mask: 0.9704, stage1_loss_cls: 0.6623, stage1_pos_acc: 66.9348, stage1_loss_bbox: 0.2265, stage1_loss_iou: 0.4790, stage1_loss_mask: 0.5528, stage2_loss_cls: 0.5544, stage2_pos_acc: 71.7264, stage2_loss_bbox: 0.1795, stage2_loss_iou: 0.3732, stage2_loss_mask: 0.5329, stage3_loss_cls: 0.4095, stage3_pos_acc: 79.1151, stage3_loss_bbox: 0.1657, stage3_loss_iou: 0.3483, stage3_loss_mask: 0.4928, stage4_loss_cls: 0.3281, stage4_pos_acc: 85.5663, stage4_loss_bbox: 0.1634, stage4_loss_iou: 0.3374, stage4_loss_mask: 0.4706, stage5_loss_cls: 0.2938, stage5_pos_acc: 87.7622, stage5_loss_bbox: 0.1635, stage5_loss_iou: 0.3311, stage5_loss_mask: 0.4729, loss: 11.1728\n",
      "2025-07-16 16:23:23,194 - mmdet - INFO - Epoch [29][700/750]\tlr: 2.500e-05, eta: 2:47:12, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0119, stage0_pos_acc: 41.9688, stage0_loss_bbox: 0.5263, stage0_loss_iou: 0.9714, stage0_loss_mask: 0.7686, stage1_loss_cls: 0.6166, stage1_pos_acc: 66.1317, stage1_loss_bbox: 0.2233, stage1_loss_iou: 0.4296, stage1_loss_mask: 0.4185, stage2_loss_cls: 0.4758, stage2_pos_acc: 73.5845, stage2_loss_bbox: 0.1865, stage2_loss_iou: 0.3269, stage2_loss_mask: 0.3659, stage3_loss_cls: 0.3918, stage3_pos_acc: 79.7621, stage3_loss_bbox: 0.1716, stage3_loss_iou: 0.3060, stage3_loss_mask: 0.3422, stage4_loss_cls: 0.3235, stage4_pos_acc: 85.2067, stage4_loss_bbox: 0.1700, stage4_loss_iou: 0.2942, stage4_loss_mask: 0.3415, stage5_loss_cls: 0.2692, stage5_pos_acc: 87.7048, stage5_loss_bbox: 0.1691, stage5_loss_iou: 0.2898, stage5_loss_mask: 0.3562, loss: 9.7465\n",
      "2025-07-16 16:23:41,366 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:23:41,366 - mmdet - INFO - Epoch [29][750/750]\tlr: 2.500e-05, eta: 2:46:52, time: 0.363, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0123, stage0_pos_acc: 30.0035, stage0_loss_bbox: 0.5078, stage0_loss_iou: 0.9731, stage0_loss_mask: 0.6945, stage1_loss_cls: 0.6396, stage1_pos_acc: 56.6224, stage1_loss_bbox: 0.2363, stage1_loss_iou: 0.4351, stage1_loss_mask: 0.3547, stage2_loss_cls: 0.5243, stage2_pos_acc: 66.4360, stage2_loss_bbox: 0.1754, stage2_loss_iou: 0.3276, stage2_loss_mask: 0.3174, stage3_loss_cls: 0.4340, stage3_pos_acc: 73.3926, stage3_loss_bbox: 0.1606, stage3_loss_iou: 0.3013, stage3_loss_mask: 0.3079, stage4_loss_cls: 0.3768, stage4_pos_acc: 82.7283, stage4_loss_bbox: 0.1541, stage4_loss_iou: 0.2824, stage4_loss_mask: 0.2895, stage5_loss_cls: 0.3534, stage5_pos_acc: 85.9885, stage5_loss_bbox: 0.1505, stage5_loss_iou: 0.2707, stage5_loss_mask: 0.2886, loss: 9.5678\n",
      "2025-07-16 16:23:41,491 - mmdet - INFO - Saving checkpoint at 29 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 88s, ETA:     0s2025-07-16 16:26:46,068 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.16s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.31s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.43s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.437\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.437\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.270\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.473\n",
      "2025-07-16 16:26:48,035 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.84s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.058\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.040\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.450\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.452\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.452\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.276\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.489\n",
      "2025-07-16 16:26:50,924 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:26:50,924 - mmdet - INFO - Epoch(val) [29][750]\tbbox_mAP: 0.0310, bbox_mAP_50: 0.0580, bbox_mAP_75: 0.0290, bbox_mAP_s: 0.0210, bbox_mAP_m: 0.0080, bbox_mAP_l: 0.0380, bbox_mAP_copypaste: 0.031 0.058 0.029 0.021 0.008 0.038, segm_mAP: 0.0320, segm_mAP_50: 0.0580, segm_mAP_75: 0.0310, segm_mAP_s: 0.0200, segm_mAP_m: 0.0080, segm_mAP_l: 0.0400, segm_mAP_copypaste: 0.032 0.058 0.031 0.020 0.008 0.040\n",
      "2025-07-16 16:27:11,826 - mmdet - INFO - Epoch [30][50/750]\tlr: 2.500e-05, eta: 2:46:44, time: 0.418, data_time: 0.052, memory: 11264, stage0_loss_cls: 1.0293, stage0_pos_acc: 39.8345, stage0_loss_bbox: 0.5333, stage0_loss_iou: 0.9713, stage0_loss_mask: 0.6860, stage1_loss_cls: 0.6244, stage1_pos_acc: 67.5337, stage1_loss_bbox: 0.2517, stage1_loss_iou: 0.4292, stage1_loss_mask: 0.4704, stage2_loss_cls: 0.4945, stage2_pos_acc: 70.8397, stage2_loss_bbox: 0.2011, stage2_loss_iou: 0.3244, stage2_loss_mask: 0.3978, stage3_loss_cls: 0.3890, stage3_pos_acc: 76.7401, stage3_loss_bbox: 0.1854, stage3_loss_iou: 0.3073, stage3_loss_mask: 0.3746, stage4_loss_cls: 0.2885, stage4_pos_acc: 86.6683, stage4_loss_bbox: 0.1639, stage4_loss_iou: 0.2864, stage4_loss_mask: 0.3748, stage5_loss_cls: 0.2444, stage5_pos_acc: 89.9472, stage5_loss_bbox: 0.1694, stage5_loss_iou: 0.2866, stage5_loss_mask: 0.3735, loss: 9.8572\n",
      "2025-07-16 16:27:30,110 - mmdet - INFO - Epoch [30][100/750]\tlr: 2.500e-05, eta: 2:46:24, time: 0.366, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0580, stage0_pos_acc: 46.0194, stage0_loss_bbox: 0.4841, stage0_loss_iou: 0.9653, stage0_loss_mask: 0.6289, stage1_loss_cls: 0.6124, stage1_pos_acc: 68.1094, stage1_loss_bbox: 0.1967, stage1_loss_iou: 0.4060, stage1_loss_mask: 0.3775, stage2_loss_cls: 0.4753, stage2_pos_acc: 78.7599, stage2_loss_bbox: 0.1465, stage2_loss_iou: 0.3114, stage2_loss_mask: 0.3156, stage3_loss_cls: 0.3423, stage3_pos_acc: 84.4746, stage3_loss_bbox: 0.1354, stage3_loss_iou: 0.2768, stage3_loss_mask: 0.3079, stage4_loss_cls: 0.2728, stage4_pos_acc: 89.1533, stage4_loss_bbox: 0.1435, stage4_loss_iou: 0.2751, stage4_loss_mask: 0.3130, stage5_loss_cls: 0.2419, stage5_pos_acc: 89.8893, stage5_loss_bbox: 0.1475, stage5_loss_iou: 0.2734, stage5_loss_mask: 0.3132, loss: 9.0204\n",
      "2025-07-16 16:27:48,280 - mmdet - INFO - Epoch [30][150/750]\tlr: 2.500e-05, eta: 2:46:04, time: 0.363, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0908, stage0_pos_acc: 39.7474, stage0_loss_bbox: 0.4813, stage0_loss_iou: 1.0004, stage0_loss_mask: 0.7817, stage1_loss_cls: 0.6126, stage1_pos_acc: 61.3203, stage1_loss_bbox: 0.2069, stage1_loss_iou: 0.4250, stage1_loss_mask: 0.5449, stage2_loss_cls: 0.4682, stage2_pos_acc: 73.5593, stage2_loss_bbox: 0.1623, stage2_loss_iou: 0.3371, stage2_loss_mask: 0.4990, stage3_loss_cls: 0.3486, stage3_pos_acc: 84.1087, stage3_loss_bbox: 0.1529, stage3_loss_iou: 0.3184, stage3_loss_mask: 0.4794, stage4_loss_cls: 0.2791, stage4_pos_acc: 87.3301, stage4_loss_bbox: 0.1553, stage4_loss_iou: 0.3221, stage4_loss_mask: 0.4813, stage5_loss_cls: 0.2470, stage5_pos_acc: 91.1896, stage5_loss_bbox: 0.1475, stage5_loss_iou: 0.3172, stage5_loss_mask: 0.4749, loss: 10.3340\n",
      "2025-07-16 16:29:39,323 - mmdet - INFO - Epoch [30][450/750]\tlr: 2.500e-05, eta: 2:44:11, time: 0.382, data_time: 0.019, memory: 11264, stage0_loss_cls: 1.0488, stage0_pos_acc: 38.9592, stage0_loss_bbox: 0.4485, stage0_loss_iou: 0.8954, stage0_loss_mask: 0.4817, stage1_loss_cls: 0.6199, stage1_pos_acc: 65.5027, stage1_loss_bbox: 0.1935, stage1_loss_iou: 0.3444, stage1_loss_mask: 0.2468, stage2_loss_cls: 0.4865, stage2_pos_acc: 74.6468, stage2_loss_bbox: 0.1551, stage2_loss_iou: 0.2654, stage2_loss_mask: 0.2339, stage3_loss_cls: 0.3907, stage3_pos_acc: 78.9366, stage3_loss_bbox: 0.1416, stage3_loss_iou: 0.2408, stage3_loss_mask: 0.2252, stage4_loss_cls: 0.3242, stage4_pos_acc: 83.2532, stage4_loss_bbox: 0.1360, stage4_loss_iou: 0.2302, stage4_loss_mask: 0.2231, stage5_loss_cls: 0.2900, stage5_pos_acc: 85.9647, stage5_loss_bbox: 0.1327, stage5_loss_iou: 0.2266, stage5_loss_mask: 0.2171, loss: 8.1980\n",
      "2025-07-16 16:29:58,069 - mmdet - INFO - Epoch [30][500/750]\tlr: 2.500e-05, eta: 2:43:54, time: 0.375, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0301, stage0_pos_acc: 37.3345, stage0_loss_bbox: 0.5118, stage0_loss_iou: 0.9831, stage0_loss_mask: 0.8684, stage1_loss_cls: 0.6369, stage1_pos_acc: 65.9961, stage1_loss_bbox: 0.2399, stage1_loss_iou: 0.4811, stage1_loss_mask: 0.5886, stage2_loss_cls: 0.5311, stage2_pos_acc: 68.4111, stage2_loss_bbox: 0.1858, stage2_loss_iou: 0.3676, stage2_loss_mask: 0.5512, stage3_loss_cls: 0.4113, stage3_pos_acc: 77.1127, stage3_loss_bbox: 0.1717, stage3_loss_iou: 0.3442, stage3_loss_mask: 0.5081, stage4_loss_cls: 0.3344, stage4_pos_acc: 82.0050, stage4_loss_bbox: 0.1687, stage4_loss_iou: 0.3342, stage4_loss_mask: 0.4901, stage5_loss_cls: 0.3086, stage5_pos_acc: 86.2820, stage5_loss_bbox: 0.1717, stage5_loss_iou: 0.3294, stage5_loss_mask: 0.4830, loss: 11.0309\n",
      "2025-07-16 16:30:16,843 - mmdet - INFO - Epoch [30][550/750]\tlr: 2.500e-05, eta: 2:43:36, time: 0.375, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0457, stage0_pos_acc: 34.5641, stage0_loss_bbox: 0.4679, stage0_loss_iou: 0.9618, stage0_loss_mask: 0.8083, stage1_loss_cls: 0.6251, stage1_pos_acc: 62.5013, stage1_loss_bbox: 0.2052, stage1_loss_iou: 0.4383, stage1_loss_mask: 0.5105, stage2_loss_cls: 0.5001, stage2_pos_acc: 71.0641, stage2_loss_bbox: 0.1646, stage2_loss_iou: 0.3523, stage2_loss_mask: 0.4698, stage3_loss_cls: 0.3879, stage3_pos_acc: 79.5212, stage3_loss_bbox: 0.1604, stage3_loss_iou: 0.3331, stage3_loss_mask: 0.4856, stage4_loss_cls: 0.3205, stage4_pos_acc: 84.1546, stage4_loss_bbox: 0.1440, stage4_loss_iou: 0.3201, stage4_loss_mask: 0.4731, stage5_loss_cls: 0.2999, stage5_pos_acc: 87.9099, stage5_loss_bbox: 0.1404, stage5_loss_iou: 0.3117, stage5_loss_mask: 0.4770, loss: 10.4035\n",
      "2025-07-16 16:30:35,899 - mmdet - INFO - Epoch [30][600/750]\tlr: 2.500e-05, eta: 2:43:19, time: 0.381, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0303, stage0_pos_acc: 39.7857, stage0_loss_bbox: 0.4914, stage0_loss_iou: 1.0182, stage0_loss_mask: 0.7096, stage1_loss_cls: 0.6291, stage1_pos_acc: 63.3024, stage1_loss_bbox: 0.1955, stage1_loss_iou: 0.3983, stage1_loss_mask: 0.3867, stage2_loss_cls: 0.4575, stage2_pos_acc: 76.1460, stage2_loss_bbox: 0.1476, stage2_loss_iou: 0.2967, stage2_loss_mask: 0.3502, stage3_loss_cls: 0.3436, stage3_pos_acc: 83.2635, stage3_loss_bbox: 0.1377, stage3_loss_iou: 0.2751, stage3_loss_mask: 0.3342, stage4_loss_cls: 0.2645, stage4_pos_acc: 87.5603, stage4_loss_bbox: 0.1327, stage4_loss_iou: 0.2678, stage4_loss_mask: 0.3209, stage5_loss_cls: 0.2246, stage5_pos_acc: 90.0770, stage5_loss_bbox: 0.1294, stage5_loss_iou: 0.2630, stage5_loss_mask: 0.3371, loss: 9.1417\n",
      "2025-07-16 16:30:55,161 - mmdet - INFO - Epoch [30][650/750]\tlr: 2.500e-05, eta: 2:43:04, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0283, stage0_pos_acc: 32.3542, stage0_loss_bbox: 0.4905, stage0_loss_iou: 0.8819, stage0_loss_mask: 0.6229, stage1_loss_cls: 0.6259, stage1_pos_acc: 63.6703, stage1_loss_bbox: 0.2043, stage1_loss_iou: 0.3816, stage1_loss_mask: 0.3842, stage2_loss_cls: 0.4826, stage2_pos_acc: 73.6450, stage2_loss_bbox: 0.1601, stage2_loss_iou: 0.2950, stage2_loss_mask: 0.3269, stage3_loss_cls: 0.3936, stage3_pos_acc: 75.4608, stage3_loss_bbox: 0.1478, stage3_loss_iou: 0.2765, stage3_loss_mask: 0.3148, stage4_loss_cls: 0.3322, stage4_pos_acc: 85.6182, stage4_loss_bbox: 0.1416, stage4_loss_iou: 0.2650, stage4_loss_mask: 0.2993, stage5_loss_cls: 0.3093, stage5_pos_acc: 83.2527, stage5_loss_bbox: 0.1377, stage5_loss_iou: 0.2605, stage5_loss_mask: 0.3009, loss: 9.0634\n",
      "2025-07-16 16:31:14,624 - mmdet - INFO - Epoch [30][700/750]\tlr: 2.500e-05, eta: 2:42:49, time: 0.389, data_time: 0.011, memory: 11264, stage0_loss_cls: 1.0073, stage0_pos_acc: 38.7851, stage0_loss_bbox: 0.5055, stage0_loss_iou: 1.0113, stage0_loss_mask: 0.7788, stage1_loss_cls: 0.6253, stage1_pos_acc: 69.9741, stage1_loss_bbox: 0.2315, stage1_loss_iou: 0.4936, stage1_loss_mask: 0.5345, stage2_loss_cls: 0.5117, stage2_pos_acc: 74.6513, stage2_loss_bbox: 0.1957, stage2_loss_iou: 0.3879, stage2_loss_mask: 0.4990, stage3_loss_cls: 0.3977, stage3_pos_acc: 80.8639, stage3_loss_bbox: 0.1836, stage3_loss_iou: 0.3615, stage3_loss_mask: 0.5036, stage4_loss_cls: 0.3250, stage4_pos_acc: 85.5108, stage4_loss_bbox: 0.1734, stage4_loss_iou: 0.3475, stage4_loss_mask: 0.4996, stage5_loss_cls: 0.3009, stage5_pos_acc: 88.3360, stage5_loss_bbox: 0.1679, stage5_loss_iou: 0.3424, stage5_loss_mask: 0.4817, loss: 10.8670\n",
      "2025-07-16 16:31:33,560 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:31:33,560 - mmdet - INFO - Epoch [30][750/750]\tlr: 2.500e-05, eta: 2:42:31, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0357, stage0_pos_acc: 34.9573, stage0_loss_bbox: 0.5258, stage0_loss_iou: 0.9832, stage0_loss_mask: 0.7037, stage1_loss_cls: 0.6193, stage1_pos_acc: 63.3971, stage1_loss_bbox: 0.2319, stage1_loss_iou: 0.4529, stage1_loss_mask: 0.4592, stage2_loss_cls: 0.5166, stage2_pos_acc: 71.9979, stage2_loss_bbox: 0.1872, stage2_loss_iou: 0.3492, stage2_loss_mask: 0.4277, stage3_loss_cls: 0.3934, stage3_pos_acc: 78.6703, stage3_loss_bbox: 0.1788, stage3_loss_iou: 0.3268, stage3_loss_mask: 0.4018, stage4_loss_cls: 0.3324, stage4_pos_acc: 84.3417, stage4_loss_bbox: 0.1655, stage4_loss_iou: 0.3047, stage4_loss_mask: 0.4075, stage5_loss_cls: 0.2803, stage5_pos_acc: 87.6958, stage5_loss_bbox: 0.1640, stage5_loss_iou: 0.3038, stage5_loss_mask: 0.4006, loss: 10.1520\n",
      "2025-07-16 16:31:33,667 - mmdet - INFO - Saving checkpoint at 30 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.2 task/s, elapsed: 86s, ETA:     0s2025-07-16 16:34:38,387 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.15s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.29s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.113\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.429\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.431\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.431\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.153\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.295\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.460\n",
      "2025-07-16 16:34:40,327 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.34s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.76s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.112\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.160\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.306\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.451\n",
      "2025-07-16 16:34:43,259 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:34:43,259 - mmdet - INFO - Epoch(val) [30][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0500, bbox_mAP_75: 0.0220, bbox_mAP_s: 0.1130, bbox_mAP_m: 0.0080, bbox_mAP_l: 0.0320, bbox_mAP_copypaste: 0.025 0.050 0.022 0.113 0.008 0.032, segm_mAP: 0.0250, segm_mAP_50: 0.0470, segm_mAP_75: 0.0210, segm_mAP_s: 0.1120, segm_mAP_m: 0.0080, segm_mAP_l: 0.0320, segm_mAP_copypaste: 0.025 0.047 0.021 0.112 0.008 0.032\n",
      "2025-07-16 16:35:04,220 - mmdet - INFO - Epoch [31][50/750]\tlr: 2.500e-05, eta: 2:42:22, time: 0.419, data_time: 0.055, memory: 11264, stage0_loss_cls: 1.0452, stage0_pos_acc: 36.8734, stage0_loss_bbox: 0.4982, stage0_loss_iou: 1.0097, stage0_loss_mask: 0.7061, stage1_loss_cls: 0.6090, stage1_pos_acc: 60.2639, stage1_loss_bbox: 0.2113, stage1_loss_iou: 0.4398, stage1_loss_mask: 0.4174, stage2_loss_cls: 0.4878, stage2_pos_acc: 75.2974, stage2_loss_bbox: 0.1754, stage2_loss_iou: 0.3440, stage2_loss_mask: 0.4159, stage3_loss_cls: 0.3677, stage3_pos_acc: 78.2156, stage3_loss_bbox: 0.1489, stage3_loss_iou: 0.2990, stage3_loss_mask: 0.3773, stage4_loss_cls: 0.2923, stage4_pos_acc: 82.2831, stage4_loss_bbox: 0.1384, stage4_loss_iou: 0.2860, stage4_loss_mask: 0.3812, stage5_loss_cls: 0.2729, stage5_pos_acc: 88.1156, stage5_loss_bbox: 0.1406, stage5_loss_iou: 0.2793, stage5_loss_mask: 0.3911, loss: 9.7345\n",
      "2025-07-16 16:35:22,965 - mmdet - INFO - Epoch [31][100/750]\tlr: 2.500e-05, eta: 2:42:04, time: 0.375, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0236, stage0_pos_acc: 38.6952, stage0_loss_bbox: 0.5542, stage0_loss_iou: 1.0305, stage0_loss_mask: 0.8458, stage1_loss_cls: 0.6374, stage1_pos_acc: 63.0643, stage1_loss_bbox: 0.2571, stage1_loss_iou: 0.4824, stage1_loss_mask: 0.4720, stage2_loss_cls: 0.5082, stage2_pos_acc: 72.5524, stage2_loss_bbox: 0.2034, stage2_loss_iou: 0.3695, stage2_loss_mask: 0.4295, stage3_loss_cls: 0.3941, stage3_pos_acc: 83.0095, stage3_loss_bbox: 0.1816, stage3_loss_iou: 0.3299, stage3_loss_mask: 0.4103, stage4_loss_cls: 0.3302, stage4_pos_acc: 85.2857, stage4_loss_bbox: 0.1723, stage4_loss_iou: 0.3140, stage4_loss_mask: 0.4009, stage5_loss_cls: 0.3018, stage5_pos_acc: 88.3873, stage5_loss_bbox: 0.1617, stage5_loss_iou: 0.3101, stage5_loss_mask: 0.4003, loss: 10.5207\n",
      "2025-07-16 16:35:41,806 - mmdet - INFO - Epoch [31][150/750]\tlr: 2.500e-05, eta: 2:41:46, time: 0.377, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0352, stage0_pos_acc: 38.5401, stage0_loss_bbox: 0.5523, stage0_loss_iou: 0.9901, stage0_loss_mask: 0.8710, stage1_loss_cls: 0.5905, stage1_pos_acc: 65.9446, stage1_loss_bbox: 0.2304, stage1_loss_iou: 0.4570, stage1_loss_mask: 0.4453, stage2_loss_cls: 0.4530, stage2_pos_acc: 76.3038, stage2_loss_bbox: 0.1774, stage2_loss_iou: 0.3528, stage2_loss_mask: 0.4004, stage3_loss_cls: 0.3149, stage3_pos_acc: 84.7369, stage3_loss_bbox: 0.1628, stage3_loss_iou: 0.3172, stage3_loss_mask: 0.3555, stage4_loss_cls: 0.2461, stage4_pos_acc: 85.7620, stage4_loss_bbox: 0.1521, stage4_loss_iou: 0.2991, stage4_loss_mask: 0.3474, stage5_loss_cls: 0.2179, stage5_pos_acc: 90.6311, stage5_loss_bbox: 0.1510, stage5_loss_iou: 0.2977, stage5_loss_mask: 0.3601, loss: 9.7772\n",
      "2025-07-16 16:36:00,525 - mmdet - INFO - Epoch [31][200/750]\tlr: 2.500e-05, eta: 2:41:28, time: 0.374, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0459, stage0_pos_acc: 34.2655, stage0_loss_bbox: 0.5285, stage0_loss_iou: 0.9665, stage0_loss_mask: 0.6736, stage1_loss_cls: 0.6166, stage1_pos_acc: 69.1867, stage1_loss_bbox: 0.2321, stage1_loss_iou: 0.4376, stage1_loss_mask: 0.4090, stage2_loss_cls: 0.4904, stage2_pos_acc: 71.6629, stage2_loss_bbox: 0.1676, stage2_loss_iou: 0.3307, stage2_loss_mask: 0.3582, stage3_loss_cls: 0.4013, stage3_pos_acc: 81.7232, stage3_loss_bbox: 0.1568, stage3_loss_iou: 0.2921, stage3_loss_mask: 0.2917, stage4_loss_cls: 0.3161, stage4_pos_acc: 85.8589, stage4_loss_bbox: 0.1474, stage4_loss_iou: 0.2852, stage4_loss_mask: 0.3021, stage5_loss_cls: 0.2928, stage5_pos_acc: 89.4827, stage5_loss_bbox: 0.1400, stage5_loss_iou: 0.2798, stage5_loss_mask: 0.3041, loss: 9.4657\n",
      "2025-07-16 16:36:19,463 - mmdet - INFO - Epoch [31][250/750]\tlr: 2.500e-05, eta: 2:41:11, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0032, stage0_pos_acc: 40.3895, stage0_loss_bbox: 0.4936, stage0_loss_iou: 0.9362, stage0_loss_mask: 0.8158, stage1_loss_cls: 0.5879, stage1_pos_acc: 65.3135, stage1_loss_bbox: 0.2218, stage1_loss_iou: 0.4231, stage1_loss_mask: 0.5128, stage2_loss_cls: 0.4656, stage2_pos_acc: 73.9947, stage2_loss_bbox: 0.1785, stage2_loss_iou: 0.3429, stage2_loss_mask: 0.4477, stage3_loss_cls: 0.3763, stage3_pos_acc: 81.7042, stage3_loss_bbox: 0.1603, stage3_loss_iou: 0.3159, stage3_loss_mask: 0.4162, stage4_loss_cls: 0.2862, stage4_pos_acc: 88.9911, stage4_loss_bbox: 0.1506, stage4_loss_iou: 0.3045, stage4_loss_mask: 0.4205, stage5_loss_cls: 0.2511, stage5_pos_acc: 89.9154, stage5_loss_bbox: 0.1465, stage5_loss_iou: 0.2968, stage5_loss_mask: 0.4165, loss: 9.9706\n",
      "2025-07-16 16:36:38,120 - mmdet - INFO - Epoch [31][300/750]\tlr: 2.500e-05, eta: 2:40:52, time: 0.373, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0488, stage0_pos_acc: 35.6286, stage0_loss_bbox: 0.5650, stage0_loss_iou: 0.9655, stage0_loss_mask: 0.8046, stage1_loss_cls: 0.6259, stage1_pos_acc: 68.4429, stage1_loss_bbox: 0.2793, stage1_loss_iou: 0.4296, stage1_loss_mask: 0.4524, stage2_loss_cls: 0.5007, stage2_pos_acc: 74.5524, stage2_loss_bbox: 0.2127, stage2_loss_iou: 0.3148, stage2_loss_mask: 0.3757, stage3_loss_cls: 0.3737, stage3_pos_acc: 78.3524, stage3_loss_bbox: 0.2058, stage3_loss_iou: 0.3013, stage3_loss_mask: 0.3287, stage4_loss_cls: 0.3163, stage4_pos_acc: 84.3667, stage4_loss_bbox: 0.1845, stage4_loss_iou: 0.2960, stage4_loss_mask: 0.3251, stage5_loss_cls: 0.2999, stage5_pos_acc: 87.5286, stage5_loss_bbox: 0.1632, stage5_loss_iou: 0.2907, stage5_loss_mask: 0.3142, loss: 9.9746\n",
      "2025-07-16 16:36:56,518 - mmdet - INFO - Epoch [31][350/750]\tlr: 2.500e-05, eta: 2:40:33, time: 0.368, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0321, stage0_pos_acc: 37.5465, stage0_loss_bbox: 0.4987, stage0_loss_iou: 0.9228, stage0_loss_mask: 0.8145, stage1_loss_cls: 0.6457, stage1_pos_acc: 62.2367, stage1_loss_bbox: 0.2187, stage1_loss_iou: 0.4261, stage1_loss_mask: 0.4770, stage2_loss_cls: 0.5412, stage2_pos_acc: 68.9516, stage2_loss_bbox: 0.1784, stage2_loss_iou: 0.3332, stage2_loss_mask: 0.4386, stage3_loss_cls: 0.4261, stage3_pos_acc: 72.6788, stage3_loss_bbox: 0.1632, stage3_loss_iou: 0.3086, stage3_loss_mask: 0.4320, stage4_loss_cls: 0.3533, stage4_pos_acc: 83.5184, stage4_loss_bbox: 0.1584, stage4_loss_iou: 0.2968, stage4_loss_mask: 0.4443, stage5_loss_cls: 0.3127, stage5_pos_acc: 87.7534, stage5_loss_bbox: 0.1609, stage5_loss_iou: 0.2949, stage5_loss_mask: 0.4545, loss: 10.3327\n",
      "2025-07-16 16:37:14,833 - mmdet - INFO - Epoch [31][400/750]\tlr: 2.500e-05, eta: 2:40:14, time: 0.366, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0603, stage0_pos_acc: 41.8510, stage0_loss_bbox: 0.4946, stage0_loss_iou: 1.0263, stage0_loss_mask: 0.9182, stage1_loss_cls: 0.6492, stage1_pos_acc: 65.3968, stage1_loss_bbox: 0.1911, stage1_loss_iou: 0.4609, stage1_loss_mask: 0.5506, stage2_loss_cls: 0.5306, stage2_pos_acc: 74.7073, stage2_loss_bbox: 0.1551, stage2_loss_iou: 0.3663, stage2_loss_mask: 0.5039, stage3_loss_cls: 0.3865, stage3_pos_acc: 82.1742, stage3_loss_bbox: 0.1497, stage3_loss_iou: 0.3411, stage3_loss_mask: 0.4863, stage4_loss_cls: 0.3260, stage4_pos_acc: 85.0826, stage4_loss_bbox: 0.1399, stage4_loss_iou: 0.3258, stage4_loss_mask: 0.4735, stage5_loss_cls: 0.2942, stage5_pos_acc: 87.1617, stage5_loss_bbox: 0.1409, stage5_loss_iou: 0.3245, stage5_loss_mask: 0.4697, loss: 10.7654\n",
      "2025-07-16 16:37:33,272 - mmdet - INFO - Epoch [31][450/750]\tlr: 2.500e-05, eta: 2:39:54, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0463, stage0_pos_acc: 40.4914, stage0_loss_bbox: 0.4915, stage0_loss_iou: 0.9899, stage0_loss_mask: 0.6433, stage1_loss_cls: 0.6171, stage1_pos_acc: 64.4880, stage1_loss_bbox: 0.2043, stage1_loss_iou: 0.4038, stage1_loss_mask: 0.3858, stage2_loss_cls: 0.4797, stage2_pos_acc: 74.2049, stage2_loss_bbox: 0.1637, stage2_loss_iou: 0.3106, stage2_loss_mask: 0.3359, stage3_loss_cls: 0.3697, stage3_pos_acc: 77.6610, stage3_loss_bbox: 0.1563, stage3_loss_iou: 0.2931, stage3_loss_mask: 0.3361, stage4_loss_cls: 0.3002, stage4_pos_acc: 81.9117, stage4_loss_bbox: 0.1477, stage4_loss_iou: 0.2832, stage4_loss_mask: 0.3070, stage5_loss_cls: 0.2566, stage5_pos_acc: 85.2138, stage5_loss_bbox: 0.1479, stage5_loss_iou: 0.2818, stage5_loss_mask: 0.3005, loss: 9.2520\n",
      "2025-07-16 16:37:51,580 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:37:51,580 - mmdet - INFO - Epoch [31][500/750]\tlr: 2.500e-05, eta: 2:39:35, time: 0.366, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0936, stage0_pos_acc: 36.8997, stage0_loss_bbox: 0.4988, stage0_loss_iou: 0.9736, stage0_loss_mask: 0.6532, stage1_loss_cls: 0.6294, stage1_pos_acc: 63.4661, stage1_loss_bbox: 0.2029, stage1_loss_iou: 0.3975, stage1_loss_mask: 0.3873, stage2_loss_cls: 0.5064, stage2_pos_acc: 73.6121, stage2_loss_bbox: 0.1580, stage2_loss_iou: 0.2947, stage2_loss_mask: 0.3410, stage3_loss_cls: 0.3723, stage3_pos_acc: 80.6431, stage3_loss_bbox: 0.1468, stage3_loss_iou: 0.2777, stage3_loss_mask: 0.3427, stage4_loss_cls: 0.3046, stage4_pos_acc: 82.6812, stage4_loss_bbox: 0.1462, stage4_loss_iou: 0.2721, stage4_loss_mask: 0.3310, stage5_loss_cls: 0.2796, stage5_pos_acc: 84.2534, stage5_loss_bbox: 0.1469, stage5_loss_iou: 0.2730, stage5_loss_mask: 0.3235, loss: 9.3528\n",
      "2025-07-16 16:38:09,889 - mmdet - INFO - Epoch [31][550/750]\tlr: 2.500e-05, eta: 2:39:15, time: 0.366, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0335, stage0_pos_acc: 31.0576, stage0_loss_bbox: 0.6167, stage0_loss_iou: 1.1538, stage0_loss_mask: 1.0961, stage1_loss_cls: 0.6153, stage1_pos_acc: 65.3593, stage1_loss_bbox: 0.2595, stage1_loss_iou: 0.5299, stage1_loss_mask: 0.5712, stage2_loss_cls: 0.4827, stage2_pos_acc: 75.6171, stage2_loss_bbox: 0.2007, stage2_loss_iou: 0.3817, stage2_loss_mask: 0.5009, stage3_loss_cls: 0.3811, stage3_pos_acc: 78.6057, stage3_loss_bbox: 0.1751, stage3_loss_iou: 0.3437, stage3_loss_mask: 0.4600, stage4_loss_cls: 0.3204, stage4_pos_acc: 84.9983, stage4_loss_bbox: 0.1595, stage4_loss_iou: 0.3267, stage4_loss_mask: 0.4498, stage5_loss_cls: 0.2971, stage5_pos_acc: 87.1771, stage5_loss_bbox: 0.1511, stage5_loss_iou: 0.3181, stage5_loss_mask: 0.4469, loss: 11.2715\n",
      "2025-07-16 16:38:28,352 - mmdet - INFO - Epoch [31][600/750]\tlr: 2.500e-05, eta: 2:38:56, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9871, stage0_pos_acc: 39.8872, stage0_loss_bbox: 0.5144, stage0_loss_iou: 0.9960, stage0_loss_mask: 0.9884, stage1_loss_cls: 0.5899, stage1_pos_acc: 68.4928, stage1_loss_bbox: 0.2293, stage1_loss_iou: 0.4810, stage1_loss_mask: 0.5883, stage2_loss_cls: 0.4875, stage2_pos_acc: 73.2413, stage2_loss_bbox: 0.1785, stage2_loss_iou: 0.3754, stage2_loss_mask: 0.5344, stage3_loss_cls: 0.3617, stage3_pos_acc: 83.6002, stage3_loss_bbox: 0.1661, stage3_loss_iou: 0.3518, stage3_loss_mask: 0.5453, stage4_loss_cls: 0.2876, stage4_pos_acc: 87.8650, stage4_loss_bbox: 0.1595, stage4_loss_iou: 0.3392, stage4_loss_mask: 0.5247, stage5_loss_cls: 0.2669, stage5_pos_acc: 90.7836, stage5_loss_bbox: 0.1574, stage5_loss_iou: 0.3353, stage5_loss_mask: 0.5337, loss: 10.9794\n",
      "2025-07-16 16:38:47,144 - mmdet - INFO - Epoch [31][650/750]\tlr: 2.500e-05, eta: 2:38:38, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0045, stage0_pos_acc: 42.5661, stage0_loss_bbox: 0.5058, stage0_loss_iou: 1.0026, stage0_loss_mask: 0.7151, stage1_loss_cls: 0.6234, stage1_pos_acc: 65.5105, stage1_loss_bbox: 0.2336, stage1_loss_iou: 0.4303, stage1_loss_mask: 0.4035, stage2_loss_cls: 0.4824, stage2_pos_acc: 77.7041, stage2_loss_bbox: 0.1821, stage2_loss_iou: 0.3376, stage2_loss_mask: 0.3548, stage3_loss_cls: 0.3828, stage3_pos_acc: 80.8930, stage3_loss_bbox: 0.1683, stage3_loss_iou: 0.3101, stage3_loss_mask: 0.3091, stage4_loss_cls: 0.3106, stage4_pos_acc: 89.7478, stage4_loss_bbox: 0.1673, stage4_loss_iou: 0.2954, stage4_loss_mask: 0.2931, stage5_loss_cls: 0.2806, stage5_pos_acc: 91.4360, stage5_loss_bbox: 0.1607, stage5_loss_iou: 0.2908, stage5_loss_mask: 0.2959, loss: 9.5405\n",
      "2025-07-16 16:39:05,578 - mmdet - INFO - Epoch [31][700/750]\tlr: 2.500e-05, eta: 2:38:19, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0674, stage0_pos_acc: 36.4906, stage0_loss_bbox: 0.4945, stage0_loss_iou: 1.0691, stage0_loss_mask: 0.8473, stage1_loss_cls: 0.6607, stage1_pos_acc: 62.8730, stage1_loss_bbox: 0.2213, stage1_loss_iou: 0.4585, stage1_loss_mask: 0.4617, stage2_loss_cls: 0.4944, stage2_pos_acc: 76.4352, stage2_loss_bbox: 0.1689, stage2_loss_iou: 0.3397, stage2_loss_mask: 0.4427, stage3_loss_cls: 0.3748, stage3_pos_acc: 81.8439, stage3_loss_bbox: 0.1548, stage3_loss_iou: 0.3110, stage3_loss_mask: 0.4180, stage4_loss_cls: 0.3183, stage4_pos_acc: 87.4537, stage4_loss_bbox: 0.1494, stage4_loss_iou: 0.3012, stage4_loss_mask: 0.4149, stage5_loss_cls: 0.2666, stage5_pos_acc: 89.9172, stage5_loss_bbox: 0.1447, stage5_loss_iou: 0.2989, stage5_loss_mask: 0.4205, loss: 10.2992\n",
      "2025-07-16 16:39:24,160 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:39:24,160 - mmdet - INFO - Epoch [31][750/750]\tlr: 2.500e-05, eta: 2:38:00, time: 0.372, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0500, stage0_pos_acc: 36.5516, stage0_loss_bbox: 0.5113, stage0_loss_iou: 1.0004, stage0_loss_mask: 0.7322, stage1_loss_cls: 0.6386, stage1_pos_acc: 68.3405, stage1_loss_bbox: 0.2135, stage1_loss_iou: 0.4090, stage1_loss_mask: 0.3453, stage2_loss_cls: 0.5122, stage2_pos_acc: 72.1159, stage2_loss_bbox: 0.1624, stage2_loss_iou: 0.2947, stage2_loss_mask: 0.3210, stage3_loss_cls: 0.3754, stage3_pos_acc: 80.9397, stage3_loss_bbox: 0.1419, stage3_loss_iou: 0.2846, stage3_loss_mask: 0.3418, stage4_loss_cls: 0.2910, stage4_pos_acc: 84.9310, stage4_loss_bbox: 0.1392, stage4_loss_iou: 0.2647, stage4_loss_mask: 0.3117, stage5_loss_cls: 0.2602, stage5_pos_acc: 89.8698, stage5_loss_bbox: 0.1347, stage5_loss_iou: 0.2588, stage5_loss_mask: 0.3060, loss: 9.3006\n",
      "2025-07-16 16:39:24,264 - mmdet - INFO - Saving checkpoint at 31 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 92s, ETA:     0s2025-07-16 16:42:33,544 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.059\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.435\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.435\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.267\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.469\n",
      "2025-07-16 16:42:35,566 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.47s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.433\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.434\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.270\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.467\n",
      "2025-07-16 16:42:38,812 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:42:38,812 - mmdet - INFO - Epoch(val) [31][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0470, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.0590, bbox_mAP_m: 0.0090, bbox_mAP_l: 0.0340, bbox_mAP_copypaste: 0.025 0.047 0.023 0.059 0.009 0.034, segm_mAP: 0.0250, segm_mAP_50: 0.0470, segm_mAP_75: 0.0220, segm_mAP_s: 0.0540, segm_mAP_m: 0.0080, segm_mAP_l: 0.0340, segm_mAP_copypaste: 0.025 0.047 0.022 0.054 0.008 0.034\n",
      "2025-07-16 16:43:00,219 - mmdet - INFO - Epoch [32][50/750]\tlr: 2.500e-05, eta: 2:37:51, time: 0.428, data_time: 0.055, memory: 11264, stage0_loss_cls: 1.0589, stage0_pos_acc: 37.1018, stage0_loss_bbox: 0.4894, stage0_loss_iou: 0.9034, stage0_loss_mask: 0.6540, stage1_loss_cls: 0.6073, stage1_pos_acc: 66.9956, stage1_loss_bbox: 0.2356, stage1_loss_iou: 0.4106, stage1_loss_mask: 0.4082, stage2_loss_cls: 0.4662, stage2_pos_acc: 75.6914, stage2_loss_bbox: 0.1918, stage2_loss_iou: 0.3174, stage2_loss_mask: 0.4074, stage3_loss_cls: 0.3738, stage3_pos_acc: 80.8747, stage3_loss_bbox: 0.1804, stage3_loss_iou: 0.3007, stage3_loss_mask: 0.3599, stage4_loss_cls: 0.2969, stage4_pos_acc: 87.8432, stage4_loss_bbox: 0.1713, stage4_loss_iou: 0.2834, stage4_loss_mask: 0.3450, stage5_loss_cls: 0.2635, stage5_pos_acc: 89.5518, stage5_loss_bbox: 0.1663, stage5_loss_iou: 0.2760, stage5_loss_mask: 0.3414, loss: 9.5089\n",
      "2025-07-16 16:43:19,543 - mmdet - INFO - Epoch [32][100/750]\tlr: 2.500e-05, eta: 2:37:35, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0327, stage0_pos_acc: 35.7645, stage0_loss_bbox: 0.4885, stage0_loss_iou: 1.0236, stage0_loss_mask: 0.9902, stage1_loss_cls: 0.6281, stage1_pos_acc: 64.4629, stage1_loss_bbox: 0.2207, stage1_loss_iou: 0.4810, stage1_loss_mask: 0.5143, stage2_loss_cls: 0.4772, stage2_pos_acc: 79.4442, stage2_loss_bbox: 0.1735, stage2_loss_iou: 0.3659, stage2_loss_mask: 0.4854, stage3_loss_cls: 0.3892, stage3_pos_acc: 83.1031, stage3_loss_bbox: 0.1620, stage3_loss_iou: 0.3309, stage3_loss_mask: 0.4603, stage4_loss_cls: 0.3009, stage4_pos_acc: 87.3706, stage4_loss_bbox: 0.1634, stage4_loss_iou: 0.3307, stage4_loss_mask: 0.4683, stage5_loss_cls: 0.2729, stage5_pos_acc: 88.4723, stage5_loss_bbox: 0.1598, stage5_loss_iou: 0.3263, stage5_loss_mask: 0.4589, loss: 10.7044\n",
      "2025-07-16 16:43:38,803 - mmdet - INFO - Epoch [32][150/750]\tlr: 2.500e-05, eta: 2:37:19, time: 0.385, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0404, stage0_pos_acc: 36.9042, stage0_loss_bbox: 0.4609, stage0_loss_iou: 0.9671, stage0_loss_mask: 0.6222, stage1_loss_cls: 0.5876, stage1_pos_acc: 65.0184, stage1_loss_bbox: 0.1991, stage1_loss_iou: 0.4052, stage1_loss_mask: 0.2907, stage2_loss_cls: 0.4620, stage2_pos_acc: 79.2259, stage2_loss_bbox: 0.1666, stage2_loss_iou: 0.3015, stage2_loss_mask: 0.2527, stage3_loss_cls: 0.3628, stage3_pos_acc: 82.6736, stage3_loss_bbox: 0.1453, stage3_loss_iou: 0.2684, stage3_loss_mask: 0.2105, stage4_loss_cls: 0.2826, stage4_pos_acc: 85.6955, stage4_loss_bbox: 0.1404, stage4_loss_iou: 0.2605, stage4_loss_mask: 0.2183, stage5_loss_cls: 0.2404, stage5_pos_acc: 91.1579, stage5_loss_bbox: 0.1360, stage5_loss_iou: 0.2546, stage5_loss_mask: 0.2264, loss: 8.5021\n",
      "2025-07-16 16:43:57,587 - mmdet - INFO - Epoch [32][200/750]\tlr: 2.500e-05, eta: 2:37:01, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0534, stage0_pos_acc: 41.3207, stage0_loss_bbox: 0.4875, stage0_loss_iou: 1.0105, stage0_loss_mask: 0.8203, stage1_loss_cls: 0.6008, stage1_pos_acc: 69.2764, stage1_loss_bbox: 0.2181, stage1_loss_iou: 0.4452, stage1_loss_mask: 0.4939, stage2_loss_cls: 0.4708, stage2_pos_acc: 75.8456, stage2_loss_bbox: 0.1691, stage2_loss_iou: 0.3347, stage2_loss_mask: 0.4040, stage3_loss_cls: 0.3609, stage3_pos_acc: 80.6325, stage3_loss_bbox: 0.1657, stage3_loss_iou: 0.3122, stage3_loss_mask: 0.3889, stage4_loss_cls: 0.2618, stage4_pos_acc: 89.1798, stage4_loss_bbox: 0.1497, stage4_loss_iou: 0.3001, stage4_loss_mask: 0.3883, stage5_loss_cls: 0.2367, stage5_pos_acc: 92.6401, stage5_loss_bbox: 0.1520, stage5_loss_iou: 0.2978, stage5_loss_mask: 0.3785, loss: 9.9009\n",
      "2025-07-16 16:44:16,248 - mmdet - INFO - Epoch [32][250/750]\tlr: 2.500e-05, eta: 2:36:42, time: 0.373, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0201, stage0_pos_acc: 39.9072, stage0_loss_bbox: 0.4694, stage0_loss_iou: 0.9375, stage0_loss_mask: 0.7158, stage1_loss_cls: 0.6061, stage1_pos_acc: 68.9554, stage1_loss_bbox: 0.2054, stage1_loss_iou: 0.4309, stage1_loss_mask: 0.5000, stage2_loss_cls: 0.4868, stage2_pos_acc: 76.7443, stage2_loss_bbox: 0.1685, stage2_loss_iou: 0.3464, stage2_loss_mask: 0.4201, stage3_loss_cls: 0.3747, stage3_pos_acc: 81.4747, stage3_loss_bbox: 0.1558, stage3_loss_iou: 0.3131, stage3_loss_mask: 0.3971, stage4_loss_cls: 0.2911, stage4_pos_acc: 86.8748, stage4_loss_bbox: 0.1488, stage4_loss_iou: 0.3027, stage4_loss_mask: 0.3976, stage5_loss_cls: 0.2546, stage5_pos_acc: 89.9045, stage5_loss_bbox: 0.1508, stage5_loss_iou: 0.3047, stage5_loss_mask: 0.4092, loss: 9.8073\n",
      "2025-07-16 16:44:34,452 - mmdet - INFO - Epoch [32][300/750]\tlr: 2.500e-05, eta: 2:36:22, time: 0.364, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0411, stage0_pos_acc: 37.3197, stage0_loss_bbox: 0.4630, stage0_loss_iou: 0.9601, stage0_loss_mask: 0.7234, stage1_loss_cls: 0.5958, stage1_pos_acc: 72.5926, stage1_loss_bbox: 0.2103, stage1_loss_iou: 0.4408, stage1_loss_mask: 0.4354, stage2_loss_cls: 0.4499, stage2_pos_acc: 76.4998, stage2_loss_bbox: 0.1585, stage2_loss_iou: 0.3066, stage2_loss_mask: 0.3598, stage3_loss_cls: 0.3458, stage3_pos_acc: 83.0165, stage3_loss_bbox: 0.1388, stage3_loss_iou: 0.2780, stage3_loss_mask: 0.3467, stage4_loss_cls: 0.2673, stage4_pos_acc: 88.1817, stage4_loss_bbox: 0.1323, stage4_loss_iou: 0.2613, stage4_loss_mask: 0.3430, stage5_loss_cls: 0.2332, stage5_pos_acc: 91.9346, stage5_loss_bbox: 0.1295, stage5_loss_iou: 0.2567, stage5_loss_mask: 0.3272, loss: 9.2044\n",
      "2025-07-16 16:44:53,132 - mmdet - INFO - Epoch [32][350/750]\tlr: 2.500e-05, eta: 2:36:04, time: 0.374, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0457, stage0_pos_acc: 34.3423, stage0_loss_bbox: 0.5618, stage0_loss_iou: 1.0088, stage0_loss_mask: 0.9222, stage1_loss_cls: 0.6505, stage1_pos_acc: 59.4241, stage1_loss_bbox: 0.2848, stage1_loss_iou: 0.4933, stage1_loss_mask: 0.6112, stage2_loss_cls: 0.5196, stage2_pos_acc: 69.7423, stage2_loss_bbox: 0.2273, stage2_loss_iou: 0.3848, stage2_loss_mask: 0.5239, stage3_loss_cls: 0.4237, stage3_pos_acc: 80.1658, stage3_loss_bbox: 0.2214, stage3_loss_iou: 0.3657, stage3_loss_mask: 0.5303, stage4_loss_cls: 0.3479, stage4_pos_acc: 84.0104, stage4_loss_bbox: 0.2144, stage4_loss_iou: 0.3469, stage4_loss_mask: 0.5234, stage5_loss_cls: 0.3277, stage5_pos_acc: 87.1489, stage5_loss_bbox: 0.1960, stage5_loss_iou: 0.3478, stage5_loss_mask: 0.5087, loss: 11.5878\n",
      "2025-07-16 16:45:11,288 - mmdet - INFO - Epoch [32][400/750]\tlr: 2.500e-05, eta: 2:35:44, time: 0.363, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0678, stage0_pos_acc: 38.3238, stage0_loss_bbox: 0.4530, stage0_loss_iou: 0.8894, stage0_loss_mask: 0.5252, stage1_loss_cls: 0.5956, stage1_pos_acc: 67.1929, stage1_loss_bbox: 0.1967, stage1_loss_iou: 0.3703, stage1_loss_mask: 0.2626, stage2_loss_cls: 0.4482, stage2_pos_acc: 75.0008, stage2_loss_bbox: 0.1501, stage2_loss_iou: 0.2703, stage2_loss_mask: 0.2224, stage3_loss_cls: 0.3149, stage3_pos_acc: 84.8587, stage3_loss_bbox: 0.1406, stage3_loss_iou: 0.2519, stage3_loss_mask: 0.2359, stage4_loss_cls: 0.2394, stage4_pos_acc: 88.3143, stage4_loss_bbox: 0.1421, stage4_loss_iou: 0.2456, stage4_loss_mask: 0.2208, stage5_loss_cls: 0.2017, stage5_pos_acc: 91.4167, stage5_loss_bbox: 0.1440, stage5_loss_iou: 0.2470, stage5_loss_mask: 0.2166, loss: 8.0520\n",
      "2025-07-16 16:45:29,394 - mmdet - INFO - Epoch [32][450/750]\tlr: 2.500e-05, eta: 2:35:23, time: 0.362, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0170, stage0_pos_acc: 32.0029, stage0_loss_bbox: 0.5034, stage0_loss_iou: 1.0261, stage0_loss_mask: 0.9118, stage1_loss_cls: 0.6286, stage1_pos_acc: 61.8594, stage1_loss_bbox: 0.2422, stage1_loss_iou: 0.4844, stage1_loss_mask: 0.5298, stage2_loss_cls: 0.4911, stage2_pos_acc: 70.4412, stage2_loss_bbox: 0.1788, stage2_loss_iou: 0.3600, stage2_loss_mask: 0.4567, stage3_loss_cls: 0.3647, stage3_pos_acc: 79.8027, stage3_loss_bbox: 0.1620, stage3_loss_iou: 0.3241, stage3_loss_mask: 0.4690, stage4_loss_cls: 0.2972, stage4_pos_acc: 84.2219, stage4_loss_bbox: 0.1557, stage4_loss_iou: 0.3096, stage4_loss_mask: 0.4423, stage5_loss_cls: 0.2582, stage5_pos_acc: 88.7552, stage5_loss_bbox: 0.1652, stage5_loss_iou: 0.3049, stage5_loss_mask: 0.4456, loss: 10.5282\n",
      "2025-07-16 16:45:47,570 - mmdet - INFO - Epoch [32][500/750]\tlr: 2.500e-05, eta: 2:35:03, time: 0.364, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0359, stage0_pos_acc: 34.8131, stage0_loss_bbox: 0.4773, stage0_loss_iou: 0.9433, stage0_loss_mask: 0.8194, stage1_loss_cls: 0.6548, stage1_pos_acc: 65.1390, stage1_loss_bbox: 0.2246, stage1_loss_iou: 0.4270, stage1_loss_mask: 0.4632, stage2_loss_cls: 0.5097, stage2_pos_acc: 72.4172, stage2_loss_bbox: 0.1736, stage2_loss_iou: 0.3498, stage2_loss_mask: 0.4292, stage3_loss_cls: 0.3984, stage3_pos_acc: 78.9427, stage3_loss_bbox: 0.1659, stage3_loss_iou: 0.3256, stage3_loss_mask: 0.3985, stage4_loss_cls: 0.3160, stage4_pos_acc: 85.2944, stage4_loss_bbox: 0.1601, stage4_loss_iou: 0.3126, stage4_loss_mask: 0.4117, stage5_loss_cls: 0.2698, stage5_pos_acc: 90.3187, stage5_loss_bbox: 0.1618, stage5_loss_iou: 0.3090, stage5_loss_mask: 0.4068, loss: 10.1440\n",
      "2025-07-16 16:46:05,625 - mmdet - INFO - Epoch [32][550/750]\tlr: 2.500e-05, eta: 2:34:43, time: 0.361, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0856, stage0_pos_acc: 30.2254, stage0_loss_bbox: 0.5374, stage0_loss_iou: 1.0645, stage0_loss_mask: 0.8591, stage1_loss_cls: 0.6113, stage1_pos_acc: 66.2302, stage1_loss_bbox: 0.2062, stage1_loss_iou: 0.4526, stage1_loss_mask: 0.3962, stage2_loss_cls: 0.4681, stage2_pos_acc: 74.5246, stage2_loss_bbox: 0.1607, stage2_loss_iou: 0.3274, stage2_loss_mask: 0.3643, stage3_loss_cls: 0.3478, stage3_pos_acc: 83.1635, stage3_loss_bbox: 0.1546, stage3_loss_iou: 0.2992, stage3_loss_mask: 0.3182, stage4_loss_cls: 0.2866, stage4_pos_acc: 86.1437, stage4_loss_bbox: 0.1437, stage4_loss_iou: 0.2923, stage4_loss_mask: 0.3343, stage5_loss_cls: 0.2371, stage5_pos_acc: 90.5603, stage5_loss_bbox: 0.1414, stage5_loss_iou: 0.2880, stage5_loss_mask: 0.3231, loss: 9.6998\n",
      "2025-07-16 16:46:23,877 - mmdet - INFO - Epoch [32][600/750]\tlr: 2.500e-05, eta: 2:34:23, time: 0.365, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0499, stage0_pos_acc: 39.1960, stage0_loss_bbox: 0.4953, stage0_loss_iou: 0.9687, stage0_loss_mask: 0.7795, stage1_loss_cls: 0.5864, stage1_pos_acc: 71.1183, stage1_loss_bbox: 0.2242, stage1_loss_iou: 0.4533, stage1_loss_mask: 0.4991, stage2_loss_cls: 0.4709, stage2_pos_acc: 78.6913, stage2_loss_bbox: 0.1681, stage2_loss_iou: 0.3306, stage2_loss_mask: 0.4416, stage3_loss_cls: 0.3666, stage3_pos_acc: 83.2571, stage3_loss_bbox: 0.1626, stage3_loss_iou: 0.3061, stage3_loss_mask: 0.4462, stage4_loss_cls: 0.3032, stage4_pos_acc: 86.3786, stage4_loss_bbox: 0.1584, stage4_loss_iou: 0.2944, stage4_loss_mask: 0.4356, stage5_loss_cls: 0.2697, stage5_pos_acc: 89.8405, stage5_loss_bbox: 0.1523, stage5_loss_iou: 0.2915, stage5_loss_mask: 0.4404, loss: 10.0946\n",
      "2025-07-16 16:46:42,678 - mmdet - INFO - Epoch [32][650/750]\tlr: 2.500e-05, eta: 2:34:05, time: 0.376, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0387, stage0_pos_acc: 39.2217, stage0_loss_bbox: 0.4759, stage0_loss_iou: 1.0031, stage0_loss_mask: 0.7090, stage1_loss_cls: 0.5815, stage1_pos_acc: 68.5979, stage1_loss_bbox: 0.2119, stage1_loss_iou: 0.4312, stage1_loss_mask: 0.4395, stage2_loss_cls: 0.4580, stage2_pos_acc: 76.5209, stage2_loss_bbox: 0.1650, stage2_loss_iou: 0.3315, stage2_loss_mask: 0.3970, stage3_loss_cls: 0.3508, stage3_pos_acc: 81.2187, stage3_loss_bbox: 0.1393, stage3_loss_iou: 0.2973, stage3_loss_mask: 0.3867, stage4_loss_cls: 0.2598, stage4_pos_acc: 87.6708, stage4_loss_bbox: 0.1401, stage4_loss_iou: 0.2949, stage4_loss_mask: 0.3844, stage5_loss_cls: 0.2340, stage5_pos_acc: 90.4420, stage5_loss_bbox: 0.1366, stage5_loss_iou: 0.2881, stage5_loss_mask: 0.3879, loss: 9.5421\n",
      "2025-07-16 16:47:01,843 - mmdet - INFO - Epoch [32][700/750]\tlr: 2.500e-05, eta: 2:33:49, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0055, stage0_pos_acc: 35.0430, stage0_loss_bbox: 0.5380, stage0_loss_iou: 0.9925, stage0_loss_mask: 0.9211, stage1_loss_cls: 0.6155, stage1_pos_acc: 64.0246, stage1_loss_bbox: 0.2698, stage1_loss_iou: 0.4883, stage1_loss_mask: 0.6151, stage2_loss_cls: 0.5117, stage2_pos_acc: 72.0740, stage2_loss_bbox: 0.2279, stage2_loss_iou: 0.4064, stage2_loss_mask: 0.5660, stage3_loss_cls: 0.4356, stage3_pos_acc: 75.8659, stage3_loss_bbox: 0.2025, stage3_loss_iou: 0.3691, stage3_loss_mask: 0.5265, stage4_loss_cls: 0.3755, stage4_pos_acc: 79.1333, stage4_loss_bbox: 0.1882, stage4_loss_iou: 0.3609, stage4_loss_mask: 0.5065, stage5_loss_cls: 0.3432, stage5_pos_acc: 84.9190, stage5_loss_bbox: 0.1870, stage5_loss_iou: 0.3575, stage5_loss_mask: 0.4864, loss: 11.4965\n",
      "2025-07-16 16:47:20,307 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:47:20,307 - mmdet - INFO - Epoch [32][750/750]\tlr: 2.500e-05, eta: 2:33:29, time: 0.369, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0716, stage0_pos_acc: 37.2278, stage0_loss_bbox: 0.4946, stage0_loss_iou: 1.0454, stage0_loss_mask: 0.8182, stage1_loss_cls: 0.6407, stage1_pos_acc: 65.6675, stage1_loss_bbox: 0.2126, stage1_loss_iou: 0.4511, stage1_loss_mask: 0.3637, stage2_loss_cls: 0.5233, stage2_pos_acc: 72.8786, stage2_loss_bbox: 0.1688, stage2_loss_iou: 0.3439, stage2_loss_mask: 0.3374, stage3_loss_cls: 0.3723, stage3_pos_acc: 81.8579, stage3_loss_bbox: 0.1565, stage3_loss_iou: 0.3098, stage3_loss_mask: 0.3278, stage4_loss_cls: 0.3004, stage4_pos_acc: 84.2238, stage4_loss_bbox: 0.1482, stage4_loss_iou: 0.2943, stage4_loss_mask: 0.3310, stage5_loss_cls: 0.2678, stage5_pos_acc: 87.4714, stage5_loss_bbox: 0.1476, stage5_loss_iou: 0.2916, stage5_loss_mask: 0.3361, loss: 9.7549\n",
      "2025-07-16 16:47:20,438 - mmdet - INFO - Saving checkpoint at 32 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 94s, ETA:     0s2025-07-16 16:50:33,518 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.54s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.030\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.396\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.399\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.399\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.239\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.433\n",
      "2025-07-16 16:50:35,800 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.80s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.004\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.448\n",
      "2025-07-16 16:50:38,750 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:50:38,750 - mmdet - INFO - Epoch(val) [32][750]\tbbox_mAP: 0.0220, bbox_mAP_50: 0.0430, bbox_mAP_75: 0.0210, bbox_mAP_s: 0.0070, bbox_mAP_m: 0.0050, bbox_mAP_l: 0.0300, bbox_mAP_copypaste: 0.022 0.043 0.021 0.007 0.005 0.030, segm_mAP: 0.0220, segm_mAP_50: 0.0430, segm_mAP_75: 0.0210, segm_mAP_s: 0.0040, segm_mAP_m: 0.0050, segm_mAP_l: 0.0310, segm_mAP_copypaste: 0.022 0.043 0.021 0.004 0.005 0.031\n",
      "2025-07-16 16:51:00,002 - mmdet - INFO - Epoch [33][50/750]\tlr: 2.500e-05, eta: 2:33:19, time: 0.425, data_time: 0.055, memory: 11264, stage0_loss_cls: 1.0170, stage0_pos_acc: 39.4491, stage0_loss_bbox: 0.5333, stage0_loss_iou: 1.0027, stage0_loss_mask: 0.9228, stage1_loss_cls: 0.6038, stage1_pos_acc: 66.4730, stage1_loss_bbox: 0.2411, stage1_loss_iou: 0.4413, stage1_loss_mask: 0.3592, stage2_loss_cls: 0.4719, stage2_pos_acc: 77.1533, stage2_loss_bbox: 0.1751, stage2_loss_iou: 0.3221, stage2_loss_mask: 0.3159, stage3_loss_cls: 0.3553, stage3_pos_acc: 82.5993, stage3_loss_bbox: 0.1608, stage3_loss_iou: 0.2835, stage3_loss_mask: 0.3058, stage4_loss_cls: 0.2682, stage4_pos_acc: 88.5598, stage4_loss_bbox: 0.1486, stage4_loss_iou: 0.2708, stage4_loss_mask: 0.2911, stage5_loss_cls: 0.2319, stage5_pos_acc: 91.5922, stage5_loss_bbox: 0.1541, stage5_loss_iou: 0.2661, stage5_loss_mask: 0.2907, loss: 9.4331\n",
      "2025-07-16 16:51:18,959 - mmdet - INFO - Epoch [33][100/750]\tlr: 2.500e-05, eta: 2:33:01, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0361, stage0_pos_acc: 38.8865, stage0_loss_bbox: 0.5125, stage0_loss_iou: 1.0032, stage0_loss_mask: 0.9630, stage1_loss_cls: 0.6035, stage1_pos_acc: 64.5992, stage1_loss_bbox: 0.2492, stage1_loss_iou: 0.4702, stage1_loss_mask: 0.6585, stage2_loss_cls: 0.4562, stage2_pos_acc: 78.2714, stage2_loss_bbox: 0.2090, stage2_loss_iou: 0.3833, stage2_loss_mask: 0.5954, stage3_loss_cls: 0.3496, stage3_pos_acc: 79.7187, stage3_loss_bbox: 0.1949, stage3_loss_iou: 0.3566, stage3_loss_mask: 0.5771, stage4_loss_cls: 0.2612, stage4_pos_acc: 86.7575, stage4_loss_bbox: 0.1810, stage4_loss_iou: 0.3476, stage4_loss_mask: 0.5780, stage5_loss_cls: 0.2235, stage5_pos_acc: 90.1881, stage5_loss_bbox: 0.1808, stage5_loss_iou: 0.3475, stage5_loss_mask: 0.5763, loss: 11.3143\n",
      "2025-07-16 16:51:37,805 - mmdet - INFO - Epoch [33][150/750]\tlr: 2.500e-05, eta: 2:32:43, time: 0.377, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0452, stage0_pos_acc: 40.2512, stage0_loss_bbox: 0.5751, stage0_loss_iou: 1.0287, stage0_loss_mask: 0.9294, stage1_loss_cls: 0.6197, stage1_pos_acc: 65.6468, stage1_loss_bbox: 0.2654, stage1_loss_iou: 0.4842, stage1_loss_mask: 0.5581, stage2_loss_cls: 0.4915, stage2_pos_acc: 78.0295, stage2_loss_bbox: 0.2275, stage2_loss_iou: 0.3814, stage2_loss_mask: 0.5088, stage3_loss_cls: 0.3652, stage3_pos_acc: 84.9227, stage3_loss_bbox: 0.2031, stage3_loss_iou: 0.3645, stage3_loss_mask: 0.4800, stage4_loss_cls: 0.2977, stage4_pos_acc: 87.9599, stage4_loss_bbox: 0.1984, stage4_loss_iou: 0.3564, stage4_loss_mask: 0.4786, stage5_loss_cls: 0.2647, stage5_pos_acc: 91.9500, stage5_loss_bbox: 0.1892, stage5_loss_iou: 0.3519, stage5_loss_mask: 0.4870, loss: 11.1515\n",
      "2025-07-16 16:51:56,729 - mmdet - INFO - Epoch [33][200/750]\tlr: 2.500e-05, eta: 2:32:25, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0259, stage0_pos_acc: 41.2210, stage0_loss_bbox: 0.4565, stage0_loss_iou: 0.9472, stage0_loss_mask: 0.6319, stage1_loss_cls: 0.5962, stage1_pos_acc: 65.4727, stage1_loss_bbox: 0.1792, stage1_loss_iou: 0.3969, stage1_loss_mask: 0.4398, stage2_loss_cls: 0.4539, stage2_pos_acc: 75.8450, stage2_loss_bbox: 0.1467, stage2_loss_iou: 0.3120, stage2_loss_mask: 0.4038, stage3_loss_cls: 0.3135, stage3_pos_acc: 82.6219, stage3_loss_bbox: 0.1348, stage3_loss_iou: 0.2903, stage3_loss_mask: 0.3954, stage4_loss_cls: 0.2590, stage4_pos_acc: 87.5123, stage4_loss_bbox: 0.1313, stage4_loss_iou: 0.2798, stage4_loss_mask: 0.3869, stage5_loss_cls: 0.2063, stage5_pos_acc: 90.9623, stage5_loss_bbox: 0.1264, stage5_loss_iou: 0.2755, stage5_loss_mask: 0.3823, loss: 9.1716\n",
      "2025-07-16 16:52:15,608 - mmdet - INFO - Epoch [33][250/750]\tlr: 2.500e-05, eta: 2:32:07, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0794, stage0_pos_acc: 35.4548, stage0_loss_bbox: 0.5063, stage0_loss_iou: 0.9653, stage0_loss_mask: 0.6349, stage1_loss_cls: 0.5837, stage1_pos_acc: 69.2548, stage1_loss_bbox: 0.2127, stage1_loss_iou: 0.4259, stage1_loss_mask: 0.3804, stage2_loss_cls: 0.4493, stage2_pos_acc: 76.2429, stage2_loss_bbox: 0.1709, stage2_loss_iou: 0.3205, stage2_loss_mask: 0.3356, stage3_loss_cls: 0.3010, stage3_pos_acc: 86.0976, stage3_loss_bbox: 0.1592, stage3_loss_iou: 0.2911, stage3_loss_mask: 0.3201, stage4_loss_cls: 0.2115, stage4_pos_acc: 91.3333, stage4_loss_bbox: 0.1531, stage4_loss_iou: 0.2829, stage4_loss_mask: 0.3108, stage5_loss_cls: 0.1847, stage5_pos_acc: 92.5833, stage5_loss_bbox: 0.1466, stage5_loss_iou: 0.2806, stage5_loss_mask: 0.3165, loss: 9.0230\n",
      "2025-07-16 16:52:34,689 - mmdet - INFO - Epoch [33][300/750]\tlr: 2.500e-05, eta: 2:31:50, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0598, stage0_pos_acc: 38.2229, stage0_loss_bbox: 0.5207, stage0_loss_iou: 1.0027, stage0_loss_mask: 0.7431, stage1_loss_cls: 0.6044, stage1_pos_acc: 66.6156, stage1_loss_bbox: 0.2130, stage1_loss_iou: 0.4176, stage1_loss_mask: 0.4491, stage2_loss_cls: 0.4848, stage2_pos_acc: 71.8346, stage2_loss_bbox: 0.1715, stage2_loss_iou: 0.3318, stage2_loss_mask: 0.4243, stage3_loss_cls: 0.3616, stage3_pos_acc: 80.2129, stage3_loss_bbox: 0.1660, stage3_loss_iou: 0.3189, stage3_loss_mask: 0.4121, stage4_loss_cls: 0.3033, stage4_pos_acc: 82.6644, stage4_loss_bbox: 0.1626, stage4_loss_iou: 0.3131, stage4_loss_mask: 0.3896, stage5_loss_cls: 0.2643, stage5_pos_acc: 90.5978, stage5_loss_bbox: 0.1642, stage5_loss_iou: 0.3111, stage5_loss_mask: 0.3981, loss: 9.9878\n",
      "2025-07-16 16:52:53,572 - mmdet - INFO - Epoch [33][350/750]\tlr: 2.500e-05, eta: 2:31:32, time: 0.378, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9876, stage0_pos_acc: 36.7395, stage0_loss_bbox: 0.4837, stage0_loss_iou: 0.8765, stage0_loss_mask: 0.6112, stage1_loss_cls: 0.6035, stage1_pos_acc: 67.7061, stage1_loss_bbox: 0.2196, stage1_loss_iou: 0.3993, stage1_loss_mask: 0.4115, stage2_loss_cls: 0.4736, stage2_pos_acc: 74.4522, stage2_loss_bbox: 0.1719, stage2_loss_iou: 0.3023, stage2_loss_mask: 0.3113, stage3_loss_cls: 0.3834, stage3_pos_acc: 78.2574, stage3_loss_bbox: 0.1661, stage3_loss_iou: 0.2678, stage3_loss_mask: 0.2680, stage4_loss_cls: 0.3214, stage4_pos_acc: 85.0859, stage4_loss_bbox: 0.1459, stage4_loss_iou: 0.2623, stage4_loss_mask: 0.2691, stage5_loss_cls: 0.2771, stage5_pos_acc: 88.5867, stage5_loss_bbox: 0.1423, stage5_loss_iou: 0.2542, stage5_loss_mask: 0.2487, loss: 8.8583\n",
      "2025-07-16 16:53:12,027 - mmdet - INFO - Epoch [33][400/750]\tlr: 2.500e-05, eta: 2:31:13, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0469, stage0_pos_acc: 35.4810, stage0_loss_bbox: 0.4800, stage0_loss_iou: 0.9554, stage0_loss_mask: 0.4781, stage1_loss_cls: 0.5838, stage1_pos_acc: 66.4444, stage1_loss_bbox: 0.2012, stage1_loss_iou: 0.3760, stage1_loss_mask: 0.3411, stage2_loss_cls: 0.4398, stage2_pos_acc: 76.0270, stage2_loss_bbox: 0.1606, stage2_loss_iou: 0.2949, stage2_loss_mask: 0.3177, stage3_loss_cls: 0.3209, stage3_pos_acc: 81.7881, stage3_loss_bbox: 0.1514, stage3_loss_iou: 0.2758, stage3_loss_mask: 0.2869, stage4_loss_cls: 0.2432, stage4_pos_acc: 88.2571, stage4_loss_bbox: 0.1375, stage4_loss_iou: 0.2649, stage4_loss_mask: 0.2818, stage5_loss_cls: 0.2125, stage5_pos_acc: 92.5381, stage5_loss_bbox: 0.1342, stage5_loss_iou: 0.2580, stage5_loss_mask: 0.2578, loss: 8.5004\n",
      "2025-07-16 16:53:30,409 - mmdet - INFO - Epoch [33][450/750]\tlr: 2.500e-05, eta: 2:30:54, time: 0.368, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0322, stage0_pos_acc: 36.4179, stage0_loss_bbox: 0.5005, stage0_loss_iou: 1.0022, stage0_loss_mask: 0.7132, stage1_loss_cls: 0.6296, stage1_pos_acc: 65.3701, stage1_loss_bbox: 0.2058, stage1_loss_iou: 0.4375, stage1_loss_mask: 0.3536, stage2_loss_cls: 0.4928, stage2_pos_acc: 73.5764, stage2_loss_bbox: 0.1535, stage2_loss_iou: 0.3134, stage2_loss_mask: 0.3140, stage3_loss_cls: 0.3626, stage3_pos_acc: 82.6275, stage3_loss_bbox: 0.1432, stage3_loss_iou: 0.2873, stage3_loss_mask: 0.2993, stage4_loss_cls: 0.2926, stage4_pos_acc: 85.7959, stage4_loss_bbox: 0.1331, stage4_loss_iou: 0.2768, stage4_loss_mask: 0.3014, stage5_loss_cls: 0.2464, stage5_pos_acc: 88.6690, stage5_loss_bbox: 0.1300, stage5_loss_iou: 0.2704, stage5_loss_mask: 0.3065, loss: 9.1979\n",
      "2025-07-16 16:53:48,773 - mmdet - INFO - Epoch [33][500/750]\tlr: 2.500e-05, eta: 2:30:34, time: 0.367, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0400, stage0_pos_acc: 38.6707, stage0_loss_bbox: 0.4755, stage0_loss_iou: 0.9613, stage0_loss_mask: 0.7565, stage1_loss_cls: 0.5942, stage1_pos_acc: 69.1802, stage1_loss_bbox: 0.2197, stage1_loss_iou: 0.4292, stage1_loss_mask: 0.3979, stage2_loss_cls: 0.4326, stage2_pos_acc: 80.1135, stage2_loss_bbox: 0.1585, stage2_loss_iou: 0.3215, stage2_loss_mask: 0.3455, stage3_loss_cls: 0.3247, stage3_pos_acc: 88.4024, stage3_loss_bbox: 0.1391, stage3_loss_iou: 0.2893, stage3_loss_mask: 0.3047, stage4_loss_cls: 0.2544, stage4_pos_acc: 87.5057, stage4_loss_bbox: 0.1369, stage4_loss_iou: 0.2858, stage4_loss_mask: 0.3071, stage5_loss_cls: 0.2366, stage5_pos_acc: 91.0326, stage5_loss_bbox: 0.1321, stage5_loss_iou: 0.2782, stage5_loss_mask: 0.3159, loss: 9.1370\n",
      "2025-07-16 16:54:07,449 - mmdet - INFO - Epoch [33][550/750]\tlr: 2.500e-05, eta: 2:30:16, time: 0.374, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0808, stage0_pos_acc: 33.1049, stage0_loss_bbox: 0.4923, stage0_loss_iou: 1.0866, stage0_loss_mask: 0.8399, stage1_loss_cls: 0.6288, stage1_pos_acc: 63.1847, stage1_loss_bbox: 0.1973, stage1_loss_iou: 0.4769, stage1_loss_mask: 0.4680, stage2_loss_cls: 0.5089, stage2_pos_acc: 75.7604, stage2_loss_bbox: 0.1482, stage2_loss_iou: 0.3577, stage2_loss_mask: 0.4702, stage3_loss_cls: 0.3672, stage3_pos_acc: 80.2357, stage3_loss_bbox: 0.1388, stage3_loss_iou: 0.3407, stage3_loss_mask: 0.4332, stage4_loss_cls: 0.3031, stage4_pos_acc: 86.2813, stage4_loss_bbox: 0.1265, stage4_loss_iou: 0.3276, stage4_loss_mask: 0.4102, stage5_loss_cls: 0.2664, stage5_pos_acc: 88.7437, stage5_loss_bbox: 0.1244, stage5_loss_iou: 0.3195, stage5_loss_mask: 0.4002, loss: 10.3135\n",
      "2025-07-16 16:54:25,960 - mmdet - INFO - Epoch [33][600/750]\tlr: 2.500e-05, eta: 2:29:57, time: 0.370, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0554, stage0_pos_acc: 34.6256, stage0_loss_bbox: 0.4572, stage0_loss_iou: 1.0106, stage0_loss_mask: 0.7832, stage1_loss_cls: 0.5776, stage1_pos_acc: 63.5634, stage1_loss_bbox: 0.1996, stage1_loss_iou: 0.4325, stage1_loss_mask: 0.3942, stage2_loss_cls: 0.4508, stage2_pos_acc: 76.1461, stage2_loss_bbox: 0.1485, stage2_loss_iou: 0.3180, stage2_loss_mask: 0.3788, stage3_loss_cls: 0.3245, stage3_pos_acc: 83.1794, stage3_loss_bbox: 0.1448, stage3_loss_iou: 0.3000, stage3_loss_mask: 0.3824, stage4_loss_cls: 0.2383, stage4_pos_acc: 90.1821, stage4_loss_bbox: 0.1363, stage4_loss_iou: 0.2869, stage4_loss_mask: 0.3853, stage5_loss_cls: 0.2053, stage5_pos_acc: 92.7096, stage5_loss_bbox: 0.1417, stage5_loss_iou: 0.2825, stage5_loss_mask: 0.3684, loss: 9.4028\n",
      "2025-07-16 16:54:44,621 - mmdet - INFO - Epoch [33][650/750]\tlr: 2.500e-05, eta: 2:29:38, time: 0.373, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0733, stage0_pos_acc: 36.3880, stage0_loss_bbox: 0.5051, stage0_loss_iou: 0.9704, stage0_loss_mask: 0.6270, stage1_loss_cls: 0.6600, stage1_pos_acc: 63.7741, stage1_loss_bbox: 0.2134, stage1_loss_iou: 0.4199, stage1_loss_mask: 0.3986, stage2_loss_cls: 0.5190, stage2_pos_acc: 74.7905, stage2_loss_bbox: 0.1605, stage2_loss_iou: 0.3127, stage2_loss_mask: 0.3261, stage3_loss_cls: 0.3941, stage3_pos_acc: 79.1018, stage3_loss_bbox: 0.1486, stage3_loss_iou: 0.2830, stage3_loss_mask: 0.3044, stage4_loss_cls: 0.3306, stage4_pos_acc: 85.8497, stage4_loss_bbox: 0.1485, stage4_loss_iou: 0.2667, stage4_loss_mask: 0.2896, stage5_loss_cls: 0.2969, stage5_pos_acc: 87.2598, stage5_loss_bbox: 0.1450, stage5_loss_iou: 0.2607, stage5_loss_mask: 0.2853, loss: 9.3396\n",
      "2025-07-16 16:55:03,176 - mmdet - INFO - Epoch [33][700/750]\tlr: 2.500e-05, eta: 2:29:20, time: 0.371, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0471, stage0_pos_acc: 39.7083, stage0_loss_bbox: 0.4676, stage0_loss_iou: 0.9839, stage0_loss_mask: 0.8933, stage1_loss_cls: 0.6459, stage1_pos_acc: 62.3921, stage1_loss_bbox: 0.2186, stage1_loss_iou: 0.4508, stage1_loss_mask: 0.4817, stage2_loss_cls: 0.5138, stage2_pos_acc: 72.1218, stage2_loss_bbox: 0.1886, stage2_loss_iou: 0.3497, stage2_loss_mask: 0.3805, stage3_loss_cls: 0.3824, stage3_pos_acc: 79.3757, stage3_loss_bbox: 0.1752, stage3_loss_iou: 0.3265, stage3_loss_mask: 0.3767, stage4_loss_cls: 0.3025, stage4_pos_acc: 83.3546, stage4_loss_bbox: 0.1637, stage4_loss_iou: 0.3136, stage4_loss_mask: 0.3562, stage5_loss_cls: 0.2766, stage5_pos_acc: 87.4880, stage5_loss_bbox: 0.1613, stage5_loss_iou: 0.3122, stage5_loss_mask: 0.3646, loss: 10.1328\n",
      "2025-07-16 16:55:21,489 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:55:21,489 - mmdet - INFO - Epoch [33][750/750]\tlr: 2.500e-05, eta: 2:29:00, time: 0.366, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0051, stage0_pos_acc: 39.2664, stage0_loss_bbox: 0.4813, stage0_loss_iou: 0.8812, stage0_loss_mask: 0.7521, stage1_loss_cls: 0.6000, stage1_pos_acc: 64.4067, stage1_loss_bbox: 0.2278, stage1_loss_iou: 0.4316, stage1_loss_mask: 0.5313, stage2_loss_cls: 0.4871, stage2_pos_acc: 77.3789, stage2_loss_bbox: 0.1841, stage2_loss_iou: 0.3500, stage2_loss_mask: 0.5045, stage3_loss_cls: 0.3737, stage3_pos_acc: 81.7847, stage3_loss_bbox: 0.1679, stage3_loss_iou: 0.3329, stage3_loss_mask: 0.4725, stage4_loss_cls: 0.3034, stage4_pos_acc: 85.3948, stage4_loss_bbox: 0.1662, stage4_loss_iou: 0.3264, stage4_loss_mask: 0.4629, stage5_loss_cls: 0.2633, stage5_pos_acc: 90.4032, stage5_loss_bbox: 0.1693, stage5_loss_iou: 0.3233, stage5_loss_mask: 0.4826, loss: 10.2804\n",
      "2025-07-16 16:55:21,603 - mmdet - INFO - Saving checkpoint at 33 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 16:58:41,714 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.249\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.449\n",
      "2025-07-16 16:58:43,758 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=2.04s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.418\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.267\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.449\n",
      "2025-07-16 16:58:46,903 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 16:58:46,904 - mmdet - INFO - Epoch(val) [33][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0480, bbox_mAP_75: 0.0240, bbox_mAP_s: 0.0180, bbox_mAP_m: 0.0060, bbox_mAP_l: 0.0330, bbox_mAP_copypaste: 0.025 0.048 0.024 0.018 0.006 0.033, segm_mAP: 0.0250, segm_mAP_50: 0.0480, segm_mAP_75: 0.0250, segm_mAP_s: 0.0150, segm_mAP_m: 0.0060, segm_mAP_l: 0.0330, segm_mAP_copypaste: 0.025 0.048 0.025 0.015 0.006 0.033\n",
      "2025-07-16 16:59:08,530 - mmdet - INFO - Epoch [34][50/750]\tlr: 2.500e-05, eta: 2:28:49, time: 0.432, data_time: 0.055, memory: 11264, stage0_loss_cls: 1.0183, stage0_pos_acc: 41.8330, stage0_loss_bbox: 0.4839, stage0_loss_iou: 0.9234, stage0_loss_mask: 0.7192, stage1_loss_cls: 0.6050, stage1_pos_acc: 70.2848, stage1_loss_bbox: 0.2219, stage1_loss_iou: 0.4354, stage1_loss_mask: 0.4493, stage2_loss_cls: 0.4737, stage2_pos_acc: 74.2602, stage2_loss_bbox: 0.1798, stage2_loss_iou: 0.3425, stage2_loss_mask: 0.3979, stage3_loss_cls: 0.3421, stage3_pos_acc: 82.9862, stage3_loss_bbox: 0.1661, stage3_loss_iou: 0.3243, stage3_loss_mask: 0.3783, stage4_loss_cls: 0.2871, stage4_pos_acc: 89.1826, stage4_loss_bbox: 0.1656, stage4_loss_iou: 0.3028, stage4_loss_mask: 0.3483, stage5_loss_cls: 0.2509, stage5_pos_acc: 92.6677, stage5_loss_bbox: 0.1641, stage5_loss_iou: 0.3011, stage5_loss_mask: 0.3537, loss: 9.6346\n",
      "2025-07-16 16:59:27,854 - mmdet - INFO - Epoch [34][100/750]\tlr: 2.500e-05, eta: 2:28:32, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0383, stage0_pos_acc: 35.5187, stage0_loss_bbox: 0.4570, stage0_loss_iou: 0.9133, stage0_loss_mask: 0.7327, stage1_loss_cls: 0.6160, stage1_pos_acc: 66.2189, stage1_loss_bbox: 0.1867, stage1_loss_iou: 0.4026, stage1_loss_mask: 0.4563, stage2_loss_cls: 0.4572, stage2_pos_acc: 78.1608, stage2_loss_bbox: 0.1572, stage2_loss_iou: 0.3190, stage2_loss_mask: 0.4527, stage3_loss_cls: 0.3420, stage3_pos_acc: 85.3117, stage3_loss_bbox: 0.1472, stage3_loss_iou: 0.3029, stage3_loss_mask: 0.3897, stage4_loss_cls: 0.2706, stage4_pos_acc: 88.3289, stage4_loss_bbox: 0.1502, stage4_loss_iou: 0.2983, stage4_loss_mask: 0.3901, stage5_loss_cls: 0.2356, stage5_pos_acc: 91.5559, stage5_loss_bbox: 0.1479, stage5_loss_iou: 0.2914, stage5_loss_mask: 0.3693, loss: 9.5242\n",
      "2025-07-16 16:59:47,160 - mmdet - INFO - Epoch [34][150/750]\tlr: 2.500e-05, eta: 2:28:16, time: 0.386, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0675, stage0_pos_acc: 34.6313, stage0_loss_bbox: 0.4808, stage0_loss_iou: 1.0324, stage0_loss_mask: 0.9298, stage1_loss_cls: 0.6092, stage1_pos_acc: 68.0635, stage1_loss_bbox: 0.1836, stage1_loss_iou: 0.4655, stage1_loss_mask: 0.4573, stage2_loss_cls: 0.4616, stage2_pos_acc: 79.0849, stage2_loss_bbox: 0.1482, stage2_loss_iou: 0.3599, stage2_loss_mask: 0.4588, stage3_loss_cls: 0.3397, stage3_pos_acc: 81.8873, stage3_loss_bbox: 0.1319, stage3_loss_iou: 0.3187, stage3_loss_mask: 0.4434, stage4_loss_cls: 0.2665, stage4_pos_acc: 89.3345, stage4_loss_bbox: 0.1232, stage4_loss_iou: 0.2976, stage4_loss_mask: 0.4217, stage5_loss_cls: 0.2191, stage5_pos_acc: 92.3452, stage5_loss_bbox: 0.1273, stage5_loss_iou: 0.2913, stage5_loss_mask: 0.4199, loss: 10.0550\n",
      "2025-07-16 17:00:06,479 - mmdet - INFO - Epoch [34][200/750]\tlr: 2.500e-05, eta: 2:27:59, time: 0.386, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0628, stage0_pos_acc: 37.6626, stage0_loss_bbox: 0.4818, stage0_loss_iou: 0.9828, stage0_loss_mask: 0.5525, stage1_loss_cls: 0.5836, stage1_pos_acc: 67.3322, stage1_loss_bbox: 0.1981, stage1_loss_iou: 0.3855, stage1_loss_mask: 0.3660, stage2_loss_cls: 0.4646, stage2_pos_acc: 74.9375, stage2_loss_bbox: 0.1544, stage2_loss_iou: 0.2773, stage2_loss_mask: 0.3256, stage3_loss_cls: 0.3414, stage3_pos_acc: 81.6395, stage3_loss_bbox: 0.1420, stage3_loss_iou: 0.2528, stage3_loss_mask: 0.3032, stage4_loss_cls: 0.2907, stage4_pos_acc: 83.9460, stage4_loss_bbox: 0.1346, stage4_loss_iou: 0.2412, stage4_loss_mask: 0.2866, stage5_loss_cls: 0.2428, stage5_pos_acc: 85.8290, stage5_loss_bbox: 0.1303, stage5_loss_iou: 0.2404, stage5_loss_mask: 0.2742, loss: 8.7152\n",
      "2025-07-16 17:00:26,160 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:00:26,161 - mmdet - INFO - Epoch [34][250/750]\tlr: 2.500e-05, eta: 2:27:43, time: 0.394, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0153, stage0_pos_acc: 41.4307, stage0_loss_bbox: 0.5274, stage0_loss_iou: 0.9388, stage0_loss_mask: 0.5729, stage1_loss_cls: 0.5598, stage1_pos_acc: 73.2101, stage1_loss_bbox: 0.2483, stage1_loss_iou: 0.3918, stage1_loss_mask: 0.3424, stage2_loss_cls: 0.4328, stage2_pos_acc: 81.0807, stage2_loss_bbox: 0.1994, stage2_loss_iou: 0.2963, stage2_loss_mask: 0.3278, stage3_loss_cls: 0.3395, stage3_pos_acc: 86.0057, stage3_loss_bbox: 0.1562, stage3_loss_iou: 0.2796, stage3_loss_mask: 0.3275, stage4_loss_cls: 0.2625, stage4_pos_acc: 90.6600, stage4_loss_bbox: 0.1444, stage4_loss_iou: 0.2654, stage4_loss_mask: 0.3181, stage5_loss_cls: 0.2268, stage5_pos_acc: 93.2864, stage5_loss_bbox: 0.1428, stage5_loss_iou: 0.2578, stage5_loss_mask: 0.3147, loss: 8.8881\n",
      "2025-07-16 17:00:45,439 - mmdet - INFO - Epoch [34][300/750]\tlr: 2.500e-05, eta: 2:27:26, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9943, stage0_pos_acc: 38.1180, stage0_loss_bbox: 0.4749, stage0_loss_iou: 0.9175, stage0_loss_mask: 0.7542, stage1_loss_cls: 0.5700, stage1_pos_acc: 65.9904, stage1_loss_bbox: 0.2282, stage1_loss_iou: 0.4416, stage1_loss_mask: 0.5560, stage2_loss_cls: 0.4611, stage2_pos_acc: 76.8366, stage2_loss_bbox: 0.1844, stage2_loss_iou: 0.3617, stage2_loss_mask: 0.5352, stage3_loss_cls: 0.3303, stage3_pos_acc: 82.8626, stage3_loss_bbox: 0.1743, stage3_loss_iou: 0.3339, stage3_loss_mask: 0.5054, stage4_loss_cls: 0.2632, stage4_pos_acc: 86.2362, stage4_loss_bbox: 0.1690, stage4_loss_iou: 0.3261, stage4_loss_mask: 0.4993, stage5_loss_cls: 0.2390, stage5_pos_acc: 90.6537, stage5_loss_bbox: 0.1658, stage5_loss_iou: 0.3175, stage5_loss_mask: 0.5064, loss: 10.3095\n",
      "2025-07-16 17:01:04,609 - mmdet - INFO - Epoch [34][350/750]\tlr: 2.500e-05, eta: 2:27:08, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0142, stage0_pos_acc: 38.7105, stage0_loss_bbox: 0.4964, stage0_loss_iou: 1.0124, stage0_loss_mask: 0.8585, stage1_loss_cls: 0.6041, stage1_pos_acc: 71.0286, stage1_loss_bbox: 0.2115, stage1_loss_iou: 0.4387, stage1_loss_mask: 0.4879, stage2_loss_cls: 0.4350, stage2_pos_acc: 82.0729, stage2_loss_bbox: 0.1650, stage2_loss_iou: 0.3357, stage2_loss_mask: 0.3884, stage3_loss_cls: 0.3037, stage3_pos_acc: 84.3670, stage3_loss_bbox: 0.1412, stage3_loss_iou: 0.3083, stage3_loss_mask: 0.4072, stage4_loss_cls: 0.2385, stage4_pos_acc: 91.3241, stage4_loss_bbox: 0.1336, stage4_loss_iou: 0.2915, stage4_loss_mask: 0.3841, stage5_loss_cls: 0.2096, stage5_pos_acc: 93.6432, stage5_loss_bbox: 0.1407, stage5_loss_iou: 0.2948, stage5_loss_mask: 0.3906, loss: 9.6915\n",
      "2025-07-16 17:01:24,030 - mmdet - INFO - Epoch [34][400/750]\tlr: 2.500e-05, eta: 2:26:51, time: 0.388, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0317, stage0_pos_acc: 37.7693, stage0_loss_bbox: 0.5011, stage0_loss_iou: 1.0025, stage0_loss_mask: 0.8448, stage1_loss_cls: 0.6200, stage1_pos_acc: 65.1877, stage1_loss_bbox: 0.2104, stage1_loss_iou: 0.4326, stage1_loss_mask: 0.4654, stage2_loss_cls: 0.4907, stage2_pos_acc: 75.3931, stage2_loss_bbox: 0.1658, stage2_loss_iou: 0.3409, stage2_loss_mask: 0.4121, stage3_loss_cls: 0.3417, stage3_pos_acc: 82.4495, stage3_loss_bbox: 0.1546, stage3_loss_iou: 0.3228, stage3_loss_mask: 0.3979, stage4_loss_cls: 0.2686, stage4_pos_acc: 86.3108, stage4_loss_bbox: 0.1488, stage4_loss_iou: 0.3114, stage4_loss_mask: 0.3910, stage5_loss_cls: 0.2379, stage5_pos_acc: 90.5365, stage5_loss_bbox: 0.1488, stage5_loss_iou: 0.3088, stage5_loss_mask: 0.3889, loss: 9.9393\n",
      "2025-07-16 17:01:42,521 - mmdet - INFO - Epoch [34][450/750]\tlr: 2.500e-05, eta: 2:26:32, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0281, stage0_pos_acc: 36.4597, stage0_loss_bbox: 0.4743, stage0_loss_iou: 0.9587, stage0_loss_mask: 0.8050, stage1_loss_cls: 0.5999, stage1_pos_acc: 64.6149, stage1_loss_bbox: 0.2104, stage1_loss_iou: 0.4506, stage1_loss_mask: 0.4453, stage2_loss_cls: 0.4829, stage2_pos_acc: 78.1340, stage2_loss_bbox: 0.1676, stage2_loss_iou: 0.3506, stage2_loss_mask: 0.4134, stage3_loss_cls: 0.3737, stage3_pos_acc: 80.6093, stage3_loss_bbox: 0.1627, stage3_loss_iou: 0.3246, stage3_loss_mask: 0.4045, stage4_loss_cls: 0.3237, stage4_pos_acc: 86.2713, stage4_loss_bbox: 0.1520, stage4_loss_iou: 0.3079, stage4_loss_mask: 0.3809, stage5_loss_cls: 0.2858, stage5_pos_acc: 88.3918, stage5_loss_bbox: 0.1560, stage5_loss_iou: 0.3065, stage5_loss_mask: 0.3726, loss: 9.9379\n",
      "2025-07-16 17:02:01,815 - mmdet - INFO - Epoch [34][500/750]\tlr: 2.500e-05, eta: 2:26:15, time: 0.386, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9997, stage0_pos_acc: 36.9071, stage0_loss_bbox: 0.4586, stage0_loss_iou: 0.8724, stage0_loss_mask: 0.6681, stage1_loss_cls: 0.6016, stage1_pos_acc: 62.6815, stage1_loss_bbox: 0.2168, stage1_loss_iou: 0.4024, stage1_loss_mask: 0.4058, stage2_loss_cls: 0.4663, stage2_pos_acc: 74.3710, stage2_loss_bbox: 0.1685, stage2_loss_iou: 0.3275, stage2_loss_mask: 0.3906, stage3_loss_cls: 0.3309, stage3_pos_acc: 83.0094, stage3_loss_bbox: 0.1557, stage3_loss_iou: 0.3054, stage3_loss_mask: 0.3858, stage4_loss_cls: 0.2574, stage4_pos_acc: 86.6043, stage4_loss_bbox: 0.1499, stage4_loss_iou: 0.2984, stage4_loss_mask: 0.3770, stage5_loss_cls: 0.2222, stage5_pos_acc: 88.7466, stage5_loss_bbox: 0.1507, stage5_loss_iou: 0.2985, stage5_loss_mask: 0.3663, loss: 9.2764\n",
      "2025-07-16 17:02:21,308 - mmdet - INFO - Epoch [34][550/750]\tlr: 2.500e-05, eta: 2:25:58, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0382, stage0_pos_acc: 33.4623, stage0_loss_bbox: 0.4827, stage0_loss_iou: 0.9977, stage0_loss_mask: 0.7751, stage1_loss_cls: 0.5658, stage1_pos_acc: 69.7603, stage1_loss_bbox: 0.2195, stage1_loss_iou: 0.4315, stage1_loss_mask: 0.4299, stage2_loss_cls: 0.4179, stage2_pos_acc: 79.1063, stage2_loss_bbox: 0.1626, stage2_loss_iou: 0.3129, stage2_loss_mask: 0.3882, stage3_loss_cls: 0.2759, stage3_pos_acc: 86.9198, stage3_loss_bbox: 0.1395, stage3_loss_iou: 0.2806, stage3_loss_mask: 0.3573, stage4_loss_cls: 0.1929, stage4_pos_acc: 92.4975, stage4_loss_bbox: 0.1311, stage4_loss_iou: 0.2691, stage4_loss_mask: 0.3598, stage5_loss_cls: 0.1514, stage5_pos_acc: 93.8794, stage5_loss_bbox: 0.1275, stage5_loss_iou: 0.2665, stage5_loss_mask: 0.3643, loss: 9.1380\n",
      "2025-07-16 17:02:40,851 - mmdet - INFO - Epoch [34][600/750]\tlr: 2.500e-05, eta: 2:25:42, time: 0.391, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0424, stage0_pos_acc: 38.9970, stage0_loss_bbox: 0.5287, stage0_loss_iou: 0.9581, stage0_loss_mask: 0.8182, stage1_loss_cls: 0.6498, stage1_pos_acc: 61.4499, stage1_loss_bbox: 0.2470, stage1_loss_iou: 0.4666, stage1_loss_mask: 0.4851, stage2_loss_cls: 0.5232, stage2_pos_acc: 66.2189, stage2_loss_bbox: 0.2044, stage2_loss_iou: 0.3696, stage2_loss_mask: 0.4592, stage3_loss_cls: 0.4010, stage3_pos_acc: 78.7368, stage3_loss_bbox: 0.1914, stage3_loss_iou: 0.3284, stage3_loss_mask: 0.4451, stage4_loss_cls: 0.3443, stage4_pos_acc: 81.7170, stage4_loss_bbox: 0.1860, stage4_loss_iou: 0.3103, stage4_loss_mask: 0.3818, stage5_loss_cls: 0.3016, stage5_pos_acc: 85.4309, stage5_loss_bbox: 0.1819, stage5_loss_iou: 0.3053, stage5_loss_mask: 0.3662, loss: 10.4957\n",
      "2025-07-16 17:02:59,989 - mmdet - INFO - Epoch [34][650/750]\tlr: 2.500e-05, eta: 2:25:24, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0738, stage0_pos_acc: 37.6225, stage0_loss_bbox: 0.4955, stage0_loss_iou: 1.0214, stage0_loss_mask: 0.6800, stage1_loss_cls: 0.6299, stage1_pos_acc: 65.1297, stage1_loss_bbox: 0.2102, stage1_loss_iou: 0.4440, stage1_loss_mask: 0.3782, stage2_loss_cls: 0.4848, stage2_pos_acc: 72.7955, stage2_loss_bbox: 0.1727, stage2_loss_iou: 0.3240, stage2_loss_mask: 0.2906, stage3_loss_cls: 0.3937, stage3_pos_acc: 80.2727, stage3_loss_bbox: 0.1514, stage3_loss_iou: 0.2895, stage3_loss_mask: 0.2663, stage4_loss_cls: 0.3301, stage4_pos_acc: 87.4781, stage4_loss_bbox: 0.1456, stage4_loss_iou: 0.2738, stage4_loss_mask: 0.2567, stage5_loss_cls: 0.2833, stage5_pos_acc: 90.2275, stage5_loss_bbox: 0.1459, stage5_loss_iou: 0.2718, stage5_loss_mask: 0.2701, loss: 9.2830\n",
      "2025-07-16 17:03:18,924 - mmdet - INFO - Epoch [34][700/750]\tlr: 2.500e-05, eta: 2:25:06, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0727, stage0_pos_acc: 31.1028, stage0_loss_bbox: 0.4999, stage0_loss_iou: 1.0304, stage0_loss_mask: 0.9012, stage1_loss_cls: 0.6410, stage1_pos_acc: 65.6092, stage1_loss_bbox: 0.2103, stage1_loss_iou: 0.4585, stage1_loss_mask: 0.4215, stage2_loss_cls: 0.4948, stage2_pos_acc: 79.5386, stage2_loss_bbox: 0.1651, stage2_loss_iou: 0.3495, stage2_loss_mask: 0.3697, stage3_loss_cls: 0.3695, stage3_pos_acc: 82.6133, stage3_loss_bbox: 0.1548, stage3_loss_iou: 0.3132, stage3_loss_mask: 0.3649, stage4_loss_cls: 0.2771, stage4_pos_acc: 87.6535, stage4_loss_bbox: 0.1437, stage4_loss_iou: 0.2959, stage4_loss_mask: 0.3661, stage5_loss_cls: 0.2368, stage5_pos_acc: 89.0780, stage5_loss_bbox: 0.1446, stage5_loss_iou: 0.2942, stage5_loss_mask: 0.3561, loss: 9.9315\n",
      "2025-07-16 17:03:38,023 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:03:38,023 - mmdet - INFO - Epoch [34][750/750]\tlr: 2.500e-05, eta: 2:24:49, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0316, stage0_pos_acc: 39.9372, stage0_loss_bbox: 0.4681, stage0_loss_iou: 0.9641, stage0_loss_mask: 0.6503, stage1_loss_cls: 0.5921, stage1_pos_acc: 70.5228, stage1_loss_bbox: 0.2004, stage1_loss_iou: 0.4046, stage1_loss_mask: 0.4012, stage2_loss_cls: 0.4493, stage2_pos_acc: 75.7211, stage2_loss_bbox: 0.1703, stage2_loss_iou: 0.2985, stage2_loss_mask: 0.3397, stage3_loss_cls: 0.3308, stage3_pos_acc: 85.9193, stage3_loss_bbox: 0.1493, stage3_loss_iou: 0.2802, stage3_loss_mask: 0.3117, stage4_loss_cls: 0.2852, stage4_pos_acc: 87.1433, stage4_loss_bbox: 0.1448, stage4_loss_iou: 0.2701, stage4_loss_mask: 0.3178, stage5_loss_cls: 0.2583, stage5_pos_acc: 89.8728, stage5_loss_bbox: 0.1477, stage5_loss_iou: 0.2693, stage5_loss_mask: 0.3063, loss: 9.0420\n",
      "2025-07-16 17:03:38,160 - mmdet - INFO - Saving checkpoint at 34 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 100s, ETA:     0s2025-07-16 17:06:57,308 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.35s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.398\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.127\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.225\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.446\n",
      "2025-07-16 17:06:59,188 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.93s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.133\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.242\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.450\n",
      "2025-07-16 17:07:02,510 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:07:02,512 - mmdet - INFO - Epoch(val) [34][750]\tbbox_mAP: 0.0240, bbox_mAP_50: 0.0450, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.0180, bbox_mAP_m: 0.0090, bbox_mAP_l: 0.0340, bbox_mAP_copypaste: 0.024 0.045 0.023 0.018 0.009 0.034, segm_mAP: 0.0240, segm_mAP_50: 0.0440, segm_mAP_75: 0.0220, segm_mAP_s: 0.0160, segm_mAP_m: 0.0100, segm_mAP_l: 0.0340, segm_mAP_copypaste: 0.024 0.044 0.022 0.016 0.010 0.034\n",
      "2025-07-16 17:09:17,798 - mmdet - INFO - Epoch [35][350/750]\tlr: 2.500e-05, eta: 2:22:48, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0154, stage0_pos_acc: 41.2250, stage0_loss_bbox: 0.5078, stage0_loss_iou: 0.9388, stage0_loss_mask: 0.7673, stage1_loss_cls: 0.6041, stage1_pos_acc: 67.8134, stage1_loss_bbox: 0.2258, stage1_loss_iou: 0.4222, stage1_loss_mask: 0.4542, stage2_loss_cls: 0.4574, stage2_pos_acc: 76.1464, stage2_loss_bbox: 0.1788, stage2_loss_iou: 0.3389, stage2_loss_mask: 0.4399, stage3_loss_cls: 0.3306, stage3_pos_acc: 83.3429, stage3_loss_bbox: 0.1710, stage3_loss_iou: 0.3160, stage3_loss_mask: 0.4297, stage4_loss_cls: 0.2657, stage4_pos_acc: 88.6439, stage4_loss_bbox: 0.1664, stage4_loss_iou: 0.3064, stage4_loss_mask: 0.4238, stage5_loss_cls: 0.2482, stage5_pos_acc: 89.7096, stage5_loss_bbox: 0.1602, stage5_loss_iou: 0.3028, stage5_loss_mask: 0.4135, loss: 9.8850\n",
      "2025-07-16 17:09:36,983 - mmdet - INFO - Epoch [35][400/750]\tlr: 2.500e-05, eta: 2:22:30, time: 0.384, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0025, stage0_pos_acc: 37.8805, stage0_loss_bbox: 0.5346, stage0_loss_iou: 0.9593, stage0_loss_mask: 1.0017, stage1_loss_cls: 0.6133, stage1_pos_acc: 71.2477, stage1_loss_bbox: 0.2597, stage1_loss_iou: 0.4696, stage1_loss_mask: 0.5612, stage2_loss_cls: 0.4960, stage2_pos_acc: 76.6170, stage2_loss_bbox: 0.2161, stage2_loss_iou: 0.3782, stage2_loss_mask: 0.5420, stage3_loss_cls: 0.3515, stage3_pos_acc: 85.7674, stage3_loss_bbox: 0.1949, stage3_loss_iou: 0.3621, stage3_loss_mask: 0.5160, stage4_loss_cls: 0.2864, stage4_pos_acc: 90.5848, stage4_loss_bbox: 0.1963, stage4_loss_iou: 0.3523, stage4_loss_mask: 0.5196, stage5_loss_cls: 0.2490, stage5_pos_acc: 93.4146, stage5_loss_bbox: 0.1890, stage5_loss_iou: 0.3421, stage5_loss_mask: 0.5078, loss: 11.1013\n",
      "2025-07-16 17:09:55,968 - mmdet - INFO - Epoch [35][450/750]\tlr: 2.500e-05, eta: 2:22:12, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0448, stage0_pos_acc: 37.9251, stage0_loss_bbox: 0.4741, stage0_loss_iou: 0.9057, stage0_loss_mask: 0.5335, stage1_loss_cls: 0.5892, stage1_pos_acc: 71.7551, stage1_loss_bbox: 0.1886, stage1_loss_iou: 0.3570, stage1_loss_mask: 0.2533, stage2_loss_cls: 0.4444, stage2_pos_acc: 77.7551, stage2_loss_bbox: 0.1463, stage2_loss_iou: 0.2733, stage2_loss_mask: 0.2180, stage3_loss_cls: 0.3340, stage3_pos_acc: 84.1923, stage3_loss_bbox: 0.1360, stage3_loss_iou: 0.2551, stage3_loss_mask: 0.2369, stage4_loss_cls: 0.2386, stage4_pos_acc: 90.0295, stage4_loss_bbox: 0.1313, stage4_loss_iou: 0.2485, stage4_loss_mask: 0.2483, stage5_loss_cls: 0.2064, stage5_pos_acc: 93.6295, stage5_loss_bbox: 0.1299, stage5_loss_iou: 0.2429, stage5_loss_mask: 0.2539, loss: 8.0899\n",
      "2025-07-16 17:10:15,162 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:10:15,162 - mmdet - INFO - Epoch [35][500/750]\tlr: 2.500e-05, eta: 2:21:55, time: 0.384, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0518, stage0_pos_acc: 35.6264, stage0_loss_bbox: 0.4756, stage0_loss_iou: 0.9610, stage0_loss_mask: 0.7084, stage1_loss_cls: 0.6039, stage1_pos_acc: 67.2312, stage1_loss_bbox: 0.2078, stage1_loss_iou: 0.4300, stage1_loss_mask: 0.4527, stage2_loss_cls: 0.4862, stage2_pos_acc: 74.5145, stage2_loss_bbox: 0.1660, stage2_loss_iou: 0.3379, stage2_loss_mask: 0.3563, stage3_loss_cls: 0.3629, stage3_pos_acc: 79.3887, stage3_loss_bbox: 0.1459, stage3_loss_iou: 0.2978, stage3_loss_mask: 0.3116, stage4_loss_cls: 0.3121, stage4_pos_acc: 85.4840, stage4_loss_bbox: 0.1461, stage4_loss_iou: 0.2821, stage4_loss_mask: 0.3126, stage5_loss_cls: 0.2735, stage5_pos_acc: 88.4840, stage5_loss_bbox: 0.1404, stage5_loss_iou: 0.2749, stage5_loss_mask: 0.3016, loss: 9.3988\n",
      "2025-07-16 17:10:34,310 - mmdet - INFO - Epoch [35][550/750]\tlr: 2.500e-05, eta: 2:21:37, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0371, stage0_pos_acc: 38.1830, stage0_loss_bbox: 0.5048, stage0_loss_iou: 0.9873, stage0_loss_mask: 0.5970, stage1_loss_cls: 0.6168, stage1_pos_acc: 68.9127, stage1_loss_bbox: 0.1822, stage1_loss_iou: 0.3886, stage1_loss_mask: 0.3179, stage2_loss_cls: 0.4560, stage2_pos_acc: 81.0687, stage2_loss_bbox: 0.1472, stage2_loss_iou: 0.3052, stage2_loss_mask: 0.3014, stage3_loss_cls: 0.3447, stage3_pos_acc: 84.1209, stage3_loss_bbox: 0.1419, stage3_loss_iou: 0.2735, stage3_loss_mask: 0.2641, stage4_loss_cls: 0.2582, stage4_pos_acc: 90.3640, stage4_loss_bbox: 0.1288, stage4_loss_iou: 0.2573, stage4_loss_mask: 0.2718, stage5_loss_cls: 0.2291, stage5_pos_acc: 92.1830, stage5_loss_bbox: 0.1344, stage5_loss_iou: 0.2535, stage5_loss_mask: 0.2680, loss: 8.6669\n",
      "2025-07-16 17:10:53,258 - mmdet - INFO - Epoch [35][600/750]\tlr: 2.500e-05, eta: 2:21:19, time: 0.379, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0622, stage0_pos_acc: 37.5496, stage0_loss_bbox: 0.5192, stage0_loss_iou: 1.0478, stage0_loss_mask: 0.8108, stage1_loss_cls: 0.6390, stage1_pos_acc: 66.2553, stage1_loss_bbox: 0.2169, stage1_loss_iou: 0.4886, stage1_loss_mask: 0.4857, stage2_loss_cls: 0.5000, stage2_pos_acc: 77.7425, stage2_loss_bbox: 0.1682, stage2_loss_iou: 0.3713, stage2_loss_mask: 0.4329, stage3_loss_cls: 0.3680, stage3_pos_acc: 82.9308, stage3_loss_bbox: 0.1520, stage3_loss_iou: 0.3320, stage3_loss_mask: 0.4138, stage4_loss_cls: 0.2927, stage4_pos_acc: 87.8831, stage4_loss_bbox: 0.1475, stage4_loss_iou: 0.3262, stage4_loss_mask: 0.4069, stage5_loss_cls: 0.2723, stage5_pos_acc: 91.9934, stage5_loss_bbox: 0.1444, stage5_loss_iou: 0.3108, stage5_loss_mask: 0.3845, loss: 10.2938\n",
      "2025-07-16 17:11:12,247 - mmdet - INFO - Epoch [35][650/750]\tlr: 2.500e-05, eta: 2:21:00, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0029, stage0_pos_acc: 38.7671, stage0_loss_bbox: 0.4719, stage0_loss_iou: 0.8977, stage0_loss_mask: 0.5972, stage1_loss_cls: 0.5756, stage1_pos_acc: 69.5624, stage1_loss_bbox: 0.2255, stage1_loss_iou: 0.4230, stage1_loss_mask: 0.3568, stage2_loss_cls: 0.4572, stage2_pos_acc: 80.3236, stage2_loss_bbox: 0.1781, stage2_loss_iou: 0.3199, stage2_loss_mask: 0.3375, stage3_loss_cls: 0.3436, stage3_pos_acc: 85.9367, stage3_loss_bbox: 0.1557, stage3_loss_iou: 0.2934, stage3_loss_mask: 0.3200, stage4_loss_cls: 0.2706, stage4_pos_acc: 88.2376, stage4_loss_bbox: 0.1490, stage4_loss_iou: 0.2827, stage4_loss_mask: 0.3216, stage5_loss_cls: 0.2398, stage5_pos_acc: 90.6518, stage5_loss_bbox: 0.1409, stage5_loss_iou: 0.2723, stage5_loss_mask: 0.3132, loss: 8.9459\n",
      "2025-07-16 17:11:31,318 - mmdet - INFO - Epoch [35][700/750]\tlr: 2.500e-05, eta: 2:20:42, time: 0.381, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0200, stage0_pos_acc: 37.0252, stage0_loss_bbox: 0.5210, stage0_loss_iou: 1.0177, stage0_loss_mask: 0.8553, stage1_loss_cls: 0.5825, stage1_pos_acc: 74.0495, stage1_loss_bbox: 0.2224, stage1_loss_iou: 0.4458, stage1_loss_mask: 0.4870, stage2_loss_cls: 0.4417, stage2_pos_acc: 81.9043, stage2_loss_bbox: 0.1633, stage2_loss_iou: 0.3250, stage2_loss_mask: 0.4445, stage3_loss_cls: 0.2919, stage3_pos_acc: 88.3157, stage3_loss_bbox: 0.1570, stage3_loss_iou: 0.3064, stage3_loss_mask: 0.4326, stage4_loss_cls: 0.2013, stage4_pos_acc: 93.5258, stage4_loss_bbox: 0.1496, stage4_loss_iou: 0.2938, stage4_loss_mask: 0.4245, stage5_loss_cls: 0.1747, stage5_pos_acc: 94.2761, stage5_loss_bbox: 0.1435, stage5_loss_iou: 0.2767, stage5_loss_mask: 0.3853, loss: 9.7634\n",
      "2025-07-16 17:11:50,231 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:11:50,231 - mmdet - INFO - Epoch [35][750/750]\tlr: 2.500e-05, eta: 2:20:24, time: 0.378, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0577, stage0_pos_acc: 42.7218, stage0_loss_bbox: 0.5360, stage0_loss_iou: 1.0443, stage0_loss_mask: 0.8135, stage1_loss_cls: 0.6216, stage1_pos_acc: 71.0783, stage1_loss_bbox: 0.2100, stage1_loss_iou: 0.4451, stage1_loss_mask: 0.4906, stage2_loss_cls: 0.4829, stage2_pos_acc: 79.5476, stage2_loss_bbox: 0.1633, stage2_loss_iou: 0.3487, stage2_loss_mask: 0.4645, stage3_loss_cls: 0.3395, stage3_pos_acc: 85.7497, stage3_loss_bbox: 0.1526, stage3_loss_iou: 0.3321, stage3_loss_mask: 0.4681, stage4_loss_cls: 0.2491, stage4_pos_acc: 88.7826, stage4_loss_bbox: 0.1400, stage4_loss_iou: 0.3077, stage4_loss_mask: 0.4457, stage5_loss_cls: 0.2158, stage5_pos_acc: 90.2119, stage5_loss_bbox: 0.1384, stage5_loss_iou: 0.3024, stage5_loss_mask: 0.4432, loss: 10.2129\n",
      "2025-07-16 17:11:50,365 - mmdet - INFO - Saving checkpoint at 35 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 98s, ETA:     0s2025-07-16 17:15:06,532 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.094\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.245\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.418\n",
      "2025-07-16 17:15:08,360 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.046\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.092\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.012\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.378\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.381\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.381\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.410\n",
      "2025-07-16 17:15:11,539 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:15:11,540 - mmdet - INFO - Epoch(val) [35][750]\tbbox_mAP: 0.0260, bbox_mAP_50: 0.0470, bbox_mAP_75: 0.0270, bbox_mAP_s: 0.0940, bbox_mAP_m: 0.0120, bbox_mAP_l: 0.0320, bbox_mAP_copypaste: 0.026 0.047 0.027 0.094 0.012 0.032, segm_mAP: 0.0250, segm_mAP_50: 0.0460, segm_mAP_75: 0.0260, segm_mAP_s: 0.0920, segm_mAP_m: 0.0120, segm_mAP_l: 0.0310, segm_mAP_copypaste: 0.025 0.046 0.026 0.092 0.012 0.031\n",
      "2025-07-16 17:15:33,045 - mmdet - INFO - Epoch [36][50/750]\tlr: 2.500e-05, eta: 2:20:11, time: 0.430, data_time: 0.057, memory: 11264, stage0_loss_cls: 1.0267, stage0_pos_acc: 37.9041, stage0_loss_bbox: 0.5065, stage0_loss_iou: 1.0429, stage0_loss_mask: 0.8879, stage1_loss_cls: 0.5743, stage1_pos_acc: 72.6538, stage1_loss_bbox: 0.2146, stage1_loss_iou: 0.4518, stage1_loss_mask: 0.4601, stage2_loss_cls: 0.4371, stage2_pos_acc: 81.6443, stage2_loss_bbox: 0.1647, stage2_loss_iou: 0.3304, stage2_loss_mask: 0.4928, stage3_loss_cls: 0.3152, stage3_pos_acc: 85.2348, stage3_loss_bbox: 0.1625, stage3_loss_iou: 0.3060, stage3_loss_mask: 0.4721, stage4_loss_cls: 0.2464, stage4_pos_acc: 89.8016, stage4_loss_bbox: 0.1520, stage4_loss_iou: 0.2896, stage4_loss_mask: 0.4403, stage5_loss_cls: 0.2222, stage5_pos_acc: 91.5286, stage5_loss_bbox: 0.1624, stage5_loss_iou: 0.2786, stage5_loss_mask: 0.4198, loss: 10.0570\n",
      "2025-07-16 17:15:51,892 - mmdet - INFO - Epoch [36][100/750]\tlr: 2.500e-05, eta: 2:19:53, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0289, stage0_pos_acc: 31.6437, stage0_loss_bbox: 0.4869, stage0_loss_iou: 1.0133, stage0_loss_mask: 0.6189, stage1_loss_cls: 0.6002, stage1_pos_acc: 67.9730, stage1_loss_bbox: 0.1918, stage1_loss_iou: 0.4081, stage1_loss_mask: 0.3395, stage2_loss_cls: 0.4487, stage2_pos_acc: 80.7032, stage2_loss_bbox: 0.1493, stage2_loss_iou: 0.3202, stage2_loss_mask: 0.3124, stage3_loss_cls: 0.3249, stage3_pos_acc: 87.3254, stage3_loss_bbox: 0.1278, stage3_loss_iou: 0.2908, stage3_loss_mask: 0.3203, stage4_loss_cls: 0.2462, stage4_pos_acc: 89.1444, stage4_loss_bbox: 0.1367, stage4_loss_iou: 0.2933, stage4_loss_mask: 0.3075, stage5_loss_cls: 0.2175, stage5_pos_acc: 91.8778, stage5_loss_bbox: 0.1379, stage5_loss_iou: 0.2918, stage5_loss_mask: 0.3100, loss: 8.9229\n",
      "2025-07-16 17:16:10,833 - mmdet - INFO - Epoch [36][150/750]\tlr: 2.500e-05, eta: 2:19:35, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0539, stage0_pos_acc: 37.0230, stage0_loss_bbox: 0.5027, stage0_loss_iou: 1.0229, stage0_loss_mask: 0.9422, stage1_loss_cls: 0.6184, stage1_pos_acc: 66.4933, stage1_loss_bbox: 0.2000, stage1_loss_iou: 0.4565, stage1_loss_mask: 0.5032, stage2_loss_cls: 0.4919, stage2_pos_acc: 77.3118, stage2_loss_bbox: 0.1561, stage2_loss_iou: 0.3470, stage2_loss_mask: 0.4016, stage3_loss_cls: 0.3364, stage3_pos_acc: 85.6841, stage3_loss_bbox: 0.1503, stage3_loss_iou: 0.3252, stage3_loss_mask: 0.4114, stage4_loss_cls: 0.2598, stage4_pos_acc: 88.9157, stage4_loss_bbox: 0.1419, stage4_loss_iou: 0.3127, stage4_loss_mask: 0.4005, stage5_loss_cls: 0.2236, stage5_pos_acc: 92.2005, stage5_loss_bbox: 0.1423, stage5_loss_iou: 0.3115, stage5_loss_mask: 0.4025, loss: 10.1147\n",
      "2025-07-16 17:16:29,673 - mmdet - INFO - Epoch [36][200/750]\tlr: 2.500e-05, eta: 2:19:16, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0342, stage0_pos_acc: 37.9966, stage0_loss_bbox: 0.4515, stage0_loss_iou: 0.9242, stage0_loss_mask: 0.7490, stage1_loss_cls: 0.6057, stage1_pos_acc: 69.1449, stage1_loss_bbox: 0.2042, stage1_loss_iou: 0.4244, stage1_loss_mask: 0.5163, stage2_loss_cls: 0.4683, stage2_pos_acc: 79.6235, stage2_loss_bbox: 0.1632, stage2_loss_iou: 0.3363, stage2_loss_mask: 0.4507, stage3_loss_cls: 0.3247, stage3_pos_acc: 85.2744, stage3_loss_bbox: 0.1570, stage3_loss_iou: 0.3120, stage3_loss_mask: 0.4381, stage4_loss_cls: 0.2511, stage4_pos_acc: 91.7833, stage4_loss_bbox: 0.1440, stage4_loss_iou: 0.2998, stage4_loss_mask: 0.4197, stage5_loss_cls: 0.2179, stage5_pos_acc: 92.1224, stage5_loss_bbox: 0.1379, stage5_loss_iou: 0.2958, stage5_loss_mask: 0.4211, loss: 9.7469\n",
      "2025-07-16 17:16:48,629 - mmdet - INFO - Epoch [36][250/750]\tlr: 2.500e-05, eta: 2:18:58, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0221, stage0_pos_acc: 39.0334, stage0_loss_bbox: 0.5323, stage0_loss_iou: 0.9559, stage0_loss_mask: 0.5183, stage1_loss_cls: 0.5652, stage1_pos_acc: 71.9159, stage1_loss_bbox: 0.2446, stage1_loss_iou: 0.3673, stage1_loss_mask: 0.3057, stage2_loss_cls: 0.4482, stage2_pos_acc: 81.9947, stage2_loss_bbox: 0.1935, stage2_loss_iou: 0.2924, stage2_loss_mask: 0.2921, stage3_loss_cls: 0.3418, stage3_pos_acc: 86.6123, stage3_loss_bbox: 0.1644, stage3_loss_iou: 0.2611, stage3_loss_mask: 0.2695, stage4_loss_cls: 0.2596, stage4_pos_acc: 89.9026, stage4_loss_bbox: 0.1523, stage4_loss_iou: 0.2543, stage4_loss_mask: 0.2412, stage5_loss_cls: 0.2404, stage5_pos_acc: 91.5794, stage5_loss_bbox: 0.1455, stage5_loss_iou: 0.2488, stage5_loss_mask: 0.2581, loss: 8.5747\n",
      "2025-07-16 17:17:07,644 - mmdet - INFO - Epoch [36][300/750]\tlr: 2.500e-05, eta: 2:18:40, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0459, stage0_pos_acc: 34.6421, stage0_loss_bbox: 0.4876, stage0_loss_iou: 0.9922, stage0_loss_mask: 0.6485, stage1_loss_cls: 0.5930, stage1_pos_acc: 65.3698, stage1_loss_bbox: 0.1899, stage1_loss_iou: 0.3881, stage1_loss_mask: 0.4065, stage2_loss_cls: 0.4466, stage2_pos_acc: 76.9730, stage2_loss_bbox: 0.1469, stage2_loss_iou: 0.3046, stage2_loss_mask: 0.4155, stage3_loss_cls: 0.3300, stage3_pos_acc: 83.1595, stage3_loss_bbox: 0.1431, stage3_loss_iou: 0.2901, stage3_loss_mask: 0.4018, stage4_loss_cls: 0.2564, stage4_pos_acc: 87.8556, stage4_loss_bbox: 0.1387, stage4_loss_iou: 0.2858, stage4_loss_mask: 0.3914, stage5_loss_cls: 0.2273, stage5_pos_acc: 88.8175, stage5_loss_bbox: 0.1357, stage5_loss_iou: 0.2811, stage5_loss_mask: 0.3581, loss: 9.3047\n",
      "2025-07-16 17:17:26,652 - mmdet - INFO - Epoch [36][350/750]\tlr: 2.500e-05, eta: 2:18:21, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0575, stage0_pos_acc: 37.3500, stage0_loss_bbox: 0.5076, stage0_loss_iou: 1.0158, stage0_loss_mask: 0.6876, stage1_loss_cls: 0.5919, stage1_pos_acc: 66.4120, stage1_loss_bbox: 0.2066, stage1_loss_iou: 0.4047, stage1_loss_mask: 0.3628, stage2_loss_cls: 0.4473, stage2_pos_acc: 76.2113, stage2_loss_bbox: 0.1539, stage2_loss_iou: 0.3118, stage2_loss_mask: 0.3337, stage3_loss_cls: 0.3288, stage3_pos_acc: 85.5652, stage3_loss_bbox: 0.1299, stage3_loss_iou: 0.2684, stage3_loss_mask: 0.3092, stage4_loss_cls: 0.2518, stage4_pos_acc: 90.3775, stage4_loss_bbox: 0.1296, stage4_loss_iou: 0.2545, stage4_loss_mask: 0.3015, stage5_loss_cls: 0.2152, stage5_pos_acc: 93.2098, stage5_loss_bbox: 0.1267, stage5_loss_iou: 0.2511, stage5_loss_mask: 0.3134, loss: 8.9612\n",
      "2025-07-16 17:17:45,663 - mmdet - INFO - Epoch [36][400/750]\tlr: 2.500e-05, eta: 2:18:03, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0317, stage0_pos_acc: 35.7038, stage0_loss_bbox: 0.4878, stage0_loss_iou: 0.9342, stage0_loss_mask: 0.5466, stage1_loss_cls: 0.5671, stage1_pos_acc: 68.6776, stage1_loss_bbox: 0.1930, stage1_loss_iou: 0.3799, stage1_loss_mask: 0.3222, stage2_loss_cls: 0.4344, stage2_pos_acc: 77.5459, stage2_loss_bbox: 0.1561, stage2_loss_iou: 0.2897, stage2_loss_mask: 0.3044, stage3_loss_cls: 0.3023, stage3_pos_acc: 84.5405, stage3_loss_bbox: 0.1417, stage3_loss_iou: 0.2707, stage3_loss_mask: 0.2990, stage4_loss_cls: 0.2473, stage4_pos_acc: 89.9102, stage4_loss_bbox: 0.1374, stage4_loss_iou: 0.2587, stage4_loss_mask: 0.2969, stage5_loss_cls: 0.1968, stage5_pos_acc: 93.0968, stage5_loss_bbox: 0.1408, stage5_loss_iou: 0.2583, stage5_loss_mask: 0.2966, loss: 8.4935\n",
      "2025-07-16 17:18:04,736 - mmdet - INFO - Epoch [36][450/750]\tlr: 2.500e-05, eta: 2:17:45, time: 0.381, data_time: 0.011, memory: 11264, stage0_loss_cls: 1.0054, stage0_pos_acc: 40.1176, stage0_loss_bbox: 0.4949, stage0_loss_iou: 0.9668, stage0_loss_mask: 0.8500, stage1_loss_cls: 0.6215, stage1_pos_acc: 73.0716, stage1_loss_bbox: 0.2256, stage1_loss_iou: 0.4614, stage1_loss_mask: 0.5273, stage2_loss_cls: 0.4939, stage2_pos_acc: 80.4556, stage2_loss_bbox: 0.1745, stage2_loss_iou: 0.3624, stage2_loss_mask: 0.5169, stage3_loss_cls: 0.3554, stage3_pos_acc: 83.4721, stage3_loss_bbox: 0.1662, stage3_loss_iou: 0.3482, stage3_loss_mask: 0.4860, stage4_loss_cls: 0.2921, stage4_pos_acc: 88.3639, stage4_loss_bbox: 0.1553, stage4_loss_iou: 0.3330, stage4_loss_mask: 0.4934, stage5_loss_cls: 0.2683, stage5_pos_acc: 91.0228, stage5_loss_bbox: 0.1549, stage5_loss_iou: 0.3304, stage5_loss_mask: 0.4889, loss: 10.5726\n",
      "2025-07-16 17:18:23,807 - mmdet - INFO - Epoch [36][500/750]\tlr: 2.500e-05, eta: 2:17:27, time: 0.381, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0419, stage0_pos_acc: 42.2368, stage0_loss_bbox: 0.4621, stage0_loss_iou: 0.9425, stage0_loss_mask: 0.6531, stage1_loss_cls: 0.5710, stage1_pos_acc: 69.6999, stage1_loss_bbox: 0.2027, stage1_loss_iou: 0.4003, stage1_loss_mask: 0.3638, stage2_loss_cls: 0.4336, stage2_pos_acc: 79.5445, stage2_loss_bbox: 0.1576, stage2_loss_iou: 0.2948, stage2_loss_mask: 0.3063, stage3_loss_cls: 0.3090, stage3_pos_acc: 87.9105, stage3_loss_bbox: 0.1432, stage3_loss_iou: 0.2698, stage3_loss_mask: 0.3079, stage4_loss_cls: 0.2256, stage4_pos_acc: 92.3183, stage4_loss_bbox: 0.1436, stage4_loss_iou: 0.2654, stage4_loss_mask: 0.2998, stage5_loss_cls: 0.1946, stage5_pos_acc: 93.1388, stage5_loss_bbox: 0.1450, stage5_loss_iou: 0.2694, stage5_loss_mask: 0.3057, loss: 8.7086\n",
      "2025-07-16 17:18:42,966 - mmdet - INFO - Epoch [36][550/750]\tlr: 2.500e-05, eta: 2:17:09, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0550, stage0_pos_acc: 39.8929, stage0_loss_bbox: 0.5021, stage0_loss_iou: 0.9882, stage0_loss_mask: 0.7629, stage1_loss_cls: 0.5994, stage1_pos_acc: 71.3821, stage1_loss_bbox: 0.2165, stage1_loss_iou: 0.4234, stage1_loss_mask: 0.4083, stage2_loss_cls: 0.4522, stage2_pos_acc: 79.9583, stage2_loss_bbox: 0.1804, stage2_loss_iou: 0.3290, stage2_loss_mask: 0.3786, stage3_loss_cls: 0.3394, stage3_pos_acc: 86.9411, stage3_loss_bbox: 0.1687, stage3_loss_iou: 0.3013, stage3_loss_mask: 0.3746, stage4_loss_cls: 0.2802, stage4_pos_acc: 89.5857, stage4_loss_bbox: 0.1571, stage4_loss_iou: 0.2920, stage4_loss_mask: 0.3250, stage5_loss_cls: 0.2628, stage5_pos_acc: 89.8226, stage5_loss_bbox: 0.1635, stage5_loss_iou: 0.2954, stage5_loss_mask: 0.3246, loss: 9.5806\n",
      "2025-07-16 17:19:01,875 - mmdet - INFO - Epoch [36][600/750]\tlr: 2.500e-05, eta: 2:16:51, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0119, stage0_pos_acc: 40.9526, stage0_loss_bbox: 0.5558, stage0_loss_iou: 1.0069, stage0_loss_mask: 0.8481, stage1_loss_cls: 0.6113, stage1_pos_acc: 63.1074, stage1_loss_bbox: 0.2483, stage1_loss_iou: 0.4606, stage1_loss_mask: 0.5477, stage2_loss_cls: 0.4955, stage2_pos_acc: 75.6883, stage2_loss_bbox: 0.1893, stage2_loss_iou: 0.3787, stage2_loss_mask: 0.4669, stage3_loss_cls: 0.3752, stage3_pos_acc: 82.8864, stage3_loss_bbox: 0.1667, stage3_loss_iou: 0.3339, stage3_loss_mask: 0.4269, stage4_loss_cls: 0.3014, stage4_pos_acc: 86.2788, stage4_loss_bbox: 0.1553, stage4_loss_iou: 0.3189, stage4_loss_mask: 0.4188, stage5_loss_cls: 0.2638, stage5_pos_acc: 90.0621, stage5_loss_bbox: 0.1526, stage5_loss_iou: 0.3150, stage5_loss_mask: 0.4326, loss: 10.4818\n",
      "2025-07-16 17:19:20,804 - mmdet - INFO - Epoch [36][650/750]\tlr: 2.500e-05, eta: 2:16:32, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0615, stage0_pos_acc: 38.6063, stage0_loss_bbox: 0.5013, stage0_loss_iou: 1.0150, stage0_loss_mask: 0.7870, stage1_loss_cls: 0.6156, stage1_pos_acc: 70.8898, stage1_loss_bbox: 0.2129, stage1_loss_iou: 0.4640, stage1_loss_mask: 0.5385, stage2_loss_cls: 0.4680, stage2_pos_acc: 79.5507, stage2_loss_bbox: 0.1597, stage2_loss_iou: 0.3601, stage2_loss_mask: 0.5055, stage3_loss_cls: 0.3346, stage3_pos_acc: 82.9365, stage3_loss_bbox: 0.1519, stage3_loss_iou: 0.3369, stage3_loss_mask: 0.4853, stage4_loss_cls: 0.2419, stage4_pos_acc: 89.2579, stage4_loss_bbox: 0.1521, stage4_loss_iou: 0.3335, stage4_loss_mask: 0.4503, stage5_loss_cls: 0.1964, stage5_pos_acc: 92.7841, stage5_loss_bbox: 0.1554, stage5_loss_iou: 0.3380, stage5_loss_mask: 0.5032, loss: 10.3685\n",
      "2025-07-16 17:19:39,955 - mmdet - INFO - Epoch [36][700/750]\tlr: 2.500e-05, eta: 2:16:14, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0695, stage0_pos_acc: 36.9987, stage0_loss_bbox: 0.5491, stage0_loss_iou: 0.9861, stage0_loss_mask: 0.6983, stage1_loss_cls: 0.6144, stage1_pos_acc: 66.9057, stage1_loss_bbox: 0.2241, stage1_loss_iou: 0.4249, stage1_loss_mask: 0.4438, stage2_loss_cls: 0.4685, stage2_pos_acc: 79.2898, stage2_loss_bbox: 0.1621, stage2_loss_iou: 0.3294, stage2_loss_mask: 0.3806, stage3_loss_cls: 0.3251, stage3_pos_acc: 86.1046, stage3_loss_bbox: 0.1493, stage3_loss_iou: 0.3029, stage3_loss_mask: 0.3561, stage4_loss_cls: 0.2413, stage4_pos_acc: 92.8848, stage4_loss_bbox: 0.1420, stage4_loss_iou: 0.2895, stage4_loss_mask: 0.3476, stage5_loss_cls: 0.2077, stage5_pos_acc: 95.0666, stage5_loss_bbox: 0.1469, stage5_loss_iou: 0.2867, stage5_loss_mask: 0.3395, loss: 9.4853\n",
      "2025-07-16 17:19:58,902 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:19:58,903 - mmdet - INFO - Epoch [36][750/750]\tlr: 2.500e-05, eta: 2:15:56, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0169, stage0_pos_acc: 33.9190, stage0_loss_bbox: 0.4887, stage0_loss_iou: 0.9147, stage0_loss_mask: 0.6396, stage1_loss_cls: 0.5772, stage1_pos_acc: 69.0452, stage1_loss_bbox: 0.1944, stage1_loss_iou: 0.3796, stage1_loss_mask: 0.3714, stage2_loss_cls: 0.4669, stage2_pos_acc: 76.5643, stage2_loss_bbox: 0.1453, stage2_loss_iou: 0.2765, stage2_loss_mask: 0.2917, stage3_loss_cls: 0.3453, stage3_pos_acc: 84.9929, stage3_loss_bbox: 0.1377, stage3_loss_iou: 0.2501, stage3_loss_mask: 0.2871, stage4_loss_cls: 0.2812, stage4_pos_acc: 88.1786, stage4_loss_bbox: 0.1276, stage4_loss_iou: 0.2347, stage4_loss_mask: 0.2710, stage5_loss_cls: 0.2683, stage5_pos_acc: 88.1619, stage5_loss_bbox: 0.1245, stage5_loss_iou: 0.2309, stage5_loss_mask: 0.2758, loss: 8.5969\n",
      "2025-07-16 17:19:59,023 - mmdet - INFO - Saving checkpoint at 36 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 17:23:19,420 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.035\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.104\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.318\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.449\n",
      "2025-07-16 17:23:21,243 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.430\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.432\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.432\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.125\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.325\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.457\n",
      "2025-07-16 17:23:24,334 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:23:24,334 - mmdet - INFO - Epoch(val) [36][750]\tbbox_mAP: 0.0280, bbox_mAP_50: 0.0540, bbox_mAP_75: 0.0250, bbox_mAP_s: 0.0350, bbox_mAP_m: 0.0090, bbox_mAP_l: 0.0350, bbox_mAP_copypaste: 0.028 0.054 0.025 0.035 0.009 0.035, segm_mAP: 0.0280, segm_mAP_50: 0.0540, segm_mAP_75: 0.0250, segm_mAP_s: 0.0450, segm_mAP_m: 0.0090, segm_mAP_l: 0.0360, segm_mAP_copypaste: 0.028 0.054 0.025 0.045 0.009 0.036\n",
      "2025-07-16 17:23:46,299 - mmdet - INFO - Epoch [37][50/750]\tlr: 2.500e-05, eta: 2:15:43, time: 0.439, data_time: 0.054, memory: 11264, stage0_loss_cls: 1.0238, stage0_pos_acc: 39.6526, stage0_loss_bbox: 0.4373, stage0_loss_iou: 0.8485, stage0_loss_mask: 0.4534, stage1_loss_cls: 0.5799, stage1_pos_acc: 67.3307, stage1_loss_bbox: 0.1856, stage1_loss_iou: 0.3363, stage1_loss_mask: 0.2408, stage2_loss_cls: 0.4178, stage2_pos_acc: 75.8931, stage2_loss_bbox: 0.1612, stage2_loss_iou: 0.2577, stage2_loss_mask: 0.2211, stage3_loss_cls: 0.2756, stage3_pos_acc: 87.6030, stage3_loss_bbox: 0.1490, stage3_loss_iou: 0.2340, stage3_loss_mask: 0.2139, stage4_loss_cls: 0.1934, stage4_pos_acc: 93.4864, stage4_loss_bbox: 0.1375, stage4_loss_iou: 0.2243, stage4_loss_mask: 0.2078, stage5_loss_cls: 0.1663, stage5_pos_acc: 93.6348, stage5_loss_bbox: 0.1317, stage5_loss_iou: 0.2218, stage5_loss_mask: 0.2193, loss: 7.5379\n",
      "2025-07-16 17:24:05,803 - mmdet - INFO - Epoch [37][100/750]\tlr: 2.500e-05, eta: 2:15:26, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0602, stage0_pos_acc: 35.1897, stage0_loss_bbox: 0.4864, stage0_loss_iou: 1.0296, stage0_loss_mask: 0.7864, stage1_loss_cls: 0.5900, stage1_pos_acc: 67.3721, stage1_loss_bbox: 0.1951, stage1_loss_iou: 0.4415, stage1_loss_mask: 0.3944, stage2_loss_cls: 0.4353, stage2_pos_acc: 81.7714, stage2_loss_bbox: 0.1469, stage2_loss_iou: 0.3298, stage2_loss_mask: 0.3864, stage3_loss_cls: 0.3226, stage3_pos_acc: 84.1645, stage3_loss_bbox: 0.1312, stage3_loss_iou: 0.2954, stage3_loss_mask: 0.3698, stage4_loss_cls: 0.2715, stage4_pos_acc: 87.5591, stage4_loss_bbox: 0.1259, stage4_loss_iou: 0.2816, stage4_loss_mask: 0.3514, stage5_loss_cls: 0.2459, stage5_pos_acc: 89.4924, stage5_loss_bbox: 0.1193, stage5_loss_iou: 0.2756, stage5_loss_mask: 0.3519, loss: 9.4240\n",
      "2025-07-16 17:24:24,727 - mmdet - INFO - Epoch [37][150/750]\tlr: 2.500e-05, eta: 2:15:07, time: 0.378, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0752, stage0_pos_acc: 36.2167, stage0_loss_bbox: 0.4915, stage0_loss_iou: 1.0232, stage0_loss_mask: 0.6004, stage1_loss_cls: 0.5957, stage1_pos_acc: 70.8452, stage1_loss_bbox: 0.1742, stage1_loss_iou: 0.3911, stage1_loss_mask: 0.3218, stage2_loss_cls: 0.4287, stage2_pos_acc: 79.4786, stage2_loss_bbox: 0.1187, stage2_loss_iou: 0.2917, stage2_loss_mask: 0.3069, stage3_loss_cls: 0.2840, stage3_pos_acc: 89.1262, stage3_loss_bbox: 0.1087, stage3_loss_iou: 0.2533, stage3_loss_mask: 0.2859, stage4_loss_cls: 0.2074, stage4_pos_acc: 91.8484, stage4_loss_bbox: 0.1074, stage4_loss_iou: 0.2517, stage4_loss_mask: 0.2811, stage5_loss_cls: 0.1740, stage5_pos_acc: 94.3817, stage5_loss_bbox: 0.1042, stage5_loss_iou: 0.2420, stage5_loss_mask: 0.2744, loss: 8.3934\n",
      "2025-07-16 17:24:43,659 - mmdet - INFO - Epoch [37][200/750]\tlr: 2.500e-05, eta: 2:14:49, time: 0.379, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0151, stage0_pos_acc: 36.1640, stage0_loss_bbox: 0.4691, stage0_loss_iou: 0.9477, stage0_loss_mask: 0.6341, stage1_loss_cls: 0.5693, stage1_pos_acc: 71.9999, stage1_loss_bbox: 0.2032, stage1_loss_iou: 0.4026, stage1_loss_mask: 0.3259, stage2_loss_cls: 0.4639, stage2_pos_acc: 74.9967, stage2_loss_bbox: 0.1523, stage2_loss_iou: 0.2888, stage2_loss_mask: 0.2795, stage3_loss_cls: 0.3309, stage3_pos_acc: 86.0864, stage3_loss_bbox: 0.1315, stage3_loss_iou: 0.2564, stage3_loss_mask: 0.2500, stage4_loss_cls: 0.2471, stage4_pos_acc: 88.6769, stage4_loss_bbox: 0.1266, stage4_loss_iou: 0.2452, stage4_loss_mask: 0.2419, stage5_loss_cls: 0.2011, stage5_pos_acc: 91.9007, stage5_loss_bbox: 0.1275, stage5_loss_iou: 0.2438, stage5_loss_mask: 0.2401, loss: 8.3938\n",
      "2025-07-16 17:25:02,710 - mmdet - INFO - Epoch [37][250/750]\tlr: 2.500e-05, eta: 2:14:31, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0603, stage0_pos_acc: 38.9802, stage0_loss_bbox: 0.5011, stage0_loss_iou: 1.0071, stage0_loss_mask: 0.8270, stage1_loss_cls: 0.5955, stage1_pos_acc: 67.8564, stage1_loss_bbox: 0.2355, stage1_loss_iou: 0.4333, stage1_loss_mask: 0.4893, stage2_loss_cls: 0.4360, stage2_pos_acc: 81.5150, stage2_loss_bbox: 0.1798, stage2_loss_iou: 0.3379, stage2_loss_mask: 0.4513, stage3_loss_cls: 0.3235, stage3_pos_acc: 84.7088, stage3_loss_bbox: 0.1704, stage3_loss_iou: 0.3171, stage3_loss_mask: 0.4442, stage4_loss_cls: 0.2702, stage4_pos_acc: 90.4341, stage4_loss_bbox: 0.1623, stage4_loss_iou: 0.3166, stage4_loss_mask: 0.4378, stage5_loss_cls: 0.2390, stage5_pos_acc: 91.8174, stage5_loss_bbox: 0.1577, stage5_loss_iou: 0.3098, stage5_loss_mask: 0.4375, loss: 10.1402\n",
      "2025-07-16 17:25:21,695 - mmdet - INFO - Epoch [37][300/750]\tlr: 2.500e-05, eta: 2:14:12, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0822, stage0_pos_acc: 36.2024, stage0_loss_bbox: 0.5404, stage0_loss_iou: 1.0198, stage0_loss_mask: 0.8679, stage1_loss_cls: 0.6151, stage1_pos_acc: 69.8452, stage1_loss_bbox: 0.2548, stage1_loss_iou: 0.4481, stage1_loss_mask: 0.5256, stage2_loss_cls: 0.4569, stage2_pos_acc: 79.0905, stage2_loss_bbox: 0.2096, stage2_loss_iou: 0.3426, stage2_loss_mask: 0.4216, stage3_loss_cls: 0.3391, stage3_pos_acc: 84.4821, stage3_loss_bbox: 0.1619, stage3_loss_iou: 0.3144, stage3_loss_mask: 0.4173, stage4_loss_cls: 0.2500, stage4_pos_acc: 89.5738, stage4_loss_bbox: 0.1529, stage4_loss_iou: 0.3067, stage4_loss_mask: 0.3908, stage5_loss_cls: 0.2058, stage5_pos_acc: 94.4167, stage5_loss_bbox: 0.1455, stage5_loss_iou: 0.2976, stage5_loss_mask: 0.3749, loss: 10.1413\n",
      "2025-07-16 17:25:40,940 - mmdet - INFO - Epoch [37][350/750]\tlr: 2.500e-05, eta: 2:13:55, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0098, stage0_pos_acc: 47.4823, stage0_loss_bbox: 0.4644, stage0_loss_iou: 0.8035, stage0_loss_mask: 0.4984, stage1_loss_cls: 0.5476, stage1_pos_acc: 75.8880, stage1_loss_bbox: 0.2161, stage1_loss_iou: 0.3511, stage1_loss_mask: 0.3082, stage2_loss_cls: 0.4362, stage2_pos_acc: 79.9102, stage2_loss_bbox: 0.1749, stage2_loss_iou: 0.2785, stage2_loss_mask: 0.2687, stage3_loss_cls: 0.3101, stage3_pos_acc: 84.1799, stage3_loss_bbox: 0.1617, stage3_loss_iou: 0.2639, stage3_loss_mask: 0.2550, stage4_loss_cls: 0.2266, stage4_pos_acc: 91.3960, stage4_loss_bbox: 0.1602, stage4_loss_iou: 0.2586, stage4_loss_mask: 0.2568, stage5_loss_cls: 0.2036, stage5_pos_acc: 94.7429, stage5_loss_bbox: 0.1559, stage5_loss_iou: 0.2520, stage5_loss_mask: 0.2548, loss: 8.1163\n",
      "2025-07-16 17:26:00,772 - mmdet - INFO - Epoch [37][400/750]\tlr: 2.500e-05, eta: 2:13:38, time: 0.397, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0099, stage0_pos_acc: 39.2964, stage0_loss_bbox: 0.5093, stage0_loss_iou: 1.0055, stage0_loss_mask: 0.8490, stage1_loss_cls: 0.5661, stage1_pos_acc: 69.0944, stage1_loss_bbox: 0.2196, stage1_loss_iou: 0.4633, stage1_loss_mask: 0.4797, stage2_loss_cls: 0.4349, stage2_pos_acc: 79.4980, stage2_loss_bbox: 0.1735, stage2_loss_iou: 0.3465, stage2_loss_mask: 0.4248, stage3_loss_cls: 0.3480, stage3_pos_acc: 84.8975, stage3_loss_bbox: 0.1542, stage3_loss_iou: 0.3106, stage3_loss_mask: 0.4019, stage4_loss_cls: 0.2751, stage4_pos_acc: 91.1647, stage4_loss_bbox: 0.1586, stage4_loss_iou: 0.3069, stage4_loss_mask: 0.3924, stage5_loss_cls: 0.2530, stage5_pos_acc: 91.4647, stage5_loss_bbox: 0.1545, stage5_loss_iou: 0.2976, stage5_loss_mask: 0.3843, loss: 9.9190\n",
      "2025-07-16 17:26:20,368 - mmdet - INFO - Epoch [37][450/750]\tlr: 2.500e-05, eta: 2:13:20, time: 0.392, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9956, stage0_pos_acc: 44.0489, stage0_loss_bbox: 0.5014, stage0_loss_iou: 1.0927, stage0_loss_mask: 0.9352, stage1_loss_cls: 0.5655, stage1_pos_acc: 68.5393, stage1_loss_bbox: 0.1942, stage1_loss_iou: 0.4747, stage1_loss_mask: 0.6189, stage2_loss_cls: 0.4436, stage2_pos_acc: 77.5339, stage2_loss_bbox: 0.1504, stage2_loss_iou: 0.3626, stage2_loss_mask: 0.5704, stage3_loss_cls: 0.3192, stage3_pos_acc: 84.8213, stage3_loss_bbox: 0.1383, stage3_loss_iou: 0.3420, stage3_loss_mask: 0.5547, stage4_loss_cls: 0.2389, stage4_pos_acc: 88.6230, stage4_loss_bbox: 0.1349, stage4_loss_iou: 0.3366, stage4_loss_mask: 0.5356, stage5_loss_cls: 0.2162, stage5_pos_acc: 93.2310, stage5_loss_bbox: 0.1388, stage5_loss_iou: 0.3347, stage5_loss_mask: 0.5360, loss: 10.7310\n",
      "2025-07-16 17:26:39,959 - mmdet - INFO - Epoch [37][500/750]\tlr: 2.500e-05, eta: 2:13:03, time: 0.392, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0153, stage0_pos_acc: 39.9143, stage0_loss_bbox: 0.4697, stage0_loss_iou: 0.9664, stage0_loss_mask: 0.8421, stage1_loss_cls: 0.5857, stage1_pos_acc: 71.6866, stage1_loss_bbox: 0.1966, stage1_loss_iou: 0.4443, stage1_loss_mask: 0.4833, stage2_loss_cls: 0.4513, stage2_pos_acc: 78.7510, stage2_loss_bbox: 0.1678, stage2_loss_iou: 0.3527, stage2_loss_mask: 0.4299, stage3_loss_cls: 0.3622, stage3_pos_acc: 83.8658, stage3_loss_bbox: 0.1576, stage3_loss_iou: 0.3222, stage3_loss_mask: 0.4195, stage4_loss_cls: 0.3058, stage4_pos_acc: 86.0459, stage4_loss_bbox: 0.1488, stage4_loss_iou: 0.3007, stage4_loss_mask: 0.3912, stage5_loss_cls: 0.2911, stage5_pos_acc: 89.3202, stage5_loss_bbox: 0.1483, stage5_loss_iou: 0.2928, stage5_loss_mask: 0.3946, loss: 9.9400\n",
      "2025-07-16 17:26:59,719 - mmdet - INFO - Epoch [37][550/750]\tlr: 2.500e-05, eta: 2:12:46, time: 0.395, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0096, stage0_pos_acc: 41.4067, stage0_loss_bbox: 0.4912, stage0_loss_iou: 0.9226, stage0_loss_mask: 0.6204, stage1_loss_cls: 0.5729, stage1_pos_acc: 70.1757, stage1_loss_bbox: 0.2162, stage1_loss_iou: 0.3833, stage1_loss_mask: 0.2786, stage2_loss_cls: 0.4451, stage2_pos_acc: 79.1644, stage2_loss_bbox: 0.1552, stage2_loss_iou: 0.2803, stage2_loss_mask: 0.2691, stage3_loss_cls: 0.3020, stage3_pos_acc: 85.5310, stage3_loss_bbox: 0.1354, stage3_loss_iou: 0.2542, stage3_loss_mask: 0.2531, stage4_loss_cls: 0.2409, stage4_pos_acc: 89.0144, stage4_loss_bbox: 0.1355, stage4_loss_iou: 0.2416, stage4_loss_mask: 0.2538, stage5_loss_cls: 0.2130, stage5_pos_acc: 94.2885, stage5_loss_bbox: 0.1406, stage5_loss_iou: 0.2408, stage5_loss_mask: 0.2631, loss: 8.3184\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 101s, ETA:     0s2025-07-16 17:31:36,197 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.37s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.394\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.394\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.160\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.182\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.440\n",
      "2025-07-16 17:31:38,106 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.39s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.89s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.406\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.406\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.190\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.454\n",
      "2025-07-16 17:31:41,371 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:31:41,371 - mmdet - INFO - Epoch(val) [37][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0520, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.0160, bbox_mAP_m: 0.0060, bbox_mAP_l: 0.0320, bbox_mAP_copypaste: 0.025 0.052 0.023 0.016 0.006 0.032, segm_mAP: 0.0260, segm_mAP_50: 0.0510, segm_mAP_75: 0.0230, segm_mAP_s: 0.0240, segm_mAP_m: 0.0060, segm_mAP_l: 0.0330, segm_mAP_copypaste: 0.026 0.051 0.023 0.024 0.006 0.033\n",
      "2025-07-16 17:32:03,166 - mmdet - INFO - Epoch [38][50/750]\tlr: 2.500e-05, eta: 2:11:19, time: 0.436, data_time: 0.055, memory: 11264, stage0_loss_cls: 1.0272, stage0_pos_acc: 41.4889, stage0_loss_bbox: 0.4719, stage0_loss_iou: 0.8768, stage0_loss_mask: 0.5613, stage1_loss_cls: 0.5578, stage1_pos_acc: 73.6372, stage1_loss_bbox: 0.2086, stage1_loss_iou: 0.3882, stage1_loss_mask: 0.3679, stage2_loss_cls: 0.3993, stage2_pos_acc: 84.8359, stage2_loss_bbox: 0.1530, stage2_loss_iou: 0.2859, stage2_loss_mask: 0.2909, stage3_loss_cls: 0.2549, stage3_pos_acc: 89.2434, stage3_loss_bbox: 0.1381, stage3_loss_iou: 0.2621, stage3_loss_mask: 0.3010, stage4_loss_cls: 0.1842, stage4_pos_acc: 94.2846, stage4_loss_bbox: 0.1336, stage4_loss_iou: 0.2529, stage4_loss_mask: 0.2913, stage5_loss_cls: 0.1386, stage5_pos_acc: 97.2527, stage5_loss_bbox: 0.1333, stage5_loss_iou: 0.2502, stage5_loss_mask: 0.3225, loss: 8.2516\n",
      "2025-07-16 17:32:22,453 - mmdet - INFO - Epoch [38][100/750]\tlr: 2.500e-05, eta: 2:11:02, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9931, stage0_pos_acc: 34.6405, stage0_loss_bbox: 0.5065, stage0_loss_iou: 0.8890, stage0_loss_mask: 0.5004, stage1_loss_cls: 0.5289, stage1_pos_acc: 76.0667, stage1_loss_bbox: 0.2323, stage1_loss_iou: 0.4027, stage1_loss_mask: 0.3985, stage2_loss_cls: 0.4102, stage2_pos_acc: 84.4444, stage2_loss_bbox: 0.1770, stage2_loss_iou: 0.3078, stage2_loss_mask: 0.3803, stage3_loss_cls: 0.2931, stage3_pos_acc: 92.3571, stage3_loss_bbox: 0.1594, stage3_loss_iou: 0.2814, stage3_loss_mask: 0.3416, stage4_loss_cls: 0.2125, stage4_pos_acc: 93.9167, stage4_loss_bbox: 0.1679, stage4_loss_iou: 0.2869, stage4_loss_mask: 0.3721, stage5_loss_cls: 0.1943, stage5_pos_acc: 96.6786, stage5_loss_bbox: 0.1643, stage5_loss_iou: 0.2787, stage5_loss_mask: 0.3722, loss: 8.8511\n",
      "2025-07-16 17:32:41,695 - mmdet - INFO - Epoch [38][150/750]\tlr: 2.500e-05, eta: 2:10:43, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0570, stage0_pos_acc: 34.5619, stage0_loss_bbox: 0.4701, stage0_loss_iou: 1.0298, stage0_loss_mask: 0.7460, stage1_loss_cls: 0.5941, stage1_pos_acc: 69.7238, stage1_loss_bbox: 0.1878, stage1_loss_iou: 0.4486, stage1_loss_mask: 0.4350, stage2_loss_cls: 0.4533, stage2_pos_acc: 79.0976, stage2_loss_bbox: 0.1430, stage2_loss_iou: 0.3270, stage2_loss_mask: 0.3738, stage3_loss_cls: 0.3345, stage3_pos_acc: 85.3548, stage3_loss_bbox: 0.1284, stage3_loss_iou: 0.2886, stage3_loss_mask: 0.3610, stage4_loss_cls: 0.2353, stage4_pos_acc: 90.9262, stage4_loss_bbox: 0.1184, stage4_loss_iou: 0.2744, stage4_loss_mask: 0.3455, stage5_loss_cls: 0.2112, stage5_pos_acc: 91.3619, stage5_loss_bbox: 0.1158, stage5_loss_iou: 0.2697, stage5_loss_mask: 0.3396, loss: 9.2880\n",
      "2025-07-16 17:33:01,008 - mmdet - INFO - Epoch [38][200/750]\tlr: 2.500e-05, eta: 2:10:25, time: 0.386, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9989, stage0_pos_acc: 37.8511, stage0_loss_bbox: 0.4886, stage0_loss_iou: 0.9222, stage0_loss_mask: 0.8141, stage1_loss_cls: 0.5882, stage1_pos_acc: 66.5424, stage1_loss_bbox: 0.2413, stage1_loss_iou: 0.4747, stage1_loss_mask: 0.6014, stage2_loss_cls: 0.4403, stage2_pos_acc: 79.9831, stage2_loss_bbox: 0.1821, stage2_loss_iou: 0.3771, stage2_loss_mask: 0.5542, stage3_loss_cls: 0.3427, stage3_pos_acc: 85.3794, stage3_loss_bbox: 0.1705, stage3_loss_iou: 0.3552, stage3_loss_mask: 0.5415, stage4_loss_cls: 0.2825, stage4_pos_acc: 89.3230, stage4_loss_bbox: 0.1655, stage4_loss_iou: 0.3460, stage4_loss_mask: 0.5081, stage5_loss_cls: 0.2528, stage5_pos_acc: 91.3801, stage5_loss_bbox: 0.1606, stage5_loss_iou: 0.3379, stage5_loss_mask: 0.5069, loss: 10.6533\n",
      "2025-07-16 17:33:20,450 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:33:20,450 - mmdet - INFO - Epoch [38][250/750]\tlr: 2.500e-05, eta: 2:10:08, time: 0.389, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9838, stage0_pos_acc: 44.7514, stage0_loss_bbox: 0.4951, stage0_loss_iou: 0.9230, stage0_loss_mask: 0.8995, stage1_loss_cls: 0.5631, stage1_pos_acc: 75.0751, stage1_loss_bbox: 0.2176, stage1_loss_iou: 0.4352, stage1_loss_mask: 0.5014, stage2_loss_cls: 0.4322, stage2_pos_acc: 80.4361, stage2_loss_bbox: 0.1748, stage2_loss_iou: 0.3539, stage2_loss_mask: 0.4424, stage3_loss_cls: 0.3286, stage3_pos_acc: 83.2088, stage3_loss_bbox: 0.1601, stage3_loss_iou: 0.3269, stage3_loss_mask: 0.4354, stage4_loss_cls: 0.2447, stage4_pos_acc: 90.0545, stage4_loss_bbox: 0.1545, stage4_loss_iou: 0.3163, stage4_loss_mask: 0.4189, stage5_loss_cls: 0.2142, stage5_pos_acc: 92.5444, stage5_loss_bbox: 0.1481, stage5_loss_iou: 0.3121, stage5_loss_mask: 0.4277, loss: 9.9094\n",
      "2025-07-16 17:33:39,751 - mmdet - INFO - Epoch [38][300/750]\tlr: 2.500e-05, eta: 2:09:50, time: 0.386, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0127, stage0_pos_acc: 38.5801, stage0_loss_bbox: 0.4527, stage0_loss_iou: 0.8921, stage0_loss_mask: 0.6779, stage1_loss_cls: 0.5691, stage1_pos_acc: 71.8505, stage1_loss_bbox: 0.2013, stage1_loss_iou: 0.3686, stage1_loss_mask: 0.3419, stage2_loss_cls: 0.4153, stage2_pos_acc: 80.8473, stage2_loss_bbox: 0.1605, stage2_loss_iou: 0.2827, stage2_loss_mask: 0.3153, stage3_loss_cls: 0.3029, stage3_pos_acc: 87.0891, stage3_loss_bbox: 0.1400, stage3_loss_iou: 0.2571, stage3_loss_mask: 0.3017, stage4_loss_cls: 0.2319, stage4_pos_acc: 91.3796, stage4_loss_bbox: 0.1372, stage4_loss_iou: 0.2450, stage4_loss_mask: 0.2903, stage5_loss_cls: 0.1995, stage5_pos_acc: 93.3128, stage5_loss_bbox: 0.1351, stage5_loss_iou: 0.2401, stage5_loss_mask: 0.2887, loss: 8.4596\n",
      "2025-07-16 17:33:59,042 - mmdet - INFO - Epoch [38][350/750]\tlr: 2.500e-05, eta: 2:09:32, time: 0.386, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9640, stage0_pos_acc: 42.7707, stage0_loss_bbox: 0.4471, stage0_loss_iou: 0.9255, stage0_loss_mask: 0.6123, stage1_loss_cls: 0.5612, stage1_pos_acc: 70.6196, stage1_loss_bbox: 0.2025, stage1_loss_iou: 0.4048, stage1_loss_mask: 0.3949, stage2_loss_cls: 0.4340, stage2_pos_acc: 80.6806, stage2_loss_bbox: 0.1507, stage2_loss_iou: 0.3125, stage2_loss_mask: 0.3981, stage3_loss_cls: 0.3266, stage3_pos_acc: 85.1865, stage3_loss_bbox: 0.1425, stage3_loss_iou: 0.2756, stage3_loss_mask: 0.3612, stage4_loss_cls: 0.2699, stage4_pos_acc: 90.8311, stage4_loss_bbox: 0.1310, stage4_loss_iou: 0.2608, stage4_loss_mask: 0.3303, stage5_loss_cls: 0.2511, stage5_pos_acc: 91.5434, stage5_loss_bbox: 0.1258, stage5_loss_iou: 0.2569, stage5_loss_mask: 0.3352, loss: 8.8747\n",
      "2025-07-16 17:34:18,271 - mmdet - INFO - Epoch [38][400/750]\tlr: 2.500e-05, eta: 2:09:14, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9988, stage0_pos_acc: 44.4160, stage0_loss_bbox: 0.4809, stage0_loss_iou: 1.0112, stage0_loss_mask: 0.7541, stage1_loss_cls: 0.5627, stage1_pos_acc: 75.2663, stage1_loss_bbox: 0.2166, stage1_loss_iou: 0.4290, stage1_loss_mask: 0.4505, stage2_loss_cls: 0.4175, stage2_pos_acc: 83.1294, stage2_loss_bbox: 0.1635, stage2_loss_iou: 0.3351, stage2_loss_mask: 0.4061, stage3_loss_cls: 0.2786, stage3_pos_acc: 89.0500, stage3_loss_bbox: 0.1554, stage3_loss_iou: 0.3105, stage3_loss_mask: 0.3917, stage4_loss_cls: 0.2209, stage4_pos_acc: 91.2857, stage4_loss_bbox: 0.1415, stage4_loss_iou: 0.2974, stage4_loss_mask: 0.3404, stage5_loss_cls: 0.1896, stage5_pos_acc: 93.2473, stage5_loss_bbox: 0.1362, stage5_loss_iou: 0.2907, stage5_loss_mask: 0.3540, loss: 9.3330\n",
      "2025-07-16 17:34:37,400 - mmdet - INFO - Epoch [38][450/750]\tlr: 2.500e-05, eta: 2:08:55, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0311, stage0_pos_acc: 36.7732, stage0_loss_bbox: 0.4916, stage0_loss_iou: 1.0025, stage0_loss_mask: 0.8130, stage1_loss_cls: 0.5760, stage1_pos_acc: 75.2708, stage1_loss_bbox: 0.1945, stage1_loss_iou: 0.4056, stage1_loss_mask: 0.3316, stage2_loss_cls: 0.4032, stage2_pos_acc: 80.7736, stage2_loss_bbox: 0.1439, stage2_loss_iou: 0.3087, stage2_loss_mask: 0.3301, stage3_loss_cls: 0.2908, stage3_pos_acc: 86.8018, stage3_loss_bbox: 0.1279, stage3_loss_iou: 0.2788, stage3_loss_mask: 0.3084, stage4_loss_cls: 0.2104, stage4_pos_acc: 91.0304, stage4_loss_bbox: 0.1229, stage4_loss_iou: 0.2719, stage4_loss_mask: 0.3320, stage5_loss_cls: 0.1786, stage5_pos_acc: 94.7971, stage5_loss_bbox: 0.1174, stage5_loss_iou: 0.2601, stage5_loss_mask: 0.3131, loss: 8.8441\n",
      "2025-07-16 17:34:56,489 - mmdet - INFO - Epoch [38][500/750]\tlr: 2.500e-05, eta: 2:08:37, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9940, stage0_pos_acc: 33.3758, stage0_loss_bbox: 0.4788, stage0_loss_iou: 1.0041, stage0_loss_mask: 0.7018, stage1_loss_cls: 0.5783, stage1_pos_acc: 67.7828, stage1_loss_bbox: 0.2027, stage1_loss_iou: 0.4391, stage1_loss_mask: 0.4533, stage2_loss_cls: 0.4414, stage2_pos_acc: 78.4641, stage2_loss_bbox: 0.1708, stage2_loss_iou: 0.3483, stage2_loss_mask: 0.4097, stage3_loss_cls: 0.3207, stage3_pos_acc: 87.4227, stage3_loss_bbox: 0.1524, stage3_loss_iou: 0.3094, stage3_loss_mask: 0.3634, stage4_loss_cls: 0.2314, stage4_pos_acc: 88.1394, stage4_loss_bbox: 0.1505, stage4_loss_iou: 0.2956, stage4_loss_mask: 0.3767, stage5_loss_cls: 0.1943, stage5_pos_acc: 91.5924, stage5_loss_bbox: 0.1454, stage5_loss_iou: 0.2920, stage5_loss_mask: 0.3625, loss: 9.4164\n",
      "2025-07-16 17:35:15,892 - mmdet - INFO - Epoch [38][550/750]\tlr: 2.500e-05, eta: 2:08:19, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0028, stage0_pos_acc: 34.0845, stage0_loss_bbox: 0.5168, stage0_loss_iou: 0.9756, stage0_loss_mask: 0.6983, stage1_loss_cls: 0.5903, stage1_pos_acc: 71.0842, stage1_loss_bbox: 0.2037, stage1_loss_iou: 0.4204, stage1_loss_mask: 0.3271, stage2_loss_cls: 0.4310, stage2_pos_acc: 83.5463, stage2_loss_bbox: 0.1657, stage2_loss_iou: 0.3060, stage2_loss_mask: 0.2857, stage3_loss_cls: 0.2989, stage3_pos_acc: 88.9044, stage3_loss_bbox: 0.1579, stage3_loss_iou: 0.2721, stage3_loss_mask: 0.2818, stage4_loss_cls: 0.2046, stage4_pos_acc: 94.1237, stage4_loss_bbox: 0.1512, stage4_loss_iou: 0.2656, stage4_loss_mask: 0.2874, stage5_loss_cls: 0.1791, stage5_pos_acc: 95.8727, stage5_loss_bbox: 0.1498, stage5_loss_iou: 0.2581, stage5_loss_mask: 0.2867, loss: 8.7165\n",
      "2025-07-16 17:35:35,142 - mmdet - INFO - Epoch [38][600/750]\tlr: 2.500e-05, eta: 2:08:01, time: 0.385, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9905, stage0_pos_acc: 38.8849, stage0_loss_bbox: 0.5169, stage0_loss_iou: 1.0017, stage0_loss_mask: 0.7698, stage1_loss_cls: 0.5622, stage1_pos_acc: 68.9048, stage1_loss_bbox: 0.2422, stage1_loss_iou: 0.4202, stage1_loss_mask: 0.3894, stage2_loss_cls: 0.4303, stage2_pos_acc: 78.3238, stage2_loss_bbox: 0.1855, stage2_loss_iou: 0.3074, stage2_loss_mask: 0.3486, stage3_loss_cls: 0.3082, stage3_pos_acc: 84.0040, stage3_loss_bbox: 0.1623, stage3_loss_iou: 0.2676, stage3_loss_mask: 0.2958, stage4_loss_cls: 0.2303, stage4_pos_acc: 91.7310, stage4_loss_bbox: 0.1481, stage4_loss_iou: 0.2489, stage4_loss_mask: 0.2754, stage5_loss_cls: 0.1970, stage5_pos_acc: 92.7722, stage5_loss_bbox: 0.1538, stage5_loss_iou: 0.2418, stage5_loss_mask: 0.2930, loss: 8.9869\n",
      "2025-07-16 17:35:54,425 - mmdet - INFO - Epoch [38][650/750]\tlr: 2.500e-05, eta: 2:07:43, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0047, stage0_pos_acc: 38.3826, stage0_loss_bbox: 0.4803, stage0_loss_iou: 0.9669, stage0_loss_mask: 0.6568, stage1_loss_cls: 0.5452, stage1_pos_acc: 68.3681, stage1_loss_bbox: 0.2203, stage1_loss_iou: 0.4169, stage1_loss_mask: 0.4112, stage2_loss_cls: 0.4081, stage2_pos_acc: 82.4266, stage2_loss_bbox: 0.1845, stage2_loss_iou: 0.3228, stage2_loss_mask: 0.3744, stage3_loss_cls: 0.2927, stage3_pos_acc: 87.6807, stage3_loss_bbox: 0.1650, stage3_loss_iou: 0.2911, stage3_loss_mask: 0.3670, stage4_loss_cls: 0.2137, stage4_pos_acc: 93.1649, stage4_loss_bbox: 0.1476, stage4_loss_iou: 0.2800, stage4_loss_mask: 0.3385, stage5_loss_cls: 0.1832, stage5_pos_acc: 93.7633, stage5_loss_bbox: 0.1530, stage5_loss_iou: 0.2781, stage5_loss_mask: 0.3494, loss: 9.0514\n",
      "2025-07-16 17:36:13,935 - mmdet - INFO - Epoch [38][700/750]\tlr: 2.500e-05, eta: 2:07:25, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0381, stage0_pos_acc: 36.6303, stage0_loss_bbox: 0.4877, stage0_loss_iou: 1.0109, stage0_loss_mask: 0.7887, stage1_loss_cls: 0.6146, stage1_pos_acc: 70.1536, stage1_loss_bbox: 0.1935, stage1_loss_iou: 0.4573, stage1_loss_mask: 0.4519, stage2_loss_cls: 0.4580, stage2_pos_acc: 78.1358, stage2_loss_bbox: 0.1517, stage2_loss_iou: 0.3535, stage2_loss_mask: 0.3586, stage3_loss_cls: 0.3545, stage3_pos_acc: 84.8666, stage3_loss_bbox: 0.1448, stage3_loss_iou: 0.3290, stage3_loss_mask: 0.3522, stage4_loss_cls: 0.2842, stage4_pos_acc: 87.4712, stage4_loss_bbox: 0.1400, stage4_loss_iou: 0.3063, stage4_loss_mask: 0.3340, stage5_loss_cls: 0.2426, stage5_pos_acc: 91.4519, stage5_loss_bbox: 0.1397, stage5_loss_iou: 0.3016, stage5_loss_mask: 0.3410, loss: 9.6347\n",
      "2025-07-16 17:36:33,190 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:36:33,190 - mmdet - INFO - Epoch [38][750/750]\tlr: 2.500e-05, eta: 2:07:07, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0537, stage0_pos_acc: 36.5680, stage0_loss_bbox: 0.5190, stage0_loss_iou: 1.0311, stage0_loss_mask: 0.6648, stage1_loss_cls: 0.5845, stage1_pos_acc: 72.4378, stage1_loss_bbox: 0.2050, stage1_loss_iou: 0.4391, stage1_loss_mask: 0.4446, stage2_loss_cls: 0.4470, stage2_pos_acc: 77.7474, stage2_loss_bbox: 0.1562, stage2_loss_iou: 0.3329, stage2_loss_mask: 0.3813, stage3_loss_cls: 0.3221, stage3_pos_acc: 86.7053, stage3_loss_bbox: 0.1484, stage3_loss_iou: 0.3096, stage3_loss_mask: 0.3707, stage4_loss_cls: 0.2413, stage4_pos_acc: 92.4109, stage4_loss_bbox: 0.1380, stage4_loss_iou: 0.2961, stage4_loss_mask: 0.3708, stage5_loss_cls: 0.2132, stage5_pos_acc: 94.9157, stage5_loss_bbox: 0.1325, stage5_loss_iou: 0.2891, stage5_loss_mask: 0.3746, loss: 9.4655\n",
      "2025-07-16 17:36:33,323 - mmdet - INFO - Saving checkpoint at 38 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 101s, ETA:     0s2025-07-16 17:39:53,292 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.38s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.355\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.360\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.360\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.194\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.395\n",
      "2025-07-16 17:39:55,205 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.32s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.85s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.005\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.006\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.364\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.368\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.368\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.214\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.400\n",
      "2025-07-16 17:39:58,302 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:39:58,303 - mmdet - INFO - Epoch(val) [38][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0470, bbox_mAP_75: 0.0220, bbox_mAP_s: 0.0080, bbox_mAP_m: 0.0060, bbox_mAP_l: 0.0320, bbox_mAP_copypaste: 0.025 0.047 0.022 0.008 0.006 0.032, segm_mAP: 0.0250, segm_mAP_50: 0.0470, segm_mAP_75: 0.0240, segm_mAP_s: 0.0050, segm_mAP_m: 0.0060, segm_mAP_l: 0.0330, segm_mAP_copypaste: 0.025 0.047 0.024 0.005 0.006 0.033\n",
      "2025-07-16 17:40:19,810 - mmdet - INFO - Epoch [39][50/750]\tlr: 2.500e-05, eta: 2:06:52, time: 0.430, data_time: 0.056, memory: 11264, stage0_loss_cls: 1.0000, stage0_pos_acc: 38.4262, stage0_loss_bbox: 0.5064, stage0_loss_iou: 1.0044, stage0_loss_mask: 0.5924, stage1_loss_cls: 0.5533, stage1_pos_acc: 70.7302, stage1_loss_bbox: 0.2074, stage1_loss_iou: 0.3979, stage1_loss_mask: 0.3892, stage2_loss_cls: 0.3995, stage2_pos_acc: 83.4381, stage2_loss_bbox: 0.1598, stage2_loss_iou: 0.2996, stage2_loss_mask: 0.3251, stage3_loss_cls: 0.2847, stage3_pos_acc: 88.5285, stage3_loss_bbox: 0.1492, stage3_loss_iou: 0.2818, stage3_loss_mask: 0.3112, stage4_loss_cls: 0.2104, stage4_pos_acc: 92.7868, stage4_loss_bbox: 0.1462, stage4_loss_iou: 0.2726, stage4_loss_mask: 0.3152, stage5_loss_cls: 0.1794, stage5_pos_acc: 93.0180, stage5_loss_bbox: 0.1426, stage5_loss_iou: 0.2636, stage5_loss_mask: 0.3078, loss: 8.6996\n",
      "2025-07-16 17:40:39,238 - mmdet - INFO - Epoch [39][100/750]\tlr: 2.500e-05, eta: 2:06:34, time: 0.389, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0097, stage0_pos_acc: 44.4020, stage0_loss_bbox: 0.5306, stage0_loss_iou: 0.9255, stage0_loss_mask: 0.6957, stage1_loss_cls: 0.5572, stage1_pos_acc: 70.1802, stage1_loss_bbox: 0.2489, stage1_loss_iou: 0.4334, stage1_loss_mask: 0.4342, stage2_loss_cls: 0.4393, stage2_pos_acc: 80.8628, stage2_loss_bbox: 0.2079, stage2_loss_iou: 0.3364, stage2_loss_mask: 0.3776, stage3_loss_cls: 0.3040, stage3_pos_acc: 85.6691, stage3_loss_bbox: 0.2009, stage3_loss_iou: 0.3129, stage3_loss_mask: 0.3692, stage4_loss_cls: 0.2417, stage4_pos_acc: 90.3506, stage4_loss_bbox: 0.1844, stage4_loss_iou: 0.3122, stage4_loss_mask: 0.3765, stage5_loss_cls: 0.2184, stage5_pos_acc: 91.4220, stage5_loss_bbox: 0.1813, stage5_loss_iou: 0.3055, stage5_loss_mask: 0.3764, loss: 9.5798\n",
      "2025-07-16 17:40:58,641 - mmdet - INFO - Epoch [39][150/750]\tlr: 2.500e-05, eta: 2:06:16, time: 0.388, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0025, stage0_pos_acc: 41.9198, stage0_loss_bbox: 0.4372, stage0_loss_iou: 0.9367, stage0_loss_mask: 0.6479, stage1_loss_cls: 0.5501, stage1_pos_acc: 71.9024, stage1_loss_bbox: 0.1973, stage1_loss_iou: 0.4097, stage1_loss_mask: 0.4475, stage2_loss_cls: 0.4004, stage2_pos_acc: 81.5817, stage2_loss_bbox: 0.1526, stage2_loss_iou: 0.3340, stage2_loss_mask: 0.4173, stage3_loss_cls: 0.2661, stage3_pos_acc: 88.2825, stage3_loss_bbox: 0.1327, stage3_loss_iou: 0.3001, stage3_loss_mask: 0.3912, stage4_loss_cls: 0.1780, stage4_pos_acc: 93.2881, stage4_loss_bbox: 0.1389, stage4_loss_iou: 0.2945, stage4_loss_mask: 0.4016, stage5_loss_cls: 0.1494, stage5_pos_acc: 95.5881, stage5_loss_bbox: 0.1299, stage5_loss_iou: 0.2862, stage5_loss_mask: 0.4013, loss: 9.0032\n",
      "2025-07-16 17:41:17,632 - mmdet - INFO - Epoch [39][200/750]\tlr: 2.500e-05, eta: 2:05:58, time: 0.380, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0264, stage0_pos_acc: 37.2702, stage0_loss_bbox: 0.4623, stage0_loss_iou: 0.9471, stage0_loss_mask: 0.5707, stage1_loss_cls: 0.5892, stage1_pos_acc: 69.3863, stage1_loss_bbox: 0.1904, stage1_loss_iou: 0.4029, stage1_loss_mask: 0.3259, stage2_loss_cls: 0.4171, stage2_pos_acc: 80.4461, stage2_loss_bbox: 0.1418, stage2_loss_iou: 0.3083, stage2_loss_mask: 0.3142, stage3_loss_cls: 0.2810, stage3_pos_acc: 86.5439, stage3_loss_bbox: 0.1275, stage3_loss_iou: 0.2796, stage3_loss_mask: 0.3108, stage4_loss_cls: 0.1989, stage4_pos_acc: 90.3877, stage4_loss_bbox: 0.1237, stage4_loss_iou: 0.2653, stage4_loss_mask: 0.3062, stage5_loss_cls: 0.1766, stage5_pos_acc: 92.7599, stage5_loss_bbox: 0.1254, stage5_loss_iou: 0.2596, stage5_loss_mask: 0.2928, loss: 8.4436\n",
      "2025-07-16 17:41:36,856 - mmdet - INFO - Epoch [39][250/750]\tlr: 2.500e-05, eta: 2:05:39, time: 0.384, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9991, stage0_pos_acc: 36.0173, stage0_loss_bbox: 0.4978, stage0_loss_iou: 0.9895, stage0_loss_mask: 0.8228, stage1_loss_cls: 0.5686, stage1_pos_acc: 72.0251, stage1_loss_bbox: 0.2322, stage1_loss_iou: 0.4631, stage1_loss_mask: 0.6143, stage2_loss_cls: 0.4462, stage2_pos_acc: 78.3922, stage2_loss_bbox: 0.1929, stage2_loss_iou: 0.3732, stage2_loss_mask: 0.5238, stage3_loss_cls: 0.3448, stage3_pos_acc: 84.9828, stage3_loss_bbox: 0.1746, stage3_loss_iou: 0.3551, stage3_loss_mask: 0.5119, stage4_loss_cls: 0.2785, stage4_pos_acc: 89.8728, stage4_loss_bbox: 0.1745, stage4_loss_iou: 0.3412, stage4_loss_mask: 0.4876, stage5_loss_cls: 0.2451, stage5_pos_acc: 90.2971, stage5_loss_bbox: 0.1707, stage5_loss_iou: 0.3346, stage5_loss_mask: 0.4963, loss: 10.6384\n",
      "2025-07-16 17:41:56,294 - mmdet - INFO - Epoch [39][300/750]\tlr: 2.500e-05, eta: 2:05:21, time: 0.389, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0118, stage0_pos_acc: 41.3167, stage0_loss_bbox: 0.5047, stage0_loss_iou: 1.0495, stage0_loss_mask: 1.0150, stage1_loss_cls: 0.5839, stage1_pos_acc: 69.0476, stage1_loss_bbox: 0.1983, stage1_loss_iou: 0.4964, stage1_loss_mask: 0.5827, stage2_loss_cls: 0.4362, stage2_pos_acc: 79.6825, stage2_loss_bbox: 0.1582, stage2_loss_iou: 0.3985, stage2_loss_mask: 0.5547, stage3_loss_cls: 0.3409, stage3_pos_acc: 84.0167, stage3_loss_bbox: 0.1436, stage3_loss_iou: 0.3619, stage3_loss_mask: 0.5309, stage4_loss_cls: 0.2523, stage4_pos_acc: 90.2405, stage4_loss_bbox: 0.1354, stage4_loss_iou: 0.3494, stage4_loss_mask: 0.5356, stage5_loss_cls: 0.2046, stage5_pos_acc: 92.1159, stage5_loss_bbox: 0.1342, stage5_loss_iou: 0.3441, stage5_loss_mask: 0.5364, loss: 10.8593\n",
      "2025-07-16 17:42:15,608 - mmdet - INFO - Epoch [39][350/750]\tlr: 2.500e-05, eta: 2:05:03, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0061, stage0_pos_acc: 39.5153, stage0_loss_bbox: 0.4685, stage0_loss_iou: 0.9244, stage0_loss_mask: 0.5763, stage1_loss_cls: 0.6053, stage1_pos_acc: 72.6638, stage1_loss_bbox: 0.1853, stage1_loss_iou: 0.3517, stage1_loss_mask: 0.2702, stage2_loss_cls: 0.4206, stage2_pos_acc: 80.3372, stage2_loss_bbox: 0.1417, stage2_loss_iou: 0.2656, stage2_loss_mask: 0.2596, stage3_loss_cls: 0.3090, stage3_pos_acc: 84.5679, stage3_loss_bbox: 0.1279, stage3_loss_iou: 0.2502, stage3_loss_mask: 0.2539, stage4_loss_cls: 0.2298, stage4_pos_acc: 90.4051, stage4_loss_bbox: 0.1203, stage4_loss_iou: 0.2391, stage4_loss_mask: 0.2431, stage5_loss_cls: 0.1996, stage5_pos_acc: 94.0202, stage5_loss_bbox: 0.1201, stage5_loss_iou: 0.2376, stage5_loss_mask: 0.2460, loss: 8.0519\n",
      "2025-07-16 17:42:35,133 - mmdet - INFO - Epoch [39][400/750]\tlr: 2.500e-05, eta: 2:04:45, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0323, stage0_pos_acc: 38.7033, stage0_loss_bbox: 0.5043, stage0_loss_iou: 1.0076, stage0_loss_mask: 0.8484, stage1_loss_cls: 0.5569, stage1_pos_acc: 76.3920, stage1_loss_bbox: 0.1941, stage1_loss_iou: 0.4160, stage1_loss_mask: 0.4836, stage2_loss_cls: 0.4018, stage2_pos_acc: 83.1325, stage2_loss_bbox: 0.1565, stage2_loss_iou: 0.3244, stage2_loss_mask: 0.4058, stage3_loss_cls: 0.2856, stage3_pos_acc: 86.3730, stage3_loss_bbox: 0.1558, stage3_loss_iou: 0.3042, stage3_loss_mask: 0.4061, stage4_loss_cls: 0.2187, stage4_pos_acc: 92.4128, stage4_loss_bbox: 0.1504, stage4_loss_iou: 0.2946, stage4_loss_mask: 0.3771, stage5_loss_cls: 0.1893, stage5_pos_acc: 93.7780, stage5_loss_bbox: 0.1492, stage5_loss_iou: 0.2919, stage5_loss_mask: 0.3743, loss: 9.5291\n",
      "2025-07-16 17:42:54,325 - mmdet - INFO - Epoch [39][450/750]\tlr: 2.500e-05, eta: 2:04:27, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9850, stage0_pos_acc: 41.2831, stage0_loss_bbox: 0.4986, stage0_loss_iou: 0.9039, stage0_loss_mask: 0.7011, stage1_loss_cls: 0.5598, stage1_pos_acc: 69.8609, stage1_loss_bbox: 0.2348, stage1_loss_iou: 0.4105, stage1_loss_mask: 0.3725, stage2_loss_cls: 0.4393, stage2_pos_acc: 77.3141, stage2_loss_bbox: 0.1746, stage2_loss_iou: 0.3063, stage2_loss_mask: 0.2909, stage3_loss_cls: 0.3097, stage3_pos_acc: 84.9840, stage3_loss_bbox: 0.1649, stage3_loss_iou: 0.2851, stage3_loss_mask: 0.3131, stage4_loss_cls: 0.2545, stage4_pos_acc: 86.6158, stage4_loss_bbox: 0.1489, stage4_loss_iou: 0.2665, stage4_loss_mask: 0.3134, stage5_loss_cls: 0.2280, stage5_pos_acc: 90.8468, stage5_loss_bbox: 0.1457, stage5_loss_iou: 0.2572, stage5_loss_mask: 0.3072, loss: 8.8715\n",
      "2025-07-16 17:43:13,832 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:43:13,832 - mmdet - INFO - Epoch [39][500/750]\tlr: 2.500e-05, eta: 2:04:09, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9969, stage0_pos_acc: 45.0961, stage0_loss_bbox: 0.4863, stage0_loss_iou: 1.0028, stage0_loss_mask: 0.7834, stage1_loss_cls: 0.5370, stage1_pos_acc: 72.7953, stage1_loss_bbox: 0.1984, stage1_loss_iou: 0.4167, stage1_loss_mask: 0.3734, stage2_loss_cls: 0.3816, stage2_pos_acc: 81.5089, stage2_loss_bbox: 0.1456, stage2_loss_iou: 0.2980, stage2_loss_mask: 0.3077, stage3_loss_cls: 0.2696, stage3_pos_acc: 87.4833, stage3_loss_bbox: 0.1345, stage3_loss_iou: 0.2609, stage3_loss_mask: 0.2873, stage4_loss_cls: 0.2059, stage4_pos_acc: 91.8533, stage4_loss_bbox: 0.1281, stage4_loss_iou: 0.2433, stage4_loss_mask: 0.2709, stage5_loss_cls: 0.1749, stage5_pos_acc: 94.2390, stage5_loss_bbox: 0.1243, stage5_loss_iou: 0.2443, stage5_loss_mask: 0.2590, loss: 8.5307\n",
      "2025-07-16 17:43:33,505 - mmdet - INFO - Epoch [39][550/750]\tlr: 2.500e-05, eta: 2:03:52, time: 0.393, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0269, stage0_pos_acc: 41.0949, stage0_loss_bbox: 0.4667, stage0_loss_iou: 0.9127, stage0_loss_mask: 0.6302, stage1_loss_cls: 0.5668, stage1_pos_acc: 71.3849, stage1_loss_bbox: 0.2084, stage1_loss_iou: 0.3976, stage1_loss_mask: 0.3178, stage2_loss_cls: 0.4400, stage2_pos_acc: 77.6338, stage2_loss_bbox: 0.1605, stage2_loss_iou: 0.3006, stage2_loss_mask: 0.3076, stage3_loss_cls: 0.2931, stage3_pos_acc: 86.5637, stage3_loss_bbox: 0.1469, stage3_loss_iou: 0.2858, stage3_loss_mask: 0.2904, stage4_loss_cls: 0.2382, stage4_pos_acc: 90.3034, stage4_loss_bbox: 0.1380, stage4_loss_iou: 0.2745, stage4_loss_mask: 0.2912, stage5_loss_cls: 0.2242, stage5_pos_acc: 93.9606, stage5_loss_bbox: 0.1292, stage5_loss_iou: 0.2611, stage5_loss_mask: 0.2751, loss: 8.5836\n",
      "2025-07-16 17:43:52,991 - mmdet - INFO - Epoch [39][600/750]\tlr: 2.500e-05, eta: 2:03:34, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9903, stage0_pos_acc: 39.6854, stage0_loss_bbox: 0.4786, stage0_loss_iou: 0.9667, stage0_loss_mask: 0.7326, stage1_loss_cls: 0.6007, stage1_pos_acc: 67.7463, stage1_loss_bbox: 0.1903, stage1_loss_iou: 0.4404, stage1_loss_mask: 0.5292, stage2_loss_cls: 0.4426, stage2_pos_acc: 75.7026, stage2_loss_bbox: 0.1642, stage2_loss_iou: 0.3465, stage2_loss_mask: 0.5000, stage3_loss_cls: 0.3216, stage3_pos_acc: 84.9015, stage3_loss_bbox: 0.1506, stage3_loss_iou: 0.3206, stage3_loss_mask: 0.4568, stage4_loss_cls: 0.2418, stage4_pos_acc: 88.7280, stage4_loss_bbox: 0.1467, stage4_loss_iou: 0.3049, stage4_loss_mask: 0.4517, stage5_loss_cls: 0.2089, stage5_pos_acc: 91.9371, stage5_loss_bbox: 0.1403, stage5_loss_iou: 0.3006, stage5_loss_mask: 0.4445, loss: 9.8712\n",
      "2025-07-16 17:44:12,357 - mmdet - INFO - Epoch [39][650/750]\tlr: 2.500e-05, eta: 2:03:16, time: 0.387, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9947, stage0_pos_acc: 32.9278, stage0_loss_bbox: 0.4473, stage0_loss_iou: 0.9340, stage0_loss_mask: 0.4774, stage1_loss_cls: 0.5300, stage1_pos_acc: 68.5516, stage1_loss_bbox: 0.1855, stage1_loss_iou: 0.3643, stage1_loss_mask: 0.2843, stage2_loss_cls: 0.3986, stage2_pos_acc: 78.5103, stage2_loss_bbox: 0.1431, stage2_loss_iou: 0.2758, stage2_loss_mask: 0.2261, stage3_loss_cls: 0.2847, stage3_pos_acc: 84.2595, stage3_loss_bbox: 0.1263, stage3_loss_iou: 0.2414, stage3_loss_mask: 0.2122, stage4_loss_cls: 0.2099, stage4_pos_acc: 90.8040, stage4_loss_bbox: 0.1256, stage4_loss_iou: 0.2347, stage4_loss_mask: 0.2250, stage5_loss_cls: 0.1770, stage5_pos_acc: 93.7921, stage5_loss_bbox: 0.1152, stage5_loss_iou: 0.2283, stage5_loss_mask: 0.2320, loss: 7.6732\n",
      "2025-07-16 17:44:32,067 - mmdet - INFO - Epoch [39][700/750]\tlr: 2.500e-05, eta: 2:02:58, time: 0.394, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0446, stage0_pos_acc: 41.2446, stage0_loss_bbox: 0.4534, stage0_loss_iou: 0.9443, stage0_loss_mask: 0.5898, stage1_loss_cls: 0.5681, stage1_pos_acc: 70.4814, stage1_loss_bbox: 0.1833, stage1_loss_iou: 0.4013, stage1_loss_mask: 0.3281, stage2_loss_cls: 0.4003, stage2_pos_acc: 81.5527, stage2_loss_bbox: 0.1418, stage2_loss_iou: 0.2990, stage2_loss_mask: 0.2762, stage3_loss_cls: 0.2659, stage3_pos_acc: 86.7275, stage3_loss_bbox: 0.1283, stage3_loss_iou: 0.2628, stage3_loss_mask: 0.2584, stage4_loss_cls: 0.2100, stage4_pos_acc: 91.1339, stage4_loss_bbox: 0.1286, stage4_loss_iou: 0.2479, stage4_loss_mask: 0.2408, stage5_loss_cls: 0.1675, stage5_pos_acc: 94.1255, stage5_loss_bbox: 0.1248, stage5_loss_iou: 0.2446, stage5_loss_mask: 0.2515, loss: 8.1613\n",
      "2025-07-16 17:44:51,603 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:44:51,603 - mmdet - INFO - Epoch [39][750/750]\tlr: 2.500e-05, eta: 2:02:40, time: 0.391, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0567, stage0_pos_acc: 37.3076, stage0_loss_bbox: 0.4625, stage0_loss_iou: 0.9618, stage0_loss_mask: 0.5396, stage1_loss_cls: 0.5507, stage1_pos_acc: 76.3013, stage1_loss_bbox: 0.1588, stage1_loss_iou: 0.3644, stage1_loss_mask: 0.2851, stage2_loss_cls: 0.3977, stage2_pos_acc: 82.2013, stage2_loss_bbox: 0.1312, stage2_loss_iou: 0.2814, stage2_loss_mask: 0.2560, stage3_loss_cls: 0.2760, stage3_pos_acc: 91.6331, stage3_loss_bbox: 0.1207, stage3_loss_iou: 0.2627, stage3_loss_mask: 0.2499, stage4_loss_cls: 0.2006, stage4_pos_acc: 93.7641, stage4_loss_bbox: 0.1158, stage4_loss_iou: 0.2472, stage4_loss_mask: 0.2438, stage5_loss_cls: 0.1676, stage5_pos_acc: 96.5141, stage5_loss_bbox: 0.1166, stage5_loss_iou: 0.2481, stage5_loss_mask: 0.2398, loss: 7.9345\n",
      "2025-07-16 17:44:51,735 - mmdet - INFO - Saving checkpoint at 39 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>     ] 85/100, 1.0 task/s, elapsed: 81s, ETA:    14s2025-07-16 17:49:32,447 - mmdet - INFO - Epoch [40][200/750]\tlr: 2.500e-05, eta: 2:01:29, time: 0.388, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9714, stage0_pos_acc: 38.9267, stage0_loss_bbox: 0.4521, stage0_loss_iou: 0.8977, stage0_loss_mask: 0.4521, stage1_loss_cls: 0.5151, stage1_pos_acc: 72.9577, stage1_loss_bbox: 0.1877, stage1_loss_iou: 0.3561, stage1_loss_mask: 0.3000, stage2_loss_cls: 0.3801, stage2_pos_acc: 82.2941, stage2_loss_bbox: 0.1451, stage2_loss_iou: 0.2671, stage2_loss_mask: 0.2635, stage3_loss_cls: 0.2485, stage3_pos_acc: 86.9274, stage3_loss_bbox: 0.1300, stage3_loss_iou: 0.2392, stage3_loss_mask: 0.2433, stage4_loss_cls: 0.1745, stage4_pos_acc: 94.1426, stage4_loss_bbox: 0.1310, stage4_loss_iou: 0.2241, stage4_loss_mask: 0.2456, stage5_loss_cls: 0.1491, stage5_pos_acc: 95.3061, stage5_loss_bbox: 0.1328, stage5_loss_iou: 0.2220, stage5_loss_mask: 0.2432, loss: 7.5712\n",
      "2025-07-16 17:49:51,581 - mmdet - INFO - Epoch [40][250/750]\tlr: 2.500e-05, eta: 2:01:10, time: 0.383, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0510, stage0_pos_acc: 39.0530, stage0_loss_bbox: 0.5579, stage0_loss_iou: 0.9987, stage0_loss_mask: 0.9197, stage1_loss_cls: 0.6250, stage1_pos_acc: 70.5131, stage1_loss_bbox: 0.2662, stage1_loss_iou: 0.5205, stage1_loss_mask: 0.6057, stage2_loss_cls: 0.4964, stage2_pos_acc: 80.0321, stage2_loss_bbox: 0.2056, stage2_loss_iou: 0.4056, stage2_loss_mask: 0.5215, stage3_loss_cls: 0.3824, stage3_pos_acc: 83.9286, stage3_loss_bbox: 0.1917, stage3_loss_iou: 0.3581, stage3_loss_mask: 0.5049, stage4_loss_cls: 0.2898, stage4_pos_acc: 86.4952, stage4_loss_bbox: 0.1853, stage4_loss_iou: 0.3511, stage4_loss_mask: 0.4909, stage5_loss_cls: 0.2647, stage5_pos_acc: 90.3167, stage5_loss_bbox: 0.1870, stage5_loss_iou: 0.3463, stage5_loss_mask: 0.4735, loss: 11.1992\n",
      "2025-07-16 17:50:10,244 - mmdet - INFO - Epoch [40][300/750]\tlr: 2.500e-05, eta: 2:00:51, time: 0.373, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9959, stage0_pos_acc: 39.3134, stage0_loss_bbox: 0.4948, stage0_loss_iou: 0.9583, stage0_loss_mask: 0.7071, stage1_loss_cls: 0.5512, stage1_pos_acc: 75.4631, stage1_loss_bbox: 0.2276, stage1_loss_iou: 0.4291, stage1_loss_mask: 0.4358, stage2_loss_cls: 0.4467, stage2_pos_acc: 84.4220, stage2_loss_bbox: 0.1641, stage2_loss_iou: 0.3111, stage2_loss_mask: 0.4124, stage3_loss_cls: 0.3063, stage3_pos_acc: 87.7634, stage3_loss_bbox: 0.1539, stage3_loss_iou: 0.2878, stage3_loss_mask: 0.3671, stage4_loss_cls: 0.2280, stage4_pos_acc: 92.0800, stage4_loss_bbox: 0.1446, stage4_loss_iou: 0.2699, stage4_loss_mask: 0.3534, stage5_loss_cls: 0.2008, stage5_pos_acc: 94.1740, stage5_loss_bbox: 0.1408, stage5_loss_iou: 0.2665, stage5_loss_mask: 0.3528, loss: 9.2059\n",
      "2025-07-16 17:50:28,600 - mmdet - INFO - Epoch [40][350/750]\tlr: 2.500e-05, eta: 2:00:32, time: 0.367, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0295, stage0_pos_acc: 39.9103, stage0_loss_bbox: 0.4411, stage0_loss_iou: 0.8907, stage0_loss_mask: 0.7467, stage1_loss_cls: 0.5688, stage1_pos_acc: 71.9532, stage1_loss_bbox: 0.2089, stage1_loss_iou: 0.4312, stage1_loss_mask: 0.5143, stage2_loss_cls: 0.4151, stage2_pos_acc: 77.6381, stage2_loss_bbox: 0.1667, stage2_loss_iou: 0.3546, stage2_loss_mask: 0.4708, stage3_loss_cls: 0.3073, stage3_pos_acc: 85.1683, stage3_loss_bbox: 0.1570, stage3_loss_iou: 0.3265, stage3_loss_mask: 0.4552, stage4_loss_cls: 0.2234, stage4_pos_acc: 92.2286, stage4_loss_bbox: 0.1553, stage4_loss_iou: 0.3140, stage4_loss_mask: 0.4314, stage5_loss_cls: 0.1960, stage5_pos_acc: 93.7206, stage5_loss_bbox: 0.1450, stage5_loss_iou: 0.3089, stage5_loss_mask: 0.4226, loss: 9.6810\n",
      "2025-07-16 17:50:47,082 - mmdet - INFO - Epoch [40][400/750]\tlr: 2.500e-05, eta: 2:00:12, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0093, stage0_pos_acc: 40.1429, stage0_loss_bbox: 0.4590, stage0_loss_iou: 0.9558, stage0_loss_mask: 0.5366, stage1_loss_cls: 0.5280, stage1_pos_acc: 76.4167, stage1_loss_bbox: 0.1833, stage1_loss_iou: 0.4103, stage1_loss_mask: 0.3650, stage2_loss_cls: 0.3909, stage2_pos_acc: 83.1429, stage2_loss_bbox: 0.1351, stage2_loss_iou: 0.2971, stage2_loss_mask: 0.3074, stage3_loss_cls: 0.2783, stage3_pos_acc: 85.9976, stage3_loss_bbox: 0.1165, stage3_loss_iou: 0.2748, stage3_loss_mask: 0.2862, stage4_loss_cls: 0.2311, stage4_pos_acc: 90.8548, stage4_loss_bbox: 0.1145, stage4_loss_iou: 0.2678, stage4_loss_mask: 0.2966, stage5_loss_cls: 0.2108, stage5_pos_acc: 92.7405, stage5_loss_bbox: 0.1103, stage5_loss_iou: 0.2596, stage5_loss_mask: 0.2956, loss: 8.3198\n",
      "2025-07-16 17:51:06,003 - mmdet - INFO - Epoch [40][450/750]\tlr: 2.500e-05, eta: 1:59:53, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9688, stage0_pos_acc: 41.5569, stage0_loss_bbox: 0.4808, stage0_loss_iou: 0.9202, stage0_loss_mask: 0.6055, stage1_loss_cls: 0.5237, stage1_pos_acc: 72.9718, stage1_loss_bbox: 0.2165, stage1_loss_iou: 0.4106, stage1_loss_mask: 0.3871, stage2_loss_cls: 0.3912, stage2_pos_acc: 83.3344, stage2_loss_bbox: 0.1693, stage2_loss_iou: 0.3022, stage2_loss_mask: 0.3075, stage3_loss_cls: 0.2651, stage3_pos_acc: 88.4087, stage3_loss_bbox: 0.1618, stage3_loss_iou: 0.2778, stage3_loss_mask: 0.3106, stage4_loss_cls: 0.1990, stage4_pos_acc: 92.7601, stage4_loss_bbox: 0.1559, stage4_loss_iou: 0.2690, stage4_loss_mask: 0.2962, stage5_loss_cls: 0.1792, stage5_pos_acc: 92.9299, stage5_loss_bbox: 0.1508, stage5_loss_iou: 0.2659, stage5_loss_mask: 0.2991, loss: 8.5138\n",
      "2025-07-16 17:51:24,908 - mmdet - INFO - Epoch [40][500/750]\tlr: 2.500e-05, eta: 1:59:34, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0289, stage0_pos_acc: 38.2108, stage0_loss_bbox: 0.4970, stage0_loss_iou: 1.0081, stage0_loss_mask: 0.8697, stage1_loss_cls: 0.5875, stage1_pos_acc: 70.7188, stage1_loss_bbox: 0.2214, stage1_loss_iou: 0.4532, stage1_loss_mask: 0.4027, stage2_loss_cls: 0.4463, stage2_pos_acc: 82.6838, stage2_loss_bbox: 0.1724, stage2_loss_iou: 0.3406, stage2_loss_mask: 0.3885, stage3_loss_cls: 0.3302, stage3_pos_acc: 87.2783, stage3_loss_bbox: 0.1676, stage3_loss_iou: 0.3065, stage3_loss_mask: 0.4074, stage4_loss_cls: 0.2622, stage4_pos_acc: 91.4061, stage4_loss_bbox: 0.1635, stage4_loss_iou: 0.2894, stage4_loss_mask: 0.3983, stage5_loss_cls: 0.2469, stage5_pos_acc: 93.1253, stage5_loss_bbox: 0.1538, stage5_loss_iou: 0.2838, stage5_loss_mask: 0.3634, loss: 9.7892\n",
      "2025-07-16 17:51:44,969 - mmdet - INFO - Epoch [40][550/750]\tlr: 2.500e-05, eta: 1:59:17, time: 0.401, data_time: 0.027, memory: 11264, stage0_loss_cls: 1.0331, stage0_pos_acc: 42.1270, stage0_loss_bbox: 0.4985, stage0_loss_iou: 1.0648, stage0_loss_mask: 0.8157, stage1_loss_cls: 0.5691, stage1_pos_acc: 74.7534, stage1_loss_bbox: 0.2040, stage1_loss_iou: 0.4378, stage1_loss_mask: 0.4499, stage2_loss_cls: 0.4247, stage2_pos_acc: 79.5366, stage2_loss_bbox: 0.1537, stage2_loss_iou: 0.3280, stage2_loss_mask: 0.3907, stage3_loss_cls: 0.2827, stage3_pos_acc: 88.0906, stage3_loss_bbox: 0.1396, stage3_loss_iou: 0.2972, stage3_loss_mask: 0.3867, stage4_loss_cls: 0.2096, stage4_pos_acc: 92.5545, stage4_loss_bbox: 0.1291, stage4_loss_iou: 0.2813, stage4_loss_mask: 0.3694, stage5_loss_cls: 0.1867, stage5_pos_acc: 92.9404, stage5_loss_bbox: 0.1274, stage5_loss_iou: 0.2751, stage5_loss_mask: 0.3631, loss: 9.4177\n",
      "2025-07-16 17:52:03,924 - mmdet - INFO - Epoch [40][600/750]\tlr: 2.500e-05, eta: 1:58:58, time: 0.379, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0056, stage0_pos_acc: 37.6614, stage0_loss_bbox: 0.5063, stage0_loss_iou: 0.9320, stage0_loss_mask: 0.5413, stage1_loss_cls: 0.5591, stage1_pos_acc: 72.0530, stage1_loss_bbox: 0.2223, stage1_loss_iou: 0.3886, stage1_loss_mask: 0.3220, stage2_loss_cls: 0.3967, stage2_pos_acc: 86.3619, stage2_loss_bbox: 0.1736, stage2_loss_iou: 0.3007, stage2_loss_mask: 0.2898, stage3_loss_cls: 0.2990, stage3_pos_acc: 88.4536, stage3_loss_bbox: 0.1610, stage3_loss_iou: 0.2861, stage3_loss_mask: 0.2677, stage4_loss_cls: 0.2421, stage4_pos_acc: 93.0058, stage4_loss_bbox: 0.1472, stage4_loss_iou: 0.2684, stage4_loss_mask: 0.2795, stage5_loss_cls: 0.2227, stage5_pos_acc: 91.9320, stage5_loss_bbox: 0.1372, stage5_loss_iou: 0.2530, stage5_loss_mask: 0.2530, loss: 8.4547\n",
      "2025-07-16 17:52:22,712 - mmdet - INFO - Epoch [40][650/750]\tlr: 2.500e-05, eta: 1:58:39, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0295, stage0_pos_acc: 33.1017, stage0_loss_bbox: 0.4913, stage0_loss_iou: 1.0132, stage0_loss_mask: 0.7072, stage1_loss_cls: 0.5779, stage1_pos_acc: 67.8430, stage1_loss_bbox: 0.1922, stage1_loss_iou: 0.4097, stage1_loss_mask: 0.3190, stage2_loss_cls: 0.4413, stage2_pos_acc: 77.1728, stage2_loss_bbox: 0.1506, stage2_loss_iou: 0.2954, stage2_loss_mask: 0.2977, stage3_loss_cls: 0.3103, stage3_pos_acc: 86.9709, stage3_loss_bbox: 0.1418, stage3_loss_iou: 0.2675, stage3_loss_mask: 0.2867, stage4_loss_cls: 0.2271, stage4_pos_acc: 92.3796, stage4_loss_bbox: 0.1429, stage4_loss_iou: 0.2620, stage4_loss_mask: 0.2967, stage5_loss_cls: 0.2013, stage5_pos_acc: 94.8864, stage5_loss_bbox: 0.1390, stage5_loss_iou: 0.2560, stage5_loss_mask: 0.2712, loss: 8.7272\n",
      "2025-07-16 17:52:41,725 - mmdet - INFO - Epoch [40][700/750]\tlr: 2.500e-05, eta: 1:58:21, time: 0.380, data_time: 0.010, memory: 11264, stage0_loss_cls: 1.0047, stage0_pos_acc: 40.4141, stage0_loss_bbox: 0.4929, stage0_loss_iou: 0.9304, stage0_loss_mask: 0.7404, stage1_loss_cls: 0.5741, stage1_pos_acc: 69.2001, stage1_loss_bbox: 0.2385, stage1_loss_iou: 0.4344, stage1_loss_mask: 0.4242, stage2_loss_cls: 0.4652, stage2_pos_acc: 79.7144, stage2_loss_bbox: 0.1843, stage2_loss_iou: 0.3428, stage2_loss_mask: 0.4060, stage3_loss_cls: 0.3106, stage3_pos_acc: 84.3501, stage3_loss_bbox: 0.1678, stage3_loss_iou: 0.3146, stage3_loss_mask: 0.3834, stage4_loss_cls: 0.2402, stage4_pos_acc: 91.0001, stage4_loss_bbox: 0.1590, stage4_loss_iou: 0.2975, stage4_loss_mask: 0.3695, stage5_loss_cls: 0.2107, stage5_pos_acc: 92.5775, stage5_loss_bbox: 0.1623, stage5_loss_iou: 0.2949, stage5_loss_mask: 0.3843, loss: 9.5328\n",
      "2025-07-16 17:53:00,585 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:53:00,585 - mmdet - INFO - Epoch [40][750/750]\tlr: 2.500e-05, eta: 1:58:02, time: 0.377, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0177, stage0_pos_acc: 35.3222, stage0_loss_bbox: 0.4944, stage0_loss_iou: 1.0142, stage0_loss_mask: 0.7716, stage1_loss_cls: 0.6244, stage1_pos_acc: 64.4263, stage1_loss_bbox: 0.2128, stage1_loss_iou: 0.4261, stage1_loss_mask: 0.3717, stage2_loss_cls: 0.4642, stage2_pos_acc: 76.8965, stage2_loss_bbox: 0.1563, stage2_loss_iou: 0.3121, stage2_loss_mask: 0.3209, stage3_loss_cls: 0.3598, stage3_pos_acc: 87.1266, stage3_loss_bbox: 0.1443, stage3_loss_iou: 0.3005, stage3_loss_mask: 0.3060, stage4_loss_cls: 0.3203, stage4_pos_acc: 88.7460, stage4_loss_bbox: 0.1422, stage4_loss_iou: 0.2911, stage4_loss_mask: 0.3040, stage5_loss_cls: 0.2664, stage5_pos_acc: 90.4418, stage5_loss_bbox: 0.1375, stage5_loss_iou: 0.2770, stage5_loss_mask: 0.2905, loss: 9.3261\n",
      "2025-07-16 17:53:00,716 - mmdet - INFO - Saving checkpoint at 40 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 100s, ETA:     0s2025-07-16 17:56:18,429 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.030\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.384\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.388\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.388\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.178\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.430\n",
      "2025-07-16 17:56:20,269 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.007\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.030\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.386\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.389\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.389\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.192\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.429\n",
      "2025-07-16 17:56:23,355 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 17:56:23,356 - mmdet - INFO - Epoch(val) [40][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0480, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.0450, bbox_mAP_m: 0.0070, bbox_mAP_l: 0.0300, bbox_mAP_copypaste: 0.025 0.048 0.023 0.045 0.007 0.030, segm_mAP: 0.0240, segm_mAP_50: 0.0480, segm_mAP_75: 0.0230, segm_mAP_s: 0.0500, segm_mAP_m: 0.0070, segm_mAP_l: 0.0300, segm_mAP_copypaste: 0.024 0.048 0.023 0.050 0.007 0.030\n",
      "2025-07-16 17:56:44,864 - mmdet - INFO - Epoch [41][50/750]\tlr: 2.500e-06, eta: 1:57:46, time: 0.430, data_time: 0.055, memory: 11264, stage0_loss_cls: 0.9913, stage0_pos_acc: 41.4751, stage0_loss_bbox: 0.4829, stage0_loss_iou: 0.9538, stage0_loss_mask: 0.6563, stage1_loss_cls: 0.5449, stage1_pos_acc: 71.9276, stage1_loss_bbox: 0.1935, stage1_loss_iou: 0.3834, stage1_loss_mask: 0.3706, stage2_loss_cls: 0.4066, stage2_pos_acc: 81.9236, stage2_loss_bbox: 0.1492, stage2_loss_iou: 0.2941, stage2_loss_mask: 0.3115, stage3_loss_cls: 0.2644, stage3_pos_acc: 88.8491, stage3_loss_bbox: 0.1390, stage3_loss_iou: 0.2731, stage3_loss_mask: 0.3022, stage4_loss_cls: 0.1997, stage4_pos_acc: 91.8419, stage4_loss_bbox: 0.1326, stage4_loss_iou: 0.2631, stage4_loss_mask: 0.2910, stage5_loss_cls: 0.1629, stage5_pos_acc: 96.3103, stage5_loss_bbox: 0.1296, stage5_loss_iou: 0.2511, stage5_loss_mask: 0.2998, loss: 8.4466\n",
      "2025-07-16 17:57:04,354 - mmdet - INFO - Epoch [41][100/750]\tlr: 2.500e-06, eta: 1:57:28, time: 0.390, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9895, stage0_pos_acc: 34.3352, stage0_loss_bbox: 0.4587, stage0_loss_iou: 0.9542, stage0_loss_mask: 0.7583, stage1_loss_cls: 0.5259, stage1_pos_acc: 73.4053, stage1_loss_bbox: 0.1891, stage1_loss_iou: 0.4256, stage1_loss_mask: 0.4524, stage2_loss_cls: 0.3987, stage2_pos_acc: 79.5379, stage2_loss_bbox: 0.1475, stage2_loss_iou: 0.3182, stage2_loss_mask: 0.3983, stage3_loss_cls: 0.2683, stage3_pos_acc: 85.4392, stage3_loss_bbox: 0.1341, stage3_loss_iou: 0.2763, stage3_loss_mask: 0.3706, stage4_loss_cls: 0.1904, stage4_pos_acc: 89.7098, stage4_loss_bbox: 0.1284, stage4_loss_iou: 0.2603, stage4_loss_mask: 0.3491, stage5_loss_cls: 0.1513, stage5_pos_acc: 95.0048, stage5_loss_bbox: 0.1282, stage5_loss_iou: 0.2593, stage5_loss_mask: 0.3677, loss: 8.9006\n",
      "2025-07-16 17:57:23,630 - mmdet - INFO - Epoch [41][150/750]\tlr: 2.500e-06, eta: 1:57:10, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0094, stage0_pos_acc: 35.5440, stage0_loss_bbox: 0.4845, stage0_loss_iou: 0.9671, stage0_loss_mask: 0.7688, stage1_loss_cls: 0.5668, stage1_pos_acc: 69.8772, stage1_loss_bbox: 0.2221, stage1_loss_iou: 0.4298, stage1_loss_mask: 0.5267, stage2_loss_cls: 0.4239, stage2_pos_acc: 80.1375, stage2_loss_bbox: 0.1726, stage2_loss_iou: 0.3275, stage2_loss_mask: 0.4653, stage3_loss_cls: 0.3051, stage3_pos_acc: 88.0042, stage3_loss_bbox: 0.1534, stage3_loss_iou: 0.3067, stage3_loss_mask: 0.4175, stage4_loss_cls: 0.2385, stage4_pos_acc: 89.9500, stage4_loss_bbox: 0.1446, stage4_loss_iou: 0.2931, stage4_loss_mask: 0.4164, stage5_loss_cls: 0.2143, stage5_pos_acc: 92.6167, stage5_loss_bbox: 0.1403, stage5_loss_iou: 0.2938, stage5_loss_mask: 0.4122, loss: 9.7003\n",
      "2025-07-16 17:57:42,789 - mmdet - INFO - Epoch [41][200/750]\tlr: 2.500e-06, eta: 1:56:51, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9749, stage0_pos_acc: 42.1003, stage0_loss_bbox: 0.4124, stage0_loss_iou: 0.9032, stage0_loss_mask: 0.7114, stage1_loss_cls: 0.5009, stage1_pos_acc: 79.1455, stage1_loss_bbox: 0.1818, stage1_loss_iou: 0.3958, stage1_loss_mask: 0.4988, stage2_loss_cls: 0.3558, stage2_pos_acc: 87.7776, stage2_loss_bbox: 0.1428, stage2_loss_iou: 0.3047, stage2_loss_mask: 0.4345, stage3_loss_cls: 0.2247, stage3_pos_acc: 90.6196, stage3_loss_bbox: 0.1316, stage3_loss_iou: 0.2774, stage3_loss_mask: 0.4077, stage4_loss_cls: 0.1703, stage4_pos_acc: 92.3087, stage4_loss_bbox: 0.1296, stage4_loss_iou: 0.2693, stage4_loss_mask: 0.4206, stage5_loss_cls: 0.1538, stage5_pos_acc: 95.9134, stage5_loss_bbox: 0.1259, stage5_loss_iou: 0.2632, stage5_loss_mask: 0.4110, loss: 8.8022\n",
      "2025-07-16 17:58:02,374 - mmdet - INFO - Epoch [41][250/750]\tlr: 2.500e-06, eta: 1:56:33, time: 0.392, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9852, stage0_pos_acc: 37.5102, stage0_loss_bbox: 0.4676, stage0_loss_iou: 0.9899, stage0_loss_mask: 0.6981, stage1_loss_cls: 0.5331, stage1_pos_acc: 74.3117, stage1_loss_bbox: 0.1837, stage1_loss_iou: 0.4027, stage1_loss_mask: 0.3930, stage2_loss_cls: 0.3827, stage2_pos_acc: 83.7112, stage2_loss_bbox: 0.1443, stage2_loss_iou: 0.2914, stage2_loss_mask: 0.3378, stage3_loss_cls: 0.2399, stage3_pos_acc: 91.0408, stage3_loss_bbox: 0.1381, stage3_loss_iou: 0.2704, stage3_loss_mask: 0.3392, stage4_loss_cls: 0.1749, stage4_pos_acc: 93.8969, stage4_loss_bbox: 0.1352, stage4_loss_iou: 0.2666, stage4_loss_mask: 0.3617, stage5_loss_cls: 0.1323, stage5_pos_acc: 97.1175, stage5_loss_bbox: 0.1326, stage5_loss_iou: 0.2633, stage5_loss_mask: 0.3527, loss: 8.6163\n",
      "2025-07-16 17:58:21,702 - mmdet - INFO - Epoch [41][300/750]\tlr: 2.500e-06, eta: 1:56:15, time: 0.387, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9786, stage0_pos_acc: 39.6072, stage0_loss_bbox: 0.4549, stage0_loss_iou: 0.9453, stage0_loss_mask: 0.6315, stage1_loss_cls: 0.5355, stage1_pos_acc: 72.8613, stage1_loss_bbox: 0.2048, stage1_loss_iou: 0.3843, stage1_loss_mask: 0.4184, stage2_loss_cls: 0.3769, stage2_pos_acc: 82.7298, stage2_loss_bbox: 0.1716, stage2_loss_iou: 0.2916, stage2_loss_mask: 0.3801, stage3_loss_cls: 0.2549, stage3_pos_acc: 91.4044, stage3_loss_bbox: 0.1368, stage3_loss_iou: 0.2632, stage3_loss_mask: 0.3626, stage4_loss_cls: 0.1850, stage4_pos_acc: 95.1720, stage4_loss_bbox: 0.1343, stage4_loss_iou: 0.2651, stage4_loss_mask: 0.3635, stage5_loss_cls: 0.1519, stage5_pos_acc: 97.3054, stage5_loss_bbox: 0.1318, stage5_loss_iou: 0.2656, stage5_loss_mask: 0.3725, loss: 8.6607\n",
      "2025-07-16 17:58:41,039 - mmdet - INFO - Epoch [41][350/750]\tlr: 2.500e-06, eta: 1:55:56, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9756, stage0_pos_acc: 37.9401, stage0_loss_bbox: 0.4286, stage0_loss_iou: 0.9615, stage0_loss_mask: 0.8596, stage1_loss_cls: 0.5472, stage1_pos_acc: 69.4401, stage1_loss_bbox: 0.1724, stage1_loss_iou: 0.4311, stage1_loss_mask: 0.4340, stage2_loss_cls: 0.3896, stage2_pos_acc: 83.9206, stage2_loss_bbox: 0.1306, stage2_loss_iou: 0.3014, stage2_loss_mask: 0.3669, stage3_loss_cls: 0.2595, stage3_pos_acc: 89.3802, stage3_loss_bbox: 0.1181, stage3_loss_iou: 0.2759, stage3_loss_mask: 0.3627, stage4_loss_cls: 0.1930, stage4_pos_acc: 93.3242, stage4_loss_bbox: 0.1123, stage4_loss_iou: 0.2662, stage4_loss_mask: 0.3599, stage5_loss_cls: 0.1684, stage5_pos_acc: 95.3381, stage5_loss_bbox: 0.1105, stage5_loss_iou: 0.2650, stage5_loss_mask: 0.3467, loss: 8.8366\n",
      "2025-07-16 17:59:00,228 - mmdet - INFO - Epoch [41][400/750]\tlr: 2.500e-06, eta: 1:55:38, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9328, stage0_pos_acc: 39.8942, stage0_loss_bbox: 0.4200, stage0_loss_iou: 0.9361, stage0_loss_mask: 0.6821, stage1_loss_cls: 0.5011, stage1_pos_acc: 78.3268, stage1_loss_bbox: 0.1739, stage1_loss_iou: 0.3988, stage1_loss_mask: 0.4166, stage2_loss_cls: 0.3442, stage2_pos_acc: 86.0737, stage2_loss_bbox: 0.1360, stage2_loss_iou: 0.2993, stage2_loss_mask: 0.3851, stage3_loss_cls: 0.2318, stage3_pos_acc: 92.9221, stage3_loss_bbox: 0.1295, stage3_loss_iou: 0.2705, stage3_loss_mask: 0.3788, stage4_loss_cls: 0.1683, stage4_pos_acc: 95.5333, stage4_loss_bbox: 0.1272, stage4_loss_iou: 0.2645, stage4_loss_mask: 0.3918, stage5_loss_cls: 0.1367, stage5_pos_acc: 96.5762, stage5_loss_bbox: 0.1265, stage5_loss_iou: 0.2662, stage5_loss_mask: 0.3762, loss: 8.4940\n",
      "2025-07-16 17:59:19,340 - mmdet - INFO - Epoch [41][450/750]\tlr: 2.500e-06, eta: 1:55:19, time: 0.382, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9786, stage0_pos_acc: 37.4927, stage0_loss_bbox: 0.4640, stage0_loss_iou: 0.9692, stage0_loss_mask: 0.6455, stage1_loss_cls: 0.5046, stage1_pos_acc: 75.0782, stage1_loss_bbox: 0.1752, stage1_loss_iou: 0.3729, stage1_loss_mask: 0.3096, stage2_loss_cls: 0.3442, stage2_pos_acc: 87.4174, stage2_loss_bbox: 0.1405, stage2_loss_iou: 0.2819, stage2_loss_mask: 0.2866, stage3_loss_cls: 0.2135, stage3_pos_acc: 94.0039, stage3_loss_bbox: 0.1240, stage3_loss_iou: 0.2460, stage3_loss_mask: 0.2539, stage4_loss_cls: 0.1512, stage4_pos_acc: 93.6017, stage4_loss_bbox: 0.1184, stage4_loss_iou: 0.2352, stage4_loss_mask: 0.2686, stage5_loss_cls: 0.1418, stage5_pos_acc: 94.6798, stage5_loss_bbox: 0.1136, stage5_loss_iou: 0.2319, stage5_loss_mask: 0.2531, loss: 7.8240\n",
      "2025-07-16 17:59:37,941 - mmdet - INFO - Epoch [41][500/750]\tlr: 2.500e-06, eta: 1:55:00, time: 0.372, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0105, stage0_pos_acc: 37.8540, stage0_loss_bbox: 0.4457, stage0_loss_iou: 0.8893, stage0_loss_mask: 0.4859, stage1_loss_cls: 0.5152, stage1_pos_acc: 75.5659, stage1_loss_bbox: 0.1594, stage1_loss_iou: 0.3228, stage1_loss_mask: 0.2391, stage2_loss_cls: 0.3578, stage2_pos_acc: 86.3008, stage2_loss_bbox: 0.1118, stage2_loss_iou: 0.2279, stage2_loss_mask: 0.2126, stage3_loss_cls: 0.2535, stage3_pos_acc: 88.5452, stage3_loss_bbox: 0.1000, stage3_loss_iou: 0.2022, stage3_loss_mask: 0.1807, stage4_loss_cls: 0.1594, stage4_pos_acc: 94.7587, stage4_loss_bbox: 0.0932, stage4_loss_iou: 0.1939, stage4_loss_mask: 0.1820, stage5_loss_cls: 0.1264, stage5_pos_acc: 95.6310, stage5_loss_bbox: 0.0936, stage5_loss_iou: 0.1923, stage5_loss_mask: 0.1830, loss: 6.9380\n",
      "2025-07-16 17:59:56,707 - mmdet - INFO - Epoch [41][550/750]\tlr: 2.500e-06, eta: 1:54:41, time: 0.375, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9622, stage0_pos_acc: 45.9747, stage0_loss_bbox: 0.4437, stage0_loss_iou: 0.9213, stage0_loss_mask: 0.5760, stage1_loss_cls: 0.5193, stage1_pos_acc: 74.6995, stage1_loss_bbox: 0.1813, stage1_loss_iou: 0.3758, stage1_loss_mask: 0.3620, stage2_loss_cls: 0.3737, stage2_pos_acc: 87.5702, stage2_loss_bbox: 0.1335, stage2_loss_iou: 0.2803, stage2_loss_mask: 0.3060, stage3_loss_cls: 0.2439, stage3_pos_acc: 92.5170, stage3_loss_bbox: 0.1269, stage3_loss_iou: 0.2604, stage3_loss_mask: 0.2848, stage4_loss_cls: 0.1846, stage4_pos_acc: 97.4155, stage4_loss_bbox: 0.1172, stage4_loss_iou: 0.2493, stage4_loss_mask: 0.2976, stage5_loss_cls: 0.1499, stage5_pos_acc: 97.5822, stage5_loss_bbox: 0.1190, stage5_loss_iou: 0.2470, stage5_loss_mask: 0.3010, loss: 8.0167\n",
      "2025-07-16 18:00:15,499 - mmdet - INFO - Epoch [41][600/750]\tlr: 2.500e-06, eta: 1:54:22, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9721, stage0_pos_acc: 41.7475, stage0_loss_bbox: 0.4106, stage0_loss_iou: 0.8694, stage0_loss_mask: 0.6143, stage1_loss_cls: 0.5109, stage1_pos_acc: 76.5075, stage1_loss_bbox: 0.1729, stage1_loss_iou: 0.3792, stage1_loss_mask: 0.3163, stage2_loss_cls: 0.3340, stage2_pos_acc: 87.8113, stage2_loss_bbox: 0.1362, stage2_loss_iou: 0.2923, stage2_loss_mask: 0.3018, stage3_loss_cls: 0.2258, stage3_pos_acc: 93.2230, stage3_loss_bbox: 0.1210, stage3_loss_iou: 0.2625, stage3_loss_mask: 0.2945, stage4_loss_cls: 0.1619, stage4_pos_acc: 95.8366, stage4_loss_bbox: 0.1199, stage4_loss_iou: 0.2524, stage4_loss_mask: 0.3265, stage5_loss_cls: 0.1309, stage5_pos_acc: 95.5709, stage5_loss_bbox: 0.1182, stage5_loss_iou: 0.2493, stage5_loss_mask: 0.2880, loss: 7.8608\n",
      "2025-07-16 18:00:34,346 - mmdet - INFO - Epoch [41][650/750]\tlr: 2.500e-06, eta: 1:54:03, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0101, stage0_pos_acc: 36.6724, stage0_loss_bbox: 0.4505, stage0_loss_iou: 0.9774, stage0_loss_mask: 0.5755, stage1_loss_cls: 0.5046, stage1_pos_acc: 79.3212, stage1_loss_bbox: 0.1725, stage1_loss_iou: 0.3823, stage1_loss_mask: 0.4147, stage2_loss_cls: 0.3601, stage2_pos_acc: 86.8274, stage2_loss_bbox: 0.1315, stage2_loss_iou: 0.2907, stage2_loss_mask: 0.3699, stage3_loss_cls: 0.2135, stage3_pos_acc: 93.4726, stage3_loss_bbox: 0.1215, stage3_loss_iou: 0.2714, stage3_loss_mask: 0.3653, stage4_loss_cls: 0.1338, stage4_pos_acc: 97.3060, stage4_loss_bbox: 0.1185, stage4_loss_iou: 0.2649, stage4_loss_mask: 0.3584, stage5_loss_cls: 0.1107, stage5_pos_acc: 97.2643, stage5_loss_bbox: 0.1145, stage5_loss_iou: 0.2601, stage5_loss_mask: 0.3525, loss: 8.3250\n",
      "2025-07-16 18:00:53,089 - mmdet - INFO - Epoch [41][700/750]\tlr: 2.500e-06, eta: 1:53:44, time: 0.375, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9883, stage0_pos_acc: 43.3748, stage0_loss_bbox: 0.4394, stage0_loss_iou: 0.9550, stage0_loss_mask: 0.7120, stage1_loss_cls: 0.5051, stage1_pos_acc: 74.9331, stage1_loss_bbox: 0.1712, stage1_loss_iou: 0.3906, stage1_loss_mask: 0.4223, stage2_loss_cls: 0.3627, stage2_pos_acc: 86.9005, stage2_loss_bbox: 0.1292, stage2_loss_iou: 0.2991, stage2_loss_mask: 0.3643, stage3_loss_cls: 0.2310, stage3_pos_acc: 91.2174, stage3_loss_bbox: 0.1199, stage3_loss_iou: 0.2721, stage3_loss_mask: 0.3457, stage4_loss_cls: 0.1622, stage4_pos_acc: 94.2191, stage4_loss_bbox: 0.1184, stage4_loss_iou: 0.2644, stage4_loss_mask: 0.3573, stage5_loss_cls: 0.1351, stage5_pos_acc: 95.8191, stage5_loss_bbox: 0.1140, stage5_loss_iou: 0.2557, stage5_loss_mask: 0.3325, loss: 8.4475\n",
      "2025-07-16 18:01:11,959 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:01:11,959 - mmdet - INFO - Epoch [41][750/750]\tlr: 2.500e-06, eta: 1:53:25, time: 0.377, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0038, stage0_pos_acc: 39.8201, stage0_loss_bbox: 0.4542, stage0_loss_iou: 0.9057, stage0_loss_mask: 0.6529, stage1_loss_cls: 0.5404, stage1_pos_acc: 74.2263, stage1_loss_bbox: 0.1941, stage1_loss_iou: 0.3589, stage1_loss_mask: 0.3017, stage2_loss_cls: 0.3931, stage2_pos_acc: 80.5789, stage2_loss_bbox: 0.1242, stage2_loss_iou: 0.2505, stage2_loss_mask: 0.2587, stage3_loss_cls: 0.2441, stage3_pos_acc: 89.3971, stage3_loss_bbox: 0.1116, stage3_loss_iou: 0.2288, stage3_loss_mask: 0.2547, stage4_loss_cls: 0.1738, stage4_pos_acc: 92.1089, stage4_loss_bbox: 0.1085, stage4_loss_iou: 0.2058, stage4_loss_mask: 0.2344, stage5_loss_cls: 0.1495, stage5_pos_acc: 94.7504, stage5_loss_bbox: 0.1030, stage5_loss_iou: 0.1977, stage5_loss_mask: 0.2227, loss: 7.6731\n",
      "2025-07-16 18:01:12,094 - mmdet - INFO - Saving checkpoint at 41 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 101s, ETA:     0s2025-07-16 18:04:31,353 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.32s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.084\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.160\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.228\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.451\n",
      "2025-07-16 18:04:33,219 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.85s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.244\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.453\n",
      "2025-07-16 18:04:36,446 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:04:36,447 - mmdet - INFO - Epoch(val) [41][750]\tbbox_mAP: 0.0300, bbox_mAP_50: 0.0550, bbox_mAP_75: 0.0290, bbox_mAP_s: 0.0840, bbox_mAP_m: 0.0190, bbox_mAP_l: 0.0350, bbox_mAP_copypaste: 0.030 0.055 0.029 0.084 0.019 0.035, segm_mAP: 0.0300, segm_mAP_50: 0.0550, segm_mAP_75: 0.0290, segm_mAP_s: 0.0520, segm_mAP_m: 0.0190, segm_mAP_l: 0.0350, segm_mAP_copypaste: 0.030 0.055 0.029 0.052 0.019 0.035\n",
      "2025-07-16 18:04:57,756 - mmdet - INFO - Epoch [42][50/750]\tlr: 2.500e-06, eta: 1:53:09, time: 0.426, data_time: 0.053, memory: 11264, stage0_loss_cls: 0.9852, stage0_pos_acc: 39.7735, stage0_loss_bbox: 0.4341, stage0_loss_iou: 0.9782, stage0_loss_mask: 0.6659, stage1_loss_cls: 0.5234, stage1_pos_acc: 72.5578, stage1_loss_bbox: 0.1564, stage1_loss_iou: 0.3944, stage1_loss_mask: 0.2915, stage2_loss_cls: 0.3675, stage2_pos_acc: 81.0497, stage2_loss_bbox: 0.1179, stage2_loss_iou: 0.2746, stage2_loss_mask: 0.2693, stage3_loss_cls: 0.2283, stage3_pos_acc: 88.5128, stage3_loss_bbox: 0.1026, stage3_loss_iou: 0.2346, stage3_loss_mask: 0.2380, stage4_loss_cls: 0.1646, stage4_pos_acc: 95.4109, stage4_loss_bbox: 0.0996, stage4_loss_iou: 0.2284, stage4_loss_mask: 0.2217, stage5_loss_cls: 0.1314, stage5_pos_acc: 97.3056, stage5_loss_bbox: 0.0976, stage5_loss_iou: 0.2220, stage5_loss_mask: 0.2267, loss: 7.6539\n",
      "2025-07-16 18:05:16,654 - mmdet - INFO - Epoch [42][100/750]\tlr: 2.500e-06, eta: 1:52:50, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0359, stage0_pos_acc: 37.1825, stage0_loss_bbox: 0.4507, stage0_loss_iou: 0.9589, stage0_loss_mask: 0.5799, stage1_loss_cls: 0.5180, stage1_pos_acc: 77.8632, stage1_loss_bbox: 0.1633, stage1_loss_iou: 0.3748, stage1_loss_mask: 0.3624, stage2_loss_cls: 0.3418, stage2_pos_acc: 87.0916, stage2_loss_bbox: 0.1265, stage2_loss_iou: 0.2801, stage2_loss_mask: 0.3190, stage3_loss_cls: 0.2049, stage3_pos_acc: 93.7042, stage3_loss_bbox: 0.1183, stage3_loss_iou: 0.2564, stage3_loss_mask: 0.3140, stage4_loss_cls: 0.1347, stage4_pos_acc: 97.6000, stage4_loss_bbox: 0.1136, stage4_loss_iou: 0.2496, stage4_loss_mask: 0.3158, stage5_loss_cls: 0.1059, stage5_pos_acc: 98.2881, stage5_loss_bbox: 0.1118, stage5_loss_iou: 0.2486, stage5_loss_mask: 0.3227, loss: 8.0074\n",
      "2025-07-16 18:05:35,489 - mmdet - INFO - Epoch [42][150/750]\tlr: 2.500e-06, eta: 1:52:31, time: 0.377, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9828, stage0_pos_acc: 35.4847, stage0_loss_bbox: 0.4159, stage0_loss_iou: 0.8645, stage0_loss_mask: 0.5798, stage1_loss_cls: 0.4995, stage1_pos_acc: 76.7967, stage1_loss_bbox: 0.1664, stage1_loss_iou: 0.3407, stage1_loss_mask: 0.3339, stage2_loss_cls: 0.3340, stage2_pos_acc: 85.8784, stage2_loss_bbox: 0.1237, stage2_loss_iou: 0.2521, stage2_loss_mask: 0.2864, stage3_loss_cls: 0.2293, stage3_pos_acc: 92.4658, stage3_loss_bbox: 0.1174, stage3_loss_iou: 0.2304, stage3_loss_mask: 0.2797, stage4_loss_cls: 0.1474, stage4_pos_acc: 95.9856, stage4_loss_bbox: 0.1118, stage4_loss_iou: 0.2253, stage4_loss_mask: 0.2678, stage5_loss_cls: 0.1283, stage5_pos_acc: 95.5269, stage5_loss_bbox: 0.1070, stage5_loss_iou: 0.2187, stage5_loss_mask: 0.2641, loss: 7.5071\n",
      "2025-07-16 18:05:54,577 - mmdet - INFO - Epoch [42][200/750]\tlr: 2.500e-06, eta: 1:52:12, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9681, stage0_pos_acc: 40.0478, stage0_loss_bbox: 0.4156, stage0_loss_iou: 0.9253, stage0_loss_mask: 0.5826, stage1_loss_cls: 0.4962, stage1_pos_acc: 78.6400, stage1_loss_bbox: 0.1554, stage1_loss_iou: 0.3479, stage1_loss_mask: 0.2719, stage2_loss_cls: 0.3342, stage2_pos_acc: 87.5321, stage2_loss_bbox: 0.1192, stage2_loss_iou: 0.2631, stage2_loss_mask: 0.2359, stage3_loss_cls: 0.2082, stage3_pos_acc: 92.5597, stage3_loss_bbox: 0.1042, stage3_loss_iou: 0.2251, stage3_loss_mask: 0.2304, stage4_loss_cls: 0.1313, stage4_pos_acc: 96.6976, stage4_loss_bbox: 0.1052, stage4_loss_iou: 0.2209, stage4_loss_mask: 0.2441, stage5_loss_cls: 0.1072, stage5_pos_acc: 97.4508, stage5_loss_bbox: 0.1036, stage5_loss_iou: 0.2156, stage5_loss_mask: 0.2349, loss: 7.2459\n",
      "2025-07-16 18:06:14,111 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:06:14,111 - mmdet - INFO - Epoch [42][250/750]\tlr: 2.500e-06, eta: 1:51:54, time: 0.391, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9638, stage0_pos_acc: 37.2005, stage0_loss_bbox: 0.4662, stage0_loss_iou: 0.9150, stage0_loss_mask: 0.5323, stage1_loss_cls: 0.4943, stage1_pos_acc: 76.5110, stage1_loss_bbox: 0.1982, stage1_loss_iou: 0.3518, stage1_loss_mask: 0.3226, stage2_loss_cls: 0.3662, stage2_pos_acc: 83.6285, stage2_loss_bbox: 0.1629, stage2_loss_iou: 0.2625, stage2_loss_mask: 0.2641, stage3_loss_cls: 0.2432, stage3_pos_acc: 88.5793, stage3_loss_bbox: 0.1429, stage3_loss_iou: 0.2361, stage3_loss_mask: 0.2482, stage4_loss_cls: 0.1550, stage4_pos_acc: 92.0897, stage4_loss_bbox: 0.1528, stage4_loss_iou: 0.2361, stage4_loss_mask: 0.2494, stage5_loss_cls: 0.1344, stage5_pos_acc: 95.5397, stage5_loss_bbox: 0.1350, stage5_loss_iou: 0.2273, stage5_loss_mask: 0.2241, loss: 7.6843\n",
      "2025-07-16 18:08:26,997 - mmdet - INFO - Epoch [42][600/750]\tlr: 2.500e-06, eta: 1:49:42, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9307, stage0_pos_acc: 42.7206, stage0_loss_bbox: 0.4773, stage0_loss_iou: 0.9195, stage0_loss_mask: 0.6719, stage1_loss_cls: 0.5128, stage1_pos_acc: 75.7505, stage1_loss_bbox: 0.2130, stage1_loss_iou: 0.3674, stage1_loss_mask: 0.4331, stage2_loss_cls: 0.3450, stage2_pos_acc: 86.9769, stage2_loss_bbox: 0.1740, stage2_loss_iou: 0.2695, stage2_loss_mask: 0.4018, stage3_loss_cls: 0.2141, stage3_pos_acc: 93.9943, stage3_loss_bbox: 0.1487, stage3_loss_iou: 0.2454, stage3_loss_mask: 0.3906, stage4_loss_cls: 0.1468, stage4_pos_acc: 97.3445, stage4_loss_bbox: 0.1362, stage4_loss_iou: 0.2467, stage4_loss_mask: 0.3864, stage5_loss_cls: 0.1158, stage5_pos_acc: 98.5926, stage5_loss_bbox: 0.1360, stage5_loss_iou: 0.2420, stage5_loss_mask: 0.3844, loss: 8.5090\n",
      "2025-07-16 18:08:46,066 - mmdet - INFO - Epoch [42][650/750]\tlr: 2.500e-06, eta: 1:49:23, time: 0.381, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0070, stage0_pos_acc: 38.0536, stage0_loss_bbox: 0.4441, stage0_loss_iou: 0.9272, stage0_loss_mask: 0.5262, stage1_loss_cls: 0.4907, stage1_pos_acc: 82.5581, stage1_loss_bbox: 0.1565, stage1_loss_iou: 0.3726, stage1_loss_mask: 0.2522, stage2_loss_cls: 0.3329, stage2_pos_acc: 88.3264, stage2_loss_bbox: 0.1183, stage2_loss_iou: 0.2601, stage2_loss_mask: 0.2485, stage3_loss_cls: 0.1931, stage3_pos_acc: 94.2798, stage3_loss_bbox: 0.1106, stage3_loss_iou: 0.2424, stage3_loss_mask: 0.2430, stage4_loss_cls: 0.1254, stage4_pos_acc: 97.9488, stage4_loss_bbox: 0.1079, stage4_loss_iou: 0.2320, stage4_loss_mask: 0.2256, stage5_loss_cls: 0.1026, stage5_pos_acc: 98.2821, stage5_loss_bbox: 0.1063, stage5_loss_iou: 0.2292, stage5_loss_mask: 0.2431, loss: 7.2976\n",
      "2025-07-16 18:09:05,464 - mmdet - INFO - Epoch [42][700/750]\tlr: 2.500e-06, eta: 1:49:05, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9847, stage0_pos_acc: 36.6976, stage0_loss_bbox: 0.4381, stage0_loss_iou: 0.9668, stage0_loss_mask: 0.7073, stage1_loss_cls: 0.5064, stage1_pos_acc: 78.0437, stage1_loss_bbox: 0.1668, stage1_loss_iou: 0.4057, stage1_loss_mask: 0.3721, stage2_loss_cls: 0.3405, stage2_pos_acc: 87.2246, stage2_loss_bbox: 0.1209, stage2_loss_iou: 0.2931, stage2_loss_mask: 0.3453, stage3_loss_cls: 0.2044, stage3_pos_acc: 93.0071, stage3_loss_bbox: 0.1148, stage3_loss_iou: 0.2674, stage3_loss_mask: 0.3073, stage4_loss_cls: 0.1303, stage4_pos_acc: 95.7675, stage4_loss_bbox: 0.1103, stage4_loss_iou: 0.2560, stage4_loss_mask: 0.3123, stage5_loss_cls: 0.1093, stage5_pos_acc: 98.0389, stage5_loss_bbox: 0.1079, stage5_loss_iou: 0.2460, stage5_loss_mask: 0.3022, loss: 8.1160\n",
      "2025-07-16 18:09:25,023 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:09:25,024 - mmdet - INFO - Epoch [42][750/750]\tlr: 2.500e-06, eta: 1:48:47, time: 0.391, data_time: 0.012, memory: 11264, stage0_loss_cls: 0.9239, stage0_pos_acc: 41.7091, stage0_loss_bbox: 0.4414, stage0_loss_iou: 0.9162, stage0_loss_mask: 0.8476, stage1_loss_cls: 0.4848, stage1_pos_acc: 79.0191, stage1_loss_bbox: 0.1889, stage1_loss_iou: 0.4212, stage1_loss_mask: 0.5190, stage2_loss_cls: 0.3336, stage2_pos_acc: 85.8183, stage2_loss_bbox: 0.1496, stage2_loss_iou: 0.3188, stage2_loss_mask: 0.4344, stage3_loss_cls: 0.2181, stage3_pos_acc: 93.7013, stage3_loss_bbox: 0.1405, stage3_loss_iou: 0.2945, stage3_loss_mask: 0.4203, stage4_loss_cls: 0.1516, stage4_pos_acc: 95.5371, stage4_loss_bbox: 0.1389, stage4_loss_iou: 0.2857, stage4_loss_mask: 0.4113, stage5_loss_cls: 0.1338, stage5_pos_acc: 96.4560, stage5_loss_bbox: 0.1379, stage5_loss_iou: 0.2811, stage5_loss_mask: 0.4174, loss: 9.0103\n",
      "2025-07-16 18:09:25,148 - mmdet - INFO - Saving checkpoint at 42 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 100s, ETA:     0s2025-07-16 18:12:42,546 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.56s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.064\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.113\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.205\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.463\n",
      "2025-07-16 18:12:44,646 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.84s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.120\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.222\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.461\n",
      "2025-07-16 18:12:47,604 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:12:47,605 - mmdet - INFO - Epoch(val) [42][750]\tbbox_mAP: 0.0300, bbox_mAP_50: 0.0560, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.0640, bbox_mAP_m: 0.0160, bbox_mAP_l: 0.0350, bbox_mAP_copypaste: 0.030 0.056 0.028 0.064 0.016 0.035, segm_mAP: 0.0290, segm_mAP_50: 0.0550, segm_mAP_75: 0.0250, segm_mAP_s: 0.0330, segm_mAP_m: 0.0160, segm_mAP_l: 0.0350, segm_mAP_copypaste: 0.029 0.055 0.025 0.033 0.016 0.035\n",
      "2025-07-16 18:13:08,994 - mmdet - INFO - Epoch [43][50/750]\tlr: 2.500e-06, eta: 1:48:30, time: 0.428, data_time: 0.056, memory: 11264, stage0_loss_cls: 0.9976, stage0_pos_acc: 37.1358, stage0_loss_bbox: 0.4189, stage0_loss_iou: 0.8744, stage0_loss_mask: 0.4328, stage1_loss_cls: 0.5144, stage1_pos_acc: 73.8098, stage1_loss_bbox: 0.1416, stage1_loss_iou: 0.3041, stage1_loss_mask: 0.2851, stage2_loss_cls: 0.3481, stage2_pos_acc: 84.0554, stage2_loss_bbox: 0.1162, stage2_loss_iou: 0.2499, stage2_loss_mask: 0.2369, stage3_loss_cls: 0.2177, stage3_pos_acc: 91.0863, stage3_loss_bbox: 0.1048, stage3_loss_iou: 0.2221, stage3_loss_mask: 0.2069, stage4_loss_cls: 0.1410, stage4_pos_acc: 95.9355, stage4_loss_bbox: 0.1045, stage4_loss_iou: 0.2234, stage4_loss_mask: 0.2275, stage5_loss_cls: 0.1067, stage5_pos_acc: 96.8911, stage5_loss_bbox: 0.1044, stage5_loss_iou: 0.2174, stage5_loss_mask: 0.2246, loss: 7.0209\n",
      "2025-07-16 18:13:27,746 - mmdet - INFO - Epoch [43][100/750]\tlr: 2.500e-06, eta: 1:48:11, time: 0.375, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9871, stage0_pos_acc: 38.3190, stage0_loss_bbox: 0.4003, stage0_loss_iou: 0.8480, stage0_loss_mask: 0.5069, stage1_loss_cls: 0.4855, stage1_pos_acc: 77.2810, stage1_loss_bbox: 0.1438, stage1_loss_iou: 0.3103, stage1_loss_mask: 0.2953, stage2_loss_cls: 0.3006, stage2_pos_acc: 87.4238, stage2_loss_bbox: 0.1119, stage2_loss_iou: 0.2309, stage2_loss_mask: 0.2583, stage3_loss_cls: 0.1715, stage3_pos_acc: 93.2952, stage3_loss_bbox: 0.1033, stage3_loss_iou: 0.2134, stage3_loss_mask: 0.2471, stage4_loss_cls: 0.1004, stage4_pos_acc: 95.8762, stage4_loss_bbox: 0.1013, stage4_loss_iou: 0.2022, stage4_loss_mask: 0.2469, stage5_loss_cls: 0.0685, stage5_pos_acc: 97.7429, stage5_loss_bbox: 0.0977, stage5_loss_iou: 0.1988, stage5_loss_mask: 0.2449, loss: 6.8750\n",
      "2025-07-16 18:13:46,532 - mmdet - INFO - Epoch [43][150/750]\tlr: 2.500e-06, eta: 1:47:52, time: 0.376, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9409, stage0_pos_acc: 42.5561, stage0_loss_bbox: 0.4310, stage0_loss_iou: 0.8499, stage0_loss_mask: 0.6033, stage1_loss_cls: 0.4853, stage1_pos_acc: 77.6959, stage1_loss_bbox: 0.1818, stage1_loss_iou: 0.3531, stage1_loss_mask: 0.3517, stage2_loss_cls: 0.3687, stage2_pos_acc: 84.3626, stage2_loss_bbox: 0.1513, stage2_loss_iou: 0.2823, stage2_loss_mask: 0.3192, stage3_loss_cls: 0.2379, stage3_pos_acc: 93.0301, stage3_loss_bbox: 0.1291, stage3_loss_iou: 0.2563, stage3_loss_mask: 0.2924, stage4_loss_cls: 0.1796, stage4_pos_acc: 94.3976, stage4_loss_bbox: 0.1389, stage4_loss_iou: 0.2466, stage4_loss_mask: 0.3167, stage5_loss_cls: 0.1545, stage5_pos_acc: 95.9405, stage5_loss_bbox: 0.1330, stage5_loss_iou: 0.2444, stage5_loss_mask: 0.3111, loss: 7.9590\n",
      "2025-07-16 18:14:05,301 - mmdet - INFO - Epoch [43][200/750]\tlr: 2.500e-06, eta: 1:47:33, time: 0.375, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9333, stage0_pos_acc: 48.3918, stage0_loss_bbox: 0.4342, stage0_loss_iou: 0.9474, stage0_loss_mask: 0.6652, stage1_loss_cls: 0.4743, stage1_pos_acc: 83.5602, stage1_loss_bbox: 0.1713, stage1_loss_iou: 0.4069, stage1_loss_mask: 0.3863, stage2_loss_cls: 0.3100, stage2_pos_acc: 90.7432, stage2_loss_bbox: 0.1415, stage2_loss_iou: 0.3001, stage2_loss_mask: 0.3368, stage3_loss_cls: 0.1765, stage3_pos_acc: 95.6064, stage3_loss_bbox: 0.1322, stage3_loss_iou: 0.2786, stage3_loss_mask: 0.3216, stage4_loss_cls: 0.1311, stage4_pos_acc: 96.7107, stage4_loss_bbox: 0.1261, stage4_loss_iou: 0.2618, stage4_loss_mask: 0.3178, stage5_loss_cls: 0.1088, stage5_pos_acc: 98.0708, stage5_loss_bbox: 0.1248, stage5_loss_iou: 0.2550, stage5_loss_mask: 0.3179, loss: 8.0595\n",
      "2025-07-16 18:14:23,759 - mmdet - INFO - Epoch [43][250/750]\tlr: 2.500e-06, eta: 1:47:14, time: 0.369, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0030, stage0_pos_acc: 35.8929, stage0_loss_bbox: 0.3967, stage0_loss_iou: 0.8181, stage0_loss_mask: 0.4082, stage1_loss_cls: 0.5004, stage1_pos_acc: 78.2524, stage1_loss_bbox: 0.1464, stage1_loss_iou: 0.2936, stage1_loss_mask: 0.2240, stage2_loss_cls: 0.3174, stage2_pos_acc: 88.3810, stage2_loss_bbox: 0.1189, stage2_loss_iou: 0.2080, stage2_loss_mask: 0.1992, stage3_loss_cls: 0.1664, stage3_pos_acc: 94.1738, stage3_loss_bbox: 0.1135, stage3_loss_iou: 0.1895, stage3_loss_mask: 0.1979, stage4_loss_cls: 0.0950, stage4_pos_acc: 96.6238, stage4_loss_bbox: 0.1099, stage4_loss_iou: 0.1832, stage4_loss_mask: 0.1982, stage5_loss_cls: 0.0702, stage5_pos_acc: 98.3952, stage5_loss_bbox: 0.1055, stage5_loss_iou: 0.1767, stage5_loss_mask: 0.1890, loss: 6.4288\n",
      "2025-07-16 18:14:42,296 - mmdet - INFO - Epoch [43][300/750]\tlr: 2.500e-06, eta: 1:46:54, time: 0.371, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9256, stage0_pos_acc: 43.2313, stage0_loss_bbox: 0.4044, stage0_loss_iou: 0.8883, stage0_loss_mask: 0.5826, stage1_loss_cls: 0.4666, stage1_pos_acc: 80.3469, stage1_loss_bbox: 0.1640, stage1_loss_iou: 0.3656, stage1_loss_mask: 0.3517, stage2_loss_cls: 0.3128, stage2_pos_acc: 89.9711, stage2_loss_bbox: 0.1341, stage2_loss_iou: 0.2794, stage2_loss_mask: 0.3193, stage3_loss_cls: 0.1893, stage3_pos_acc: 93.5106, stage3_loss_bbox: 0.1236, stage3_loss_iou: 0.2584, stage3_loss_mask: 0.3086, stage4_loss_cls: 0.1294, stage4_pos_acc: 96.0391, stage4_loss_bbox: 0.1184, stage4_loss_iou: 0.2477, stage4_loss_mask: 0.2971, stage5_loss_cls: 0.1159, stage5_pos_acc: 97.3391, stage5_loss_bbox: 0.1150, stage5_loss_iou: 0.2362, stage5_loss_mask: 0.3017, loss: 7.6358\n",
      "2025-07-16 18:15:00,681 - mmdet - INFO - Epoch [43][350/750]\tlr: 2.500e-06, eta: 1:46:35, time: 0.368, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9900, stage0_pos_acc: 38.4738, stage0_loss_bbox: 0.4371, stage0_loss_iou: 0.9192, stage0_loss_mask: 0.6098, stage1_loss_cls: 0.4977, stage1_pos_acc: 77.5419, stage1_loss_bbox: 0.1543, stage1_loss_iou: 0.3553, stage1_loss_mask: 0.3014, stage2_loss_cls: 0.3395, stage2_pos_acc: 87.2326, stage2_loss_bbox: 0.1173, stage2_loss_iou: 0.2696, stage2_loss_mask: 0.2903, stage3_loss_cls: 0.1971, stage3_pos_acc: 92.9792, stage3_loss_bbox: 0.1167, stage3_loss_iou: 0.2606, stage3_loss_mask: 0.2998, stage4_loss_cls: 0.1401, stage4_pos_acc: 96.1453, stage4_loss_bbox: 0.1110, stage4_loss_iou: 0.2425, stage4_loss_mask: 0.2883, stage5_loss_cls: 0.1155, stage5_pos_acc: 97.9339, stage5_loss_bbox: 0.1114, stage5_loss_iou: 0.2377, stage5_loss_mask: 0.2730, loss: 7.6752\n",
      "2025-07-16 18:15:19,392 - mmdet - INFO - Epoch [43][400/750]\tlr: 2.500e-06, eta: 1:46:16, time: 0.374, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9964, stage0_pos_acc: 42.0918, stage0_loss_bbox: 0.3968, stage0_loss_iou: 0.8878, stage0_loss_mask: 0.5167, stage1_loss_cls: 0.4718, stage1_pos_acc: 79.6340, stage1_loss_bbox: 0.1444, stage1_loss_iou: 0.3134, stage1_loss_mask: 0.2709, stage2_loss_cls: 0.3124, stage2_pos_acc: 87.9269, stage2_loss_bbox: 0.1036, stage2_loss_iou: 0.2223, stage2_loss_mask: 0.2249, stage3_loss_cls: 0.1758, stage3_pos_acc: 94.2737, stage3_loss_bbox: 0.1016, stage3_loss_iou: 0.2023, stage3_loss_mask: 0.2125, stage4_loss_cls: 0.1196, stage4_pos_acc: 97.1889, stage4_loss_bbox: 0.0917, stage4_loss_iou: 0.1916, stage4_loss_mask: 0.1974, stage5_loss_cls: 0.0930, stage5_pos_acc: 97.5444, stage5_loss_bbox: 0.0901, stage5_loss_iou: 0.1910, stage5_loss_mask: 0.1963, loss: 6.7246\n",
      "2025-07-16 18:15:37,854 - mmdet - INFO - Epoch [43][450/750]\tlr: 2.500e-06, eta: 1:45:56, time: 0.369, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9354, stage0_pos_acc: 45.3988, stage0_loss_bbox: 0.4309, stage0_loss_iou: 0.8627, stage0_loss_mask: 0.7889, stage1_loss_cls: 0.4983, stage1_pos_acc: 78.4907, stage1_loss_bbox: 0.2095, stage1_loss_iou: 0.4202, stage1_loss_mask: 0.5034, stage2_loss_cls: 0.3737, stage2_pos_acc: 83.2933, stage2_loss_bbox: 0.1785, stage2_loss_iou: 0.3511, stage2_loss_mask: 0.4688, stage3_loss_cls: 0.2313, stage3_pos_acc: 91.8676, stage3_loss_bbox: 0.1728, stage3_loss_iou: 0.3297, stage3_loss_mask: 0.4646, stage4_loss_cls: 0.1748, stage4_pos_acc: 94.3697, stage4_loss_bbox: 0.1667, stage4_loss_iou: 0.3221, stage4_loss_mask: 0.4790, stage5_loss_cls: 0.1528, stage5_pos_acc: 95.9621, stage5_loss_bbox: 0.1660, stage5_loss_iou: 0.3166, stage5_loss_mask: 0.4737, loss: 9.4714\n",
      "2025-07-16 18:15:56,288 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:15:56,288 - mmdet - INFO - Epoch [43][500/750]\tlr: 2.500e-06, eta: 1:45:37, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9562, stage0_pos_acc: 40.1518, stage0_loss_bbox: 0.4128, stage0_loss_iou: 0.9112, stage0_loss_mask: 0.7221, stage1_loss_cls: 0.5070, stage1_pos_acc: 76.1877, stage1_loss_bbox: 0.1671, stage1_loss_iou: 0.3815, stage1_loss_mask: 0.3484, stage2_loss_cls: 0.3563, stage2_pos_acc: 85.7290, stage2_loss_bbox: 0.1338, stage2_loss_iou: 0.2898, stage2_loss_mask: 0.3042, stage3_loss_cls: 0.1889, stage3_pos_acc: 94.9314, stage3_loss_bbox: 0.1250, stage3_loss_iou: 0.2755, stage3_loss_mask: 0.2901, stage4_loss_cls: 0.1230, stage4_pos_acc: 97.4412, stage4_loss_bbox: 0.1182, stage4_loss_iou: 0.2614, stage4_loss_mask: 0.2664, stage5_loss_cls: 0.1029, stage5_pos_acc: 98.4324, stage5_loss_bbox: 0.1114, stage5_loss_iou: 0.2489, stage5_loss_mask: 0.2631, loss: 7.8653\n",
      "2025-07-16 18:16:14,719 - mmdet - INFO - Epoch [43][550/750]\tlr: 2.500e-06, eta: 1:45:18, time: 0.369, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9953, stage0_pos_acc: 39.1408, stage0_loss_bbox: 0.4015, stage0_loss_iou: 0.8994, stage0_loss_mask: 0.5508, stage1_loss_cls: 0.4741, stage1_pos_acc: 74.9752, stage1_loss_bbox: 0.1409, stage1_loss_iou: 0.3379, stage1_loss_mask: 0.2771, stage2_loss_cls: 0.3103, stage2_pos_acc: 88.0055, stage2_loss_bbox: 0.1098, stage2_loss_iou: 0.2440, stage2_loss_mask: 0.2361, stage3_loss_cls: 0.1957, stage3_pos_acc: 92.1833, stage3_loss_bbox: 0.0961, stage3_loss_iou: 0.2063, stage3_loss_mask: 0.1908, stage4_loss_cls: 0.1111, stage4_pos_acc: 95.3126, stage4_loss_bbox: 0.0924, stage4_loss_iou: 0.1981, stage4_loss_mask: 0.1998, stage5_loss_cls: 0.0760, stage5_pos_acc: 98.8658, stage5_loss_bbox: 0.0890, stage5_loss_iou: 0.1956, stage5_loss_mask: 0.1862, loss: 6.8145\n",
      "2025-07-16 18:16:33,399 - mmdet - INFO - Epoch [43][600/750]\tlr: 2.500e-06, eta: 1:44:58, time: 0.374, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9577, stage0_pos_acc: 40.6897, stage0_loss_bbox: 0.4113, stage0_loss_iou: 0.9342, stage0_loss_mask: 0.7229, stage1_loss_cls: 0.4797, stage1_pos_acc: 80.5591, stage1_loss_bbox: 0.1495, stage1_loss_iou: 0.3778, stage1_loss_mask: 0.4015, stage2_loss_cls: 0.3257, stage2_pos_acc: 87.6799, stage2_loss_bbox: 0.1161, stage2_loss_iou: 0.2775, stage2_loss_mask: 0.3562, stage3_loss_cls: 0.2046, stage3_pos_acc: 91.0403, stage3_loss_bbox: 0.1064, stage3_loss_iou: 0.2527, stage3_loss_mask: 0.3827, stage4_loss_cls: 0.1369, stage4_pos_acc: 94.2181, stage4_loss_bbox: 0.1042, stage4_loss_iou: 0.2482, stage4_loss_mask: 0.3866, stage5_loss_cls: 0.1107, stage5_pos_acc: 95.7713, stage5_loss_bbox: 0.0997, stage5_loss_iou: 0.2428, stage5_loss_mask: 0.3745, loss: 8.1600\n",
      "2025-07-16 18:16:52,675 - mmdet - INFO - Epoch [43][650/750]\tlr: 2.500e-06, eta: 1:44:40, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9828, stage0_pos_acc: 40.2769, stage0_loss_bbox: 0.4767, stage0_loss_iou: 0.9698, stage0_loss_mask: 0.6929, stage1_loss_cls: 0.5034, stage1_pos_acc: 76.4209, stage1_loss_bbox: 0.2150, stage1_loss_iou: 0.3813, stage1_loss_mask: 0.4156, stage2_loss_cls: 0.3607, stage2_pos_acc: 86.9819, stage2_loss_bbox: 0.1721, stage2_loss_iou: 0.2871, stage2_loss_mask: 0.4033, stage3_loss_cls: 0.2283, stage3_pos_acc: 93.1652, stage3_loss_bbox: 0.1427, stage3_loss_iou: 0.2682, stage3_loss_mask: 0.3749, stage4_loss_cls: 0.1485, stage4_pos_acc: 97.9167, stage4_loss_bbox: 0.1251, stage4_loss_iou: 0.2651, stage4_loss_mask: 0.3867, stage5_loss_cls: 0.1149, stage5_pos_acc: 98.3190, stage5_loss_bbox: 0.1237, stage5_loss_iou: 0.2585, stage5_loss_mask: 0.3593, loss: 8.6564\n",
      "2025-07-16 18:17:11,960 - mmdet - INFO - Epoch [43][700/750]\tlr: 2.500e-06, eta: 1:44:21, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9763, stage0_pos_acc: 38.0311, stage0_loss_bbox: 0.3921, stage0_loss_iou: 0.9280, stage0_loss_mask: 0.4868, stage1_loss_cls: 0.4826, stage1_pos_acc: 79.2887, stage1_loss_bbox: 0.1326, stage1_loss_iou: 0.3086, stage1_loss_mask: 0.1963, stage2_loss_cls: 0.3130, stage2_pos_acc: 87.9189, stage2_loss_bbox: 0.0981, stage2_loss_iou: 0.2177, stage2_loss_mask: 0.1673, stage3_loss_cls: 0.1869, stage3_pos_acc: 94.1166, stage3_loss_bbox: 0.0873, stage3_loss_iou: 0.1868, stage3_loss_mask: 0.1634, stage4_loss_cls: 0.1020, stage4_pos_acc: 96.4944, stage4_loss_bbox: 0.0859, stage4_loss_iou: 0.1825, stage4_loss_mask: 0.1625, stage5_loss_cls: 0.0821, stage5_pos_acc: 97.9698, stage5_loss_bbox: 0.0823, stage5_loss_iou: 0.1769, stage5_loss_mask: 0.1554, loss: 6.3535\n",
      "2025-07-16 18:17:31,005 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:17:31,005 - mmdet - INFO - Epoch [43][750/750]\tlr: 2.500e-06, eta: 1:44:02, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9967, stage0_pos_acc: 43.2357, stage0_loss_bbox: 0.4534, stage0_loss_iou: 1.0592, stage0_loss_mask: 0.7905, stage1_loss_cls: 0.5274, stage1_pos_acc: 76.0310, stage1_loss_bbox: 0.1663, stage1_loss_iou: 0.4600, stage1_loss_mask: 0.4971, stage2_loss_cls: 0.3510, stage2_pos_acc: 85.6317, stage2_loss_bbox: 0.1303, stage2_loss_iou: 0.3364, stage2_loss_mask: 0.4658, stage3_loss_cls: 0.2138, stage3_pos_acc: 91.4587, stage3_loss_bbox: 0.1177, stage3_loss_iou: 0.3126, stage3_loss_mask: 0.4600, stage4_loss_cls: 0.1475, stage4_pos_acc: 95.8484, stage4_loss_bbox: 0.1152, stage4_loss_iou: 0.3011, stage4_loss_mask: 0.4477, stage5_loss_cls: 0.1230, stage5_pos_acc: 96.7722, stage5_loss_bbox: 0.1188, stage5_loss_iou: 0.2973, stage5_loss_mask: 0.4576, loss: 9.3467\n",
      "2025-07-16 18:17:31,145 - mmdet - INFO - Saving checkpoint at 43 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 18:20:50,246 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.085\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.423\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.153\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.246\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.462\n",
      "2025-07-16 18:20:52,305 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.82s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.425\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.264\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.461\n",
      "2025-07-16 18:20:55,515 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:20:55,515 - mmdet - INFO - Epoch(val) [43][750]\tbbox_mAP: 0.0290, bbox_mAP_50: 0.0530, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.0850, bbox_mAP_m: 0.0170, bbox_mAP_l: 0.0360, bbox_mAP_copypaste: 0.029 0.053 0.028 0.085 0.017 0.036, segm_mAP: 0.0290, segm_mAP_50: 0.0530, segm_mAP_75: 0.0270, segm_mAP_s: 0.0530, segm_mAP_m: 0.0170, segm_mAP_l: 0.0360, segm_mAP_copypaste: 0.029 0.053 0.027 0.053 0.017 0.036\n",
      "2025-07-16 18:21:16,754 - mmdet - INFO - Epoch [44][50/750]\tlr: 2.500e-06, eta: 1:43:46, time: 0.425, data_time: 0.054, memory: 11264, stage0_loss_cls: 0.9006, stage0_pos_acc: 44.8110, stage0_loss_bbox: 0.4084, stage0_loss_iou: 0.8493, stage0_loss_mask: 0.5340, stage1_loss_cls: 0.4603, stage1_pos_acc: 83.0300, stage1_loss_bbox: 0.1656, stage1_loss_iou: 0.3300, stage1_loss_mask: 0.3272, stage2_loss_cls: 0.3124, stage2_pos_acc: 91.0954, stage2_loss_bbox: 0.1354, stage2_loss_iou: 0.2507, stage2_loss_mask: 0.2860, stage3_loss_cls: 0.1779, stage3_pos_acc: 95.1387, stage3_loss_bbox: 0.1264, stage3_loss_iou: 0.2286, stage3_loss_mask: 0.2637, stage4_loss_cls: 0.1171, stage4_pos_acc: 97.3975, stage4_loss_bbox: 0.1207, stage4_loss_iou: 0.2204, stage4_loss_mask: 0.2542, stage5_loss_cls: 0.0858, stage5_pos_acc: 98.9872, stage5_loss_bbox: 0.1198, stage5_loss_iou: 0.2148, stage5_loss_mask: 0.2564, loss: 7.1454\n",
      "2025-07-16 18:21:35,622 - mmdet - INFO - Epoch [44][100/750]\tlr: 2.500e-06, eta: 1:43:27, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9517, stage0_pos_acc: 43.3623, stage0_loss_bbox: 0.4258, stage0_loss_iou: 0.9504, stage0_loss_mask: 0.6208, stage1_loss_cls: 0.4750, stage1_pos_acc: 82.2574, stage1_loss_bbox: 0.1680, stage1_loss_iou: 0.3650, stage1_loss_mask: 0.3027, stage2_loss_cls: 0.2966, stage2_pos_acc: 90.9041, stage2_loss_bbox: 0.1238, stage2_loss_iou: 0.2533, stage2_loss_mask: 0.2700, stage3_loss_cls: 0.1690, stage3_pos_acc: 94.4651, stage3_loss_bbox: 0.1125, stage3_loss_iou: 0.2232, stage3_loss_mask: 0.2667, stage4_loss_cls: 0.0959, stage4_pos_acc: 97.9881, stage4_loss_bbox: 0.1104, stage4_loss_iou: 0.2134, stage4_loss_mask: 0.2680, stage5_loss_cls: 0.0712, stage5_pos_acc: 98.5168, stage5_loss_bbox: 0.1090, stage5_loss_iou: 0.2080, stage5_loss_mask: 0.2694, loss: 7.3197\n",
      "2025-07-16 18:21:54,963 - mmdet - INFO - Epoch [44][150/750]\tlr: 2.500e-06, eta: 1:43:08, time: 0.387, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9477, stage0_pos_acc: 41.6484, stage0_loss_bbox: 0.3798, stage0_loss_iou: 0.8171, stage0_loss_mask: 0.5913, stage1_loss_cls: 0.4842, stage1_pos_acc: 77.7587, stage1_loss_bbox: 0.1699, stage1_loss_iou: 0.3578, stage1_loss_mask: 0.3647, stage2_loss_cls: 0.3372, stage2_pos_acc: 86.6411, stage2_loss_bbox: 0.1382, stage2_loss_iou: 0.2844, stage2_loss_mask: 0.3342, stage3_loss_cls: 0.2050, stage3_pos_acc: 92.5306, stage3_loss_bbox: 0.1314, stage3_loss_iou: 0.2601, stage3_loss_mask: 0.3401, stage4_loss_cls: 0.1302, stage4_pos_acc: 95.6244, stage4_loss_bbox: 0.1270, stage4_loss_iou: 0.2480, stage4_loss_mask: 0.3278, stage5_loss_cls: 0.1028, stage5_pos_acc: 97.5417, stage5_loss_bbox: 0.1238, stage5_loss_iou: 0.2431, stage5_loss_mask: 0.3274, loss: 7.7730\n",
      "2025-07-16 18:22:14,508 - mmdet - INFO - Epoch [44][200/750]\tlr: 2.500e-06, eta: 1:42:50, time: 0.391, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9760, stage0_pos_acc: 40.9745, stage0_loss_bbox: 0.3922, stage0_loss_iou: 0.8836, stage0_loss_mask: 0.4533, stage1_loss_cls: 0.4609, stage1_pos_acc: 83.5613, stage1_loss_bbox: 0.1482, stage1_loss_iou: 0.3208, stage1_loss_mask: 0.2436, stage2_loss_cls: 0.3155, stage2_pos_acc: 91.0717, stage2_loss_bbox: 0.1161, stage2_loss_iou: 0.2408, stage2_loss_mask: 0.2095, stage3_loss_cls: 0.1703, stage3_pos_acc: 95.0365, stage3_loss_bbox: 0.1037, stage3_loss_iou: 0.2169, stage3_loss_mask: 0.1989, stage4_loss_cls: 0.0956, stage4_pos_acc: 97.3619, stage4_loss_bbox: 0.1083, stage4_loss_iou: 0.2103, stage4_loss_mask: 0.1933, stage5_loss_cls: 0.0717, stage5_pos_acc: 98.3048, stage5_loss_bbox: 0.1053, stage5_loss_iou: 0.2064, stage5_loss_mask: 0.1940, loss: 6.6354\n",
      "2025-07-16 18:22:33,927 - mmdet - INFO - Epoch [44][250/750]\tlr: 2.500e-06, eta: 1:42:32, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9774, stage0_pos_acc: 46.3025, stage0_loss_bbox: 0.4393, stage0_loss_iou: 0.9296, stage0_loss_mask: 0.5680, stage1_loss_cls: 0.5099, stage1_pos_acc: 78.1371, stage1_loss_bbox: 0.1654, stage1_loss_iou: 0.3509, stage1_loss_mask: 0.3131, stage2_loss_cls: 0.3467, stage2_pos_acc: 87.4556, stage2_loss_bbox: 0.1276, stage2_loss_iou: 0.2623, stage2_loss_mask: 0.2858, stage3_loss_cls: 0.1948, stage3_pos_acc: 94.1852, stage3_loss_bbox: 0.1156, stage3_loss_iou: 0.2461, stage3_loss_mask: 0.2746, stage4_loss_cls: 0.1357, stage4_pos_acc: 95.2947, stage4_loss_bbox: 0.1078, stage4_loss_iou: 0.2329, stage4_loss_mask: 0.2727, stage5_loss_cls: 0.1189, stage5_pos_acc: 95.2728, stage5_loss_bbox: 0.1062, stage5_loss_iou: 0.2260, stage5_loss_mask: 0.2682, loss: 7.5754\n",
      "2025-07-16 18:22:53,604 - mmdet - INFO - Epoch [44][300/750]\tlr: 2.500e-06, eta: 1:42:13, time: 0.394, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9802, stage0_pos_acc: 41.0575, stage0_loss_bbox: 0.5109, stage0_loss_iou: 1.0087, stage0_loss_mask: 1.0374, stage1_loss_cls: 0.5439, stage1_pos_acc: 76.1260, stage1_loss_bbox: 0.2315, stage1_loss_iou: 0.4736, stage1_loss_mask: 0.6324, stage2_loss_cls: 0.3726, stage2_pos_acc: 86.9825, stage2_loss_bbox: 0.1875, stage2_loss_iou: 0.3805, stage2_loss_mask: 0.6174, stage3_loss_cls: 0.2035, stage3_pos_acc: 93.8046, stage3_loss_bbox: 0.1639, stage3_loss_iou: 0.3503, stage3_loss_mask: 0.6113, stage4_loss_cls: 0.1477, stage4_pos_acc: 96.8132, stage4_loss_bbox: 0.1567, stage4_loss_iou: 0.3276, stage4_loss_mask: 0.5920, stage5_loss_cls: 0.1383, stage5_pos_acc: 96.6734, stage5_loss_bbox: 0.1497, stage5_loss_iou: 0.3312, stage5_loss_mask: 0.6063, loss: 10.7552\n",
      "2025-07-16 18:23:12,168 - mmdet - INFO - Epoch [44][350/750]\tlr: 2.500e-06, eta: 1:41:54, time: 0.371, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9605, stage0_pos_acc: 42.5844, stage0_loss_bbox: 0.3949, stage0_loss_iou: 0.9007, stage0_loss_mask: 0.6361, stage1_loss_cls: 0.5013, stage1_pos_acc: 78.5049, stage1_loss_bbox: 0.1501, stage1_loss_iou: 0.3655, stage1_loss_mask: 0.2774, stage2_loss_cls: 0.3256, stage2_pos_acc: 85.6700, stage2_loss_bbox: 0.1145, stage2_loss_iou: 0.2592, stage2_loss_mask: 0.2514, stage3_loss_cls: 0.2064, stage3_pos_acc: 93.6816, stage3_loss_bbox: 0.1135, stage3_loss_iou: 0.2317, stage3_loss_mask: 0.2414, stage4_loss_cls: 0.1249, stage4_pos_acc: 97.6935, stage4_loss_bbox: 0.1098, stage4_loss_iou: 0.2300, stage4_loss_mask: 0.2435, stage5_loss_cls: 0.1011, stage5_pos_acc: 97.8741, stage5_loss_bbox: 0.1068, stage5_loss_iou: 0.2296, stage5_loss_mask: 0.2364, loss: 7.3126\n",
      "2025-07-16 18:23:30,957 - mmdet - INFO - Epoch [44][400/750]\tlr: 2.500e-06, eta: 1:41:35, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9675, stage0_pos_acc: 44.6918, stage0_loss_bbox: 0.3966, stage0_loss_iou: 0.8619, stage0_loss_mask: 0.4993, stage1_loss_cls: 0.4801, stage1_pos_acc: 80.6657, stage1_loss_bbox: 0.1470, stage1_loss_iou: 0.3000, stage1_loss_mask: 0.2248, stage2_loss_cls: 0.3258, stage2_pos_acc: 84.8531, stage2_loss_bbox: 0.1170, stage2_loss_iou: 0.2192, stage2_loss_mask: 0.2102, stage3_loss_cls: 0.2016, stage3_pos_acc: 92.4181, stage3_loss_bbox: 0.1076, stage3_loss_iou: 0.2039, stage3_loss_mask: 0.1917, stage4_loss_cls: 0.1304, stage4_pos_acc: 97.8365, stage4_loss_bbox: 0.1019, stage4_loss_iou: 0.1962, stage4_loss_mask: 0.1842, stage5_loss_cls: 0.1004, stage5_pos_acc: 97.7856, stage5_loss_bbox: 0.0996, stage5_loss_iou: 0.1925, stage5_loss_mask: 0.1780, loss: 6.6372\n",
      "2025-07-16 18:23:50,327 - mmdet - INFO - Epoch [44][450/750]\tlr: 2.500e-06, eta: 1:41:16, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9536, stage0_pos_acc: 41.4494, stage0_loss_bbox: 0.3858, stage0_loss_iou: 0.8419, stage0_loss_mask: 0.6317, stage1_loss_cls: 0.4960, stage1_pos_acc: 79.3658, stage1_loss_bbox: 0.1628, stage1_loss_iou: 0.3367, stage1_loss_mask: 0.4026, stage2_loss_cls: 0.3294, stage2_pos_acc: 86.5446, stage2_loss_bbox: 0.1310, stage2_loss_iou: 0.2600, stage2_loss_mask: 0.3820, stage3_loss_cls: 0.2112, stage3_pos_acc: 91.6056, stage3_loss_bbox: 0.1230, stage3_loss_iou: 0.2406, stage3_loss_mask: 0.3698, stage4_loss_cls: 0.1388, stage4_pos_acc: 95.2198, stage4_loss_bbox: 0.1219, stage4_loss_iou: 0.2352, stage4_loss_mask: 0.3673, stage5_loss_cls: 0.1123, stage5_pos_acc: 96.7976, stage5_loss_bbox: 0.1214, stage5_loss_iou: 0.2340, stage5_loss_mask: 0.3637, loss: 7.9527\n",
      "2025-07-16 18:24:09,744 - mmdet - INFO - Epoch [44][500/750]\tlr: 2.500e-06, eta: 1:40:58, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9671, stage0_pos_acc: 44.1222, stage0_loss_bbox: 0.4240, stage0_loss_iou: 0.9000, stage0_loss_mask: 0.6426, stage1_loss_cls: 0.4532, stage1_pos_acc: 81.5500, stage1_loss_bbox: 0.1420, stage1_loss_iou: 0.3230, stage1_loss_mask: 0.2407, stage2_loss_cls: 0.2951, stage2_pos_acc: 91.1833, stage2_loss_bbox: 0.1109, stage2_loss_iou: 0.2446, stage2_loss_mask: 0.2351, stage3_loss_cls: 0.1612, stage3_pos_acc: 97.3500, stage3_loss_bbox: 0.1004, stage3_loss_iou: 0.2196, stage3_loss_mask: 0.2206, stage4_loss_cls: 0.0938, stage4_pos_acc: 98.6444, stage4_loss_bbox: 0.0986, stage4_loss_iou: 0.2154, stage4_loss_mask: 0.2218, stage5_loss_cls: 0.0762, stage5_pos_acc: 99.4444, stage5_loss_bbox: 0.0969, stage5_loss_iou: 0.2103, stage5_loss_mask: 0.2154, loss: 6.9086\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 99s, ETA:     0s2025-07-16 18:29:02,090 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.33s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.407\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.448\n",
      "2025-07-16 18:29:04,159 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.37s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.87s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.042\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.019\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.409\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.412\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.412\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.259\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.445\n",
      "2025-07-16 18:29:07,399 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:29:07,399 - mmdet - INFO - Epoch(val) [44][750]\tbbox_mAP: 0.0310, bbox_mAP_50: 0.0560, bbox_mAP_75: 0.0300, bbox_mAP_s: 0.1230, bbox_mAP_m: 0.0180, bbox_mAP_l: 0.0370, bbox_mAP_copypaste: 0.031 0.056 0.030 0.123 0.018 0.037, segm_mAP: 0.0300, segm_mAP_50: 0.0550, segm_mAP_75: 0.0290, segm_mAP_s: 0.0420, segm_mAP_m: 0.0190, segm_mAP_l: 0.0370, segm_mAP_copypaste: 0.030 0.055 0.029 0.042 0.019 0.037\n",
      "2025-07-16 18:29:28,537 - mmdet - INFO - Epoch [45][50/750]\tlr: 2.500e-06, eta: 1:39:08, time: 0.423, data_time: 0.054, memory: 11264, stage0_loss_cls: 0.9365, stage0_pos_acc: 38.2251, stage0_loss_bbox: 0.4048, stage0_loss_iou: 0.8688, stage0_loss_mask: 0.5053, stage1_loss_cls: 0.4588, stage1_pos_acc: 79.6526, stage1_loss_bbox: 0.1516, stage1_loss_iou: 0.3293, stage1_loss_mask: 0.3094, stage2_loss_cls: 0.3146, stage2_pos_acc: 88.3018, stage2_loss_bbox: 0.1067, stage2_loss_iou: 0.2464, stage2_loss_mask: 0.2577, stage3_loss_cls: 0.1543, stage3_pos_acc: 94.2798, stage3_loss_bbox: 0.1112, stage3_loss_iou: 0.2266, stage3_loss_mask: 0.2743, stage4_loss_cls: 0.0828, stage4_pos_acc: 97.4623, stage4_loss_bbox: 0.1077, stage4_loss_iou: 0.2151, stage4_loss_mask: 0.2585, stage5_loss_cls: 0.0543, stage5_pos_acc: 98.8842, stage5_loss_bbox: 0.1073, stage5_loss_iou: 0.2148, stage5_loss_mask: 0.2618, loss: 6.9588\n",
      "2025-07-16 18:29:47,387 - mmdet - INFO - Epoch [45][100/750]\tlr: 2.500e-06, eta: 1:38:49, time: 0.377, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9445, stage0_pos_acc: 43.1455, stage0_loss_bbox: 0.4380, stage0_loss_iou: 0.9287, stage0_loss_mask: 0.6584, stage1_loss_cls: 0.4492, stage1_pos_acc: 84.7417, stage1_loss_bbox: 0.1498, stage1_loss_iou: 0.3481, stage1_loss_mask: 0.3359, stage2_loss_cls: 0.2936, stage2_pos_acc: 92.4020, stage2_loss_bbox: 0.1181, stage2_loss_iou: 0.2694, stage2_loss_mask: 0.3144, stage3_loss_cls: 0.1843, stage3_pos_acc: 93.1734, stage3_loss_bbox: 0.1119, stage3_loss_iou: 0.2486, stage3_loss_mask: 0.3132, stage4_loss_cls: 0.1143, stage4_pos_acc: 96.8460, stage4_loss_bbox: 0.1140, stage4_loss_iou: 0.2441, stage4_loss_mask: 0.3031, stage5_loss_cls: 0.0987, stage5_pos_acc: 98.1516, stage5_loss_bbox: 0.1104, stage5_loss_iou: 0.2392, stage5_loss_mask: 0.2999, loss: 7.6297\n",
      "2025-07-16 18:30:06,745 - mmdet - INFO - Epoch [45][150/750]\tlr: 2.500e-06, eta: 1:38:30, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9450, stage0_pos_acc: 40.6772, stage0_loss_bbox: 0.3983, stage0_loss_iou: 0.9123, stage0_loss_mask: 0.6606, stage1_loss_cls: 0.4453, stage1_pos_acc: 79.8759, stage1_loss_bbox: 0.1567, stage1_loss_iou: 0.3564, stage1_loss_mask: 0.3488, stage2_loss_cls: 0.2850, stage2_pos_acc: 90.8812, stage2_loss_bbox: 0.1222, stage2_loss_iou: 0.2785, stage2_loss_mask: 0.3147, stage3_loss_cls: 0.1738, stage3_pos_acc: 95.4329, stage3_loss_bbox: 0.1101, stage3_loss_iou: 0.2515, stage3_loss_mask: 0.2948, stage4_loss_cls: 0.1220, stage4_pos_acc: 95.8994, stage4_loss_bbox: 0.1112, stage4_loss_iou: 0.2437, stage4_loss_mask: 0.2959, stage5_loss_cls: 0.0996, stage5_pos_acc: 98.3983, stage5_loss_bbox: 0.1064, stage5_loss_iou: 0.2329, stage5_loss_mask: 0.2985, loss: 7.5640\n",
      "2025-07-16 18:30:26,291 - mmdet - INFO - Epoch [45][200/750]\tlr: 2.500e-06, eta: 1:38:12, time: 0.391, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9181, stage0_pos_acc: 41.9930, stage0_loss_bbox: 0.3898, stage0_loss_iou: 0.7681, stage0_loss_mask: 0.4818, stage1_loss_cls: 0.4929, stage1_pos_acc: 79.0099, stage1_loss_bbox: 0.1634, stage1_loss_iou: 0.3234, stage1_loss_mask: 0.2960, stage2_loss_cls: 0.3301, stage2_pos_acc: 89.2809, stage2_loss_bbox: 0.1295, stage2_loss_iou: 0.2536, stage2_loss_mask: 0.2715, stage3_loss_cls: 0.1938, stage3_pos_acc: 93.4510, stage3_loss_bbox: 0.1148, stage3_loss_iou: 0.2309, stage3_loss_mask: 0.2692, stage4_loss_cls: 0.1252, stage4_pos_acc: 96.1453, stage4_loss_bbox: 0.1115, stage4_loss_iou: 0.2230, stage4_loss_mask: 0.2591, stage5_loss_cls: 0.1065, stage5_pos_acc: 97.6405, stage5_loss_bbox: 0.1066, stage5_loss_iou: 0.2170, stage5_loss_mask: 0.2542, loss: 7.0302\n",
      "2025-07-16 18:30:45,437 - mmdet - INFO - Epoch [45][250/750]\tlr: 2.500e-06, eta: 1:37:53, time: 0.383, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9938, stage0_pos_acc: 37.9061, stage0_loss_bbox: 0.4087, stage0_loss_iou: 0.9629, stage0_loss_mask: 0.5618, stage1_loss_cls: 0.4713, stage1_pos_acc: 79.3583, stage1_loss_bbox: 0.1372, stage1_loss_iou: 0.3551, stage1_loss_mask: 0.3412, stage2_loss_cls: 0.2861, stage2_pos_acc: 88.5417, stage2_loss_bbox: 0.1063, stage2_loss_iou: 0.2608, stage2_loss_mask: 0.3270, stage3_loss_cls: 0.1561, stage3_pos_acc: 95.8250, stage3_loss_bbox: 0.0918, stage3_loss_iou: 0.2233, stage3_loss_mask: 0.2625, stage4_loss_cls: 0.0965, stage4_pos_acc: 97.8167, stage4_loss_bbox: 0.0893, stage4_loss_iou: 0.2115, stage4_loss_mask: 0.2674, stage5_loss_cls: 0.0722, stage5_pos_acc: 99.6083, stage5_loss_bbox: 0.0876, stage5_loss_iou: 0.2062, stage5_loss_mask: 0.2701, loss: 7.2467\n",
      "2025-07-16 18:31:04,620 - mmdet - INFO - Epoch [45][300/750]\tlr: 2.500e-06, eta: 1:37:34, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9617, stage0_pos_acc: 42.3030, stage0_loss_bbox: 0.3863, stage0_loss_iou: 0.8269, stage0_loss_mask: 0.5155, stage1_loss_cls: 0.4727, stage1_pos_acc: 76.9840, stage1_loss_bbox: 0.1407, stage1_loss_iou: 0.3188, stage1_loss_mask: 0.2571, stage2_loss_cls: 0.3000, stage2_pos_acc: 89.2319, stage2_loss_bbox: 0.1106, stage2_loss_iou: 0.2360, stage2_loss_mask: 0.2441, stage3_loss_cls: 0.1505, stage3_pos_acc: 94.8063, stage3_loss_bbox: 0.1034, stage3_loss_iou: 0.2193, stage3_loss_mask: 0.2375, stage4_loss_cls: 0.0895, stage4_pos_acc: 98.5539, stage4_loss_bbox: 0.0978, stage4_loss_iou: 0.2120, stage4_loss_mask: 0.2261, stage5_loss_cls: 0.0545, stage5_pos_acc: 99.5286, stage5_loss_bbox: 0.0950, stage5_loss_iou: 0.2076, stage5_loss_mask: 0.2274, loss: 6.6912\n",
      "2025-07-16 18:31:24,106 - mmdet - INFO - Epoch [45][350/750]\tlr: 2.500e-06, eta: 1:37:16, time: 0.390, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9230, stage0_pos_acc: 45.8551, stage0_loss_bbox: 0.3895, stage0_loss_iou: 0.8989, stage0_loss_mask: 0.6099, stage1_loss_cls: 0.4892, stage1_pos_acc: 80.7885, stage1_loss_bbox: 0.1673, stage1_loss_iou: 0.3575, stage1_loss_mask: 0.3380, stage2_loss_cls: 0.3274, stage2_pos_acc: 89.5408, stage2_loss_bbox: 0.1285, stage2_loss_iou: 0.2715, stage2_loss_mask: 0.3086, stage3_loss_cls: 0.2088, stage3_pos_acc: 93.3802, stage3_loss_bbox: 0.1143, stage3_loss_iou: 0.2417, stage3_loss_mask: 0.2874, stage4_loss_cls: 0.1469, stage4_pos_acc: 96.9072, stage4_loss_bbox: 0.1079, stage4_loss_iou: 0.2267, stage4_loss_mask: 0.2727, stage5_loss_cls: 0.1264, stage5_pos_acc: 98.1025, stage5_loss_bbox: 0.1027, stage5_loss_iou: 0.2183, stage5_loss_mask: 0.2670, loss: 7.5303\n",
      "2025-07-16 18:31:43,174 - mmdet - INFO - Epoch [45][400/750]\tlr: 2.500e-06, eta: 1:36:57, time: 0.381, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9414, stage0_pos_acc: 39.2322, stage0_loss_bbox: 0.4333, stage0_loss_iou: 0.8391, stage0_loss_mask: 0.5449, stage1_loss_cls: 0.4806, stage1_pos_acc: 82.3143, stage1_loss_bbox: 0.2016, stage1_loss_iou: 0.3398, stage1_loss_mask: 0.3189, stage2_loss_cls: 0.2895, stage2_pos_acc: 88.8995, stage2_loss_bbox: 0.1687, stage2_loss_iou: 0.2565, stage2_loss_mask: 0.2954, stage3_loss_cls: 0.1805, stage3_pos_acc: 94.7405, stage3_loss_bbox: 0.1365, stage3_loss_iou: 0.2487, stage3_loss_mask: 0.2912, stage4_loss_cls: 0.1344, stage4_pos_acc: 97.0071, stage4_loss_bbox: 0.1281, stage4_loss_iou: 0.2374, stage4_loss_mask: 0.2965, stage5_loss_cls: 0.1115, stage5_pos_acc: 98.2143, stage5_loss_bbox: 0.1208, stage5_loss_iou: 0.2331, stage5_loss_mask: 0.2861, loss: 7.5144\n",
      "2025-07-16 18:32:01,981 - mmdet - INFO - Epoch [45][450/750]\tlr: 2.500e-06, eta: 1:36:38, time: 0.376, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9620, stage0_pos_acc: 38.4218, stage0_loss_bbox: 0.3921, stage0_loss_iou: 0.8617, stage0_loss_mask: 0.5131, stage1_loss_cls: 0.4965, stage1_pos_acc: 77.0353, stage1_loss_bbox: 0.1547, stage1_loss_iou: 0.3245, stage1_loss_mask: 0.3024, stage2_loss_cls: 0.3139, stage2_pos_acc: 86.8700, stage2_loss_bbox: 0.1295, stage2_loss_iou: 0.2485, stage2_loss_mask: 0.2657, stage3_loss_cls: 0.1838, stage3_pos_acc: 93.4209, stage3_loss_bbox: 0.1200, stage3_loss_iou: 0.2289, stage3_loss_mask: 0.2587, stage4_loss_cls: 0.1132, stage4_pos_acc: 95.4251, stage4_loss_bbox: 0.1159, stage4_loss_iou: 0.2233, stage4_loss_mask: 0.2729, stage5_loss_cls: 0.0882, stage5_pos_acc: 98.4578, stage5_loss_bbox: 0.1147, stage5_loss_iou: 0.2214, stage5_loss_mask: 0.2699, loss: 7.1752\n",
      "2025-07-16 18:32:21,281 - mmdet - INFO - Epoch [45][500/750]\tlr: 2.500e-06, eta: 1:36:19, time: 0.386, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9937, stage0_pos_acc: 37.1801, stage0_loss_bbox: 0.3758, stage0_loss_iou: 0.9073, stage0_loss_mask: 0.5533, stage1_loss_cls: 0.4856, stage1_pos_acc: 80.8640, stage1_loss_bbox: 0.1446, stage1_loss_iou: 0.3539, stage1_loss_mask: 0.3788, stage2_loss_cls: 0.3194, stage2_pos_acc: 90.0465, stage2_loss_bbox: 0.1149, stage2_loss_iou: 0.2678, stage2_loss_mask: 0.3305, stage3_loss_cls: 0.1776, stage3_pos_acc: 94.3641, stage3_loss_bbox: 0.1026, stage3_loss_iou: 0.2391, stage3_loss_mask: 0.3226, stage4_loss_cls: 0.0972, stage4_pos_acc: 96.8308, stage4_loss_bbox: 0.0990, stage4_loss_iou: 0.2304, stage4_loss_mask: 0.3224, stage5_loss_cls: 0.0757, stage5_pos_acc: 98.1846, stage5_loss_bbox: 0.0975, stage5_loss_iou: 0.2278, stage5_loss_mask: 0.3189, loss: 7.5365\n",
      "2025-07-16 18:32:40,788 - mmdet - INFO - Epoch [45][550/750]\tlr: 2.500e-06, eta: 1:36:01, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9731, stage0_pos_acc: 37.6396, stage0_loss_bbox: 0.4341, stage0_loss_iou: 0.8672, stage0_loss_mask: 0.5169, stage1_loss_cls: 0.5154, stage1_pos_acc: 77.1872, stage1_loss_bbox: 0.1741, stage1_loss_iou: 0.3525, stage1_loss_mask: 0.2822, stage2_loss_cls: 0.3503, stage2_pos_acc: 85.2229, stage2_loss_bbox: 0.1320, stage2_loss_iou: 0.2521, stage2_loss_mask: 0.2617, stage3_loss_cls: 0.2188, stage3_pos_acc: 89.0585, stage3_loss_bbox: 0.1227, stage3_loss_iou: 0.2240, stage3_loss_mask: 0.2480, stage4_loss_cls: 0.1510, stage4_pos_acc: 93.0364, stage4_loss_bbox: 0.1143, stage4_loss_iou: 0.2114, stage4_loss_mask: 0.2370, stage5_loss_cls: 0.1271, stage5_pos_acc: 96.1652, stage5_loss_bbox: 0.1136, stage5_loss_iou: 0.2052, stage5_loss_mask: 0.2333, loss: 7.3178\n",
      "2025-07-16 18:33:00,274 - mmdet - INFO - Epoch [45][600/750]\tlr: 2.500e-06, eta: 1:35:42, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 1.0083, stage0_pos_acc: 38.5946, stage0_loss_bbox: 0.4355, stage0_loss_iou: 0.9692, stage0_loss_mask: 0.6353, stage1_loss_cls: 0.4758, stage1_pos_acc: 82.4917, stage1_loss_bbox: 0.1559, stage1_loss_iou: 0.3521, stage1_loss_mask: 0.3407, stage2_loss_cls: 0.3138, stage2_pos_acc: 89.3439, stage2_loss_bbox: 0.1206, stage2_loss_iou: 0.2643, stage2_loss_mask: 0.2728, stage3_loss_cls: 0.1644, stage3_pos_acc: 95.8613, stage3_loss_bbox: 0.1079, stage3_loss_iou: 0.2407, stage3_loss_mask: 0.2496, stage4_loss_cls: 0.1132, stage4_pos_acc: 97.2150, stage4_loss_bbox: 0.1053, stage4_loss_iou: 0.2305, stage4_loss_mask: 0.2419, stage5_loss_cls: 0.0855, stage5_pos_acc: 98.2412, stage5_loss_bbox: 0.1027, stage5_loss_iou: 0.2264, stage5_loss_mask: 0.2446, loss: 7.4568\n",
      "2025-07-16 18:33:19,899 - mmdet - INFO - Epoch [45][650/750]\tlr: 2.500e-06, eta: 1:35:24, time: 0.392, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9675, stage0_pos_acc: 45.9024, stage0_loss_bbox: 0.4384, stage0_loss_iou: 0.9546, stage0_loss_mask: 0.5083, stage1_loss_cls: 0.4608, stage1_pos_acc: 84.3119, stage1_loss_bbox: 0.1481, stage1_loss_iou: 0.3380, stage1_loss_mask: 0.2071, stage2_loss_cls: 0.2987, stage2_pos_acc: 88.3310, stage2_loss_bbox: 0.1107, stage2_loss_iou: 0.2446, stage2_loss_mask: 0.1953, stage3_loss_cls: 0.1575, stage3_pos_acc: 95.2976, stage3_loss_bbox: 0.0978, stage3_loss_iou: 0.2162, stage3_loss_mask: 0.1908, stage4_loss_cls: 0.0810, stage4_pos_acc: 98.3167, stage4_loss_bbox: 0.0963, stage4_loss_iou: 0.2092, stage4_loss_mask: 0.1934, stage5_loss_cls: 0.0603, stage5_pos_acc: 98.3143, stage5_loss_bbox: 0.0919, stage5_loss_iou: 0.2048, stage5_loss_mask: 0.1893, loss: 6.6606\n",
      "2025-07-16 18:33:39,503 - mmdet - INFO - Epoch [45][700/750]\tlr: 2.500e-06, eta: 1:35:05, time: 0.392, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9352, stage0_pos_acc: 44.1821, stage0_loss_bbox: 0.4287, stage0_loss_iou: 0.9006, stage0_loss_mask: 0.6096, stage1_loss_cls: 0.4963, stage1_pos_acc: 77.0375, stage1_loss_bbox: 0.1684, stage1_loss_iou: 0.3474, stage1_loss_mask: 0.3863, stage2_loss_cls: 0.3522, stage2_pos_acc: 85.8633, stage2_loss_bbox: 0.1437, stage2_loss_iou: 0.2900, stage2_loss_mask: 0.3798, stage3_loss_cls: 0.2054, stage3_pos_acc: 91.5919, stage3_loss_bbox: 0.1393, stage3_loss_iou: 0.2776, stage3_loss_mask: 0.3563, stage4_loss_cls: 0.1407, stage4_pos_acc: 96.8401, stage4_loss_bbox: 0.1349, stage4_loss_iou: 0.2673, stage4_loss_mask: 0.3511, stage5_loss_cls: 0.1157, stage5_pos_acc: 97.4274, stage5_loss_bbox: 0.1318, stage5_loss_iou: 0.2636, stage5_loss_mask: 0.3431, loss: 8.1650\n",
      "2025-07-16 18:33:58,918 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:33:58,919 - mmdet - INFO - Epoch [45][750/750]\tlr: 2.500e-06, eta: 1:34:47, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9340, stage0_pos_acc: 39.7340, stage0_loss_bbox: 0.3969, stage0_loss_iou: 0.8559, stage0_loss_mask: 0.6793, stage1_loss_cls: 0.4611, stage1_pos_acc: 79.9358, stage1_loss_bbox: 0.1509, stage1_loss_iou: 0.3599, stage1_loss_mask: 0.3516, stage2_loss_cls: 0.3141, stage2_pos_acc: 87.1715, stage2_loss_bbox: 0.1158, stage2_loss_iou: 0.2716, stage2_loss_mask: 0.2860, stage3_loss_cls: 0.1895, stage3_pos_acc: 93.9081, stage3_loss_bbox: 0.1111, stage3_loss_iou: 0.2510, stage3_loss_mask: 0.2862, stage4_loss_cls: 0.1255, stage4_pos_acc: 97.0438, stage4_loss_bbox: 0.1067, stage4_loss_iou: 0.2439, stage4_loss_mask: 0.2789, stage5_loss_cls: 0.0973, stage5_pos_acc: 98.7162, stage5_loss_bbox: 0.1056, stage5_loss_iou: 0.2361, stage5_loss_mask: 0.2736, loss: 7.4823\n",
      "2025-07-16 18:33:59,048 - mmdet - INFO - Saving checkpoint at 45 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 98s, ETA:     0s2025-07-16 18:37:15,903 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.031\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.056\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.070\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.182\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.240\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.455\n",
      "2025-07-16 18:37:17,982 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.073\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.209\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.455\n",
      "2025-07-16 18:37:21,178 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:37:21,180 - mmdet - INFO - Epoch(val) [45][750]\tbbox_mAP: 0.0310, bbox_mAP_50: 0.0560, bbox_mAP_75: 0.0300, bbox_mAP_s: 0.0700, bbox_mAP_m: 0.0140, bbox_mAP_l: 0.0380, bbox_mAP_copypaste: 0.031 0.056 0.030 0.070 0.014 0.038, segm_mAP: 0.0300, segm_mAP_50: 0.0550, segm_mAP_75: 0.0290, segm_mAP_s: 0.0730, segm_mAP_m: 0.0150, segm_mAP_l: 0.0380, segm_mAP_copypaste: 0.030 0.055 0.029 0.073 0.015 0.038\n",
      "2025-07-16 18:37:42,571 - mmdet - INFO - Epoch [46][50/750]\tlr: 2.500e-06, eta: 1:34:30, time: 0.428, data_time: 0.055, memory: 11264, stage0_loss_cls: 0.9702, stage0_pos_acc: 36.9965, stage0_loss_bbox: 0.3907, stage0_loss_iou: 0.8567, stage0_loss_mask: 0.5065, stage1_loss_cls: 0.4815, stage1_pos_acc: 79.5905, stage1_loss_bbox: 0.1439, stage1_loss_iou: 0.3185, stage1_loss_mask: 0.3138, stage2_loss_cls: 0.2986, stage2_pos_acc: 89.2825, stage2_loss_bbox: 0.1209, stage2_loss_iou: 0.2590, stage2_loss_mask: 0.2855, stage3_loss_cls: 0.1579, stage3_pos_acc: 95.4175, stage3_loss_bbox: 0.1014, stage3_loss_iou: 0.2335, stage3_loss_mask: 0.2606, stage4_loss_cls: 0.0874, stage4_pos_acc: 97.9095, stage4_loss_bbox: 0.1009, stage4_loss_iou: 0.2255, stage4_loss_mask: 0.2607, stage5_loss_cls: 0.0631, stage5_pos_acc: 98.5429, stage5_loss_bbox: 0.0999, stage5_loss_iou: 0.2212, stage5_loss_mask: 0.2563, loss: 7.0141\n",
      "2025-07-16 18:38:01,445 - mmdet - INFO - Epoch [46][100/750]\tlr: 2.500e-06, eta: 1:34:11, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9703, stage0_pos_acc: 39.2219, stage0_loss_bbox: 0.4000, stage0_loss_iou: 0.9073, stage0_loss_mask: 0.6383, stage1_loss_cls: 0.4837, stage1_pos_acc: 78.4850, stage1_loss_bbox: 0.1522, stage1_loss_iou: 0.3576, stage1_loss_mask: 0.3652, stage2_loss_cls: 0.3142, stage2_pos_acc: 87.5611, stage2_loss_bbox: 0.1234, stage2_loss_iou: 0.2715, stage2_loss_mask: 0.3544, stage3_loss_cls: 0.1963, stage3_pos_acc: 95.9864, stage3_loss_bbox: 0.1134, stage3_loss_iou: 0.2492, stage3_loss_mask: 0.3477, stage4_loss_cls: 0.1250, stage4_pos_acc: 98.2500, stage4_loss_bbox: 0.1096, stage4_loss_iou: 0.2412, stage4_loss_mask: 0.3477, stage5_loss_cls: 0.0921, stage5_pos_acc: 98.4500, stage5_loss_bbox: 0.1094, stage5_loss_iou: 0.2368, stage5_loss_mask: 0.3444, loss: 7.8509\n",
      "2025-07-16 18:38:20,616 - mmdet - INFO - Epoch [46][150/750]\tlr: 2.500e-06, eta: 1:33:52, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9711, stage0_pos_acc: 39.4845, stage0_loss_bbox: 0.4443, stage0_loss_iou: 0.9135, stage0_loss_mask: 0.5555, stage1_loss_cls: 0.4456, stage1_pos_acc: 79.9171, stage1_loss_bbox: 0.1760, stage1_loss_iou: 0.3460, stage1_loss_mask: 0.3524, stage2_loss_cls: 0.2843, stage2_pos_acc: 92.2680, stage2_loss_bbox: 0.1457, stage2_loss_iou: 0.2457, stage2_loss_mask: 0.3251, stage3_loss_cls: 0.1617, stage3_pos_acc: 95.2870, stage3_loss_bbox: 0.1089, stage3_loss_iou: 0.2381, stage3_loss_mask: 0.3297, stage4_loss_cls: 0.1187, stage4_pos_acc: 96.5022, stage4_loss_bbox: 0.1036, stage4_loss_iou: 0.2294, stage4_loss_mask: 0.3214, stage5_loss_cls: 0.0908, stage5_pos_acc: 96.4355, stage5_loss_bbox: 0.1003, stage5_loss_iou: 0.2234, stage5_loss_mask: 0.3187, loss: 7.5495\n",
      "2025-07-16 18:38:39,524 - mmdet - INFO - Epoch [46][200/750]\tlr: 2.500e-06, eta: 1:33:33, time: 0.378, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9442, stage0_pos_acc: 44.5542, stage0_loss_bbox: 0.4005, stage0_loss_iou: 0.8781, stage0_loss_mask: 0.5531, stage1_loss_cls: 0.4493, stage1_pos_acc: 79.8288, stage1_loss_bbox: 0.1552, stage1_loss_iou: 0.3312, stage1_loss_mask: 0.2944, stage2_loss_cls: 0.2922, stage2_pos_acc: 89.9621, stage2_loss_bbox: 0.1146, stage2_loss_iou: 0.2452, stage2_loss_mask: 0.2437, stage3_loss_cls: 0.1674, stage3_pos_acc: 95.0822, stage3_loss_bbox: 0.1091, stage3_loss_iou: 0.2227, stage3_loss_mask: 0.2310, stage4_loss_cls: 0.1156, stage4_pos_acc: 97.6807, stage4_loss_bbox: 0.1015, stage4_loss_iou: 0.2116, stage4_loss_mask: 0.2218, stage5_loss_cls: 0.0970, stage5_pos_acc: 98.1944, stage5_loss_bbox: 0.0998, stage5_loss_iou: 0.2082, stage5_loss_mask: 0.2186, loss: 6.9060\n",
      "2025-07-16 18:38:58,633 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:38:58,634 - mmdet - INFO - Epoch [46][250/750]\tlr: 2.500e-06, eta: 1:33:14, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9556, stage0_pos_acc: 41.3240, stage0_loss_bbox: 0.3913, stage0_loss_iou: 0.9581, stage0_loss_mask: 0.6785, stage1_loss_cls: 0.4679, stage1_pos_acc: 82.1028, stage1_loss_bbox: 0.1467, stage1_loss_iou: 0.3560, stage1_loss_mask: 0.3472, stage2_loss_cls: 0.2926, stage2_pos_acc: 87.9362, stage2_loss_bbox: 0.1169, stage2_loss_iou: 0.2716, stage2_loss_mask: 0.3039, stage3_loss_cls: 0.1617, stage3_pos_acc: 93.7877, stage3_loss_bbox: 0.1101, stage3_loss_iou: 0.2443, stage3_loss_mask: 0.2988, stage4_loss_cls: 0.1122, stage4_pos_acc: 96.4854, stage4_loss_bbox: 0.1040, stage4_loss_iou: 0.2331, stage4_loss_mask: 0.2936, stage5_loss_cls: 0.0901, stage5_pos_acc: 98.1523, stage5_loss_bbox: 0.1018, stage5_loss_iou: 0.2280, stage5_loss_mask: 0.2889, loss: 7.5528\n",
      "2025-07-16 18:39:17,953 - mmdet - INFO - Epoch [46][300/750]\tlr: 2.500e-06, eta: 1:32:55, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9512, stage0_pos_acc: 38.5751, stage0_loss_bbox: 0.4076, stage0_loss_iou: 0.9507, stage0_loss_mask: 0.5645, stage1_loss_cls: 0.4819, stage1_pos_acc: 81.4582, stage1_loss_bbox: 0.1625, stage1_loss_iou: 0.3481, stage1_loss_mask: 0.2433, stage2_loss_cls: 0.3166, stage2_pos_acc: 87.1551, stage2_loss_bbox: 0.1200, stage2_loss_iou: 0.2448, stage2_loss_mask: 0.2188, stage3_loss_cls: 0.1909, stage3_pos_acc: 93.4447, stage3_loss_bbox: 0.1125, stage3_loss_iou: 0.2107, stage3_loss_mask: 0.1804, stage4_loss_cls: 0.1133, stage4_pos_acc: 97.6357, stage4_loss_bbox: 0.1060, stage4_loss_iou: 0.2012, stage4_loss_mask: 0.2057, stage5_loss_cls: 0.0824, stage5_pos_acc: 98.0357, stage5_loss_bbox: 0.1009, stage5_loss_iou: 0.1966, stage5_loss_mask: 0.1958, loss: 6.9064\n",
      "2025-07-16 18:39:37,042 - mmdet - INFO - Epoch [46][350/750]\tlr: 2.500e-06, eta: 1:32:37, time: 0.382, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9217, stage0_pos_acc: 43.7523, stage0_loss_bbox: 0.4178, stage0_loss_iou: 0.8290, stage0_loss_mask: 0.5388, stage1_loss_cls: 0.4669, stage1_pos_acc: 79.8461, stage1_loss_bbox: 0.1688, stage1_loss_iou: 0.3386, stage1_loss_mask: 0.3299, stage2_loss_cls: 0.3152, stage2_pos_acc: 88.5439, stage2_loss_bbox: 0.1349, stage2_loss_iou: 0.2620, stage2_loss_mask: 0.2949, stage3_loss_cls: 0.1939, stage3_pos_acc: 94.1866, stage3_loss_bbox: 0.1280, stage3_loss_iou: 0.2527, stage3_loss_mask: 0.3006, stage4_loss_cls: 0.1348, stage4_pos_acc: 96.2373, stage4_loss_bbox: 0.1252, stage4_loss_iou: 0.2448, stage4_loss_mask: 0.2978, stage5_loss_cls: 0.1054, stage5_pos_acc: 98.3970, stage5_loss_bbox: 0.1259, stage5_loss_iou: 0.2459, stage5_loss_mask: 0.3151, loss: 7.4889\n",
      "2025-07-16 18:39:55,863 - mmdet - INFO - Epoch [46][400/750]\tlr: 2.500e-06, eta: 1:32:17, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9613, stage0_pos_acc: 40.6429, stage0_loss_bbox: 0.3562, stage0_loss_iou: 0.7797, stage0_loss_mask: 0.4801, stage1_loss_cls: 0.4701, stage1_pos_acc: 79.8810, stage1_loss_bbox: 0.1539, stage1_loss_iou: 0.3155, stage1_loss_mask: 0.2492, stage2_loss_cls: 0.3229, stage2_pos_acc: 86.1762, stage2_loss_bbox: 0.1237, stage2_loss_iou: 0.2474, stage2_loss_mask: 0.2327, stage3_loss_cls: 0.1807, stage3_pos_acc: 93.6952, stage3_loss_bbox: 0.1090, stage3_loss_iou: 0.2215, stage3_loss_mask: 0.2231, stage4_loss_cls: 0.1090, stage4_pos_acc: 97.2095, stage4_loss_bbox: 0.1017, stage4_loss_iou: 0.2098, stage4_loss_mask: 0.1997, stage5_loss_cls: 0.0890, stage5_pos_acc: 98.3952, stage5_loss_bbox: 0.0966, stage5_loss_iou: 0.2055, stage5_loss_mask: 0.2036, loss: 6.6418\n",
      "2025-07-16 18:40:15,057 - mmdet - INFO - Epoch [46][450/750]\tlr: 2.500e-06, eta: 1:31:59, time: 0.384, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9292, stage0_pos_acc: 41.0059, stage0_loss_bbox: 0.4253, stage0_loss_iou: 0.8964, stage0_loss_mask: 0.5814, stage1_loss_cls: 0.4821, stage1_pos_acc: 81.3200, stage1_loss_bbox: 0.1591, stage1_loss_iou: 0.3550, stage1_loss_mask: 0.3714, stage2_loss_cls: 0.3124, stage2_pos_acc: 86.7154, stage2_loss_bbox: 0.1341, stage2_loss_iou: 0.2823, stage2_loss_mask: 0.3551, stage3_loss_cls: 0.1909, stage3_pos_acc: 93.4812, stage3_loss_bbox: 0.1242, stage3_loss_iou: 0.2599, stage3_loss_mask: 0.3428, stage4_loss_cls: 0.1328, stage4_pos_acc: 96.6128, stage4_loss_bbox: 0.1191, stage4_loss_iou: 0.2515, stage4_loss_mask: 0.3435, stage5_loss_cls: 0.1188, stage5_pos_acc: 97.9532, stage5_loss_bbox: 0.1150, stage5_loss_iou: 0.2500, stage5_loss_mask: 0.3487, loss: 7.8811\n",
      "2025-07-16 18:40:34,214 - mmdet - INFO - Epoch [46][500/750]\tlr: 2.500e-06, eta: 1:31:40, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9773, stage0_pos_acc: 41.1069, stage0_loss_bbox: 0.4198, stage0_loss_iou: 0.8468, stage0_loss_mask: 0.5160, stage1_loss_cls: 0.4612, stage1_pos_acc: 83.3621, stage1_loss_bbox: 0.1721, stage1_loss_iou: 0.3091, stage1_loss_mask: 0.3293, stage2_loss_cls: 0.2818, stage2_pos_acc: 91.1673, stage2_loss_bbox: 0.1414, stage2_loss_iou: 0.2470, stage2_loss_mask: 0.3017, stage3_loss_cls: 0.1737, stage3_pos_acc: 93.6971, stage3_loss_bbox: 0.1185, stage3_loss_iou: 0.2276, stage3_loss_mask: 0.3025, stage4_loss_cls: 0.1159, stage4_pos_acc: 95.6162, stage4_loss_bbox: 0.1250, stage4_loss_iou: 0.2198, stage4_loss_mask: 0.2973, stage5_loss_cls: 0.1085, stage5_pos_acc: 97.9258, stage5_loss_bbox: 0.1176, stage5_loss_iou: 0.2174, stage5_loss_mask: 0.2798, loss: 7.3074\n",
      "2025-07-16 18:40:52,942 - mmdet - INFO - Epoch [46][550/750]\tlr: 2.500e-06, eta: 1:31:21, time: 0.375, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9615, stage0_pos_acc: 39.6463, stage0_loss_bbox: 0.3943, stage0_loss_iou: 0.8434, stage0_loss_mask: 0.5689, stage1_loss_cls: 0.4588, stage1_pos_acc: 80.5602, stage1_loss_bbox: 0.1556, stage1_loss_iou: 0.3493, stage1_loss_mask: 0.3098, stage2_loss_cls: 0.3193, stage2_pos_acc: 86.8671, stage2_loss_bbox: 0.1195, stage2_loss_iou: 0.2665, stage2_loss_mask: 0.2772, stage3_loss_cls: 0.1868, stage3_pos_acc: 93.7251, stage3_loss_bbox: 0.1117, stage3_loss_iou: 0.2479, stage3_loss_mask: 0.2754, stage4_loss_cls: 0.1191, stage4_pos_acc: 97.2593, stage4_loss_bbox: 0.1099, stage4_loss_iou: 0.2370, stage4_loss_mask: 0.2617, stage5_loss_cls: 0.1013, stage5_pos_acc: 97.4645, stage5_loss_bbox: 0.1079, stage5_loss_iou: 0.2346, stage5_loss_mask: 0.2628, loss: 7.2801\n",
      "2025-07-16 18:41:11,738 - mmdet - INFO - Epoch [46][600/750]\tlr: 2.500e-06, eta: 1:31:02, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9680, stage0_pos_acc: 39.0939, stage0_loss_bbox: 0.4026, stage0_loss_iou: 0.8743, stage0_loss_mask: 0.6134, stage1_loss_cls: 0.4702, stage1_pos_acc: 76.4883, stage1_loss_bbox: 0.1638, stage1_loss_iou: 0.3706, stage1_loss_mask: 0.3632, stage2_loss_cls: 0.3367, stage2_pos_acc: 87.1697, stage2_loss_bbox: 0.1348, stage2_loss_iou: 0.2825, stage2_loss_mask: 0.3141, stage3_loss_cls: 0.1858, stage3_pos_acc: 92.9712, stage3_loss_bbox: 0.1294, stage3_loss_iou: 0.2702, stage3_loss_mask: 0.3113, stage4_loss_cls: 0.1189, stage4_pos_acc: 96.2826, stage4_loss_bbox: 0.1216, stage4_loss_iou: 0.2565, stage4_loss_mask: 0.3056, stage5_loss_cls: 0.0911, stage5_pos_acc: 96.3626, stage5_loss_bbox: 0.1202, stage5_loss_iou: 0.2527, stage5_loss_mask: 0.3036, loss: 7.7612\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 100s, ETA:     0s2025-07-16 18:45:26,107 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.31s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.124\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.407\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.232\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.452\n",
      "2025-07-16 18:45:28,161 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.142\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.038\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.213\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.244\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.457\n",
      "2025-07-16 18:45:31,316 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:45:31,316 - mmdet - INFO - Epoch(val) [46][750]\tbbox_mAP: 0.0300, bbox_mAP_50: 0.0550, bbox_mAP_75: 0.0300, bbox_mAP_s: 0.1240, bbox_mAP_m: 0.0200, bbox_mAP_l: 0.0370, bbox_mAP_copypaste: 0.030 0.055 0.030 0.124 0.020 0.037, segm_mAP: 0.0300, segm_mAP_50: 0.0550, segm_mAP_75: 0.0300, segm_mAP_s: 0.1420, segm_mAP_m: 0.0220, segm_mAP_l: 0.0380, segm_mAP_copypaste: 0.030 0.055 0.030 0.142 0.022 0.038\n",
      "2025-07-16 18:45:53,191 - mmdet - INFO - Epoch [47][50/750]\tlr: 2.500e-06, eta: 1:29:47, time: 0.437, data_time: 0.055, memory: 11264, stage0_loss_cls: 0.9777, stage0_pos_acc: 41.0333, stage0_loss_bbox: 0.4102, stage0_loss_iou: 0.8900, stage0_loss_mask: 0.5063, stage1_loss_cls: 0.4677, stage1_pos_acc: 77.4357, stage1_loss_bbox: 0.1516, stage1_loss_iou: 0.3362, stage1_loss_mask: 0.2744, stage2_loss_cls: 0.3092, stage2_pos_acc: 90.6762, stage2_loss_bbox: 0.1125, stage2_loss_iou: 0.2423, stage2_loss_mask: 0.2650, stage3_loss_cls: 0.1559, stage3_pos_acc: 94.7762, stage3_loss_bbox: 0.0994, stage3_loss_iou: 0.2257, stage3_loss_mask: 0.2567, stage4_loss_cls: 0.0967, stage4_pos_acc: 98.3238, stage4_loss_bbox: 0.0967, stage4_loss_iou: 0.2193, stage4_loss_mask: 0.2664, stage5_loss_cls: 0.0726, stage5_pos_acc: 98.1500, stage5_loss_bbox: 0.0944, stage5_loss_iou: 0.2113, stage5_loss_mask: 0.2625, loss: 7.0006\n",
      "2025-07-16 18:46:12,661 - mmdet - INFO - Epoch [47][100/750]\tlr: 2.500e-06, eta: 1:29:29, time: 0.389, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9391, stage0_pos_acc: 41.4040, stage0_loss_bbox: 0.3953, stage0_loss_iou: 0.8441, stage0_loss_mask: 0.5117, stage1_loss_cls: 0.4637, stage1_pos_acc: 82.9760, stage1_loss_bbox: 0.1481, stage1_loss_iou: 0.2985, stage1_loss_mask: 0.3068, stage2_loss_cls: 0.2922, stage2_pos_acc: 91.6583, stage2_loss_bbox: 0.1220, stage2_loss_iou: 0.2508, stage2_loss_mask: 0.2844, stage3_loss_cls: 0.1749, stage3_pos_acc: 96.0234, stage3_loss_bbox: 0.1099, stage3_loss_iou: 0.2236, stage3_loss_mask: 0.2787, stage4_loss_cls: 0.1264, stage4_pos_acc: 97.9934, stage4_loss_bbox: 0.1061, stage4_loss_iou: 0.2207, stage4_loss_mask: 0.2711, stage5_loss_cls: 0.1055, stage5_pos_acc: 97.8215, stage5_loss_bbox: 0.1042, stage5_loss_iou: 0.2209, stage5_loss_mask: 0.2949, loss: 7.0935\n",
      "2025-07-16 18:46:31,793 - mmdet - INFO - Epoch [47][150/750]\tlr: 2.500e-06, eta: 1:29:10, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9518, stage0_pos_acc: 39.7594, stage0_loss_bbox: 0.4320, stage0_loss_iou: 0.8832, stage0_loss_mask: 0.6683, stage1_loss_cls: 0.4830, stage1_pos_acc: 80.9131, stage1_loss_bbox: 0.2137, stage1_loss_iou: 0.3672, stage1_loss_mask: 0.4454, stage2_loss_cls: 0.2982, stage2_pos_acc: 89.8151, stage2_loss_bbox: 0.1739, stage2_loss_iou: 0.2961, stage2_loss_mask: 0.3898, stage3_loss_cls: 0.1791, stage3_pos_acc: 93.5406, stage3_loss_bbox: 0.1350, stage3_loss_iou: 0.2814, stage3_loss_mask: 0.3734, stage4_loss_cls: 0.1277, stage4_pos_acc: 97.6911, stage4_loss_bbox: 0.1386, stage4_loss_iou: 0.2774, stage4_loss_mask: 0.3736, stage5_loss_cls: 0.0997, stage5_pos_acc: 97.9706, stage5_loss_bbox: 0.1333, stage5_loss_iou: 0.2791, stage5_loss_mask: 0.3808, loss: 8.3817\n",
      "2025-07-16 18:46:50,904 - mmdet - INFO - Epoch [47][200/750]\tlr: 2.500e-06, eta: 1:28:51, time: 0.382, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9423, stage0_pos_acc: 41.6490, stage0_loss_bbox: 0.3938, stage0_loss_iou: 0.7996, stage0_loss_mask: 0.5350, stage1_loss_cls: 0.4685, stage1_pos_acc: 80.3753, stage1_loss_bbox: 0.1682, stage1_loss_iou: 0.3325, stage1_loss_mask: 0.3694, stage2_loss_cls: 0.2860, stage2_pos_acc: 90.8169, stage2_loss_bbox: 0.1387, stage2_loss_iou: 0.2636, stage2_loss_mask: 0.3461, stage3_loss_cls: 0.1632, stage3_pos_acc: 95.3864, stage3_loss_bbox: 0.1332, stage3_loss_iou: 0.2438, stage3_loss_mask: 0.3303, stage4_loss_cls: 0.1050, stage4_pos_acc: 97.1376, stage4_loss_bbox: 0.1274, stage4_loss_iou: 0.2379, stage4_loss_mask: 0.3214, stage5_loss_cls: 0.0899, stage5_pos_acc: 98.2121, stage5_loss_bbox: 0.1257, stage5_loss_iou: 0.2354, stage5_loss_mask: 0.3162, loss: 7.4730\n",
      "2025-07-16 18:47:10,144 - mmdet - INFO - Epoch [47][250/750]\tlr: 2.500e-06, eta: 1:28:32, time: 0.385, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9633, stage0_pos_acc: 41.5291, stage0_loss_bbox: 0.4368, stage0_loss_iou: 0.9463, stage0_loss_mask: 0.6416, stage1_loss_cls: 0.4783, stage1_pos_acc: 79.5514, stage1_loss_bbox: 0.1631, stage1_loss_iou: 0.3699, stage1_loss_mask: 0.3663, stage2_loss_cls: 0.3174, stage2_pos_acc: 86.1482, stage2_loss_bbox: 0.1317, stage2_loss_iou: 0.2862, stage2_loss_mask: 0.3412, stage3_loss_cls: 0.2009, stage3_pos_acc: 91.5972, stage3_loss_bbox: 0.1268, stage3_loss_iou: 0.2655, stage3_loss_mask: 0.2940, stage4_loss_cls: 0.1311, stage4_pos_acc: 95.8330, stage4_loss_bbox: 0.1259, stage4_loss_iou: 0.2577, stage4_loss_mask: 0.2916, stage5_loss_cls: 0.1092, stage5_pos_acc: 96.6321, stage5_loss_bbox: 0.1258, stage5_loss_iou: 0.2521, stage5_loss_mask: 0.2940, loss: 7.9168\n",
      "2025-07-16 18:47:29,142 - mmdet - INFO - Epoch [47][300/750]\tlr: 2.500e-06, eta: 1:28:13, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9169, stage0_pos_acc: 42.1576, stage0_loss_bbox: 0.3912, stage0_loss_iou: 0.8364, stage0_loss_mask: 0.5300, stage1_loss_cls: 0.4706, stage1_pos_acc: 76.8501, stage1_loss_bbox: 0.1610, stage1_loss_iou: 0.3361, stage1_loss_mask: 0.3472, stage2_loss_cls: 0.3217, stage2_pos_acc: 86.9717, stage2_loss_bbox: 0.1315, stage2_loss_iou: 0.2545, stage2_loss_mask: 0.2802, stage3_loss_cls: 0.1962, stage3_pos_acc: 93.7255, stage3_loss_bbox: 0.1182, stage3_loss_iou: 0.2345, stage3_loss_mask: 0.2638, stage4_loss_cls: 0.1375, stage4_pos_acc: 96.7573, stage4_loss_bbox: 0.1154, stage4_loss_iou: 0.2258, stage4_loss_mask: 0.2634, stage5_loss_cls: 0.1190, stage5_pos_acc: 97.9333, stage5_loss_bbox: 0.1146, stage5_loss_iou: 0.2262, stage5_loss_mask: 0.2662, loss: 7.2578\n",
      "2025-07-16 18:47:48,142 - mmdet - INFO - Epoch [47][350/750]\tlr: 2.500e-06, eta: 1:27:54, time: 0.380, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9333, stage0_pos_acc: 39.8460, stage0_loss_bbox: 0.3924, stage0_loss_iou: 0.8733, stage0_loss_mask: 0.5150, stage1_loss_cls: 0.4593, stage1_pos_acc: 79.2000, stage1_loss_bbox: 0.1392, stage1_loss_iou: 0.3374, stage1_loss_mask: 0.2825, stage2_loss_cls: 0.2803, stage2_pos_acc: 89.0706, stage2_loss_bbox: 0.1150, stage2_loss_iou: 0.2473, stage2_loss_mask: 0.2650, stage3_loss_cls: 0.1578, stage3_pos_acc: 94.0857, stage3_loss_bbox: 0.1020, stage3_loss_iou: 0.2225, stage3_loss_mask: 0.2286, stage4_loss_cls: 0.0865, stage4_pos_acc: 97.1738, stage4_loss_bbox: 0.0946, stage4_loss_iou: 0.2127, stage4_loss_mask: 0.1961, stage5_loss_cls: 0.0725, stage5_pos_acc: 98.6452, stage5_loss_bbox: 0.0911, stage5_loss_iou: 0.2046, stage5_loss_mask: 0.2143, loss: 6.7235\n",
      "2025-07-16 18:48:07,068 - mmdet - INFO - Epoch [47][400/750]\tlr: 2.500e-06, eta: 1:27:35, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0058, stage0_pos_acc: 40.0692, stage0_loss_bbox: 0.4623, stage0_loss_iou: 0.9404, stage0_loss_mask: 0.6852, stage1_loss_cls: 0.5022, stage1_pos_acc: 79.7301, stage1_loss_bbox: 0.1705, stage1_loss_iou: 0.3474, stage1_loss_mask: 0.3109, stage2_loss_cls: 0.3198, stage2_pos_acc: 89.4781, stage2_loss_bbox: 0.1344, stage2_loss_iou: 0.2647, stage2_loss_mask: 0.2821, stage3_loss_cls: 0.1755, stage3_pos_acc: 92.7013, stage3_loss_bbox: 0.1172, stage3_loss_iou: 0.2398, stage3_loss_mask: 0.2423, stage4_loss_cls: 0.1180, stage4_pos_acc: 95.5135, stage4_loss_bbox: 0.1107, stage4_loss_iou: 0.2285, stage4_loss_mask: 0.2238, stage5_loss_cls: 0.0972, stage5_pos_acc: 95.9234, stage5_loss_bbox: 0.1076, stage5_loss_iou: 0.2272, stage5_loss_mask: 0.2262, loss: 7.5398\n",
      "2025-07-16 18:48:26,247 - mmdet - INFO - Epoch [47][450/750]\tlr: 2.500e-06, eta: 1:27:16, time: 0.384, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9663, stage0_pos_acc: 46.0636, stage0_loss_bbox: 0.3888, stage0_loss_iou: 0.8821, stage0_loss_mask: 0.5846, stage1_loss_cls: 0.4492, stage1_pos_acc: 83.0465, stage1_loss_bbox: 0.1299, stage1_loss_iou: 0.3170, stage1_loss_mask: 0.3241, stage2_loss_cls: 0.2762, stage2_pos_acc: 92.3197, stage2_loss_bbox: 0.0996, stage2_loss_iou: 0.2421, stage2_loss_mask: 0.2883, stage3_loss_cls: 0.1609, stage3_pos_acc: 94.0586, stage3_loss_bbox: 0.0878, stage3_loss_iou: 0.2053, stage3_loss_mask: 0.2719, stage4_loss_cls: 0.1012, stage4_pos_acc: 98.3793, stage4_loss_bbox: 0.0878, stage4_loss_iou: 0.2018, stage4_loss_mask: 0.2685, stage5_loss_cls: 0.0706, stage5_pos_acc: 98.8071, stage5_loss_bbox: 0.0835, stage5_loss_iou: 0.1963, stage5_loss_mask: 0.2662, loss: 6.9501\n",
      "2025-07-16 18:48:45,717 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:48:45,717 - mmdet - INFO - Epoch [47][500/750]\tlr: 2.500e-06, eta: 1:26:58, time: 0.389, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9859, stage0_pos_acc: 38.7403, stage0_loss_bbox: 0.3969, stage0_loss_iou: 0.9221, stage0_loss_mask: 0.5400, stage1_loss_cls: 0.4585, stage1_pos_acc: 83.0550, stage1_loss_bbox: 0.1432, stage1_loss_iou: 0.3415, stage1_loss_mask: 0.2722, stage2_loss_cls: 0.2786, stage2_pos_acc: 91.2855, stage2_loss_bbox: 0.1048, stage2_loss_iou: 0.2410, stage2_loss_mask: 0.2467, stage3_loss_cls: 0.1428, stage3_pos_acc: 97.8149, stage3_loss_bbox: 0.0955, stage3_loss_iou: 0.2091, stage3_loss_mask: 0.2232, stage4_loss_cls: 0.0803, stage4_pos_acc: 98.3240, stage4_loss_bbox: 0.0945, stage4_loss_iou: 0.2054, stage4_loss_mask: 0.2149, stage5_loss_cls: 0.0572, stage5_pos_acc: 98.6426, stage5_loss_bbox: 0.0908, stage5_loss_iou: 0.2002, stage5_loss_mask: 0.2045, loss: 6.7498\n",
      "2025-07-16 18:49:04,722 - mmdet - INFO - Epoch [47][550/750]\tlr: 2.500e-06, eta: 1:26:39, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9521, stage0_pos_acc: 47.6949, stage0_loss_bbox: 0.3762, stage0_loss_iou: 0.8256, stage0_loss_mask: 0.4399, stage1_loss_cls: 0.4539, stage1_pos_acc: 84.1950, stage1_loss_bbox: 0.1476, stage1_loss_iou: 0.3319, stage1_loss_mask: 0.2637, stage2_loss_cls: 0.2777, stage2_pos_acc: 91.9051, stage2_loss_bbox: 0.1201, stage2_loss_iou: 0.2321, stage2_loss_mask: 0.2408, stage3_loss_cls: 0.1545, stage3_pos_acc: 95.3421, stage3_loss_bbox: 0.1095, stage3_loss_iou: 0.2128, stage3_loss_mask: 0.2388, stage4_loss_cls: 0.0814, stage4_pos_acc: 97.8875, stage4_loss_bbox: 0.1069, stage4_loss_iou: 0.2061, stage4_loss_mask: 0.2304, stage5_loss_cls: 0.0706, stage5_pos_acc: 98.5923, stage5_loss_bbox: 0.1044, stage5_loss_iou: 0.2033, stage5_loss_mask: 0.2276, loss: 6.6078\n",
      "2025-07-16 18:49:23,702 - mmdet - INFO - Epoch [47][600/750]\tlr: 2.500e-06, eta: 1:26:20, time: 0.380, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9373, stage0_pos_acc: 43.6901, stage0_loss_bbox: 0.3608, stage0_loss_iou: 0.8157, stage0_loss_mask: 0.4382, stage1_loss_cls: 0.4380, stage1_pos_acc: 81.5649, stage1_loss_bbox: 0.1350, stage1_loss_iou: 0.2991, stage1_loss_mask: 0.2571, stage2_loss_cls: 0.2873, stage2_pos_acc: 89.0222, stage2_loss_bbox: 0.1083, stage2_loss_iou: 0.2389, stage2_loss_mask: 0.2235, stage3_loss_cls: 0.1543, stage3_pos_acc: 96.0714, stage3_loss_bbox: 0.0990, stage3_loss_iou: 0.2153, stage3_loss_mask: 0.2124, stage4_loss_cls: 0.0995, stage4_pos_acc: 97.2159, stage4_loss_bbox: 0.0925, stage4_loss_iou: 0.2034, stage4_loss_mask: 0.2085, stage5_loss_cls: 0.0767, stage5_pos_acc: 98.7238, stage5_loss_bbox: 0.0915, stage5_loss_iou: 0.1974, stage5_loss_mask: 0.2082, loss: 6.3977\n",
      "2025-07-16 18:49:42,778 - mmdet - INFO - Epoch [47][650/750]\tlr: 2.500e-06, eta: 1:26:01, time: 0.382, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9422, stage0_pos_acc: 40.1202, stage0_loss_bbox: 0.3995, stage0_loss_iou: 0.8853, stage0_loss_mask: 0.7138, stage1_loss_cls: 0.4702, stage1_pos_acc: 78.1840, stage1_loss_bbox: 0.1543, stage1_loss_iou: 0.3694, stage1_loss_mask: 0.3766, stage2_loss_cls: 0.3101, stage2_pos_acc: 87.1582, stage2_loss_bbox: 0.1215, stage2_loss_iou: 0.2843, stage2_loss_mask: 0.3734, stage3_loss_cls: 0.1791, stage3_pos_acc: 92.4234, stage3_loss_bbox: 0.1160, stage3_loss_iou: 0.2653, stage3_loss_mask: 0.3247, stage4_loss_cls: 0.1288, stage4_pos_acc: 95.7400, stage4_loss_bbox: 0.1125, stage4_loss_iou: 0.2542, stage4_loss_mask: 0.3093, stage5_loss_cls: 0.1188, stage5_pos_acc: 96.3965, stage5_loss_bbox: 0.1090, stage5_loss_iou: 0.2471, stage5_loss_mask: 0.3109, loss: 7.8764\n",
      "2025-07-16 18:50:01,813 - mmdet - INFO - Epoch [47][700/750]\tlr: 2.500e-06, eta: 1:25:42, time: 0.381, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9554, stage0_pos_acc: 41.4853, stage0_loss_bbox: 0.3764, stage0_loss_iou: 0.8913, stage0_loss_mask: 0.6251, stage1_loss_cls: 0.4634, stage1_pos_acc: 83.0197, stage1_loss_bbox: 0.1514, stage1_loss_iou: 0.3623, stage1_loss_mask: 0.3813, stage2_loss_cls: 0.2973, stage2_pos_acc: 90.9753, stage2_loss_bbox: 0.1253, stage2_loss_iou: 0.2939, stage2_loss_mask: 0.3586, stage3_loss_cls: 0.1682, stage3_pos_acc: 95.8920, stage3_loss_bbox: 0.1116, stage3_loss_iou: 0.2635, stage3_loss_mask: 0.3365, stage4_loss_cls: 0.1010, stage4_pos_acc: 98.0317, stage4_loss_bbox: 0.1093, stage4_loss_iou: 0.2588, stage4_loss_mask: 0.3327, stage5_loss_cls: 0.0792, stage5_pos_acc: 98.5317, stage5_loss_bbox: 0.1080, stage5_loss_iou: 0.2549, stage5_loss_mask: 0.3388, loss: 7.7440\n",
      "2025-07-16 18:50:20,718 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:50:20,719 - mmdet - INFO - Epoch [47][750/750]\tlr: 2.500e-06, eta: 1:25:23, time: 0.378, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9148, stage0_pos_acc: 41.9798, stage0_loss_bbox: 0.3607, stage0_loss_iou: 0.7621, stage0_loss_mask: 0.3853, stage1_loss_cls: 0.4475, stage1_pos_acc: 80.7310, stage1_loss_bbox: 0.1227, stage1_loss_iou: 0.2785, stage1_loss_mask: 0.2017, stage2_loss_cls: 0.2727, stage2_pos_acc: 89.5310, stage2_loss_bbox: 0.0961, stage2_loss_iou: 0.2084, stage2_loss_mask: 0.1807, stage3_loss_cls: 0.1547, stage3_pos_acc: 92.5976, stage3_loss_bbox: 0.0963, stage3_loss_iou: 0.1963, stage3_loss_mask: 0.1847, stage4_loss_cls: 0.0949, stage4_pos_acc: 95.7452, stage4_loss_bbox: 0.0924, stage4_loss_iou: 0.1898, stage4_loss_mask: 0.1663, stage5_loss_cls: 0.0777, stage5_pos_acc: 97.0702, stage5_loss_bbox: 0.0886, stage5_loss_iou: 0.1850, stage5_loss_mask: 0.1563, loss: 5.9142\n",
      "2025-07-16 18:50:20,861 - mmdet - INFO - Saving checkpoint at 47 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 18:53:40,186 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.011\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.239\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.452\n",
      "2025-07-16 18:53:42,280 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.411\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.253\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.452\n",
      "2025-07-16 18:53:45,464 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:53:45,464 - mmdet - INFO - Epoch(val) [47][750]\tbbox_mAP: 0.0270, bbox_mAP_50: 0.0510, bbox_mAP_75: 0.0260, bbox_mAP_s: 0.1230, bbox_mAP_m: 0.0110, bbox_mAP_l: 0.0340, bbox_mAP_copypaste: 0.027 0.051 0.026 0.123 0.011 0.034, segm_mAP: 0.0270, segm_mAP_50: 0.0500, segm_mAP_75: 0.0260, segm_mAP_s: 0.1210, segm_mAP_m: 0.0130, segm_mAP_l: 0.0350, segm_mAP_copypaste: 0.027 0.050 0.026 0.121 0.013 0.035\n",
      "2025-07-16 18:54:06,825 - mmdet - INFO - Epoch [48][50/750]\tlr: 2.500e-06, eta: 1:25:06, time: 0.427, data_time: 0.056, memory: 11264, stage0_loss_cls: 0.9364, stage0_pos_acc: 46.3786, stage0_loss_bbox: 0.4137, stage0_loss_iou: 0.9061, stage0_loss_mask: 0.5254, stage1_loss_cls: 0.4611, stage1_pos_acc: 79.5689, stage1_loss_bbox: 0.1446, stage1_loss_iou: 0.3198, stage1_loss_mask: 0.2309, stage2_loss_cls: 0.2976, stage2_pos_acc: 89.4978, stage2_loss_bbox: 0.1163, stage2_loss_iou: 0.2373, stage2_loss_mask: 0.2002, stage3_loss_cls: 0.1718, stage3_pos_acc: 93.7895, stage3_loss_bbox: 0.1059, stage3_loss_iou: 0.2123, stage3_loss_mask: 0.1948, stage4_loss_cls: 0.1043, stage4_pos_acc: 97.4900, stage4_loss_bbox: 0.1050, stage4_loss_iou: 0.2119, stage4_loss_mask: 0.2306, stage5_loss_cls: 0.0951, stage5_pos_acc: 98.3877, stage5_loss_bbox: 0.1027, stage5_loss_iou: 0.2081, stage5_loss_mask: 0.2348, loss: 6.7667\n",
      "2025-07-16 18:54:25,759 - mmdet - INFO - Epoch [48][100/750]\tlr: 2.500e-06, eta: 1:24:47, time: 0.379, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9505, stage0_pos_acc: 34.9633, stage0_loss_bbox: 0.4045, stage0_loss_iou: 0.8318, stage0_loss_mask: 0.4669, stage1_loss_cls: 0.4461, stage1_pos_acc: 80.8140, stage1_loss_bbox: 0.1538, stage1_loss_iou: 0.3187, stage1_loss_mask: 0.2762, stage2_loss_cls: 0.2759, stage2_pos_acc: 91.4555, stage2_loss_bbox: 0.1185, stage2_loss_iou: 0.2173, stage2_loss_mask: 0.2339, stage3_loss_cls: 0.1591, stage3_pos_acc: 94.6850, stage3_loss_bbox: 0.1067, stage3_loss_iou: 0.2011, stage3_loss_mask: 0.2208, stage4_loss_cls: 0.0821, stage4_pos_acc: 97.5260, stage4_loss_bbox: 0.1028, stage4_loss_iou: 0.1948, stage4_loss_mask: 0.2183, stage5_loss_cls: 0.0642, stage5_pos_acc: 98.2564, stage5_loss_bbox: 0.1010, stage5_loss_iou: 0.1931, stage5_loss_mask: 0.2080, loss: 6.5460\n",
      "2025-07-16 18:54:44,733 - mmdet - INFO - Epoch [48][150/750]\tlr: 2.500e-06, eta: 1:24:28, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9437, stage0_pos_acc: 39.0282, stage0_loss_bbox: 0.3740, stage0_loss_iou: 0.8278, stage0_loss_mask: 0.3806, stage1_loss_cls: 0.4541, stage1_pos_acc: 78.4592, stage1_loss_bbox: 0.1381, stage1_loss_iou: 0.2752, stage1_loss_mask: 0.1988, stage2_loss_cls: 0.2803, stage2_pos_acc: 89.1258, stage2_loss_bbox: 0.1059, stage2_loss_iou: 0.2009, stage2_loss_mask: 0.1742, stage3_loss_cls: 0.1495, stage3_pos_acc: 95.3308, stage3_loss_bbox: 0.0953, stage3_loss_iou: 0.1799, stage3_loss_mask: 0.1593, stage4_loss_cls: 0.0843, stage4_pos_acc: 98.3189, stage4_loss_bbox: 0.0925, stage4_loss_iou: 0.1739, stage4_loss_mask: 0.1586, stage5_loss_cls: 0.0658, stage5_pos_acc: 99.6970, stage5_loss_bbox: 0.0927, stage5_loss_iou: 0.1722, stage5_loss_mask: 0.1544, loss: 5.9321\n",
      "2025-07-16 18:55:03,762 - mmdet - INFO - Epoch [48][200/750]\tlr: 2.500e-06, eta: 1:24:09, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9435, stage0_pos_acc: 47.8973, stage0_loss_bbox: 0.4223, stage0_loss_iou: 0.8454, stage0_loss_mask: 0.4903, stage1_loss_cls: 0.4424, stage1_pos_acc: 79.6735, stage1_loss_bbox: 0.1724, stage1_loss_iou: 0.3000, stage1_loss_mask: 0.2673, stage2_loss_cls: 0.2718, stage2_pos_acc: 90.8315, stage2_loss_bbox: 0.1443, stage2_loss_iou: 0.2229, stage2_loss_mask: 0.2486, stage3_loss_cls: 0.1404, stage3_pos_acc: 95.7815, stage3_loss_bbox: 0.1222, stage3_loss_iou: 0.2032, stage3_loss_mask: 0.2402, stage4_loss_cls: 0.0800, stage4_pos_acc: 98.2465, stage4_loss_bbox: 0.1065, stage4_loss_iou: 0.1931, stage4_loss_mask: 0.2386, stage5_loss_cls: 0.0644, stage5_pos_acc: 98.8727, stage5_loss_bbox: 0.1012, stage5_loss_iou: 0.1850, stage5_loss_mask: 0.2357, loss: 6.6818\n",
      "2025-07-16 18:55:22,971 - mmdet - INFO - Epoch [48][250/750]\tlr: 2.500e-06, eta: 1:23:50, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9665, stage0_pos_acc: 39.1949, stage0_loss_bbox: 0.3753, stage0_loss_iou: 0.8735, stage0_loss_mask: 0.5173, stage1_loss_cls: 0.4590, stage1_pos_acc: 77.1685, stage1_loss_bbox: 0.1460, stage1_loss_iou: 0.3309, stage1_loss_mask: 0.3288, stage2_loss_cls: 0.2965, stage2_pos_acc: 88.4911, stage2_loss_bbox: 0.1164, stage2_loss_iou: 0.2444, stage2_loss_mask: 0.2929, stage3_loss_cls: 0.1708, stage3_pos_acc: 96.2935, stage3_loss_bbox: 0.1057, stage3_loss_iou: 0.2250, stage3_loss_mask: 0.2846, stage4_loss_cls: 0.0969, stage4_pos_acc: 98.0958, stage4_loss_bbox: 0.1049, stage4_loss_iou: 0.2223, stage4_loss_mask: 0.2801, stage5_loss_cls: 0.0770, stage5_pos_acc: 98.1583, stage5_loss_bbox: 0.1033, stage5_loss_iou: 0.2195, stage5_loss_mask: 0.2708, loss: 7.1086\n",
      "2025-07-16 18:55:41,878 - mmdet - INFO - Epoch [48][300/750]\tlr: 2.500e-06, eta: 1:23:31, time: 0.378, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9401, stage0_pos_acc: 36.8891, stage0_loss_bbox: 0.3976, stage0_loss_iou: 0.8043, stage0_loss_mask: 0.6222, stage1_loss_cls: 0.4800, stage1_pos_acc: 81.7220, stage1_loss_bbox: 0.1773, stage1_loss_iou: 0.3436, stage1_loss_mask: 0.4551, stage2_loss_cls: 0.3004, stage2_pos_acc: 89.7714, stage2_loss_bbox: 0.1423, stage2_loss_iou: 0.2874, stage2_loss_mask: 0.4099, stage3_loss_cls: 0.1668, stage3_pos_acc: 94.5110, stage3_loss_bbox: 0.1323, stage3_loss_iou: 0.2640, stage3_loss_mask: 0.3874, stage4_loss_cls: 0.1069, stage4_pos_acc: 98.1439, stage4_loss_bbox: 0.1294, stage4_loss_iou: 0.2615, stage4_loss_mask: 0.3820, stage5_loss_cls: 0.0820, stage5_pos_acc: 98.5682, stage5_loss_bbox: 0.1259, stage5_loss_iou: 0.2589, stage5_loss_mask: 0.3817, loss: 8.0390\n",
      "2025-07-16 18:56:01,090 - mmdet - INFO - Epoch [48][350/750]\tlr: 2.500e-06, eta: 1:23:12, time: 0.384, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9298, stage0_pos_acc: 44.4229, stage0_loss_bbox: 0.3988, stage0_loss_iou: 0.9172, stage0_loss_mask: 0.8133, stage1_loss_cls: 0.4646, stage1_pos_acc: 79.3489, stage1_loss_bbox: 0.1625, stage1_loss_iou: 0.4208, stage1_loss_mask: 0.5556, stage2_loss_cls: 0.3182, stage2_pos_acc: 90.5245, stage2_loss_bbox: 0.1321, stage2_loss_iou: 0.3388, stage2_loss_mask: 0.5252, stage3_loss_cls: 0.1975, stage3_pos_acc: 94.0983, stage3_loss_bbox: 0.1188, stage3_loss_iou: 0.3124, stage3_loss_mask: 0.4751, stage4_loss_cls: 0.1236, stage4_pos_acc: 97.7071, stage4_loss_bbox: 0.1182, stage4_loss_iou: 0.3010, stage4_loss_mask: 0.4772, stage5_loss_cls: 0.1086, stage5_pos_acc: 98.8643, stage5_loss_bbox: 0.1150, stage5_loss_iou: 0.2919, stage5_loss_mask: 0.4671, loss: 9.0832\n",
      "2025-07-16 18:56:20,138 - mmdet - INFO - Epoch [48][400/750]\tlr: 2.500e-06, eta: 1:22:53, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9692, stage0_pos_acc: 42.7455, stage0_loss_bbox: 0.3803, stage0_loss_iou: 0.8208, stage0_loss_mask: 0.3418, stage1_loss_cls: 0.4331, stage1_pos_acc: 79.6595, stage1_loss_bbox: 0.1372, stage1_loss_iou: 0.2801, stage1_loss_mask: 0.2010, stage2_loss_cls: 0.2704, stage2_pos_acc: 93.3284, stage2_loss_bbox: 0.1053, stage2_loss_iou: 0.2048, stage2_loss_mask: 0.1781, stage3_loss_cls: 0.1338, stage3_pos_acc: 97.1952, stage3_loss_bbox: 0.0971, stage3_loss_iou: 0.1863, stage3_loss_mask: 0.1681, stage4_loss_cls: 0.0734, stage4_pos_acc: 98.3190, stage4_loss_bbox: 0.0931, stage4_loss_iou: 0.1758, stage4_loss_mask: 0.1644, stage5_loss_cls: 0.0567, stage5_pos_acc: 98.7238, stage5_loss_bbox: 0.0894, stage5_loss_iou: 0.1707, stage5_loss_mask: 0.1580, loss: 5.8889\n",
      "2025-07-16 18:56:39,138 - mmdet - INFO - Epoch [48][450/750]\tlr: 2.500e-06, eta: 1:22:34, time: 0.380, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9131, stage0_pos_acc: 42.7754, stage0_loss_bbox: 0.3650, stage0_loss_iou: 0.8719, stage0_loss_mask: 0.7410, stage1_loss_cls: 0.4521, stage1_pos_acc: 85.7843, stage1_loss_bbox: 0.1412, stage1_loss_iou: 0.3796, stage1_loss_mask: 0.3995, stage2_loss_cls: 0.3182, stage2_pos_acc: 85.9849, stage2_loss_bbox: 0.1161, stage2_loss_iou: 0.2887, stage2_loss_mask: 0.2856, stage3_loss_cls: 0.2065, stage3_pos_acc: 91.9891, stage3_loss_bbox: 0.1036, stage3_loss_iou: 0.2504, stage3_loss_mask: 0.2716, stage4_loss_cls: 0.1502, stage4_pos_acc: 95.4435, stage4_loss_bbox: 0.0999, stage4_loss_iou: 0.2461, stage4_loss_mask: 0.2570, stage5_loss_cls: 0.1141, stage5_pos_acc: 97.8924, stage5_loss_bbox: 0.0999, stage5_loss_iou: 0.2378, stage5_loss_mask: 0.2602, loss: 7.5694\n",
      "2025-07-16 18:56:58,135 - mmdet - INFO - Epoch [48][500/750]\tlr: 2.500e-06, eta: 1:22:15, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9724, stage0_pos_acc: 42.6718, stage0_loss_bbox: 0.4110, stage0_loss_iou: 0.9079, stage0_loss_mask: 0.4934, stage1_loss_cls: 0.4304, stage1_pos_acc: 84.1877, stage1_loss_bbox: 0.1347, stage1_loss_iou: 0.3196, stage1_loss_mask: 0.2436, stage2_loss_cls: 0.2616, stage2_pos_acc: 89.3849, stage2_loss_bbox: 0.1043, stage2_loss_iou: 0.2415, stage2_loss_mask: 0.2034, stage3_loss_cls: 0.1472, stage3_pos_acc: 93.3972, stage3_loss_bbox: 0.0918, stage3_loss_iou: 0.2102, stage3_loss_mask: 0.1900, stage4_loss_cls: 0.0917, stage4_pos_acc: 96.0254, stage4_loss_bbox: 0.0890, stage4_loss_iou: 0.2029, stage4_loss_mask: 0.1869, stage5_loss_cls: 0.0684, stage5_pos_acc: 98.1944, stage5_loss_bbox: 0.0868, stage5_loss_iou: 0.1968, stage5_loss_mask: 0.1849, loss: 6.4707\n",
      "2025-07-16 18:57:17,192 - mmdet - INFO - Epoch [48][550/750]\tlr: 2.500e-06, eta: 1:21:56, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9907, stage0_pos_acc: 37.4496, stage0_loss_bbox: 0.3697, stage0_loss_iou: 0.8705, stage0_loss_mask: 0.4570, stage1_loss_cls: 0.4629, stage1_pos_acc: 83.4408, stage1_loss_bbox: 0.1361, stage1_loss_iou: 0.3039, stage1_loss_mask: 0.2570, stage2_loss_cls: 0.2765, stage2_pos_acc: 91.7500, stage2_loss_bbox: 0.1094, stage2_loss_iou: 0.2348, stage2_loss_mask: 0.2492, stage3_loss_cls: 0.1533, stage3_pos_acc: 95.6932, stage3_loss_bbox: 0.1004, stage3_loss_iou: 0.2089, stage3_loss_mask: 0.2238, stage4_loss_cls: 0.0906, stage4_pos_acc: 98.2667, stage4_loss_bbox: 0.1005, stage4_loss_iou: 0.2061, stage4_loss_mask: 0.2418, stage5_loss_cls: 0.0697, stage5_pos_acc: 98.9333, stage5_loss_bbox: 0.0970, stage5_loss_iou: 0.2005, stage5_loss_mask: 0.2336, loss: 6.6441\n",
      "2025-07-16 18:57:36,321 - mmdet - INFO - Epoch [48][600/750]\tlr: 2.500e-06, eta: 1:21:37, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9307, stage0_pos_acc: 41.9593, stage0_loss_bbox: 0.3956, stage0_loss_iou: 0.8494, stage0_loss_mask: 0.6163, stage1_loss_cls: 0.4573, stage1_pos_acc: 81.0483, stage1_loss_bbox: 0.1711, stage1_loss_iou: 0.3529, stage1_loss_mask: 0.3460, stage2_loss_cls: 0.3001, stage2_pos_acc: 87.4919, stage2_loss_bbox: 0.1457, stage2_loss_iou: 0.2728, stage2_loss_mask: 0.3346, stage3_loss_cls: 0.1662, stage3_pos_acc: 94.1214, stage3_loss_bbox: 0.1332, stage3_loss_iou: 0.2559, stage3_loss_mask: 0.3202, stage4_loss_cls: 0.1185, stage4_pos_acc: 96.7167, stage4_loss_bbox: 0.1143, stage4_loss_iou: 0.2486, stage4_loss_mask: 0.3176, stage5_loss_cls: 0.0982, stage5_pos_acc: 98.0095, stage5_loss_bbox: 0.1126, stage5_loss_iou: 0.2462, stage5_loss_mask: 0.3099, loss: 7.6138\n",
      "2025-07-16 18:57:55,446 - mmdet - INFO - Epoch [48][650/750]\tlr: 2.500e-06, eta: 1:21:18, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9686, stage0_pos_acc: 39.9522, stage0_loss_bbox: 0.3980, stage0_loss_iou: 0.9103, stage0_loss_mask: 0.6042, stage1_loss_cls: 0.4501, stage1_pos_acc: 84.4243, stage1_loss_bbox: 0.1616, stage1_loss_iou: 0.3669, stage1_loss_mask: 0.3876, stage2_loss_cls: 0.2810, stage2_pos_acc: 92.6790, stage2_loss_bbox: 0.1284, stage2_loss_iou: 0.2891, stage2_loss_mask: 0.3752, stage3_loss_cls: 0.1413, stage3_pos_acc: 96.5749, stage3_loss_bbox: 0.1159, stage3_loss_iou: 0.2598, stage3_loss_mask: 0.3612, stage4_loss_cls: 0.1041, stage4_pos_acc: 96.9862, stage4_loss_bbox: 0.1117, stage4_loss_iou: 0.2496, stage4_loss_mask: 0.3546, stage5_loss_cls: 0.0747, stage5_pos_acc: 99.7395, stage5_loss_bbox: 0.1092, stage5_loss_iou: 0.2450, stage5_loss_mask: 0.3530, loss: 7.8010\n",
      "2025-07-16 18:58:14,537 - mmdet - INFO - Epoch [48][700/750]\tlr: 2.500e-06, eta: 1:20:59, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9442, stage0_pos_acc: 39.5549, stage0_loss_bbox: 0.3958, stage0_loss_iou: 0.8668, stage0_loss_mask: 0.5938, stage1_loss_cls: 0.4536, stage1_pos_acc: 82.0399, stage1_loss_bbox: 0.1612, stage1_loss_iou: 0.3782, stage1_loss_mask: 0.3851, stage2_loss_cls: 0.2948, stage2_pos_acc: 89.0946, stage2_loss_bbox: 0.1396, stage2_loss_iou: 0.3045, stage2_loss_mask: 0.3573, stage3_loss_cls: 0.1629, stage3_pos_acc: 94.6881, stage3_loss_bbox: 0.1276, stage3_loss_iou: 0.2747, stage3_loss_mask: 0.3332, stage4_loss_cls: 0.1021, stage4_pos_acc: 97.2873, stage4_loss_bbox: 0.1198, stage4_loss_iou: 0.2652, stage4_loss_mask: 0.3246, stage5_loss_cls: 0.0926, stage5_pos_acc: 97.1507, stage5_loss_bbox: 0.1186, stage5_loss_iou: 0.2637, stage5_loss_mask: 0.3209, loss: 7.7810\n",
      "2025-07-16 18:58:33,898 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 18:58:33,899 - mmdet - INFO - Epoch [48][750/750]\tlr: 2.500e-06, eta: 1:20:41, time: 0.387, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9423, stage0_pos_acc: 43.4800, stage0_loss_bbox: 0.3952, stage0_loss_iou: 0.9171, stage0_loss_mask: 0.5539, stage1_loss_cls: 0.4787, stage1_pos_acc: 77.9069, stage1_loss_bbox: 0.1558, stage1_loss_iou: 0.3482, stage1_loss_mask: 0.2577, stage2_loss_cls: 0.3058, stage2_pos_acc: 88.6001, stage2_loss_bbox: 0.1225, stage2_loss_iou: 0.2495, stage2_loss_mask: 0.2196, stage3_loss_cls: 0.1909, stage3_pos_acc: 93.7766, stage3_loss_bbox: 0.1124, stage3_loss_iou: 0.2308, stage3_loss_mask: 0.2238, stage4_loss_cls: 0.1404, stage4_pos_acc: 96.1599, stage4_loss_bbox: 0.1046, stage4_loss_iou: 0.2206, stage4_loss_mask: 0.2062, stage5_loss_cls: 0.1218, stage5_pos_acc: 95.7099, stage5_loss_bbox: 0.1028, stage5_loss_iou: 0.2145, stage5_loss_mask: 0.2223, loss: 7.0374\n",
      "2025-07-16 18:58:34,021 - mmdet - INFO - Saving checkpoint at 48 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 19:01:53,795 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.34s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.066\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.409\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.236\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.456\n",
      "2025-07-16 19:01:55,668 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.85s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.213\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.255\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.458\n",
      "2025-07-16 19:01:58,886 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:01:58,887 - mmdet - INFO - Epoch(val) [48][750]\tbbox_mAP: 0.0300, bbox_mAP_50: 0.0550, bbox_mAP_75: 0.0290, bbox_mAP_s: 0.0660, bbox_mAP_m: 0.0280, bbox_mAP_l: 0.0370, bbox_mAP_copypaste: 0.030 0.055 0.029 0.066 0.028 0.037, segm_mAP: 0.0300, segm_mAP_50: 0.0530, segm_mAP_75: 0.0290, segm_mAP_s: 0.0720, segm_mAP_m: 0.0280, segm_mAP_l: 0.0370, segm_mAP_copypaste: 0.030 0.053 0.029 0.072 0.028 0.037\n",
      "2025-07-16 19:02:20,945 - mmdet - INFO - Epoch [49][50/750]\tlr: 2.500e-06, eta: 1:20:24, time: 0.441, data_time: 0.058, memory: 11264, stage0_loss_cls: 0.9671, stage0_pos_acc: 44.7754, stage0_loss_bbox: 0.4169, stage0_loss_iou: 0.9768, stage0_loss_mask: 0.7179, stage1_loss_cls: 0.4631, stage1_pos_acc: 84.1966, stage1_loss_bbox: 0.1605, stage1_loss_iou: 0.4036, stage1_loss_mask: 0.4717, stage2_loss_cls: 0.3348, stage2_pos_acc: 86.7681, stage2_loss_bbox: 0.1261, stage2_loss_iou: 0.3050, stage2_loss_mask: 0.4131, stage3_loss_cls: 0.1822, stage3_pos_acc: 94.5036, stage3_loss_bbox: 0.1165, stage3_loss_iou: 0.2868, stage3_loss_mask: 0.3923, stage4_loss_cls: 0.1090, stage4_pos_acc: 97.5679, stage4_loss_bbox: 0.1174, stage4_loss_iou: 0.2805, stage4_loss_mask: 0.3943, stage5_loss_cls: 0.0900, stage5_pos_acc: 98.7798, stage5_loss_bbox: 0.1117, stage5_loss_iou: 0.2720, stage5_loss_mask: 0.3937, loss: 8.5029\n",
      "2025-07-16 19:02:40,722 - mmdet - INFO - Epoch [49][100/750]\tlr: 2.500e-06, eta: 1:20:05, time: 0.396, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9338, stage0_pos_acc: 47.9361, stage0_loss_bbox: 0.3855, stage0_loss_iou: 0.8515, stage0_loss_mask: 0.6161, stage1_loss_cls: 0.4351, stage1_pos_acc: 81.8663, stage1_loss_bbox: 0.1553, stage1_loss_iou: 0.3427, stage1_loss_mask: 0.3350, stage2_loss_cls: 0.2840, stage2_pos_acc: 90.5281, stage2_loss_bbox: 0.1299, stage2_loss_iou: 0.2761, stage2_loss_mask: 0.3091, stage3_loss_cls: 0.1474, stage3_pos_acc: 95.6912, stage3_loss_bbox: 0.1246, stage3_loss_iou: 0.2614, stage3_loss_mask: 0.2931, stage4_loss_cls: 0.0984, stage4_pos_acc: 97.9167, stage4_loss_bbox: 0.1204, stage4_loss_iou: 0.2524, stage4_loss_mask: 0.2945, stage5_loss_cls: 0.0833, stage5_pos_acc: 98.7952, stage5_loss_bbox: 0.1227, stage5_loss_iou: 0.2464, stage5_loss_mask: 0.2943, loss: 7.3929\n",
      "2025-07-16 19:03:00,251 - mmdet - INFO - Epoch [49][150/750]\tlr: 2.500e-06, eta: 1:19:46, time: 0.391, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9583, stage0_pos_acc: 39.9115, stage0_loss_bbox: 0.3762, stage0_loss_iou: 0.8770, stage0_loss_mask: 0.5465, stage1_loss_cls: 0.4506, stage1_pos_acc: 80.7232, stage1_loss_bbox: 0.1407, stage1_loss_iou: 0.3337, stage1_loss_mask: 0.2780, stage2_loss_cls: 0.2882, stage2_pos_acc: 90.4177, stage2_loss_bbox: 0.1124, stage2_loss_iou: 0.2554, stage2_loss_mask: 0.2698, stage3_loss_cls: 0.1850, stage3_pos_acc: 94.1288, stage3_loss_bbox: 0.1024, stage3_loss_iou: 0.2344, stage3_loss_mask: 0.2592, stage4_loss_cls: 0.1175, stage4_pos_acc: 96.9034, stage4_loss_bbox: 0.1040, stage4_loss_iou: 0.2292, stage4_loss_mask: 0.2642, stage5_loss_cls: 0.0978, stage5_pos_acc: 96.7034, stage5_loss_bbox: 0.1036, stage5_loss_iou: 0.2245, stage5_loss_mask: 0.2636, loss: 7.0722\n",
      "2025-07-16 19:03:19,500 - mmdet - INFO - Epoch [49][200/750]\tlr: 2.500e-06, eta: 1:19:28, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8936, stage0_pos_acc: 45.6694, stage0_loss_bbox: 0.3923, stage0_loss_iou: 0.7684, stage0_loss_mask: 0.5792, stage1_loss_cls: 0.4347, stage1_pos_acc: 81.4099, stage1_loss_bbox: 0.1850, stage1_loss_iou: 0.3076, stage1_loss_mask: 0.3417, stage2_loss_cls: 0.2948, stage2_pos_acc: 90.5373, stage2_loss_bbox: 0.1547, stage2_loss_iou: 0.2413, stage2_loss_mask: 0.2863, stage3_loss_cls: 0.1686, stage3_pos_acc: 94.4921, stage3_loss_bbox: 0.1156, stage3_loss_iou: 0.2294, stage3_loss_mask: 0.2769, stage4_loss_cls: 0.1114, stage4_pos_acc: 98.3278, stage4_loss_bbox: 0.1286, stage4_loss_iou: 0.2213, stage4_loss_mask: 0.2594, stage5_loss_cls: 0.0952, stage5_pos_acc: 98.3278, stage5_loss_bbox: 0.1078, stage5_loss_iou: 0.2191, stage5_loss_mask: 0.2456, loss: 7.0585\n",
      "2025-07-16 19:03:38,785 - mmdet - INFO - Epoch [49][250/750]\tlr: 2.500e-06, eta: 1:19:09, time: 0.386, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9765, stage0_pos_acc: 42.0108, stage0_loss_bbox: 0.4014, stage0_loss_iou: 0.8789, stage0_loss_mask: 0.5251, stage1_loss_cls: 0.4514, stage1_pos_acc: 85.0062, stage1_loss_bbox: 0.1435, stage1_loss_iou: 0.3241, stage1_loss_mask: 0.2865, stage2_loss_cls: 0.2987, stage2_pos_acc: 89.6579, stage2_loss_bbox: 0.1096, stage2_loss_iou: 0.2294, stage2_loss_mask: 0.2362, stage3_loss_cls: 0.1680, stage3_pos_acc: 93.8421, stage3_loss_bbox: 0.1014, stage3_loss_iou: 0.2051, stage3_loss_mask: 0.2496, stage4_loss_cls: 0.1014, stage4_pos_acc: 97.1111, stage4_loss_bbox: 0.0994, stage4_loss_iou: 0.1982, stage4_loss_mask: 0.2401, stage5_loss_cls: 0.0733, stage5_pos_acc: 98.8667, stage5_loss_bbox: 0.0960, stage5_loss_iou: 0.1918, stage5_loss_mask: 0.2377, loss: 6.8232\n",
      "2025-07-16 19:03:57,941 - mmdet - INFO - Epoch [49][300/750]\tlr: 2.500e-06, eta: 1:18:50, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9435, stage0_pos_acc: 41.3509, stage0_loss_bbox: 0.3607, stage0_loss_iou: 0.7464, stage0_loss_mask: 0.4403, stage1_loss_cls: 0.4732, stage1_pos_acc: 76.9937, stage1_loss_bbox: 0.1635, stage1_loss_iou: 0.3090, stage1_loss_mask: 0.2723, stage2_loss_cls: 0.3030, stage2_pos_acc: 89.4031, stage2_loss_bbox: 0.1386, stage2_loss_iou: 0.2404, stage2_loss_mask: 0.2654, stage3_loss_cls: 0.1720, stage3_pos_acc: 93.0210, stage3_loss_bbox: 0.1310, stage3_loss_iou: 0.2285, stage3_loss_mask: 0.2604, stage4_loss_cls: 0.1094, stage4_pos_acc: 95.9552, stage4_loss_bbox: 0.1250, stage4_loss_iou: 0.2236, stage4_loss_mask: 0.2553, stage5_loss_cls: 0.0891, stage5_pos_acc: 98.1451, stage5_loss_bbox: 0.1274, stage5_loss_iou: 0.2218, stage5_loss_mask: 0.2585, loss: 6.8583\n",
      "2025-07-16 19:04:17,127 - mmdet - INFO - Epoch [49][350/750]\tlr: 2.500e-06, eta: 1:18:31, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9508, stage0_pos_acc: 45.1376, stage0_loss_bbox: 0.3807, stage0_loss_iou: 0.7999, stage0_loss_mask: 0.4598, stage1_loss_cls: 0.4395, stage1_pos_acc: 81.7695, stage1_loss_bbox: 0.1598, stage1_loss_iou: 0.2976, stage1_loss_mask: 0.2786, stage2_loss_cls: 0.2760, stage2_pos_acc: 89.5632, stage2_loss_bbox: 0.1228, stage2_loss_iou: 0.2286, stage2_loss_mask: 0.2394, stage3_loss_cls: 0.1731, stage3_pos_acc: 94.7211, stage3_loss_bbox: 0.1150, stage3_loss_iou: 0.2073, stage3_loss_mask: 0.2195, stage4_loss_cls: 0.1185, stage4_pos_acc: 96.8385, stage4_loss_bbox: 0.1028, stage4_loss_iou: 0.1959, stage4_loss_mask: 0.2172, stage5_loss_cls: 0.0978, stage5_pos_acc: 97.1363, stage5_loss_bbox: 0.1019, stage5_loss_iou: 0.1887, stage5_loss_mask: 0.1987, loss: 6.5699\n",
      "2025-07-16 19:04:36,570 - mmdet - INFO - Epoch [49][400/750]\tlr: 2.500e-06, eta: 1:18:12, time: 0.389, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9235, stage0_pos_acc: 41.6472, stage0_loss_bbox: 0.3927, stage0_loss_iou: 0.7977, stage0_loss_mask: 0.4586, stage1_loss_cls: 0.4506, stage1_pos_acc: 83.1107, stage1_loss_bbox: 0.1504, stage1_loss_iou: 0.3018, stage1_loss_mask: 0.2980, stage2_loss_cls: 0.2807, stage2_pos_acc: 91.1715, stage2_loss_bbox: 0.1230, stage2_loss_iou: 0.2339, stage2_loss_mask: 0.2778, stage3_loss_cls: 0.1485, stage3_pos_acc: 95.5221, stage3_loss_bbox: 0.1129, stage3_loss_iou: 0.2156, stage3_loss_mask: 0.2707, stage4_loss_cls: 0.1109, stage4_pos_acc: 97.1551, stage4_loss_bbox: 0.1096, stage4_loss_iou: 0.2114, stage4_loss_mask: 0.2476, stage5_loss_cls: 0.0922, stage5_pos_acc: 97.7983, stage5_loss_bbox: 0.1086, stage5_loss_iou: 0.2093, stage5_loss_mask: 0.2609, loss: 6.7867\n",
      "2025-07-16 19:04:56,094 - mmdet - INFO - Epoch [49][450/750]\tlr: 2.500e-06, eta: 1:17:54, time: 0.390, data_time: 0.012, memory: 11264, stage0_loss_cls: 0.9453, stage0_pos_acc: 46.2468, stage0_loss_bbox: 0.4018, stage0_loss_iou: 0.8825, stage0_loss_mask: 0.6651, stage1_loss_cls: 0.4756, stage1_pos_acc: 80.1414, stage1_loss_bbox: 0.1564, stage1_loss_iou: 0.3651, stage1_loss_mask: 0.3870, stage2_loss_cls: 0.3214, stage2_pos_acc: 86.6597, stage2_loss_bbox: 0.1200, stage2_loss_iou: 0.2993, stage2_loss_mask: 0.3695, stage3_loss_cls: 0.1897, stage3_pos_acc: 93.8251, stage3_loss_bbox: 0.1069, stage3_loss_iou: 0.2812, stage3_loss_mask: 0.3514, stage4_loss_cls: 0.1413, stage4_pos_acc: 96.2748, stage4_loss_bbox: 0.1025, stage4_loss_iou: 0.2658, stage4_loss_mask: 0.3345, stage5_loss_cls: 0.1207, stage5_pos_acc: 97.0367, stage5_loss_bbox: 0.1010, stage5_loss_iou: 0.2602, stage5_loss_mask: 0.3344, loss: 7.9786\n",
      "2025-07-16 19:05:15,150 - mmdet - INFO - Epoch [49][500/750]\tlr: 2.500e-06, eta: 1:17:35, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9986, stage0_pos_acc: 38.1786, stage0_loss_bbox: 0.3809, stage0_loss_iou: 0.8939, stage0_loss_mask: 0.4776, stage1_loss_cls: 0.4500, stage1_pos_acc: 80.7810, stage1_loss_bbox: 0.1301, stage1_loss_iou: 0.3137, stage1_loss_mask: 0.3063, stage2_loss_cls: 0.2773, stage2_pos_acc: 91.3071, stage2_loss_bbox: 0.1054, stage2_loss_iou: 0.2275, stage2_loss_mask: 0.2795, stage3_loss_cls: 0.1296, stage3_pos_acc: 96.4476, stage3_loss_bbox: 0.0945, stage3_loss_iou: 0.2052, stage3_loss_mask: 0.2748, stage4_loss_cls: 0.0570, stage4_pos_acc: 99.1238, stage4_loss_bbox: 0.0979, stage4_loss_iou: 0.1975, stage4_loss_mask: 0.2768, stage5_loss_cls: 0.0452, stage5_pos_acc: 99.7238, stage5_loss_bbox: 0.0932, stage5_loss_iou: 0.1937, stage5_loss_mask: 0.2696, loss: 6.7757\n",
      "2025-07-16 19:05:34,103 - mmdet - INFO - Epoch [49][550/750]\tlr: 2.500e-06, eta: 1:17:16, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9505, stage0_pos_acc: 40.1062, stage0_loss_bbox: 0.3639, stage0_loss_iou: 0.8008, stage0_loss_mask: 0.5343, stage1_loss_cls: 0.4471, stage1_pos_acc: 77.2866, stage1_loss_bbox: 0.1309, stage1_loss_iou: 0.3181, stage1_loss_mask: 0.2803, stage2_loss_cls: 0.2691, stage2_pos_acc: 87.6317, stage2_loss_bbox: 0.1061, stage2_loss_iou: 0.2431, stage2_loss_mask: 0.2384, stage3_loss_cls: 0.1601, stage3_pos_acc: 94.9310, stage3_loss_bbox: 0.0945, stage3_loss_iou: 0.2151, stage3_loss_mask: 0.2252, stage4_loss_cls: 0.0843, stage4_pos_acc: 97.0643, stage4_loss_bbox: 0.0928, stage4_loss_iou: 0.2054, stage4_loss_mask: 0.2430, stage5_loss_cls: 0.0691, stage5_pos_acc: 97.7500, stage5_loss_bbox: 0.0882, stage5_loss_iou: 0.1977, stage5_loss_mask: 0.2288, loss: 6.5868\n",
      "2025-07-16 19:05:53,440 - mmdet - INFO - Epoch [49][600/750]\tlr: 2.500e-06, eta: 1:16:57, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9788, stage0_pos_acc: 42.1703, stage0_loss_bbox: 0.3923, stage0_loss_iou: 0.8988, stage0_loss_mask: 0.3660, stage1_loss_cls: 0.4621, stage1_pos_acc: 82.8857, stage1_loss_bbox: 0.1369, stage1_loss_iou: 0.2944, stage1_loss_mask: 0.2097, stage2_loss_cls: 0.2912, stage2_pos_acc: 88.9524, stage2_loss_bbox: 0.1113, stage2_loss_iou: 0.2206, stage2_loss_mask: 0.1999, stage3_loss_cls: 0.1497, stage3_pos_acc: 94.0952, stage3_loss_bbox: 0.1034, stage3_loss_iou: 0.1931, stage3_loss_mask: 0.1785, stage4_loss_cls: 0.0908, stage4_pos_acc: 98.6571, stage4_loss_bbox: 0.0950, stage4_loss_iou: 0.1786, stage4_loss_mask: 0.1674, stage5_loss_cls: 0.0637, stage5_pos_acc: 99.8000, stage5_loss_bbox: 0.0901, stage5_loss_iou: 0.1720, stage5_loss_mask: 0.1662, loss: 6.2107\n",
      "2025-07-16 19:06:12,081 - mmdet - INFO - Epoch [49][650/750]\tlr: 2.500e-06, eta: 1:16:38, time: 0.373, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9569, stage0_pos_acc: 40.8742, stage0_loss_bbox: 0.4115, stage0_loss_iou: 0.9776, stage0_loss_mask: 0.7469, stage1_loss_cls: 0.4783, stage1_pos_acc: 85.2649, stage1_loss_bbox: 0.1525, stage1_loss_iou: 0.3838, stage1_loss_mask: 0.3692, stage2_loss_cls: 0.3041, stage2_pos_acc: 87.8593, stage2_loss_bbox: 0.1094, stage2_loss_iou: 0.2828, stage2_loss_mask: 0.3324, stage3_loss_cls: 0.1775, stage3_pos_acc: 93.9703, stage3_loss_bbox: 0.0992, stage3_loss_iou: 0.2521, stage3_loss_mask: 0.3202, stage4_loss_cls: 0.1229, stage4_pos_acc: 97.1894, stage4_loss_bbox: 0.0964, stage4_loss_iou: 0.2487, stage4_loss_mask: 0.3152, stage5_loss_cls: 0.0895, stage5_pos_acc: 97.9584, stage5_loss_bbox: 0.0944, stage5_loss_iou: 0.2470, stage5_loss_mask: 0.3315, loss: 7.9003\n",
      "2025-07-16 19:06:30,482 - mmdet - INFO - Epoch [49][700/750]\tlr: 2.500e-06, eta: 1:16:18, time: 0.368, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9486, stage0_pos_acc: 37.0891, stage0_loss_bbox: 0.3779, stage0_loss_iou: 0.8324, stage0_loss_mask: 0.3992, stage1_loss_cls: 0.4369, stage1_pos_acc: 79.9941, stage1_loss_bbox: 0.1367, stage1_loss_iou: 0.2868, stage1_loss_mask: 0.2118, stage2_loss_cls: 0.2680, stage2_pos_acc: 89.2524, stage2_loss_bbox: 0.1089, stage2_loss_iou: 0.2145, stage2_loss_mask: 0.2132, stage3_loss_cls: 0.1460, stage3_pos_acc: 95.6087, stage3_loss_bbox: 0.0939, stage3_loss_iou: 0.1942, stage3_loss_mask: 0.2039, stage4_loss_cls: 0.0910, stage4_pos_acc: 97.3194, stage4_loss_bbox: 0.0883, stage4_loss_iou: 0.1849, stage4_loss_mask: 0.1989, stage5_loss_cls: 0.0715, stage5_pos_acc: 98.7027, stage5_loss_bbox: 0.0873, stage5_loss_iou: 0.1833, stage5_loss_mask: 0.2035, loss: 6.1813\n",
      "2025-07-16 19:06:49,301 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:06:49,301 - mmdet - INFO - Epoch [49][750/750]\tlr: 2.500e-06, eta: 1:15:59, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9464, stage0_pos_acc: 36.8725, stage0_loss_bbox: 0.3672, stage0_loss_iou: 0.7985, stage0_loss_mask: 0.4167, stage1_loss_cls: 0.4488, stage1_pos_acc: 83.4126, stage1_loss_bbox: 0.1457, stage1_loss_iou: 0.2911, stage1_loss_mask: 0.2590, stage2_loss_cls: 0.2859, stage2_pos_acc: 89.4201, stage2_loss_bbox: 0.1133, stage2_loss_iou: 0.2090, stage2_loss_mask: 0.2401, stage3_loss_cls: 0.1636, stage3_pos_acc: 95.7582, stage3_loss_bbox: 0.1064, stage3_loss_iou: 0.1943, stage3_loss_mask: 0.2283, stage4_loss_cls: 0.0992, stage4_pos_acc: 98.3734, stage4_loss_bbox: 0.1050, stage4_loss_iou: 0.1895, stage4_loss_mask: 0.2240, stage5_loss_cls: 0.0814, stage5_pos_acc: 98.9924, stage5_loss_bbox: 0.1017, stage5_loss_iou: 0.1858, stage5_loss_mask: 0.2242, loss: 6.4251\n",
      "2025-07-16 19:06:49,437 - mmdet - INFO - Saving checkpoint at 49 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 19:10:08,308 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.125\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.219\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.458\n",
      "2025-07-16 19:10:10,148 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.82s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.037\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.417\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.238\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.458\n",
      "2025-07-16 19:10:13,345 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:10:13,345 - mmdet - INFO - Epoch(val) [49][750]\tbbox_mAP: 0.0290, bbox_mAP_50: 0.0510, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.1250, bbox_mAP_m: 0.0160, bbox_mAP_l: 0.0360, bbox_mAP_copypaste: 0.029 0.051 0.028 0.125 0.016 0.036, segm_mAP: 0.0290, segm_mAP_50: 0.0510, segm_mAP_75: 0.0280, segm_mAP_s: 0.1230, segm_mAP_m: 0.0170, segm_mAP_l: 0.0370, segm_mAP_copypaste: 0.029 0.051 0.028 0.123 0.017 0.037\n",
      "2025-07-16 19:10:35,055 - mmdet - INFO - Epoch [50][50/750]\tlr: 2.500e-06, eta: 1:15:42, time: 0.434, data_time: 0.053, memory: 11264, stage0_loss_cls: 0.9338, stage0_pos_acc: 39.9345, stage0_loss_bbox: 0.3521, stage0_loss_iou: 0.7789, stage0_loss_mask: 0.5055, stage1_loss_cls: 0.4139, stage1_pos_acc: 79.7833, stage1_loss_bbox: 0.1341, stage1_loss_iou: 0.2989, stage1_loss_mask: 0.2766, stage2_loss_cls: 0.2520, stage2_pos_acc: 92.5583, stage2_loss_bbox: 0.1130, stage2_loss_iou: 0.2345, stage2_loss_mask: 0.2683, stage3_loss_cls: 0.1415, stage3_pos_acc: 97.3667, stage3_loss_bbox: 0.1000, stage3_loss_iou: 0.2091, stage3_loss_mask: 0.2554, stage4_loss_cls: 0.0916, stage4_pos_acc: 98.3333, stage4_loss_bbox: 0.0956, stage4_loss_iou: 0.2043, stage4_loss_mask: 0.2508, stage5_loss_cls: 0.0780, stage5_pos_acc: 98.9333, stage5_loss_bbox: 0.0935, stage5_loss_iou: 0.1970, stage5_loss_mask: 0.2572, loss: 6.5356\n",
      "2025-07-16 19:10:54,262 - mmdet - INFO - Epoch [50][100/750]\tlr: 2.500e-06, eta: 1:15:23, time: 0.384, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9170, stage0_pos_acc: 48.1096, stage0_loss_bbox: 0.3952, stage0_loss_iou: 0.7944, stage0_loss_mask: 0.5557, stage1_loss_cls: 0.4649, stage1_pos_acc: 80.5842, stage1_loss_bbox: 0.1509, stage1_loss_iou: 0.3042, stage1_loss_mask: 0.3119, stage2_loss_cls: 0.3212, stage2_pos_acc: 90.0295, stage2_loss_bbox: 0.1226, stage2_loss_iou: 0.2284, stage2_loss_mask: 0.2721, stage3_loss_cls: 0.1972, stage3_pos_acc: 93.8868, stage3_loss_bbox: 0.1121, stage3_loss_iou: 0.2099, stage3_loss_mask: 0.2563, stage4_loss_cls: 0.1368, stage4_pos_acc: 97.8167, stage4_loss_bbox: 0.1099, stage4_loss_iou: 0.1997, stage4_loss_mask: 0.2442, stage5_loss_cls: 0.1103, stage5_pos_acc: 98.8363, stage5_loss_bbox: 0.1020, stage5_loss_iou: 0.1952, stage5_loss_mask: 0.2424, loss: 6.9545\n",
      "2025-07-16 19:11:13,101 - mmdet - INFO - Epoch [50][150/750]\tlr: 2.500e-06, eta: 1:15:04, time: 0.377, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9459, stage0_pos_acc: 42.1441, stage0_loss_bbox: 0.4064, stage0_loss_iou: 0.9251, stage0_loss_mask: 0.5588, stage1_loss_cls: 0.4481, stage1_pos_acc: 79.3728, stage1_loss_bbox: 0.1491, stage1_loss_iou: 0.3533, stage1_loss_mask: 0.3531, stage2_loss_cls: 0.2849, stage2_pos_acc: 91.5753, stage2_loss_bbox: 0.1292, stage2_loss_iou: 0.2669, stage2_loss_mask: 0.3090, stage3_loss_cls: 0.1577, stage3_pos_acc: 95.7001, stage3_loss_bbox: 0.1171, stage3_loss_iou: 0.2424, stage3_loss_mask: 0.3134, stage4_loss_cls: 0.1143, stage4_pos_acc: 97.3955, stage4_loss_bbox: 0.1078, stage4_loss_iou: 0.2251, stage4_loss_mask: 0.3010, stage5_loss_cls: 0.0901, stage5_pos_acc: 98.0838, stage5_loss_bbox: 0.1054, stage5_loss_iou: 0.2193, stage5_loss_mask: 0.2928, loss: 7.4164\n",
      "2025-07-16 19:11:31,879 - mmdet - INFO - Epoch [50][200/750]\tlr: 2.500e-06, eta: 1:14:45, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9126, stage0_pos_acc: 41.6703, stage0_loss_bbox: 0.4012, stage0_loss_iou: 0.8233, stage0_loss_mask: 0.4583, stage1_loss_cls: 0.4304, stage1_pos_acc: 82.5160, stage1_loss_bbox: 0.1414, stage1_loss_iou: 0.2972, stage1_loss_mask: 0.3234, stage2_loss_cls: 0.2662, stage2_pos_acc: 91.3684, stage2_loss_bbox: 0.1128, stage2_loss_iou: 0.2272, stage2_loss_mask: 0.2987, stage3_loss_cls: 0.1516, stage3_pos_acc: 96.6778, stage3_loss_bbox: 0.1049, stage3_loss_iou: 0.2153, stage3_loss_mask: 0.2887, stage4_loss_cls: 0.0882, stage4_pos_acc: 98.6500, stage4_loss_bbox: 0.1019, stage4_loss_iou: 0.2105, stage4_loss_mask: 0.2976, stage5_loss_cls: 0.0680, stage5_pos_acc: 98.5833, stage5_loss_bbox: 0.1053, stage5_loss_iou: 0.2091, stage5_loss_mask: 0.3152, loss: 6.8490\n",
      "2025-07-16 19:11:51,199 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:11:51,199 - mmdet - INFO - Epoch [50][250/750]\tlr: 2.500e-06, eta: 1:14:26, time: 0.386, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9222, stage0_pos_acc: 41.7784, stage0_loss_bbox: 0.3557, stage0_loss_iou: 0.8229, stage0_loss_mask: 0.5880, stage1_loss_cls: 0.4629, stage1_pos_acc: 77.6596, stage1_loss_bbox: 0.1524, stage1_loss_iou: 0.3406, stage1_loss_mask: 0.3029, stage2_loss_cls: 0.3107, stage2_pos_acc: 87.4977, stage2_loss_bbox: 0.1156, stage2_loss_iou: 0.2576, stage2_loss_mask: 0.2932, stage3_loss_cls: 0.1694, stage3_pos_acc: 95.9310, stage3_loss_bbox: 0.1035, stage3_loss_iou: 0.2336, stage3_loss_mask: 0.2707, stage4_loss_cls: 0.1099, stage4_pos_acc: 97.9599, stage4_loss_bbox: 0.1005, stage4_loss_iou: 0.2238, stage4_loss_mask: 0.2477, stage5_loss_cls: 0.0934, stage5_pos_acc: 98.0670, stage5_loss_bbox: 0.0965, stage5_loss_iou: 0.2176, stage5_loss_mask: 0.2437, loss: 7.0348\n",
      "2025-07-16 19:12:10,268 - mmdet - INFO - Epoch [50][300/750]\tlr: 2.500e-06, eta: 1:14:07, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9230, stage0_pos_acc: 42.2643, stage0_loss_bbox: 0.3515, stage0_loss_iou: 0.8064, stage0_loss_mask: 0.3726, stage1_loss_cls: 0.4075, stage1_pos_acc: 85.5833, stage1_loss_bbox: 0.1323, stage1_loss_iou: 0.3113, stage1_loss_mask: 0.2384, stage2_loss_cls: 0.2509, stage2_pos_acc: 92.7363, stage2_loss_bbox: 0.1042, stage2_loss_iou: 0.2349, stage2_loss_mask: 0.1948, stage3_loss_cls: 0.1509, stage3_pos_acc: 95.5964, stage3_loss_bbox: 0.0996, stage3_loss_iou: 0.2218, stage3_loss_mask: 0.1834, stage4_loss_cls: 0.0843, stage4_pos_acc: 97.6631, stage4_loss_bbox: 0.0944, stage4_loss_iou: 0.2141, stage4_loss_mask: 0.1827, stage5_loss_cls: 0.0671, stage5_pos_acc: 98.3280, stage5_loss_bbox: 0.0936, stage5_loss_iou: 0.2069, stage5_loss_mask: 0.1812, loss: 6.1080\n",
      "2025-07-16 19:12:29,133 - mmdet - INFO - Epoch [50][350/750]\tlr: 2.500e-06, eta: 1:13:48, time: 0.377, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9467, stage0_pos_acc: 36.5339, stage0_loss_bbox: 0.3682, stage0_loss_iou: 0.8623, stage0_loss_mask: 0.4894, stage1_loss_cls: 0.4413, stage1_pos_acc: 82.1802, stage1_loss_bbox: 0.1324, stage1_loss_iou: 0.3140, stage1_loss_mask: 0.3095, stage2_loss_cls: 0.2821, stage2_pos_acc: 92.5764, stage2_loss_bbox: 0.1024, stage2_loss_iou: 0.2237, stage2_loss_mask: 0.2485, stage3_loss_cls: 0.1439, stage3_pos_acc: 95.0892, stage3_loss_bbox: 0.0946, stage3_loss_iou: 0.1999, stage3_loss_mask: 0.2337, stage4_loss_cls: 0.1070, stage4_pos_acc: 97.8113, stage4_loss_bbox: 0.0897, stage4_loss_iou: 0.1903, stage4_loss_mask: 0.2248, stage5_loss_cls: 0.0685, stage5_pos_acc: 98.3060, stage5_loss_bbox: 0.0912, stage5_loss_iou: 0.1907, stage5_loss_mask: 0.2190, loss: 6.5738\n",
      "2025-07-16 19:12:48,452 - mmdet - INFO - Epoch [50][400/750]\tlr: 2.500e-06, eta: 1:13:29, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9825, stage0_pos_acc: 42.0406, stage0_loss_bbox: 0.3883, stage0_loss_iou: 0.8854, stage0_loss_mask: 0.6381, stage1_loss_cls: 0.4549, stage1_pos_acc: 82.5149, stage1_loss_bbox: 0.1447, stage1_loss_iou: 0.3496, stage1_loss_mask: 0.3516, stage2_loss_cls: 0.2888, stage2_pos_acc: 88.9959, stage2_loss_bbox: 0.1132, stage2_loss_iou: 0.2738, stage2_loss_mask: 0.3430, stage3_loss_cls: 0.1504, stage3_pos_acc: 93.8332, stage3_loss_bbox: 0.1006, stage3_loss_iou: 0.2492, stage3_loss_mask: 0.3199, stage4_loss_cls: 0.1028, stage4_pos_acc: 97.5538, stage4_loss_bbox: 0.0985, stage4_loss_iou: 0.2400, stage4_loss_mask: 0.3147, stage5_loss_cls: 0.0774, stage5_pos_acc: 98.5280, stage5_loss_bbox: 0.0985, stage5_loss_iou: 0.2377, stage5_loss_mask: 0.3310, loss: 7.5346\n",
      "2025-07-16 19:13:07,353 - mmdet - INFO - Epoch [50][450/750]\tlr: 2.500e-06, eta: 1:13:10, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9662, stage0_pos_acc: 41.7358, stage0_loss_bbox: 0.4088, stage0_loss_iou: 0.8938, stage0_loss_mask: 0.6724, stage1_loss_cls: 0.4638, stage1_pos_acc: 81.2638, stage1_loss_bbox: 0.1693, stage1_loss_iou: 0.3700, stage1_loss_mask: 0.4172, stage2_loss_cls: 0.2999, stage2_pos_acc: 91.1374, stage2_loss_bbox: 0.1306, stage2_loss_iou: 0.2914, stage2_loss_mask: 0.3695, stage3_loss_cls: 0.1659, stage3_pos_acc: 95.6123, stage3_loss_bbox: 0.1192, stage3_loss_iou: 0.2775, stage3_loss_mask: 0.3448, stage4_loss_cls: 0.1103, stage4_pos_acc: 96.8052, stage4_loss_bbox: 0.1166, stage4_loss_iou: 0.2677, stage4_loss_mask: 0.3356, stage5_loss_cls: 0.0868, stage5_pos_acc: 97.3528, stage5_loss_bbox: 0.1160, stage5_loss_iou: 0.2635, stage5_loss_mask: 0.3432, loss: 7.9999\n",
      "2025-07-16 19:13:26,422 - mmdet - INFO - Epoch [50][500/750]\tlr: 2.500e-06, eta: 1:12:51, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9563, stage0_pos_acc: 45.1662, stage0_loss_bbox: 0.4039, stage0_loss_iou: 0.8730, stage0_loss_mask: 0.4201, stage1_loss_cls: 0.4428, stage1_pos_acc: 84.9125, stage1_loss_bbox: 0.1436, stage1_loss_iou: 0.3261, stage1_loss_mask: 0.2612, stage2_loss_cls: 0.2844, stage2_pos_acc: 93.7889, stage2_loss_bbox: 0.1147, stage2_loss_iou: 0.2506, stage2_loss_mask: 0.2295, stage3_loss_cls: 0.1605, stage3_pos_acc: 96.1627, stage3_loss_bbox: 0.1104, stage3_loss_iou: 0.2365, stage3_loss_mask: 0.2665, stage4_loss_cls: 0.1250, stage4_pos_acc: 98.0421, stage4_loss_bbox: 0.1031, stage4_loss_iou: 0.2268, stage4_loss_mask: 0.2583, stage5_loss_cls: 0.1115, stage5_pos_acc: 99.1000, stage5_loss_bbox: 0.0950, stage5_loss_iou: 0.2187, stage5_loss_mask: 0.2456, loss: 6.8640\n",
      "2025-07-16 19:13:45,729 - mmdet - INFO - Epoch [50][550/750]\tlr: 2.500e-06, eta: 1:12:32, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9838, stage0_pos_acc: 39.8159, stage0_loss_bbox: 0.4040, stage0_loss_iou: 0.8488, stage0_loss_mask: 0.5442, stage1_loss_cls: 0.4629, stage1_pos_acc: 83.3778, stage1_loss_bbox: 0.1651, stage1_loss_iou: 0.3503, stage1_loss_mask: 0.3329, stage2_loss_cls: 0.2887, stage2_pos_acc: 91.9071, stage2_loss_bbox: 0.1341, stage2_loss_iou: 0.2721, stage2_loss_mask: 0.2423, stage3_loss_cls: 0.1792, stage3_pos_acc: 96.5603, stage3_loss_bbox: 0.1241, stage3_loss_iou: 0.2472, stage3_loss_mask: 0.2492, stage4_loss_cls: 0.1009, stage4_pos_acc: 98.4381, stage4_loss_bbox: 0.1191, stage4_loss_iou: 0.2317, stage4_loss_mask: 0.2525, stage5_loss_cls: 0.0792, stage5_pos_acc: 98.5357, stage5_loss_bbox: 0.1174, stage5_loss_iou: 0.2303, stage5_loss_mask: 0.2427, loss: 7.2027\n",
      "2025-07-16 19:14:05,030 - mmdet - INFO - Epoch [50][600/750]\tlr: 2.500e-06, eta: 1:12:13, time: 0.386, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9365, stage0_pos_acc: 43.0715, stage0_loss_bbox: 0.4155, stage0_loss_iou: 0.8443, stage0_loss_mask: 0.4626, stage1_loss_cls: 0.4377, stage1_pos_acc: 83.3466, stage1_loss_bbox: 0.1879, stage1_loss_iou: 0.3058, stage1_loss_mask: 0.3062, stage2_loss_cls: 0.2745, stage2_pos_acc: 91.8451, stage2_loss_bbox: 0.1568, stage2_loss_iou: 0.2364, stage2_loss_mask: 0.2764, stage3_loss_cls: 0.1498, stage3_pos_acc: 96.0825, stage3_loss_bbox: 0.1366, stage3_loss_iou: 0.2252, stage3_loss_mask: 0.2588, stage4_loss_cls: 0.0916, stage4_pos_acc: 98.5848, stage4_loss_bbox: 0.1189, stage4_loss_iou: 0.2110, stage4_loss_mask: 0.2311, stage5_loss_cls: 0.0706, stage5_pos_acc: 98.3182, stage5_loss_bbox: 0.1137, stage5_loss_iou: 0.2116, stage5_loss_mask: 0.2518, loss: 6.9114\n",
      "2025-07-16 19:14:24,250 - mmdet - INFO - Epoch [50][650/750]\tlr: 2.500e-06, eta: 1:11:54, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9786, stage0_pos_acc: 36.6634, stage0_loss_bbox: 0.4115, stage0_loss_iou: 0.9307, stage0_loss_mask: 0.5398, stage1_loss_cls: 0.4673, stage1_pos_acc: 77.4890, stage1_loss_bbox: 0.1317, stage1_loss_iou: 0.3393, stage1_loss_mask: 0.2404, stage2_loss_cls: 0.2928, stage2_pos_acc: 85.8994, stage2_loss_bbox: 0.1050, stage2_loss_iou: 0.2261, stage2_loss_mask: 0.2328, stage3_loss_cls: 0.1509, stage3_pos_acc: 94.3840, stage3_loss_bbox: 0.1004, stage3_loss_iou: 0.2088, stage3_loss_mask: 0.2246, stage4_loss_cls: 0.0901, stage4_pos_acc: 97.8833, stage4_loss_bbox: 0.0980, stage4_loss_iou: 0.1985, stage4_loss_mask: 0.2004, stage5_loss_cls: 0.0661, stage5_pos_acc: 98.9258, stage5_loss_bbox: 0.0946, stage5_loss_iou: 0.1941, stage5_loss_mask: 0.1982, loss: 6.7208\n",
      "2025-07-16 19:14:43,608 - mmdet - INFO - Epoch [50][700/750]\tlr: 2.500e-06, eta: 1:11:35, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9274, stage0_pos_acc: 40.0994, stage0_loss_bbox: 0.3974, stage0_loss_iou: 0.8621, stage0_loss_mask: 0.5475, stage1_loss_cls: 0.4513, stage1_pos_acc: 79.5202, stage1_loss_bbox: 0.1641, stage1_loss_iou: 0.3546, stage1_loss_mask: 0.3532, stage2_loss_cls: 0.2784, stage2_pos_acc: 89.3348, stage2_loss_bbox: 0.1239, stage2_loss_iou: 0.2711, stage2_loss_mask: 0.3131, stage3_loss_cls: 0.1582, stage3_pos_acc: 94.6194, stage3_loss_bbox: 0.1196, stage3_loss_iou: 0.2436, stage3_loss_mask: 0.3230, stage4_loss_cls: 0.0876, stage4_pos_acc: 97.9897, stage4_loss_bbox: 0.1072, stage4_loss_iou: 0.2338, stage4_loss_mask: 0.2954, stage5_loss_cls: 0.0679, stage5_pos_acc: 98.2397, stage5_loss_bbox: 0.1038, stage5_loss_iou: 0.2283, stage5_loss_mask: 0.2939, loss: 7.3066\n",
      "2025-07-16 19:15:03,061 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:15:03,061 - mmdet - INFO - Epoch [50][750/750]\tlr: 2.500e-06, eta: 1:11:17, time: 0.389, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9469, stage0_pos_acc: 45.2253, stage0_loss_bbox: 0.4146, stage0_loss_iou: 0.9027, stage0_loss_mask: 0.5198, stage1_loss_cls: 0.4470, stage1_pos_acc: 80.4769, stage1_loss_bbox: 0.1586, stage1_loss_iou: 0.3312, stage1_loss_mask: 0.3299, stage2_loss_cls: 0.2951, stage2_pos_acc: 90.4127, stage2_loss_bbox: 0.1202, stage2_loss_iou: 0.2375, stage2_loss_mask: 0.3069, stage3_loss_cls: 0.1581, stage3_pos_acc: 94.6270, stage3_loss_bbox: 0.1202, stage3_loss_iou: 0.2288, stage3_loss_mask: 0.2952, stage4_loss_cls: 0.1065, stage4_pos_acc: 98.5901, stage4_loss_bbox: 0.1186, stage4_loss_iou: 0.2209, stage4_loss_mask: 0.2945, stage5_loss_cls: 0.0941, stage5_pos_acc: 98.1056, stage5_loss_bbox: 0.1139, stage5_loss_iou: 0.2154, stage5_loss_mask: 0.2920, loss: 7.2688\n",
      "2025-07-16 19:15:03,199 - mmdet - INFO - Saving checkpoint at 50 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 92s, ETA:     0s2025-07-16 19:18:13,340 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.57s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.054\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.127\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.421\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.421\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.188\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.463\n",
      "2025-07-16 19:18:15,653 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.82s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.030\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.053\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.122\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.039\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.426\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.208\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.245\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.463\n",
      "2025-07-16 19:18:18,601 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:18:18,601 - mmdet - INFO - Epoch(val) [50][750]\tbbox_mAP: 0.0300, bbox_mAP_50: 0.0540, bbox_mAP_75: 0.0300, bbox_mAP_s: 0.1270, bbox_mAP_m: 0.0230, bbox_mAP_l: 0.0390, bbox_mAP_copypaste: 0.030 0.054 0.030 0.127 0.023 0.039, segm_mAP: 0.0300, segm_mAP_50: 0.0530, segm_mAP_75: 0.0290, segm_mAP_s: 0.1220, segm_mAP_m: 0.0230, segm_mAP_l: 0.0390, segm_mAP_copypaste: 0.030 0.053 0.029 0.122 0.023 0.039\n",
      "2025-07-16 19:18:40,514 - mmdet - INFO - Epoch [51][50/750]\tlr: 2.500e-06, eta: 1:10:59, time: 0.438, data_time: 0.056, memory: 11264, stage0_loss_cls: 0.9120, stage0_pos_acc: 42.4941, stage0_loss_bbox: 0.3542, stage0_loss_iou: 0.7987, stage0_loss_mask: 0.5169, stage1_loss_cls: 0.4288, stage1_pos_acc: 79.3375, stage1_loss_bbox: 0.1405, stage1_loss_iou: 0.3028, stage1_loss_mask: 0.2833, stage2_loss_cls: 0.2838, stage2_pos_acc: 87.9533, stage2_loss_bbox: 0.1134, stage2_loss_iou: 0.2337, stage2_loss_mask: 0.2593, stage3_loss_cls: 0.1490, stage3_pos_acc: 96.1866, stage3_loss_bbox: 0.1043, stage3_loss_iou: 0.2146, stage3_loss_mask: 0.2512, stage4_loss_cls: 0.0869, stage4_pos_acc: 97.8500, stage4_loss_bbox: 0.1003, stage4_loss_iou: 0.2067, stage4_loss_mask: 0.2460, stage5_loss_cls: 0.0594, stage5_pos_acc: 99.3333, stage5_loss_bbox: 0.0985, stage5_loss_iou: 0.2029, stage5_loss_mask: 0.2456, loss: 6.5927\n",
      "2025-07-16 19:18:59,324 - mmdet - INFO - Epoch [51][100/750]\tlr: 2.500e-06, eta: 1:10:40, time: 0.376, data_time: 0.007, memory: 11264, stage0_loss_cls: 1.0072, stage0_pos_acc: 34.6079, stage0_loss_bbox: 0.3796, stage0_loss_iou: 0.9298, stage0_loss_mask: 0.4068, stage1_loss_cls: 0.4582, stage1_pos_acc: 85.4683, stage1_loss_bbox: 0.1230, stage1_loss_iou: 0.3033, stage1_loss_mask: 0.2707, stage2_loss_cls: 0.2835, stage2_pos_acc: 89.3222, stage2_loss_bbox: 0.0965, stage2_loss_iou: 0.2176, stage2_loss_mask: 0.2605, stage3_loss_cls: 0.1363, stage3_pos_acc: 95.2317, stage3_loss_bbox: 0.0848, stage3_loss_iou: 0.1970, stage3_loss_mask: 0.2527, stage4_loss_cls: 0.0734, stage4_pos_acc: 98.0222, stage4_loss_bbox: 0.0832, stage4_loss_iou: 0.1916, stage4_loss_mask: 0.2539, stage5_loss_cls: 0.0571, stage5_pos_acc: 99.4603, stage5_loss_bbox: 0.0801, stage5_loss_iou: 0.1868, stage5_loss_mask: 0.2467, loss: 6.5804\n",
      "2025-07-16 19:19:18,573 - mmdet - INFO - Epoch [51][150/750]\tlr: 2.500e-06, eta: 1:10:21, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9142, stage0_pos_acc: 44.5356, stage0_loss_bbox: 0.3816, stage0_loss_iou: 0.8174, stage0_loss_mask: 0.4248, stage1_loss_cls: 0.4367, stage1_pos_acc: 79.2497, stage1_loss_bbox: 0.1260, stage1_loss_iou: 0.2940, stage1_loss_mask: 0.2209, stage2_loss_cls: 0.2681, stage2_pos_acc: 92.1101, stage2_loss_bbox: 0.1004, stage2_loss_iou: 0.2293, stage2_loss_mask: 0.2121, stage3_loss_cls: 0.1563, stage3_pos_acc: 93.8579, stage3_loss_bbox: 0.0883, stage3_loss_iou: 0.1996, stage3_loss_mask: 0.1816, stage4_loss_cls: 0.0925, stage4_pos_acc: 98.2840, stage4_loss_bbox: 0.0852, stage4_loss_iou: 0.1921, stage4_loss_mask: 0.1909, stage5_loss_cls: 0.0677, stage5_pos_acc: 99.0454, stage5_loss_bbox: 0.0881, stage5_loss_iou: 0.1910, stage5_loss_mask: 0.1961, loss: 6.1548\n",
      "2025-07-16 19:19:37,935 - mmdet - INFO - Epoch [51][200/750]\tlr: 2.500e-06, eta: 1:10:02, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9115, stage0_pos_acc: 39.3957, stage0_loss_bbox: 0.3801, stage0_loss_iou: 0.8034, stage0_loss_mask: 0.4548, stage1_loss_cls: 0.4394, stage1_pos_acc: 81.2164, stage1_loss_bbox: 0.1496, stage1_loss_iou: 0.3212, stage1_loss_mask: 0.2544, stage2_loss_cls: 0.2635, stage2_pos_acc: 90.4735, stage2_loss_bbox: 0.1313, stage2_loss_iou: 0.2570, stage2_loss_mask: 0.2467, stage3_loss_cls: 0.1302, stage3_pos_acc: 96.0537, stage3_loss_bbox: 0.1152, stage3_loss_iou: 0.2318, stage3_loss_mask: 0.2180, stage4_loss_cls: 0.0830, stage4_pos_acc: 97.2831, stage4_loss_bbox: 0.1095, stage4_loss_iou: 0.2228, stage4_loss_mask: 0.2324, stage5_loss_cls: 0.0629, stage5_pos_acc: 98.0061, stage5_loss_bbox: 0.1055, stage5_loss_iou: 0.2201, stage5_loss_mask: 0.2306, loss: 6.5749\n",
      "2025-07-16 19:19:57,314 - mmdet - INFO - Epoch [51][250/750]\tlr: 2.500e-06, eta: 1:09:43, time: 0.388, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9469, stage0_pos_acc: 46.1637, stage0_loss_bbox: 0.4363, stage0_loss_iou: 0.8675, stage0_loss_mask: 0.6062, stage1_loss_cls: 0.4498, stage1_pos_acc: 80.3991, stage1_loss_bbox: 0.1940, stage1_loss_iou: 0.3510, stage1_loss_mask: 0.4220, stage2_loss_cls: 0.2734, stage2_pos_acc: 90.3791, stage2_loss_bbox: 0.1633, stage2_loss_iou: 0.2895, stage2_loss_mask: 0.3924, stage3_loss_cls: 0.1648, stage3_pos_acc: 95.2968, stage3_loss_bbox: 0.1257, stage3_loss_iou: 0.2722, stage3_loss_mask: 0.3706, stage4_loss_cls: 0.1210, stage4_pos_acc: 97.3500, stage4_loss_bbox: 0.1238, stage4_loss_iou: 0.2629, stage4_loss_mask: 0.3622, stage5_loss_cls: 0.1032, stage5_pos_acc: 98.2667, stage5_loss_bbox: 0.1190, stage5_loss_iou: 0.2505, stage5_loss_mask: 0.3586, loss: 8.0266\n",
      "2025-07-16 19:20:16,964 - mmdet - INFO - Epoch [51][300/750]\tlr: 2.500e-06, eta: 1:09:25, time: 0.393, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9277, stage0_pos_acc: 45.2241, stage0_loss_bbox: 0.3914, stage0_loss_iou: 0.8793, stage0_loss_mask: 0.4860, stage1_loss_cls: 0.4466, stage1_pos_acc: 80.8124, stage1_loss_bbox: 0.1501, stage1_loss_iou: 0.3278, stage1_loss_mask: 0.2779, stage2_loss_cls: 0.2919, stage2_pos_acc: 92.1436, stage2_loss_bbox: 0.1155, stage2_loss_iou: 0.2458, stage2_loss_mask: 0.2377, stage3_loss_cls: 0.1635, stage3_pos_acc: 95.5363, stage3_loss_bbox: 0.1088, stage3_loss_iou: 0.2241, stage3_loss_mask: 0.2320, stage4_loss_cls: 0.1063, stage4_pos_acc: 97.8079, stage4_loss_bbox: 0.1067, stage4_loss_iou: 0.2136, stage4_loss_mask: 0.2226, stage5_loss_cls: 0.0889, stage5_pos_acc: 98.2388, stage5_loss_bbox: 0.1011, stage5_loss_iou: 0.2071, stage5_loss_mask: 0.2096, loss: 6.7618\n",
      "2025-07-16 19:22:29,683 - mmdet - INFO - Epoch [51][650/750]\tlr: 2.500e-06, eta: 1:07:11, time: 0.372, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9853, stage0_pos_acc: 39.5867, stage0_loss_bbox: 0.4061, stage0_loss_iou: 0.9641, stage0_loss_mask: 0.7580, stage1_loss_cls: 0.4619, stage1_pos_acc: 84.0107, stage1_loss_bbox: 0.1421, stage1_loss_iou: 0.3964, stage1_loss_mask: 0.4187, stage2_loss_cls: 0.2958, stage2_pos_acc: 88.9569, stage2_loss_bbox: 0.1044, stage2_loss_iou: 0.2875, stage2_loss_mask: 0.3570, stage3_loss_cls: 0.1715, stage3_pos_acc: 93.9733, stage3_loss_bbox: 0.0945, stage3_loss_iou: 0.2575, stage3_loss_mask: 0.3555, stage4_loss_cls: 0.1099, stage4_pos_acc: 97.2257, stage4_loss_bbox: 0.0946, stage4_loss_iou: 0.2550, stage4_loss_mask: 0.3476, stage5_loss_cls: 0.0932, stage5_pos_acc: 98.6114, stage5_loss_bbox: 0.0925, stage5_loss_iou: 0.2491, stage5_loss_mask: 0.3457, loss: 8.0441\n",
      "2025-07-16 19:22:48,375 - mmdet - INFO - Epoch [51][700/750]\tlr: 2.500e-06, eta: 1:06:52, time: 0.374, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9343, stage0_pos_acc: 41.7653, stage0_loss_bbox: 0.3828, stage0_loss_iou: 0.7853, stage0_loss_mask: 0.5907, stage1_loss_cls: 0.4454, stage1_pos_acc: 80.3655, stage1_loss_bbox: 0.1570, stage1_loss_iou: 0.3227, stage1_loss_mask: 0.3475, stage2_loss_cls: 0.2798, stage2_pos_acc: 90.2002, stage2_loss_bbox: 0.1292, stage2_loss_iou: 0.2582, stage2_loss_mask: 0.3055, stage3_loss_cls: 0.1549, stage3_pos_acc: 95.4415, stage3_loss_bbox: 0.1170, stage3_loss_iou: 0.2373, stage3_loss_mask: 0.3010, stage4_loss_cls: 0.0916, stage4_pos_acc: 98.4144, stage4_loss_bbox: 0.1132, stage4_loss_iou: 0.2287, stage4_loss_mask: 0.3033, stage5_loss_cls: 0.0749, stage5_pos_acc: 98.7564, stage5_loss_bbox: 0.1105, stage5_loss_iou: 0.2242, stage5_loss_mask: 0.2839, loss: 7.1790\n",
      "2025-07-16 19:23:06,981 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:23:06,981 - mmdet - INFO - Epoch [51][750/750]\tlr: 2.500e-06, eta: 1:06:33, time: 0.372, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9550, stage0_pos_acc: 41.3107, stage0_loss_bbox: 0.3615, stage0_loss_iou: 0.8639, stage0_loss_mask: 0.5331, stage1_loss_cls: 0.4499, stage1_pos_acc: 77.7131, stage1_loss_bbox: 0.1224, stage1_loss_iou: 0.3065, stage1_loss_mask: 0.2545, stage2_loss_cls: 0.2642, stage2_pos_acc: 88.7185, stage2_loss_bbox: 0.0994, stage2_loss_iou: 0.2449, stage2_loss_mask: 0.2455, stage3_loss_cls: 0.1512, stage3_pos_acc: 93.5911, stage3_loss_bbox: 0.0893, stage3_loss_iou: 0.2236, stage3_loss_mask: 0.2346, stage4_loss_cls: 0.1040, stage4_pos_acc: 96.5958, stage4_loss_bbox: 0.0858, stage4_loss_iou: 0.2134, stage4_loss_mask: 0.2186, stage5_loss_cls: 0.0888, stage5_pos_acc: 97.8083, stage5_loss_bbox: 0.0850, stage5_loss_iou: 0.2096, stage5_loss_mask: 0.2288, loss: 6.6334\n",
      "2025-07-16 19:23:07,110 - mmdet - INFO - Saving checkpoint at 51 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 101s, ETA:     0s2025-07-16 19:26:26,219 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.24s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.29s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.126\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.399\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.170\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.443\n",
      "2025-07-16 19:26:28,262 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=2.02s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.197\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.235\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.441\n",
      "2025-07-16 19:26:31,406 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:26:31,407 - mmdet - INFO - Epoch(val) [51][750]\tbbox_mAP: 0.0270, bbox_mAP_50: 0.0500, bbox_mAP_75: 0.0270, bbox_mAP_s: 0.1260, bbox_mAP_m: 0.0250, bbox_mAP_l: 0.0340, bbox_mAP_copypaste: 0.027 0.050 0.027 0.126 0.025 0.034, segm_mAP: 0.0270, segm_mAP_50: 0.0490, segm_mAP_75: 0.0260, segm_mAP_s: 0.1210, segm_mAP_m: 0.0280, segm_mAP_l: 0.0350, segm_mAP_copypaste: 0.027 0.049 0.026 0.121 0.028 0.035\n",
      "2025-07-16 19:26:53,207 - mmdet - INFO - Epoch [52][50/750]\tlr: 2.500e-06, eta: 1:06:15, time: 0.436, data_time: 0.055, memory: 11264, stage0_loss_cls: 0.9243, stage0_pos_acc: 40.4750, stage0_loss_bbox: 0.3284, stage0_loss_iou: 0.7556, stage0_loss_mask: 0.3855, stage1_loss_cls: 0.4368, stage1_pos_acc: 83.8567, stage1_loss_bbox: 0.1371, stage1_loss_iou: 0.2717, stage1_loss_mask: 0.2575, stage2_loss_cls: 0.2842, stage2_pos_acc: 89.7398, stage2_loss_bbox: 0.1106, stage2_loss_iou: 0.2176, stage2_loss_mask: 0.2265, stage3_loss_cls: 0.1540, stage3_pos_acc: 95.5117, stage3_loss_bbox: 0.1003, stage3_loss_iou: 0.2037, stage3_loss_mask: 0.2266, stage4_loss_cls: 0.0982, stage4_pos_acc: 98.3167, stage4_loss_bbox: 0.0937, stage4_loss_iou: 0.1975, stage4_loss_mask: 0.2152, stage5_loss_cls: 0.0808, stage5_pos_acc: 97.8500, stage5_loss_bbox: 0.0912, stage5_loss_iou: 0.1898, stage5_loss_mask: 0.1953, loss: 6.1821\n",
      "2025-07-16 19:27:12,567 - mmdet - INFO - Epoch [52][100/750]\tlr: 2.500e-06, eta: 1:05:56, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9387, stage0_pos_acc: 38.8198, stage0_loss_bbox: 0.3696, stage0_loss_iou: 0.8566, stage0_loss_mask: 0.5576, stage1_loss_cls: 0.4612, stage1_pos_acc: 80.0571, stage1_loss_bbox: 0.1475, stage1_loss_iou: 0.3263, stage1_loss_mask: 0.3532, stage2_loss_cls: 0.2918, stage2_pos_acc: 88.6405, stage2_loss_bbox: 0.1161, stage2_loss_iou: 0.2550, stage2_loss_mask: 0.3363, stage3_loss_cls: 0.1575, stage3_pos_acc: 94.4405, stage3_loss_bbox: 0.1031, stage3_loss_iou: 0.2234, stage3_loss_mask: 0.3096, stage4_loss_cls: 0.0906, stage4_pos_acc: 98.0905, stage4_loss_bbox: 0.1012, stage4_loss_iou: 0.2169, stage4_loss_mask: 0.3027, stage5_loss_cls: 0.0714, stage5_pos_acc: 98.6333, stage5_loss_bbox: 0.0969, stage5_loss_iou: 0.2154, stage5_loss_mask: 0.3071, loss: 7.2059\n",
      "2025-07-16 19:27:32,052 - mmdet - INFO - Epoch [52][150/750]\tlr: 2.500e-06, eta: 1:05:38, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9669, stage0_pos_acc: 40.7444, stage0_loss_bbox: 0.3671, stage0_loss_iou: 0.8993, stage0_loss_mask: 0.4394, stage1_loss_cls: 0.4627, stage1_pos_acc: 82.5197, stage1_loss_bbox: 0.1284, stage1_loss_iou: 0.3103, stage1_loss_mask: 0.2489, stage2_loss_cls: 0.2678, stage2_pos_acc: 89.1188, stage2_loss_bbox: 0.1016, stage2_loss_iou: 0.2375, stage2_loss_mask: 0.2274, stage3_loss_cls: 0.1198, stage3_pos_acc: 97.9582, stage3_loss_bbox: 0.0975, stage3_loss_iou: 0.2160, stage3_loss_mask: 0.2003, stage4_loss_cls: 0.0668, stage4_pos_acc: 98.0879, stage4_loss_bbox: 0.0932, stage4_loss_iou: 0.2053, stage4_loss_mask: 0.2080, stage5_loss_cls: 0.0598, stage5_pos_acc: 98.7909, stage5_loss_bbox: 0.0916, stage5_loss_iou: 0.2002, stage5_loss_mask: 0.2145, loss: 6.4301\n",
      "2025-07-16 19:27:51,668 - mmdet - INFO - Epoch [52][200/750]\tlr: 2.500e-06, eta: 1:05:19, time: 0.392, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9331, stage0_pos_acc: 41.3484, stage0_loss_bbox: 0.3971, stage0_loss_iou: 0.8124, stage0_loss_mask: 0.4856, stage1_loss_cls: 0.4610, stage1_pos_acc: 84.4984, stage1_loss_bbox: 0.1572, stage1_loss_iou: 0.3171, stage1_loss_mask: 0.3055, stage2_loss_cls: 0.2921, stage2_pos_acc: 90.8714, stage2_loss_bbox: 0.1339, stage2_loss_iou: 0.2541, stage2_loss_mask: 0.2786, stage3_loss_cls: 0.1721, stage3_pos_acc: 93.6119, stage3_loss_bbox: 0.1230, stage3_loss_iou: 0.2294, stage3_loss_mask: 0.2704, stage4_loss_cls: 0.1023, stage4_pos_acc: 97.9770, stage4_loss_bbox: 0.1157, stage4_loss_iou: 0.2151, stage4_loss_mask: 0.2663, stage5_loss_cls: 0.0732, stage5_pos_acc: 98.7889, stage5_loss_bbox: 0.1122, stage5_loss_iou: 0.2095, stage5_loss_mask: 0.2653, loss: 6.9824\n",
      "2025-07-16 19:28:11,533 - mmdet - INFO - Epoch [52][250/750]\tlr: 2.500e-06, eta: 1:05:00, time: 0.397, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8907, stage0_pos_acc: 45.0624, stage0_loss_bbox: 0.4148, stage0_loss_iou: 0.8421, stage0_loss_mask: 0.5163, stage1_loss_cls: 0.4020, stage1_pos_acc: 85.3911, stage1_loss_bbox: 0.1537, stage1_loss_iou: 0.3238, stage1_loss_mask: 0.2583, stage2_loss_cls: 0.2364, stage2_pos_acc: 92.7137, stage2_loss_bbox: 0.1196, stage2_loss_iou: 0.2498, stage2_loss_mask: 0.2583, stage3_loss_cls: 0.1417, stage3_pos_acc: 96.4417, stage3_loss_bbox: 0.1083, stage3_loss_iou: 0.2220, stage3_loss_mask: 0.2371, stage4_loss_cls: 0.1049, stage4_pos_acc: 97.1253, stage4_loss_bbox: 0.1078, stage4_loss_iou: 0.2143, stage4_loss_mask: 0.2322, stage5_loss_cls: 0.0883, stage5_pos_acc: 98.4003, stage5_loss_bbox: 0.1061, stage5_loss_iou: 0.2109, stage5_loss_mask: 0.2386, loss: 6.6778\n",
      "2025-07-16 19:28:31,108 - mmdet - INFO - Epoch [52][300/750]\tlr: 2.500e-06, eta: 1:04:41, time: 0.391, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9532, stage0_pos_acc: 44.8433, stage0_loss_bbox: 0.3912, stage0_loss_iou: 0.7497, stage0_loss_mask: 0.4074, stage1_loss_cls: 0.4568, stage1_pos_acc: 77.9406, stage1_loss_bbox: 0.1639, stage1_loss_iou: 0.2992, stage1_loss_mask: 0.2537, stage2_loss_cls: 0.2923, stage2_pos_acc: 88.4919, stage2_loss_bbox: 0.1330, stage2_loss_iou: 0.2395, stage2_loss_mask: 0.2512, stage3_loss_cls: 0.1753, stage3_pos_acc: 94.2780, stage3_loss_bbox: 0.1205, stage3_loss_iou: 0.2166, stage3_loss_mask: 0.2320, stage4_loss_cls: 0.1165, stage4_pos_acc: 98.6375, stage4_loss_bbox: 0.1143, stage4_loss_iou: 0.2108, stage4_loss_mask: 0.2232, stage5_loss_cls: 0.0888, stage5_pos_acc: 98.5542, stage5_loss_bbox: 0.1119, stage5_loss_iou: 0.2081, stage5_loss_mask: 0.2233, loss: 6.6321\n",
      "2025-07-16 19:28:51,175 - mmdet - INFO - Epoch [52][350/750]\tlr: 2.500e-06, eta: 1:04:23, time: 0.401, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9195, stage0_pos_acc: 42.0753, stage0_loss_bbox: 0.3813, stage0_loss_iou: 0.7949, stage0_loss_mask: 0.4803, stage1_loss_cls: 0.4477, stage1_pos_acc: 83.5863, stage1_loss_bbox: 0.1467, stage1_loss_iou: 0.2911, stage1_loss_mask: 0.2831, stage2_loss_cls: 0.2697, stage2_pos_acc: 91.2925, stage2_loss_bbox: 0.1153, stage2_loss_iou: 0.2228, stage2_loss_mask: 0.2693, stage3_loss_cls: 0.1484, stage3_pos_acc: 95.1360, stage3_loss_bbox: 0.1089, stage3_loss_iou: 0.2060, stage3_loss_mask: 0.2595, stage4_loss_cls: 0.0922, stage4_pos_acc: 97.0651, stage4_loss_bbox: 0.0999, stage4_loss_iou: 0.1955, stage4_loss_mask: 0.2561, stage5_loss_cls: 0.0690, stage5_pos_acc: 97.9266, stage5_loss_bbox: 0.1031, stage5_loss_iou: 0.1940, stage5_loss_mask: 0.2590, loss: 6.6135\n",
      "2025-07-16 19:29:10,802 - mmdet - INFO - Epoch [52][400/750]\tlr: 2.500e-06, eta: 1:04:04, time: 0.393, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9313, stage0_pos_acc: 37.6933, stage0_loss_bbox: 0.3847, stage0_loss_iou: 0.9106, stage0_loss_mask: 0.4691, stage1_loss_cls: 0.4339, stage1_pos_acc: 82.9405, stage1_loss_bbox: 0.1258, stage1_loss_iou: 0.3202, stage1_loss_mask: 0.2621, stage2_loss_cls: 0.2750, stage2_pos_acc: 92.7087, stage2_loss_bbox: 0.0999, stage2_loss_iou: 0.2194, stage2_loss_mask: 0.2165, stage3_loss_cls: 0.1500, stage3_pos_acc: 95.2619, stage3_loss_bbox: 0.0902, stage3_loss_iou: 0.1998, stage3_loss_mask: 0.2052, stage4_loss_cls: 0.0860, stage4_pos_acc: 98.9500, stage4_loss_bbox: 0.0865, stage4_loss_iou: 0.1896, stage4_loss_mask: 0.1956, stage5_loss_cls: 0.0647, stage5_pos_acc: 99.2000, stage5_loss_bbox: 0.0845, stage5_loss_iou: 0.1875, stage5_loss_mask: 0.1943, loss: 6.3823\n",
      "2025-07-16 19:29:30,336 - mmdet - INFO - Epoch [52][450/750]\tlr: 2.500e-06, eta: 1:03:45, time: 0.391, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9480, stage0_pos_acc: 41.1429, stage0_loss_bbox: 0.3859, stage0_loss_iou: 0.8841, stage0_loss_mask: 0.4980, stage1_loss_cls: 0.4502, stage1_pos_acc: 83.1452, stage1_loss_bbox: 0.1412, stage1_loss_iou: 0.3217, stage1_loss_mask: 0.2471, stage2_loss_cls: 0.2664, stage2_pos_acc: 92.1292, stage2_loss_bbox: 0.1085, stage2_loss_iou: 0.2292, stage2_loss_mask: 0.2444, stage3_loss_cls: 0.1275, stage3_pos_acc: 97.9750, stage3_loss_bbox: 0.0941, stage3_loss_iou: 0.2060, stage3_loss_mask: 0.1898, stage4_loss_cls: 0.0766, stage4_pos_acc: 99.1417, stage4_loss_bbox: 0.0879, stage4_loss_iou: 0.1980, stage4_loss_mask: 0.1978, stage5_loss_cls: 0.0690, stage5_pos_acc: 99.4708, stage5_loss_bbox: 0.0862, stage5_loss_iou: 0.1951, stage5_loss_mask: 0.1924, loss: 6.4452\n",
      "2025-07-16 19:29:49,861 - mmdet - INFO - Epoch [52][500/750]\tlr: 2.500e-06, eta: 1:03:26, time: 0.390, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9360, stage0_pos_acc: 39.9888, stage0_loss_bbox: 0.3558, stage0_loss_iou: 0.7913, stage0_loss_mask: 0.5063, stage1_loss_cls: 0.4435, stage1_pos_acc: 83.5005, stage1_loss_bbox: 0.1369, stage1_loss_iou: 0.3317, stage1_loss_mask: 0.2862, stage2_loss_cls: 0.2668, stage2_pos_acc: 92.6981, stage2_loss_bbox: 0.1086, stage2_loss_iou: 0.2375, stage2_loss_mask: 0.2073, stage3_loss_cls: 0.1352, stage3_pos_acc: 96.7704, stage3_loss_bbox: 0.0986, stage3_loss_iou: 0.2159, stage3_loss_mask: 0.2057, stage4_loss_cls: 0.0805, stage4_pos_acc: 98.6325, stage4_loss_bbox: 0.0897, stage4_loss_iou: 0.2035, stage4_loss_mask: 0.1956, stage5_loss_cls: 0.0718, stage5_pos_acc: 98.7221, stage5_loss_bbox: 0.0863, stage5_loss_iou: 0.1950, stage5_loss_mask: 0.1930, loss: 6.3789\n",
      "2025-07-16 19:30:09,467 - mmdet - INFO - Epoch [52][550/750]\tlr: 2.500e-06, eta: 1:03:08, time: 0.392, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9326, stage0_pos_acc: 40.7452, stage0_loss_bbox: 0.4302, stage0_loss_iou: 0.8897, stage0_loss_mask: 0.7196, stage1_loss_cls: 0.4646, stage1_pos_acc: 78.6107, stage1_loss_bbox: 0.2019, stage1_loss_iou: 0.3945, stage1_loss_mask: 0.4241, stage2_loss_cls: 0.3064, stage2_pos_acc: 87.3833, stage2_loss_bbox: 0.1714, stage2_loss_iou: 0.2972, stage2_loss_mask: 0.3851, stage3_loss_cls: 0.1946, stage3_pos_acc: 92.7000, stage3_loss_bbox: 0.1313, stage3_loss_iou: 0.2734, stage3_loss_mask: 0.3705, stage4_loss_cls: 0.1392, stage4_pos_acc: 96.7500, stage4_loss_bbox: 0.1251, stage4_loss_iou: 0.2693, stage4_loss_mask: 0.3351, stage5_loss_cls: 0.1063, stage5_pos_acc: 98.1000, stage5_loss_bbox: 0.1208, stage5_loss_iou: 0.2640, stage5_loss_mask: 0.3417, loss: 8.2886\n",
      "2025-07-16 19:30:28,724 - mmdet - INFO - Epoch [52][600/750]\tlr: 2.500e-06, eta: 1:02:49, time: 0.385, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9492, stage0_pos_acc: 48.3064, stage0_loss_bbox: 0.3678, stage0_loss_iou: 0.8949, stage0_loss_mask: 0.5159, stage1_loss_cls: 0.4176, stage1_pos_acc: 85.5966, stage1_loss_bbox: 0.1273, stage1_loss_iou: 0.3080, stage1_loss_mask: 0.2581, stage2_loss_cls: 0.2530, stage2_pos_acc: 91.2422, stage2_loss_bbox: 0.0960, stage2_loss_iou: 0.2202, stage2_loss_mask: 0.2206, stage3_loss_cls: 0.1284, stage3_pos_acc: 96.6561, stage3_loss_bbox: 0.0861, stage3_loss_iou: 0.1973, stage3_loss_mask: 0.2078, stage4_loss_cls: 0.0777, stage4_pos_acc: 97.7816, stage4_loss_bbox: 0.0836, stage4_loss_iou: 0.1896, stage4_loss_mask: 0.1949, stage5_loss_cls: 0.0595, stage5_pos_acc: 99.2965, stage5_loss_bbox: 0.0813, stage5_loss_iou: 0.1839, stage5_loss_mask: 0.1951, loss: 6.3139\n",
      "2025-07-16 19:30:47,739 - mmdet - INFO - Epoch [52][650/750]\tlr: 2.500e-06, eta: 1:02:30, time: 0.380, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9607, stage0_pos_acc: 45.6143, stage0_loss_bbox: 0.3949, stage0_loss_iou: 0.9179, stage0_loss_mask: 0.6364, stage1_loss_cls: 0.4356, stage1_pos_acc: 86.0040, stage1_loss_bbox: 0.1370, stage1_loss_iou: 0.3546, stage1_loss_mask: 0.3912, stage2_loss_cls: 0.2643, stage2_pos_acc: 92.3492, stage2_loss_bbox: 0.1016, stage2_loss_iou: 0.2682, stage2_loss_mask: 0.3854, stage3_loss_cls: 0.1390, stage3_pos_acc: 97.4286, stage3_loss_bbox: 0.0956, stage3_loss_iou: 0.2447, stage3_loss_mask: 0.3546, stage4_loss_cls: 0.1046, stage4_pos_acc: 98.2905, stage4_loss_bbox: 0.0933, stage4_loss_iou: 0.2384, stage4_loss_mask: 0.3456, stage5_loss_cls: 0.0801, stage5_pos_acc: 98.8643, stage5_loss_bbox: 0.0906, stage5_loss_iou: 0.2341, stage5_loss_mask: 0.3400, loss: 7.6083\n",
      "2025-07-16 19:31:07,035 - mmdet - INFO - Epoch [52][700/750]\tlr: 2.500e-06, eta: 1:02:11, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9299, stage0_pos_acc: 43.2102, stage0_loss_bbox: 0.3955, stage0_loss_iou: 0.8657, stage0_loss_mask: 0.5651, stage1_loss_cls: 0.4594, stage1_pos_acc: 82.3693, stage1_loss_bbox: 0.1738, stage1_loss_iou: 0.3819, stage1_loss_mask: 0.3350, stage2_loss_cls: 0.2865, stage2_pos_acc: 92.1360, stage2_loss_bbox: 0.1455, stage2_loss_iou: 0.2986, stage2_loss_mask: 0.3103, stage3_loss_cls: 0.1726, stage3_pos_acc: 95.2830, stage3_loss_bbox: 0.1289, stage3_loss_iou: 0.2643, stage3_loss_mask: 0.2759, stage4_loss_cls: 0.1080, stage4_pos_acc: 98.1137, stage4_loss_bbox: 0.1252, stage4_loss_iou: 0.2474, stage4_loss_mask: 0.2671, stage5_loss_cls: 0.0898, stage5_pos_acc: 98.3867, stage5_loss_bbox: 0.1238, stage5_loss_iou: 0.2437, stage5_loss_mask: 0.2646, loss: 7.4584\n",
      "2025-07-16 19:31:26,607 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:31:26,607 - mmdet - INFO - Epoch [52][750/750]\tlr: 2.500e-06, eta: 1:01:52, time: 0.391, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9337, stage0_pos_acc: 43.1327, stage0_loss_bbox: 0.3540, stage0_loss_iou: 0.8458, stage0_loss_mask: 0.4818, stage1_loss_cls: 0.4168, stage1_pos_acc: 85.0842, stage1_loss_bbox: 0.1199, stage1_loss_iou: 0.2995, stage1_loss_mask: 0.2720, stage2_loss_cls: 0.2507, stage2_pos_acc: 91.7198, stage2_loss_bbox: 0.0996, stage2_loss_iou: 0.2257, stage2_loss_mask: 0.2583, stage3_loss_cls: 0.1183, stage3_pos_acc: 97.0660, stage3_loss_bbox: 0.0931, stage3_loss_iou: 0.1990, stage3_loss_mask: 0.2540, stage4_loss_cls: 0.0815, stage4_pos_acc: 97.5699, stage4_loss_bbox: 0.0891, stage4_loss_iou: 0.1924, stage4_loss_mask: 0.2462, stage5_loss_cls: 0.0588, stage5_pos_acc: 98.6875, stage5_loss_bbox: 0.0868, stage5_loss_iou: 0.1872, stage5_loss_mask: 0.2407, loss: 6.4051\n",
      "2025-07-16 19:31:26,741 - mmdet - INFO - Saving checkpoint at 52 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 101s, ETA:     0s2025-07-16 19:34:44,222 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.125\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.412\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.414\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.230\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.455\n",
      "2025-07-16 19:34:46,049 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.77s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.141\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.030\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.213\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.245\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.453\n",
      "2025-07-16 19:34:49,158 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:34:49,159 - mmdet - INFO - Epoch(val) [52][750]\tbbox_mAP: 0.0240, bbox_mAP_50: 0.0440, bbox_mAP_75: 0.0240, bbox_mAP_s: 0.1250, bbox_mAP_m: 0.0130, bbox_mAP_l: 0.0290, bbox_mAP_copypaste: 0.024 0.044 0.024 0.125 0.013 0.029, segm_mAP: 0.0240, segm_mAP_50: 0.0430, segm_mAP_75: 0.0230, segm_mAP_s: 0.1410, segm_mAP_m: 0.0130, segm_mAP_l: 0.0300, segm_mAP_copypaste: 0.024 0.043 0.023 0.141 0.013 0.030\n",
      "2025-07-16 19:35:11,085 - mmdet - INFO - Epoch [53][50/750]\tlr: 2.500e-06, eta: 1:01:34, time: 0.438, data_time: 0.055, memory: 11264, stage0_loss_cls: 0.9170, stage0_pos_acc: 41.2470, stage0_loss_bbox: 0.3519, stage0_loss_iou: 0.7490, stage0_loss_mask: 0.3245, stage1_loss_cls: 0.4063, stage1_pos_acc: 81.4080, stage1_loss_bbox: 0.1229, stage1_loss_iou: 0.2461, stage1_loss_mask: 0.1792, stage2_loss_cls: 0.2234, stage2_pos_acc: 91.7609, stage2_loss_bbox: 0.0924, stage2_loss_iou: 0.1724, stage2_loss_mask: 0.1665, stage3_loss_cls: 0.0934, stage3_pos_acc: 97.5876, stage3_loss_bbox: 0.0855, stage3_loss_iou: 0.1616, stage3_loss_mask: 0.1716, stage4_loss_cls: 0.0488, stage4_pos_acc: 97.7323, stage4_loss_bbox: 0.0805, stage4_loss_iou: 0.1502, stage4_loss_mask: 0.1603, stage5_loss_cls: 0.0353, stage5_pos_acc: 98.8090, stage5_loss_bbox: 0.0792, stage5_loss_iou: 0.1467, stage5_loss_mask: 0.1546, loss: 5.3192\n",
      "2025-07-16 19:35:30,769 - mmdet - INFO - Epoch [53][100/750]\tlr: 2.500e-06, eta: 1:01:15, time: 0.394, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9665, stage0_pos_acc: 41.4603, stage0_loss_bbox: 0.3927, stage0_loss_iou: 0.9301, stage0_loss_mask: 0.5966, stage1_loss_cls: 0.4657, stage1_pos_acc: 85.0071, stage1_loss_bbox: 0.1415, stage1_loss_iou: 0.3381, stage1_loss_mask: 0.2518, stage2_loss_cls: 0.2637, stage2_pos_acc: 92.8794, stage2_loss_bbox: 0.1111, stage2_loss_iou: 0.2440, stage2_loss_mask: 0.2470, stage3_loss_cls: 0.1274, stage3_pos_acc: 97.3397, stage3_loss_bbox: 0.1034, stage3_loss_iou: 0.2133, stage3_loss_mask: 0.1932, stage4_loss_cls: 0.0742, stage4_pos_acc: 98.8619, stage4_loss_bbox: 0.0999, stage4_loss_iou: 0.2016, stage4_loss_mask: 0.1907, stage5_loss_cls: 0.0543, stage5_pos_acc: 99.4333, stage5_loss_bbox: 0.0989, stage5_loss_iou: 0.1953, stage5_loss_mask: 0.1909, loss: 6.6916\n",
      "2025-07-16 19:35:50,864 - mmdet - INFO - Epoch [53][150/750]\tlr: 2.500e-06, eta: 1:00:57, time: 0.402, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9355, stage0_pos_acc: 48.0289, stage0_loss_bbox: 0.3469, stage0_loss_iou: 0.8653, stage0_loss_mask: 0.4886, stage1_loss_cls: 0.4346, stage1_pos_acc: 82.7793, stage1_loss_bbox: 0.1338, stage1_loss_iou: 0.3407, stage1_loss_mask: 0.3061, stage2_loss_cls: 0.2654, stage2_pos_acc: 91.7406, stage2_loss_bbox: 0.1054, stage2_loss_iou: 0.2532, stage2_loss_mask: 0.3044, stage3_loss_cls: 0.1354, stage3_pos_acc: 95.7313, stage3_loss_bbox: 0.0996, stage3_loss_iou: 0.2375, stage3_loss_mask: 0.2884, stage4_loss_cls: 0.0812, stage4_pos_acc: 98.4072, stage4_loss_bbox: 0.0947, stage4_loss_iou: 0.2350, stage4_loss_mask: 0.2860, stage5_loss_cls: 0.0694, stage5_pos_acc: 97.6147, stage5_loss_bbox: 0.0927, stage5_loss_iou: 0.2294, stage5_loss_mask: 0.2781, loss: 6.9074\n",
      "2025-07-16 19:36:10,358 - mmdet - INFO - Epoch [53][200/750]\tlr: 2.500e-06, eta: 1:00:38, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9311, stage0_pos_acc: 41.1853, stage0_loss_bbox: 0.3963, stage0_loss_iou: 0.8235, stage0_loss_mask: 0.4647, stage1_loss_cls: 0.4208, stage1_pos_acc: 83.4619, stage1_loss_bbox: 0.1542, stage1_loss_iou: 0.3365, stage1_loss_mask: 0.2708, stage2_loss_cls: 0.2841, stage2_pos_acc: 89.4377, stage2_loss_bbox: 0.1209, stage2_loss_iou: 0.2525, stage2_loss_mask: 0.2563, stage3_loss_cls: 0.1564, stage3_pos_acc: 96.0290, stage3_loss_bbox: 0.1066, stage3_loss_iou: 0.2277, stage3_loss_mask: 0.2303, stage4_loss_cls: 0.1005, stage4_pos_acc: 97.3837, stage4_loss_bbox: 0.1017, stage4_loss_iou: 0.2197, stage4_loss_mask: 0.2245, stage5_loss_cls: 0.0724, stage5_pos_acc: 98.8516, stage5_loss_bbox: 0.1020, stage5_loss_iou: 0.2194, stage5_loss_mask: 0.2187, loss: 6.6919\n",
      "2025-07-16 19:36:29,955 - mmdet - INFO - Epoch [53][250/750]\tlr: 2.500e-06, eta: 1:00:19, time: 0.392, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9136, stage0_pos_acc: 52.1462, stage0_loss_bbox: 0.3755, stage0_loss_iou: 0.8909, stage0_loss_mask: 0.6156, stage1_loss_cls: 0.4309, stage1_pos_acc: 83.2517, stage1_loss_bbox: 0.1512, stage1_loss_iou: 0.3561, stage1_loss_mask: 0.3468, stage2_loss_cls: 0.2732, stage2_pos_acc: 90.3321, stage2_loss_bbox: 0.1222, stage2_loss_iou: 0.2707, stage2_loss_mask: 0.3280, stage3_loss_cls: 0.1531, stage3_pos_acc: 96.1518, stage3_loss_bbox: 0.1128, stage3_loss_iou: 0.2510, stage3_loss_mask: 0.3076, stage4_loss_cls: 0.0901, stage4_pos_acc: 97.3398, stage4_loss_bbox: 0.1084, stage4_loss_iou: 0.2403, stage4_loss_mask: 0.3138, stage5_loss_cls: 0.0714, stage5_pos_acc: 99.1032, stage5_loss_bbox: 0.1063, stage5_loss_iou: 0.2332, stage5_loss_mask: 0.3080, loss: 7.3706\n",
      "2025-07-16 19:36:49,286 - mmdet - INFO - Epoch [53][300/750]\tlr: 2.500e-06, eta: 1:00:00, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9201, stage0_pos_acc: 44.5702, stage0_loss_bbox: 0.3846, stage0_loss_iou: 0.8367, stage0_loss_mask: 0.4543, stage1_loss_cls: 0.3984, stage1_pos_acc: 85.4658, stage1_loss_bbox: 0.1429, stage1_loss_iou: 0.3038, stage1_loss_mask: 0.2572, stage2_loss_cls: 0.2352, stage2_pos_acc: 92.5702, stage2_loss_bbox: 0.1129, stage2_loss_iou: 0.2209, stage2_loss_mask: 0.2155, stage3_loss_cls: 0.1164, stage3_pos_acc: 98.3972, stage3_loss_bbox: 0.1003, stage3_loss_iou: 0.1972, stage3_loss_mask: 0.1929, stage4_loss_cls: 0.0713, stage4_pos_acc: 99.4920, stage4_loss_bbox: 0.0955, stage4_loss_iou: 0.1890, stage4_loss_mask: 0.1914, stage5_loss_cls: 0.0587, stage5_pos_acc: 99.5829, stage5_loss_bbox: 0.0916, stage5_loss_iou: 0.1849, stage5_loss_mask: 0.1908, loss: 6.1625\n",
      "2025-07-16 19:37:08,275 - mmdet - INFO - Epoch [53][350/750]\tlr: 2.500e-06, eta: 0:59:41, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9261, stage0_pos_acc: 39.6956, stage0_loss_bbox: 0.3650, stage0_loss_iou: 0.8193, stage0_loss_mask: 0.4390, stage1_loss_cls: 0.4004, stage1_pos_acc: 86.7722, stage1_loss_bbox: 0.1395, stage1_loss_iou: 0.2948, stage1_loss_mask: 0.2720, stage2_loss_cls: 0.2363, stage2_pos_acc: 92.7503, stage2_loss_bbox: 0.1237, stage2_loss_iou: 0.2301, stage2_loss_mask: 0.2636, stage3_loss_cls: 0.1288, stage3_pos_acc: 96.7444, stage3_loss_bbox: 0.0982, stage3_loss_iou: 0.2077, stage3_loss_mask: 0.2405, stage4_loss_cls: 0.0958, stage4_pos_acc: 97.8333, stage4_loss_bbox: 0.1013, stage4_loss_iou: 0.2025, stage4_loss_mask: 0.2197, stage5_loss_cls: 0.0645, stage5_pos_acc: 99.0778, stage5_loss_bbox: 0.0920, stage5_loss_iou: 0.1968, stage5_loss_mask: 0.2207, loss: 6.3784\n",
      "2025-07-16 19:37:27,943 - mmdet - INFO - Epoch [53][400/750]\tlr: 2.500e-06, eta: 0:59:22, time: 0.393, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9496, stage0_pos_acc: 42.8692, stage0_loss_bbox: 0.3769, stage0_loss_iou: 0.8768, stage0_loss_mask: 0.5856, stage1_loss_cls: 0.4486, stage1_pos_acc: 77.4363, stage1_loss_bbox: 0.1541, stage1_loss_iou: 0.3469, stage1_loss_mask: 0.3594, stage2_loss_cls: 0.2753, stage2_pos_acc: 89.6785, stage2_loss_bbox: 0.1247, stage2_loss_iou: 0.2740, stage2_loss_mask: 0.3312, stage3_loss_cls: 0.1401, stage3_pos_acc: 93.9526, stage3_loss_bbox: 0.1120, stage3_loss_iou: 0.2463, stage3_loss_mask: 0.3037, stage4_loss_cls: 0.0874, stage4_pos_acc: 96.9052, stage4_loss_bbox: 0.1082, stage4_loss_iou: 0.2376, stage4_loss_mask: 0.2879, stage5_loss_cls: 0.0753, stage5_pos_acc: 97.2143, stage5_loss_bbox: 0.1063, stage5_loss_iou: 0.2326, stage5_loss_mask: 0.2976, loss: 7.3380\n",
      "2025-07-16 19:37:47,433 - mmdet - INFO - Epoch [53][450/750]\tlr: 2.500e-06, eta: 0:59:03, time: 0.390, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9281, stage0_pos_acc: 34.8954, stage0_loss_bbox: 0.3620, stage0_loss_iou: 0.7765, stage0_loss_mask: 0.4659, stage1_loss_cls: 0.4270, stage1_pos_acc: 79.6646, stage1_loss_bbox: 0.1414, stage1_loss_iou: 0.3005, stage1_loss_mask: 0.2949, stage2_loss_cls: 0.2481, stage2_pos_acc: 89.5354, stage2_loss_bbox: 0.1115, stage2_loss_iou: 0.2245, stage2_loss_mask: 0.2564, stage3_loss_cls: 0.1342, stage3_pos_acc: 94.9520, stage3_loss_bbox: 0.1032, stage3_loss_iou: 0.2066, stage3_loss_mask: 0.2568, stage4_loss_cls: 0.0894, stage4_pos_acc: 98.2999, stage4_loss_bbox: 0.0983, stage4_loss_iou: 0.1990, stage4_loss_mask: 0.2412, stage5_loss_cls: 0.0706, stage5_pos_acc: 97.9138, stage5_loss_bbox: 0.0976, stage5_loss_iou: 0.1952, stage5_loss_mask: 0.2410, loss: 6.4699\n",
      "2025-07-16 19:38:07,133 - mmdet - INFO - Epoch [53][500/750]\tlr: 2.500e-06, eta: 0:58:44, time: 0.394, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9420, stage0_pos_acc: 42.8468, stage0_loss_bbox: 0.3872, stage0_loss_iou: 0.8430, stage0_loss_mask: 0.5562, stage1_loss_cls: 0.4271, stage1_pos_acc: 80.7199, stage1_loss_bbox: 0.1427, stage1_loss_iou: 0.3311, stage1_loss_mask: 0.3211, stage2_loss_cls: 0.2720, stage2_pos_acc: 89.6887, stage2_loss_bbox: 0.1197, stage2_loss_iou: 0.2435, stage2_loss_mask: 0.2860, stage3_loss_cls: 0.1577, stage3_pos_acc: 94.8215, stage3_loss_bbox: 0.1124, stage3_loss_iou: 0.2254, stage3_loss_mask: 0.2676, stage4_loss_cls: 0.0914, stage4_pos_acc: 98.7313, stage4_loss_bbox: 0.1073, stage4_loss_iou: 0.2204, stage4_loss_mask: 0.2696, stage5_loss_cls: 0.0744, stage5_pos_acc: 99.5492, stage5_loss_bbox: 0.1028, stage5_loss_iou: 0.2136, stage5_loss_mask: 0.2664, loss: 6.9806\n",
      "2025-07-16 19:38:27,117 - mmdet - INFO - Epoch [53][550/750]\tlr: 2.500e-06, eta: 0:58:26, time: 0.400, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9564, stage0_pos_acc: 38.9627, stage0_loss_bbox: 0.3583, stage0_loss_iou: 0.7956, stage0_loss_mask: 0.4118, stage1_loss_cls: 0.4446, stage1_pos_acc: 83.7504, stage1_loss_bbox: 0.1273, stage1_loss_iou: 0.2812, stage1_loss_mask: 0.2523, stage2_loss_cls: 0.2738, stage2_pos_acc: 90.6861, stage2_loss_bbox: 0.1047, stage2_loss_iou: 0.2225, stage2_loss_mask: 0.2362, stage3_loss_cls: 0.1336, stage3_pos_acc: 96.6000, stage3_loss_bbox: 0.0963, stage3_loss_iou: 0.2006, stage3_loss_mask: 0.2097, stage4_loss_cls: 0.0710, stage4_pos_acc: 98.6921, stage4_loss_bbox: 0.0898, stage4_loss_iou: 0.1895, stage4_loss_mask: 0.2003, stage5_loss_cls: 0.0528, stage5_pos_acc: 99.7500, stage5_loss_bbox: 0.0863, stage5_loss_iou: 0.1856, stage5_loss_mask: 0.2059, loss: 6.1860\n",
      "2025-07-16 19:38:46,998 - mmdet - INFO - Epoch [53][600/750]\tlr: 2.500e-06, eta: 0:58:07, time: 0.398, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9160, stage0_pos_acc: 41.9574, stage0_loss_bbox: 0.3853, stage0_loss_iou: 0.8731, stage0_loss_mask: 0.5770, stage1_loss_cls: 0.4083, stage1_pos_acc: 82.2189, stage1_loss_bbox: 0.1430, stage1_loss_iou: 0.3530, stage1_loss_mask: 0.3527, stage2_loss_cls: 0.2585, stage2_pos_acc: 90.5843, stage2_loss_bbox: 0.1118, stage2_loss_iou: 0.2731, stage2_loss_mask: 0.3193, stage3_loss_cls: 0.1508, stage3_pos_acc: 95.5427, stage3_loss_bbox: 0.0978, stage3_loss_iou: 0.2327, stage3_loss_mask: 0.2875, stage4_loss_cls: 0.0843, stage4_pos_acc: 98.0851, stage4_loss_bbox: 0.0948, stage4_loss_iou: 0.2204, stage4_loss_mask: 0.2915, stage5_loss_cls: 0.0663, stage5_pos_acc: 99.0225, stage5_loss_bbox: 0.0918, stage5_loss_iou: 0.2136, stage5_loss_mask: 0.2883, loss: 7.0909\n",
      "2025-07-16 19:39:06,422 - mmdet - INFO - Epoch [53][650/750]\tlr: 2.500e-06, eta: 0:57:48, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9319, stage0_pos_acc: 41.8095, stage0_loss_bbox: 0.4302, stage0_loss_iou: 0.9183, stage0_loss_mask: 0.7409, stage1_loss_cls: 0.4376, stage1_pos_acc: 83.7667, stage1_loss_bbox: 0.1873, stage1_loss_iou: 0.3732, stage1_loss_mask: 0.4754, stage2_loss_cls: 0.2721, stage2_pos_acc: 89.3167, stage2_loss_bbox: 0.1681, stage2_loss_iou: 0.2904, stage2_loss_mask: 0.4578, stage3_loss_cls: 0.1623, stage3_pos_acc: 94.6333, stage3_loss_bbox: 0.1229, stage3_loss_iou: 0.2750, stage3_loss_mask: 0.4342, stage4_loss_cls: 0.1143, stage4_pos_acc: 97.4500, stage4_loss_bbox: 0.1159, stage4_loss_iou: 0.2616, stage4_loss_mask: 0.3909, stage5_loss_cls: 0.0868, stage5_pos_acc: 97.9667, stage5_loss_bbox: 0.1047, stage5_loss_iou: 0.2555, stage5_loss_mask: 0.3996, loss: 8.4070\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 19:43:05,009 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.105\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.408\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.144\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.197\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.454\n",
      "2025-07-16 19:43:06,854 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.016\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.036\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.406\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.410\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.170\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.210\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.454\n",
      "2025-07-16 19:43:10,012 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:43:10,014 - mmdet - INFO - Epoch(val) [53][750]\tbbox_mAP: 0.0280, bbox_mAP_50: 0.0520, bbox_mAP_75: 0.0280, bbox_mAP_s: 0.1050, bbox_mAP_m: 0.0150, bbox_mAP_l: 0.0350, bbox_mAP_copypaste: 0.028 0.052 0.028 0.105 0.015 0.035, segm_mAP: 0.0290, segm_mAP_50: 0.0520, segm_mAP_75: 0.0280, segm_mAP_s: 0.0600, segm_mAP_m: 0.0160, segm_mAP_l: 0.0360, segm_mAP_copypaste: 0.029 0.052 0.028 0.060 0.016 0.036\n",
      "2025-07-16 19:43:31,461 - mmdet - INFO - Epoch [54][50/750]\tlr: 2.500e-06, eta: 0:56:52, time: 0.429, data_time: 0.055, memory: 11264, stage0_loss_cls: 0.8695, stage0_pos_acc: 51.6000, stage0_loss_bbox: 0.3604, stage0_loss_iou: 0.7589, stage0_loss_mask: 0.5555, stage1_loss_cls: 0.3732, stage1_pos_acc: 83.9988, stage1_loss_bbox: 0.1724, stage1_loss_iou: 0.2950, stage1_loss_mask: 0.3130, stage2_loss_cls: 0.2272, stage2_pos_acc: 92.8060, stage2_loss_bbox: 0.1483, stage2_loss_iou: 0.2353, stage2_loss_mask: 0.3071, stage3_loss_cls: 0.1427, stage3_pos_acc: 95.6429, stage3_loss_bbox: 0.1122, stage3_loss_iou: 0.2131, stage3_loss_mask: 0.2622, stage4_loss_cls: 0.0905, stage4_pos_acc: 96.9325, stage4_loss_bbox: 0.1040, stage4_loss_iou: 0.2130, stage4_loss_mask: 0.2699, stage5_loss_cls: 0.0747, stage5_pos_acc: 97.5734, stage5_loss_bbox: 0.0996, stage5_loss_iou: 0.2068, stage5_loss_mask: 0.2912, loss: 6.6956\n",
      "2025-07-16 19:43:50,669 - mmdet - INFO - Epoch [54][100/750]\tlr: 2.500e-06, eta: 0:56:33, time: 0.384, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9861, stage0_pos_acc: 42.0303, stage0_loss_bbox: 0.3618, stage0_loss_iou: 0.9330, stage0_loss_mask: 0.5018, stage1_loss_cls: 0.4394, stage1_pos_acc: 81.5245, stage1_loss_bbox: 0.1352, stage1_loss_iou: 0.3642, stage1_loss_mask: 0.2905, stage2_loss_cls: 0.2494, stage2_pos_acc: 91.9476, stage2_loss_bbox: 0.1044, stage2_loss_iou: 0.2717, stage2_loss_mask: 0.2655, stage3_loss_cls: 0.1090, stage3_pos_acc: 97.6690, stage3_loss_bbox: 0.0977, stage3_loss_iou: 0.2467, stage3_loss_mask: 0.2772, stage4_loss_cls: 0.0731, stage4_pos_acc: 98.0571, stage4_loss_bbox: 0.0973, stage4_loss_iou: 0.2340, stage4_loss_mask: 0.2726, stage5_loss_cls: 0.0639, stage5_pos_acc: 98.3786, stage5_loss_bbox: 0.0938, stage5_loss_iou: 0.2278, stage5_loss_mask: 0.2660, loss: 6.9621\n",
      "2025-07-16 19:44:09,992 - mmdet - INFO - Epoch [54][150/750]\tlr: 2.500e-06, eta: 0:56:14, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9371, stage0_pos_acc: 43.3105, stage0_loss_bbox: 0.3590, stage0_loss_iou: 0.8150, stage0_loss_mask: 0.3446, stage1_loss_cls: 0.4108, stage1_pos_acc: 84.0193, stage1_loss_bbox: 0.1327, stage1_loss_iou: 0.2747, stage1_loss_mask: 0.2270, stage2_loss_cls: 0.2422, stage2_pos_acc: 94.0436, stage2_loss_bbox: 0.1053, stage2_loss_iou: 0.2056, stage2_loss_mask: 0.2069, stage3_loss_cls: 0.1276, stage3_pos_acc: 98.6626, stage3_loss_bbox: 0.0932, stage3_loss_iou: 0.1840, stage3_loss_mask: 0.1854, stage4_loss_cls: 0.0707, stage4_pos_acc: 99.7778, stage4_loss_bbox: 0.0915, stage4_loss_iou: 0.1800, stage4_loss_mask: 0.1882, stage5_loss_cls: 0.0551, stage5_pos_acc: 99.3778, stage5_loss_bbox: 0.0895, stage5_loss_iou: 0.1747, stage5_loss_mask: 0.1853, loss: 5.8861\n",
      "2025-07-16 19:44:29,327 - mmdet - INFO - Epoch [54][200/750]\tlr: 2.500e-06, eta: 0:55:55, time: 0.387, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9334, stage0_pos_acc: 42.3747, stage0_loss_bbox: 0.3788, stage0_loss_iou: 0.8281, stage0_loss_mask: 0.7018, stage1_loss_cls: 0.4207, stage1_pos_acc: 83.5296, stage1_loss_bbox: 0.1634, stage1_loss_iou: 0.3868, stage1_loss_mask: 0.4394, stage2_loss_cls: 0.2502, stage2_pos_acc: 91.2309, stage2_loss_bbox: 0.1362, stage2_loss_iou: 0.3047, stage2_loss_mask: 0.4021, stage3_loss_cls: 0.1342, stage3_pos_acc: 97.2497, stage3_loss_bbox: 0.1253, stage3_loss_iou: 0.2823, stage3_loss_mask: 0.3767, stage4_loss_cls: 0.0886, stage4_pos_acc: 98.6479, stage4_loss_bbox: 0.1193, stage4_loss_iou: 0.2703, stage4_loss_mask: 0.3677, stage5_loss_cls: 0.0714, stage5_pos_acc: 99.1183, stage5_loss_bbox: 0.1200, stage5_loss_iou: 0.2701, stage5_loss_mask: 0.3723, loss: 7.9438\n",
      "2025-07-16 19:44:48,573 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:44:48,573 - mmdet - INFO - Epoch [54][250/750]\tlr: 2.500e-06, eta: 0:55:36, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9022, stage0_pos_acc: 45.2045, stage0_loss_bbox: 0.3645, stage0_loss_iou: 0.8417, stage0_loss_mask: 0.4041, stage1_loss_cls: 0.4184, stage1_pos_acc: 81.7657, stage1_loss_bbox: 0.1382, stage1_loss_iou: 0.3164, stage1_loss_mask: 0.2587, stage2_loss_cls: 0.2264, stage2_pos_acc: 92.3331, stage2_loss_bbox: 0.1108, stage2_loss_iou: 0.2407, stage2_loss_mask: 0.2393, stage3_loss_cls: 0.1225, stage3_pos_acc: 95.6339, stage3_loss_bbox: 0.0987, stage3_loss_iou: 0.2152, stage3_loss_mask: 0.2247, stage4_loss_cls: 0.0729, stage4_pos_acc: 97.8353, stage4_loss_bbox: 0.0931, stage4_loss_iou: 0.2087, stage4_loss_mask: 0.2252, stage5_loss_cls: 0.0636, stage5_pos_acc: 98.5005, stage5_loss_bbox: 0.0902, stage5_loss_iou: 0.2055, stage5_loss_mask: 0.2145, loss: 6.2963\n",
      "2025-07-16 19:45:07,534 - mmdet - INFO - Epoch [54][300/750]\tlr: 2.500e-06, eta: 0:55:17, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9720, stage0_pos_acc: 41.7777, stage0_loss_bbox: 0.3843, stage0_loss_iou: 0.8661, stage0_loss_mask: 0.5898, stage1_loss_cls: 0.4387, stage1_pos_acc: 82.2519, stage1_loss_bbox: 0.1434, stage1_loss_iou: 0.3378, stage1_loss_mask: 0.2610, stage2_loss_cls: 0.2742, stage2_pos_acc: 91.5084, stage2_loss_bbox: 0.1128, stage2_loss_iou: 0.2487, stage2_loss_mask: 0.2324, stage3_loss_cls: 0.1568, stage3_pos_acc: 94.6435, stage3_loss_bbox: 0.1033, stage3_loss_iou: 0.2208, stage3_loss_mask: 0.2300, stage4_loss_cls: 0.1035, stage4_pos_acc: 96.8442, stage4_loss_bbox: 0.0992, stage4_loss_iou: 0.2105, stage4_loss_mask: 0.2255, stage5_loss_cls: 0.0755, stage5_pos_acc: 97.4260, stage5_loss_bbox: 0.0987, stage5_loss_iou: 0.2099, stage5_loss_mask: 0.2126, loss: 6.8078\n",
      "2025-07-16 19:45:26,885 - mmdet - INFO - Epoch [54][350/750]\tlr: 2.500e-06, eta: 0:54:58, time: 0.387, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9438, stage0_pos_acc: 40.5318, stage0_loss_bbox: 0.3981, stage0_loss_iou: 0.8824, stage0_loss_mask: 0.4583, stage1_loss_cls: 0.4342, stage1_pos_acc: 81.9261, stage1_loss_bbox: 0.1312, stage1_loss_iou: 0.3219, stage1_loss_mask: 0.2853, stage2_loss_cls: 0.2632, stage2_pos_acc: 92.5880, stage2_loss_bbox: 0.1001, stage2_loss_iou: 0.2380, stage2_loss_mask: 0.2581, stage3_loss_cls: 0.1178, stage3_pos_acc: 97.5647, stage3_loss_bbox: 0.0950, stage3_loss_iou: 0.2154, stage3_loss_mask: 0.2467, stage4_loss_cls: 0.0583, stage4_pos_acc: 99.4524, stage4_loss_bbox: 0.0903, stage4_loss_iou: 0.2080, stage4_loss_mask: 0.2376, stage5_loss_cls: 0.0461, stage5_pos_acc: 99.8571, stage5_loss_bbox: 0.0871, stage5_loss_iou: 0.2020, stage5_loss_mask: 0.2365, loss: 6.5555\n",
      "2025-07-16 19:45:45,718 - mmdet - INFO - Epoch [54][400/750]\tlr: 2.500e-06, eta: 0:54:39, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9711, stage0_pos_acc: 34.3722, stage0_loss_bbox: 0.3744, stage0_loss_iou: 0.8752, stage0_loss_mask: 0.5614, stage1_loss_cls: 0.4601, stage1_pos_acc: 81.4151, stage1_loss_bbox: 0.1350, stage1_loss_iou: 0.3265, stage1_loss_mask: 0.2597, stage2_loss_cls: 0.2664, stage2_pos_acc: 89.6716, stage2_loss_bbox: 0.1055, stage2_loss_iou: 0.2561, stage2_loss_mask: 0.2305, stage3_loss_cls: 0.1510, stage3_pos_acc: 94.6421, stage3_loss_bbox: 0.0916, stage3_loss_iou: 0.2130, stage3_loss_mask: 0.2280, stage4_loss_cls: 0.0939, stage4_pos_acc: 98.3935, stage4_loss_bbox: 0.0877, stage4_loss_iou: 0.2009, stage4_loss_mask: 0.2202, stage5_loss_cls: 0.0763, stage5_pos_acc: 99.4792, stage5_loss_bbox: 0.0856, stage5_loss_iou: 0.1965, stage5_loss_mask: 0.2109, loss: 6.6775\n",
      "2025-07-16 19:46:04,990 - mmdet - INFO - Epoch [54][450/750]\tlr: 2.500e-06, eta: 0:54:20, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9087, stage0_pos_acc: 44.4416, stage0_loss_bbox: 0.3769, stage0_loss_iou: 0.7759, stage0_loss_mask: 0.4900, stage1_loss_cls: 0.4268, stage1_pos_acc: 82.5922, stage1_loss_bbox: 0.1542, stage1_loss_iou: 0.2982, stage1_loss_mask: 0.2254, stage2_loss_cls: 0.2715, stage2_pos_acc: 89.7970, stage2_loss_bbox: 0.1206, stage2_loss_iou: 0.2266, stage2_loss_mask: 0.2117, stage3_loss_cls: 0.1486, stage3_pos_acc: 95.3697, stage3_loss_bbox: 0.1149, stage3_loss_iou: 0.2102, stage3_loss_mask: 0.1908, stage4_loss_cls: 0.0896, stage4_pos_acc: 98.3212, stage4_loss_bbox: 0.1136, stage4_loss_iou: 0.2031, stage4_loss_mask: 0.1904, stage5_loss_cls: 0.0729, stage5_pos_acc: 99.2045, stage5_loss_bbox: 0.1081, stage5_loss_iou: 0.1995, stage5_loss_mask: 0.1859, loss: 6.3142\n",
      "2025-07-16 19:46:24,206 - mmdet - INFO - Epoch [54][500/750]\tlr: 2.500e-06, eta: 0:54:01, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9224, stage0_pos_acc: 42.8553, stage0_loss_bbox: 0.3840, stage0_loss_iou: 0.7980, stage0_loss_mask: 0.4044, stage1_loss_cls: 0.4190, stage1_pos_acc: 80.2337, stage1_loss_bbox: 0.1559, stage1_loss_iou: 0.2912, stage1_loss_mask: 0.2558, stage2_loss_cls: 0.2561, stage2_pos_acc: 91.0464, stage2_loss_bbox: 0.1228, stage2_loss_iou: 0.2255, stage2_loss_mask: 0.2554, stage3_loss_cls: 0.1285, stage3_pos_acc: 97.9750, stage3_loss_bbox: 0.1019, stage3_loss_iou: 0.2023, stage3_loss_mask: 0.2186, stage4_loss_cls: 0.0755, stage4_pos_acc: 99.1810, stage4_loss_bbox: 0.1026, stage4_loss_iou: 0.1923, stage4_loss_mask: 0.2169, stage5_loss_cls: 0.0573, stage5_pos_acc: 99.0833, stage5_loss_bbox: 0.0959, stage5_loss_iou: 0.1885, stage5_loss_mask: 0.2065, loss: 6.2774\n",
      "2025-07-16 19:46:43,494 - mmdet - INFO - Epoch [54][550/750]\tlr: 2.500e-06, eta: 0:53:42, time: 0.386, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9124, stage0_pos_acc: 45.6970, stage0_loss_bbox: 0.3584, stage0_loss_iou: 0.8378, stage0_loss_mask: 0.5771, stage1_loss_cls: 0.4305, stage1_pos_acc: 85.1201, stage1_loss_bbox: 0.1463, stage1_loss_iou: 0.3339, stage1_loss_mask: 0.3337, stage2_loss_cls: 0.2894, stage2_pos_acc: 90.1765, stage2_loss_bbox: 0.1185, stage2_loss_iou: 0.2565, stage2_loss_mask: 0.2902, stage3_loss_cls: 0.1716, stage3_pos_acc: 96.2685, stage3_loss_bbox: 0.1099, stage3_loss_iou: 0.2291, stage3_loss_mask: 0.2829, stage4_loss_cls: 0.1036, stage4_pos_acc: 97.8321, stage4_loss_bbox: 0.1077, stage4_loss_iou: 0.2261, stage4_loss_mask: 0.2912, stage5_loss_cls: 0.0862, stage5_pos_acc: 98.4587, stage5_loss_bbox: 0.1029, stage5_loss_iou: 0.2199, stage5_loss_mask: 0.2947, loss: 7.1102\n",
      "2025-07-16 19:47:03,024 - mmdet - INFO - Epoch [54][600/750]\tlr: 2.500e-06, eta: 0:53:23, time: 0.391, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9285, stage0_pos_acc: 46.4331, stage0_loss_bbox: 0.3488, stage0_loss_iou: 0.7425, stage0_loss_mask: 0.4399, stage1_loss_cls: 0.4690, stage1_pos_acc: 78.3073, stage1_loss_bbox: 0.1504, stage1_loss_iou: 0.3051, stage1_loss_mask: 0.2445, stage2_loss_cls: 0.2833, stage2_pos_acc: 89.0696, stage2_loss_bbox: 0.1309, stage2_loss_iou: 0.2414, stage2_loss_mask: 0.2571, stage3_loss_cls: 0.1759, stage3_pos_acc: 94.3906, stage3_loss_bbox: 0.1209, stage3_loss_iou: 0.2264, stage3_loss_mask: 0.2358, stage4_loss_cls: 0.1144, stage4_pos_acc: 96.9533, stage4_loss_bbox: 0.1230, stage4_loss_iou: 0.2209, stage4_loss_mask: 0.2523, stage5_loss_cls: 0.1074, stage5_pos_acc: 97.5794, stage5_loss_bbox: 0.1182, stage5_loss_iou: 0.2128, stage5_loss_mask: 0.2537, loss: 6.7032\n",
      "2025-07-16 19:47:22,151 - mmdet - INFO - Epoch [54][650/750]\tlr: 2.500e-06, eta: 0:53:04, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9449, stage0_pos_acc: 47.0910, stage0_loss_bbox: 0.3874, stage0_loss_iou: 0.8888, stage0_loss_mask: 0.5800, stage1_loss_cls: 0.4751, stage1_pos_acc: 81.0562, stage1_loss_bbox: 0.1424, stage1_loss_iou: 0.3410, stage1_loss_mask: 0.4090, stage2_loss_cls: 0.3002, stage2_pos_acc: 91.7239, stage2_loss_bbox: 0.1128, stage2_loss_iou: 0.2668, stage2_loss_mask: 0.3863, stage3_loss_cls: 0.1667, stage3_pos_acc: 95.0863, stage3_loss_bbox: 0.1016, stage3_loss_iou: 0.2521, stage3_loss_mask: 0.3604, stage4_loss_cls: 0.1160, stage4_pos_acc: 98.3678, stage4_loss_bbox: 0.0957, stage4_loss_iou: 0.2390, stage4_loss_mask: 0.3571, stage5_loss_cls: 0.0982, stage5_pos_acc: 99.0058, stage5_loss_bbox: 0.0927, stage5_loss_iou: 0.2329, stage5_loss_mask: 0.3528, loss: 7.6996\n",
      "2025-07-16 19:47:41,135 - mmdet - INFO - Epoch [54][700/750]\tlr: 2.500e-06, eta: 0:52:45, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9422, stage0_pos_acc: 39.0048, stage0_loss_bbox: 0.3675, stage0_loss_iou: 0.8486, stage0_loss_mask: 0.5381, stage1_loss_cls: 0.4685, stage1_pos_acc: 79.2867, stage1_loss_bbox: 0.1443, stage1_loss_iou: 0.3294, stage1_loss_mask: 0.2780, stage2_loss_cls: 0.2963, stage2_pos_acc: 88.3797, stage2_loss_bbox: 0.1266, stage2_loss_iou: 0.2671, stage2_loss_mask: 0.2718, stage3_loss_cls: 0.1613, stage3_pos_acc: 92.7955, stage3_loss_bbox: 0.1140, stage3_loss_iou: 0.2402, stage3_loss_mask: 0.2537, stage4_loss_cls: 0.1003, stage4_pos_acc: 97.5879, stage4_loss_bbox: 0.1076, stage4_loss_iou: 0.2273, stage4_loss_mask: 0.2484, stage5_loss_cls: 0.0864, stage5_pos_acc: 98.9394, stage5_loss_bbox: 0.1022, stage5_loss_iou: 0.2198, stage5_loss_mask: 0.2453, loss: 6.9851\n",
      "2025-07-16 19:48:00,655 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:48:00,655 - mmdet - INFO - Epoch [54][750/750]\tlr: 2.500e-06, eta: 0:52:26, time: 0.390, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9510, stage0_pos_acc: 42.0129, stage0_loss_bbox: 0.3473, stage0_loss_iou: 0.8474, stage0_loss_mask: 0.3666, stage1_loss_cls: 0.4168, stage1_pos_acc: 83.4447, stage1_loss_bbox: 0.1268, stage1_loss_iou: 0.2765, stage1_loss_mask: 0.2019, stage2_loss_cls: 0.2365, stage2_pos_acc: 92.2447, stage2_loss_bbox: 0.0989, stage2_loss_iou: 0.1961, stage2_loss_mask: 0.1794, stage3_loss_cls: 0.0959, stage3_pos_acc: 98.0572, stage3_loss_bbox: 0.0930, stage3_loss_iou: 0.1803, stage3_loss_mask: 0.1816, stage4_loss_cls: 0.0548, stage4_pos_acc: 98.2072, stage4_loss_bbox: 0.0873, stage4_loss_iou: 0.1668, stage4_loss_mask: 0.1671, stage5_loss_cls: 0.0418, stage5_pos_acc: 99.1697, stage5_loss_bbox: 0.0856, stage5_loss_iou: 0.1641, stage5_loss_mask: 0.1703, loss: 5.7338\n",
      "2025-07-16 19:48:00,795 - mmdet - INFO - Saving checkpoint at 54 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 100s, ETA:     0s2025-07-16 19:51:18,684 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.31s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.124\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.412\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.415\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.170\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.229\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.457\n",
      "2025-07-16 19:51:20,741 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.81s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.071\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.413\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.416\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.217\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.241\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.455\n",
      "2025-07-16 19:51:23,929 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:51:23,929 - mmdet - INFO - Epoch(val) [54][750]\tbbox_mAP: 0.0250, bbox_mAP_50: 0.0470, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.1240, bbox_mAP_m: 0.0100, bbox_mAP_l: 0.0320, bbox_mAP_copypaste: 0.025 0.047 0.023 0.124 0.010 0.032, segm_mAP: 0.0250, segm_mAP_50: 0.0470, segm_mAP_75: 0.0220, segm_mAP_s: 0.0710, segm_mAP_m: 0.0090, segm_mAP_l: 0.0320, segm_mAP_copypaste: 0.025 0.047 0.022 0.071 0.009 0.032\n",
      "2025-07-16 19:51:45,494 - mmdet - INFO - Epoch [55][50/750]\tlr: 2.500e-06, eta: 0:52:08, time: 0.431, data_time: 0.056, memory: 11264, stage0_loss_cls: 0.9111, stage0_pos_acc: 36.1737, stage0_loss_bbox: 0.3119, stage0_loss_iou: 0.7864, stage0_loss_mask: 0.3551, stage1_loss_cls: 0.4073, stage1_pos_acc: 83.6931, stage1_loss_bbox: 0.1238, stage1_loss_iou: 0.2945, stage1_loss_mask: 0.2212, stage2_loss_cls: 0.2310, stage2_pos_acc: 93.7510, stage2_loss_bbox: 0.0985, stage2_loss_iou: 0.2251, stage2_loss_mask: 0.2073, stage3_loss_cls: 0.1277, stage3_pos_acc: 95.7949, stage3_loss_bbox: 0.0889, stage3_loss_iou: 0.2008, stage3_loss_mask: 0.1985, stage4_loss_cls: 0.0812, stage4_pos_acc: 97.7939, stage4_loss_bbox: 0.0876, stage4_loss_iou: 0.1922, stage4_loss_mask: 0.1976, stage5_loss_cls: 0.0695, stage5_pos_acc: 98.4516, stage5_loss_bbox: 0.0872, stage5_loss_iou: 0.1905, stage5_loss_mask: 0.1993, loss: 5.8941\n",
      "2025-07-16 19:52:04,645 - mmdet - INFO - Epoch [55][100/750]\tlr: 2.500e-06, eta: 0:51:49, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9609, stage0_pos_acc: 42.1726, stage0_loss_bbox: 0.3783, stage0_loss_iou: 0.8635, stage0_loss_mask: 0.5233, stage1_loss_cls: 0.4665, stage1_pos_acc: 84.0558, stage1_loss_bbox: 0.1517, stage1_loss_iou: 0.3386, stage1_loss_mask: 0.3344, stage2_loss_cls: 0.2762, stage2_pos_acc: 91.6335, stage2_loss_bbox: 0.1249, stage2_loss_iou: 0.2610, stage2_loss_mask: 0.3038, stage3_loss_cls: 0.1576, stage3_pos_acc: 95.9825, stage3_loss_bbox: 0.1118, stage3_loss_iou: 0.2390, stage3_loss_mask: 0.2882, stage4_loss_cls: 0.1188, stage4_pos_acc: 97.0842, stage4_loss_bbox: 0.1105, stage4_loss_iou: 0.2336, stage4_loss_mask: 0.2813, stage5_loss_cls: 0.0949, stage5_pos_acc: 97.8717, stage5_loss_bbox: 0.1065, stage5_loss_iou: 0.2251, stage5_loss_mask: 0.2764, loss: 7.2266\n",
      "2025-07-16 19:52:23,538 - mmdet - INFO - Epoch [55][150/750]\tlr: 2.500e-06, eta: 0:51:29, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9280, stage0_pos_acc: 41.2911, stage0_loss_bbox: 0.3624, stage0_loss_iou: 0.8037, stage0_loss_mask: 0.4286, stage1_loss_cls: 0.4477, stage1_pos_acc: 79.6698, stage1_loss_bbox: 0.1501, stage1_loss_iou: 0.2972, stage1_loss_mask: 0.2432, stage2_loss_cls: 0.2960, stage2_pos_acc: 88.4833, stage2_loss_bbox: 0.1195, stage2_loss_iou: 0.2269, stage2_loss_mask: 0.2285, stage3_loss_cls: 0.1482, stage3_pos_acc: 96.7595, stage3_loss_bbox: 0.1128, stage3_loss_iou: 0.2135, stage3_loss_mask: 0.2090, stage4_loss_cls: 0.0936, stage4_pos_acc: 98.3412, stage4_loss_bbox: 0.1092, stage4_loss_iou: 0.2066, stage4_loss_mask: 0.2105, stage5_loss_cls: 0.0865, stage5_pos_acc: 99.3452, stage5_loss_bbox: 0.1069, stage5_loss_iou: 0.2012, stage5_loss_mask: 0.2036, loss: 6.4334\n",
      "2025-07-16 19:52:42,760 - mmdet - INFO - Epoch [55][200/750]\tlr: 2.500e-06, eta: 0:51:10, time: 0.384, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9032, stage0_pos_acc: 44.3456, stage0_loss_bbox: 0.3786, stage0_loss_iou: 0.8565, stage0_loss_mask: 0.6324, stage1_loss_cls: 0.4446, stage1_pos_acc: 81.4794, stage1_loss_bbox: 0.1525, stage1_loss_iou: 0.3217, stage1_loss_mask: 0.3764, stage2_loss_cls: 0.2757, stage2_pos_acc: 91.4196, stage2_loss_bbox: 0.1176, stage2_loss_iou: 0.2491, stage2_loss_mask: 0.3893, stage3_loss_cls: 0.1489, stage3_pos_acc: 95.4649, stage3_loss_bbox: 0.1070, stage3_loss_iou: 0.2242, stage3_loss_mask: 0.3301, stage4_loss_cls: 0.0996, stage4_pos_acc: 96.3588, stage4_loss_bbox: 0.1006, stage4_loss_iou: 0.2122, stage4_loss_mask: 0.3226, stage5_loss_cls: 0.0840, stage5_pos_acc: 97.7578, stage5_loss_bbox: 0.0982, stage5_loss_iou: 0.2102, stage5_loss_mask: 0.3215, loss: 7.3565\n",
      "2025-07-16 19:53:02,018 - mmdet - INFO - Epoch [55][250/750]\tlr: 2.500e-06, eta: 0:50:51, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9432, stage0_pos_acc: 38.3944, stage0_loss_bbox: 0.3433, stage0_loss_iou: 0.7925, stage0_loss_mask: 0.3395, stage1_loss_cls: 0.4271, stage1_pos_acc: 79.9459, stage1_loss_bbox: 0.1319, stage1_loss_iou: 0.2863, stage1_loss_mask: 0.2356, stage2_loss_cls: 0.2480, stage2_pos_acc: 92.5913, stage2_loss_bbox: 0.1043, stage2_loss_iou: 0.2104, stage2_loss_mask: 0.2335, stage3_loss_cls: 0.1109, stage3_pos_acc: 97.2087, stage3_loss_bbox: 0.0961, stage3_loss_iou: 0.1955, stage3_loss_mask: 0.2043, stage4_loss_cls: 0.0622, stage4_pos_acc: 98.0312, stage4_loss_bbox: 0.0939, stage4_loss_iou: 0.1885, stage4_loss_mask: 0.2103, stage5_loss_cls: 0.0454, stage5_pos_acc: 98.8312, stage5_loss_bbox: 0.0925, stage5_loss_iou: 0.1856, stage5_loss_mask: 0.1992, loss: 5.9801\n",
      "2025-07-16 19:53:21,198 - mmdet - INFO - Epoch [55][300/750]\tlr: 2.500e-06, eta: 0:50:32, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8963, stage0_pos_acc: 48.6297, stage0_loss_bbox: 0.3822, stage0_loss_iou: 0.7446, stage0_loss_mask: 0.5151, stage1_loss_cls: 0.4117, stage1_pos_acc: 81.9402, stage1_loss_bbox: 0.1669, stage1_loss_iou: 0.3312, stage1_loss_mask: 0.3613, stage2_loss_cls: 0.2738, stage2_pos_acc: 88.8356, stage2_loss_bbox: 0.1358, stage2_loss_iou: 0.2554, stage2_loss_mask: 0.3410, stage3_loss_cls: 0.1526, stage3_pos_acc: 95.3674, stage3_loss_bbox: 0.1243, stage3_loss_iou: 0.2384, stage3_loss_mask: 0.3119, stage4_loss_cls: 0.0921, stage4_pos_acc: 98.5762, stage4_loss_bbox: 0.1218, stage4_loss_iou: 0.2296, stage4_loss_mask: 0.3153, stage5_loss_cls: 0.0743, stage5_pos_acc: 98.8500, stage5_loss_bbox: 0.1163, stage5_loss_iou: 0.2239, stage5_loss_mask: 0.3107, loss: 7.1264\n",
      "2025-07-16 19:53:40,368 - mmdet - INFO - Epoch [55][350/750]\tlr: 2.500e-06, eta: 0:50:13, time: 0.383, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9522, stage0_pos_acc: 44.8048, stage0_loss_bbox: 0.3531, stage0_loss_iou: 0.7749, stage0_loss_mask: 0.3674, stage1_loss_cls: 0.3989, stage1_pos_acc: 87.3849, stage1_loss_bbox: 0.1222, stage1_loss_iou: 0.2510, stage1_loss_mask: 0.1818, stage2_loss_cls: 0.2093, stage2_pos_acc: 93.5714, stage2_loss_bbox: 0.0997, stage2_loss_iou: 0.1986, stage2_loss_mask: 0.1769, stage3_loss_cls: 0.0927, stage3_pos_acc: 98.6571, stage3_loss_bbox: 0.0926, stage3_loss_iou: 0.1822, stage3_loss_mask: 0.1702, stage4_loss_cls: 0.0556, stage4_pos_acc: 99.6548, stage4_loss_bbox: 0.0852, stage4_loss_iou: 0.1725, stage4_loss_mask: 0.1652, stage5_loss_cls: 0.0401, stage5_pos_acc: 99.4643, stage5_loss_bbox: 0.0859, stage5_loss_iou: 0.1703, stage5_loss_mask: 0.1668, loss: 5.5653\n",
      "2025-07-16 19:53:59,759 - mmdet - INFO - Epoch [55][400/750]\tlr: 2.500e-06, eta: 0:49:54, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9483, stage0_pos_acc: 47.6261, stage0_loss_bbox: 0.4136, stage0_loss_iou: 0.9490, stage0_loss_mask: 0.5622, stage1_loss_cls: 0.4291, stage1_pos_acc: 80.3382, stage1_loss_bbox: 0.1537, stage1_loss_iou: 0.3599, stage1_loss_mask: 0.3828, stage2_loss_cls: 0.2659, stage2_pos_acc: 91.7163, stage2_loss_bbox: 0.1219, stage2_loss_iou: 0.2623, stage2_loss_mask: 0.3412, stage3_loss_cls: 0.1489, stage3_pos_acc: 96.1449, stage3_loss_bbox: 0.1148, stage3_loss_iou: 0.2411, stage3_loss_mask: 0.3403, stage4_loss_cls: 0.1119, stage4_pos_acc: 97.4211, stage4_loss_bbox: 0.1133, stage4_loss_iou: 0.2322, stage4_loss_mask: 0.3283, stage5_loss_cls: 0.0863, stage5_pos_acc: 98.4417, stage5_loss_bbox: 0.1126, stage5_loss_iou: 0.2265, stage5_loss_mask: 0.3215, loss: 7.5676\n",
      "2025-07-16 19:54:18,986 - mmdet - INFO - Epoch [55][450/750]\tlr: 2.500e-06, eta: 0:49:35, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9282, stage0_pos_acc: 44.8705, stage0_loss_bbox: 0.3802, stage0_loss_iou: 0.7639, stage0_loss_mask: 0.5436, stage1_loss_cls: 0.4478, stage1_pos_acc: 81.5133, stage1_loss_bbox: 0.1405, stage1_loss_iou: 0.3169, stage1_loss_mask: 0.2617, stage2_loss_cls: 0.2766, stage2_pos_acc: 89.8608, stage2_loss_bbox: 0.1212, stage2_loss_iou: 0.2542, stage2_loss_mask: 0.2419, stage3_loss_cls: 0.1618, stage3_pos_acc: 94.2035, stage3_loss_bbox: 0.1117, stage3_loss_iou: 0.2217, stage3_loss_mask: 0.2215, stage4_loss_cls: 0.0940, stage4_pos_acc: 97.3194, stage4_loss_bbox: 0.1056, stage4_loss_iou: 0.2127, stage4_loss_mask: 0.2114, stage5_loss_cls: 0.0750, stage5_pos_acc: 97.8899, stage5_loss_bbox: 0.1038, stage5_loss_iou: 0.2072, stage5_loss_mask: 0.2079, loss: 6.6112\n",
      "2025-07-16 19:54:38,175 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:54:38,175 - mmdet - INFO - Epoch [55][500/750]\tlr: 2.500e-06, eta: 0:49:16, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8852, stage0_pos_acc: 47.4857, stage0_loss_bbox: 0.3554, stage0_loss_iou: 0.7964, stage0_loss_mask: 0.3842, stage1_loss_cls: 0.4146, stage1_pos_acc: 79.4405, stage1_loss_bbox: 0.1229, stage1_loss_iou: 0.2681, stage1_loss_mask: 0.2240, stage2_loss_cls: 0.2537, stage2_pos_acc: 89.4849, stage2_loss_bbox: 0.0949, stage2_loss_iou: 0.1972, stage2_loss_mask: 0.1923, stage3_loss_cls: 0.1164, stage3_pos_acc: 96.3635, stage3_loss_bbox: 0.0933, stage3_loss_iou: 0.1837, stage3_loss_mask: 0.1788, stage4_loss_cls: 0.0630, stage4_pos_acc: 98.6159, stage4_loss_bbox: 0.0871, stage4_loss_iou: 0.1748, stage4_loss_mask: 0.1773, stage5_loss_cls: 0.0477, stage5_pos_acc: 99.3167, stage5_loss_bbox: 0.0822, stage5_loss_iou: 0.1688, stage5_loss_mask: 0.1756, loss: 5.7377\n",
      "2025-07-16 19:54:57,040 - mmdet - INFO - Epoch [55][550/750]\tlr: 2.500e-06, eta: 0:48:57, time: 0.377, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9312, stage0_pos_acc: 43.7397, stage0_loss_bbox: 0.3639, stage0_loss_iou: 0.7837, stage0_loss_mask: 0.4108, stage1_loss_cls: 0.4154, stage1_pos_acc: 83.6052, stage1_loss_bbox: 0.1543, stage1_loss_iou: 0.3019, stage1_loss_mask: 0.2332, stage2_loss_cls: 0.2756, stage2_pos_acc: 90.3115, stage2_loss_bbox: 0.1212, stage2_loss_iou: 0.2235, stage2_loss_mask: 0.1865, stage3_loss_cls: 0.1472, stage3_pos_acc: 96.3852, stage3_loss_bbox: 0.1038, stage3_loss_iou: 0.2086, stage3_loss_mask: 0.1624, stage4_loss_cls: 0.0892, stage4_pos_acc: 98.4833, stage4_loss_bbox: 0.1045, stage4_loss_iou: 0.2057, stage4_loss_mask: 0.1697, stage5_loss_cls: 0.0680, stage5_pos_acc: 99.2000, stage5_loss_bbox: 0.0965, stage5_loss_iou: 0.1964, stage5_loss_mask: 0.1672, loss: 6.1204\n",
      "2025-07-16 19:55:15,974 - mmdet - INFO - Epoch [55][600/750]\tlr: 2.500e-06, eta: 0:48:38, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9838, stage0_pos_acc: 40.1030, stage0_loss_bbox: 0.3891, stage0_loss_iou: 0.9822, stage0_loss_mask: 0.6863, stage1_loss_cls: 0.4337, stage1_pos_acc: 85.1142, stage1_loss_bbox: 0.1478, stage1_loss_iou: 0.3878, stage1_loss_mask: 0.4187, stage2_loss_cls: 0.2666, stage2_pos_acc: 91.0550, stage2_loss_bbox: 0.1174, stage2_loss_iou: 0.2971, stage2_loss_mask: 0.3791, stage3_loss_cls: 0.1447, stage3_pos_acc: 94.6721, stage3_loss_bbox: 0.1114, stage3_loss_iou: 0.2698, stage3_loss_mask: 0.3737, stage4_loss_cls: 0.1048, stage4_pos_acc: 96.5693, stage4_loss_bbox: 0.1026, stage4_loss_iou: 0.2546, stage4_loss_mask: 0.3495, stage5_loss_cls: 0.0896, stage5_pos_acc: 97.2210, stage5_loss_bbox: 0.1025, stage5_loss_iou: 0.2505, stage5_loss_mask: 0.3564, loss: 7.9996\n",
      "2025-07-16 19:55:35,317 - mmdet - INFO - Epoch [55][650/750]\tlr: 2.500e-06, eta: 0:48:19, time: 0.387, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9135, stage0_pos_acc: 47.0325, stage0_loss_bbox: 0.3780, stage0_loss_iou: 0.9199, stage0_loss_mask: 0.6168, stage1_loss_cls: 0.4006, stage1_pos_acc: 83.7397, stage1_loss_bbox: 0.1438, stage1_loss_iou: 0.3603, stage1_loss_mask: 0.3270, stage2_loss_cls: 0.2516, stage2_pos_acc: 92.7774, stage2_loss_bbox: 0.1111, stage2_loss_iou: 0.2633, stage2_loss_mask: 0.2981, stage3_loss_cls: 0.1338, stage3_pos_acc: 95.9702, stage3_loss_bbox: 0.1017, stage3_loss_iou: 0.2287, stage3_loss_mask: 0.2684, stage4_loss_cls: 0.0839, stage4_pos_acc: 99.0690, stage4_loss_bbox: 0.0952, stage4_loss_iou: 0.2196, stage4_loss_mask: 0.2737, stage5_loss_cls: 0.0738, stage5_pos_acc: 98.4667, stage5_loss_bbox: 0.0948, stage5_loss_iou: 0.2160, stage5_loss_mask: 0.2825, loss: 7.0561\n",
      "2025-07-16 19:55:54,621 - mmdet - INFO - Epoch [55][700/750]\tlr: 2.500e-06, eta: 0:48:00, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9563, stage0_pos_acc: 38.5976, stage0_loss_bbox: 0.3690, stage0_loss_iou: 0.8034, stage0_loss_mask: 0.4159, stage1_loss_cls: 0.4193, stage1_pos_acc: 80.0833, stage1_loss_bbox: 0.1290, stage1_loss_iou: 0.2922, stage1_loss_mask: 0.2123, stage2_loss_cls: 0.2445, stage2_pos_acc: 90.8333, stage2_loss_bbox: 0.0979, stage2_loss_iou: 0.2180, stage2_loss_mask: 0.2118, stage3_loss_cls: 0.1252, stage3_pos_acc: 98.3714, stage3_loss_bbox: 0.0897, stage3_loss_iou: 0.1955, stage3_loss_mask: 0.1998, stage4_loss_cls: 0.0616, stage4_pos_acc: 99.8571, stage4_loss_bbox: 0.0866, stage4_loss_iou: 0.1830, stage4_loss_mask: 0.1746, stage5_loss_cls: 0.0395, stage5_pos_acc: 99.8571, stage5_loss_bbox: 0.0858, stage5_loss_iou: 0.1852, stage5_loss_mask: 0.1970, loss: 5.9931\n",
      "2025-07-16 19:56:13,967 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:56:13,968 - mmdet - INFO - Epoch [55][750/750]\tlr: 2.500e-06, eta: 0:47:41, time: 0.387, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9379, stage0_pos_acc: 39.9505, stage0_loss_bbox: 0.3976, stage0_loss_iou: 0.8810, stage0_loss_mask: 0.5668, stage1_loss_cls: 0.4381, stage1_pos_acc: 82.7197, stage1_loss_bbox: 0.1619, stage1_loss_iou: 0.3370, stage1_loss_mask: 0.3486, stage2_loss_cls: 0.2639, stage2_pos_acc: 89.2515, stage2_loss_bbox: 0.1439, stage2_loss_iou: 0.2627, stage2_loss_mask: 0.3181, stage3_loss_cls: 0.1404, stage3_pos_acc: 97.8182, stage3_loss_bbox: 0.1107, stage3_loss_iou: 0.2339, stage3_loss_mask: 0.2791, stage4_loss_cls: 0.0824, stage4_pos_acc: 99.4182, stage4_loss_bbox: 0.1002, stage4_loss_iou: 0.2223, stage4_loss_mask: 0.3027, stage5_loss_cls: 0.0547, stage5_pos_acc: 99.6364, stage5_loss_bbox: 0.0918, stage5_loss_iou: 0.2231, stage5_loss_mask: 0.2907, loss: 7.1897\n",
      "2025-07-16 19:56:14,098 - mmdet - INFO - Saving checkpoint at 55 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 101s, ETA:     0s2025-07-16 19:59:33,065 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.062\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.398\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.211\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.444\n",
      "2025-07-16 19:59:34,885 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.071\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.030\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.443\n",
      "2025-07-16 19:59:38,018 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 19:59:38,018 - mmdet - INFO - Epoch(val) [55][750]\tbbox_mAP: 0.0240, bbox_mAP_50: 0.0450, bbox_mAP_75: 0.0240, bbox_mAP_s: 0.0620, bbox_mAP_m: 0.0170, bbox_mAP_l: 0.0290, bbox_mAP_copypaste: 0.024 0.045 0.024 0.062 0.017 0.029, segm_mAP: 0.0240, segm_mAP_50: 0.0440, segm_mAP_75: 0.0220, segm_mAP_s: 0.0710, segm_mAP_m: 0.0170, segm_mAP_l: 0.0300, segm_mAP_copypaste: 0.024 0.044 0.022 0.071 0.017 0.030\n",
      "2025-07-16 19:59:59,420 - mmdet - INFO - Epoch [56][50/750]\tlr: 2.500e-06, eta: 0:47:23, time: 0.428, data_time: 0.056, memory: 11264, stage0_loss_cls: 0.9294, stage0_pos_acc: 44.5097, stage0_loss_bbox: 0.3763, stage0_loss_iou: 0.8412, stage0_loss_mask: 0.4810, stage1_loss_cls: 0.4497, stage1_pos_acc: 84.1260, stage1_loss_bbox: 0.1297, stage1_loss_iou: 0.2961, stage1_loss_mask: 0.2334, stage2_loss_cls: 0.2786, stage2_pos_acc: 94.2777, stage2_loss_bbox: 0.0988, stage2_loss_iou: 0.2143, stage2_loss_mask: 0.2184, stage3_loss_cls: 0.1435, stage3_pos_acc: 95.3435, stage3_loss_bbox: 0.0939, stage3_loss_iou: 0.1965, stage3_loss_mask: 0.2021, stage4_loss_cls: 0.0890, stage4_pos_acc: 99.3896, stage4_loss_bbox: 0.0898, stage4_loss_iou: 0.1852, stage4_loss_mask: 0.2027, stage5_loss_cls: 0.0698, stage5_pos_acc: 99.3381, stage5_loss_bbox: 0.0874, stage5_loss_iou: 0.1759, stage5_loss_mask: 0.1911, loss: 6.2738\n",
      "2025-07-16 20:00:18,554 - mmdet - INFO - Epoch [56][100/750]\tlr: 2.500e-06, eta: 0:47:04, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9585, stage0_pos_acc: 42.1282, stage0_loss_bbox: 0.4173, stage0_loss_iou: 0.9311, stage0_loss_mask: 0.6909, stage1_loss_cls: 0.4563, stage1_pos_acc: 83.1359, stage1_loss_bbox: 0.1868, stage1_loss_iou: 0.3789, stage1_loss_mask: 0.3946, stage2_loss_cls: 0.2803, stage2_pos_acc: 90.2846, stage2_loss_bbox: 0.1536, stage2_loss_iou: 0.2642, stage2_loss_mask: 0.3184, stage3_loss_cls: 0.1507, stage3_pos_acc: 96.5295, stage3_loss_bbox: 0.1145, stage3_loss_iou: 0.2377, stage3_loss_mask: 0.2559, stage4_loss_cls: 0.1012, stage4_pos_acc: 96.3628, stage4_loss_bbox: 0.1052, stage4_loss_iou: 0.2275, stage4_loss_mask: 0.2558, stage5_loss_cls: 0.0822, stage5_pos_acc: 98.8333, stage5_loss_bbox: 0.0975, stage5_loss_iou: 0.2208, stage5_loss_mask: 0.2454, loss: 7.5253\n",
      "2025-07-16 20:00:37,646 - mmdet - INFO - Epoch [56][150/750]\tlr: 2.500e-06, eta: 0:46:44, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9077, stage0_pos_acc: 46.8589, stage0_loss_bbox: 0.3607, stage0_loss_iou: 0.8415, stage0_loss_mask: 0.4325, stage1_loss_cls: 0.4317, stage1_pos_acc: 82.5166, stage1_loss_bbox: 0.1249, stage1_loss_iou: 0.2884, stage1_loss_mask: 0.2119, stage2_loss_cls: 0.2515, stage2_pos_acc: 94.6730, stage2_loss_bbox: 0.0995, stage2_loss_iou: 0.2055, stage2_loss_mask: 0.1973, stage3_loss_cls: 0.1320, stage3_pos_acc: 96.3436, stage3_loss_bbox: 0.0861, stage3_loss_iou: 0.1833, stage3_loss_mask: 0.1745, stage4_loss_cls: 0.1005, stage4_pos_acc: 98.5658, stage4_loss_bbox: 0.0813, stage4_loss_iou: 0.1743, stage4_loss_mask: 0.1695, stage5_loss_cls: 0.0802, stage5_pos_acc: 99.0976, stage5_loss_bbox: 0.0801, stage5_loss_iou: 0.1731, stage5_loss_mask: 0.1756, loss: 5.9637\n",
      "2025-07-16 20:00:56,624 - mmdet - INFO - Epoch [56][200/750]\tlr: 2.500e-06, eta: 0:46:25, time: 0.380, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9563, stage0_pos_acc: 41.6380, stage0_loss_bbox: 0.4039, stage0_loss_iou: 0.9113, stage0_loss_mask: 0.3839, stage1_loss_cls: 0.4495, stage1_pos_acc: 80.0937, stage1_loss_bbox: 0.1435, stage1_loss_iou: 0.3083, stage1_loss_mask: 0.2271, stage2_loss_cls: 0.2681, stage2_pos_acc: 90.9066, stage2_loss_bbox: 0.1012, stage2_loss_iou: 0.2166, stage2_loss_mask: 0.1971, stage3_loss_cls: 0.1191, stage3_pos_acc: 96.3447, stage3_loss_bbox: 0.0997, stage3_loss_iou: 0.2002, stage3_loss_mask: 0.2106, stage4_loss_cls: 0.0775, stage4_pos_acc: 98.3333, stage4_loss_bbox: 0.0950, stage4_loss_iou: 0.1916, stage4_loss_mask: 0.2011, stage5_loss_cls: 0.0549, stage5_pos_acc: 99.2333, stage5_loss_bbox: 0.0919, stage5_loss_iou: 0.1871, stage5_loss_mask: 0.1871, loss: 6.2825\n",
      "2025-07-16 20:01:15,781 - mmdet - INFO - Epoch [56][250/750]\tlr: 2.500e-06, eta: 0:46:06, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8660, stage0_pos_acc: 42.6885, stage0_loss_bbox: 0.3317, stage0_loss_iou: 0.7853, stage0_loss_mask: 0.6349, stage1_loss_cls: 0.4052, stage1_pos_acc: 85.3592, stage1_loss_bbox: 0.1519, stage1_loss_iou: 0.3526, stage1_loss_mask: 0.4458, stage2_loss_cls: 0.2761, stage2_pos_acc: 93.2307, stage2_loss_bbox: 0.1258, stage2_loss_iou: 0.2873, stage2_loss_mask: 0.4174, stage3_loss_cls: 0.1604, stage3_pos_acc: 96.5910, stage3_loss_bbox: 0.1166, stage3_loss_iou: 0.2689, stage3_loss_mask: 0.3902, stage4_loss_cls: 0.1051, stage4_pos_acc: 97.4496, stage4_loss_bbox: 0.1147, stage4_loss_iou: 0.2603, stage4_loss_mask: 0.3939, stage5_loss_cls: 0.0898, stage5_pos_acc: 97.8622, stage5_loss_bbox: 0.1127, stage5_loss_iou: 0.2555, stage5_loss_mask: 0.3929, loss: 7.7410\n",
      "2025-07-16 20:01:34,793 - mmdet - INFO - Epoch [56][300/750]\tlr: 2.500e-06, eta: 0:45:47, time: 0.380, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9049, stage0_pos_acc: 43.4092, stage0_loss_bbox: 0.3732, stage0_loss_iou: 0.8623, stage0_loss_mask: 0.7457, stage1_loss_cls: 0.4112, stage1_pos_acc: 84.5445, stage1_loss_bbox: 0.1619, stage1_loss_iou: 0.3790, stage1_loss_mask: 0.4661, stage2_loss_cls: 0.2539, stage2_pos_acc: 91.2374, stage2_loss_bbox: 0.1326, stage2_loss_iou: 0.3124, stage2_loss_mask: 0.4725, stage3_loss_cls: 0.1320, stage3_pos_acc: 96.7496, stage3_loss_bbox: 0.1248, stage3_loss_iou: 0.2888, stage3_loss_mask: 0.4506, stage4_loss_cls: 0.0826, stage4_pos_acc: 97.7357, stage4_loss_bbox: 0.1259, stage4_loss_iou: 0.2807, stage4_loss_mask: 0.4451, stage5_loss_cls: 0.0658, stage5_pos_acc: 97.7175, stage5_loss_bbox: 0.1241, stage5_loss_iou: 0.2759, stage5_loss_mask: 0.4387, loss: 8.3106\n",
      "2025-07-16 20:01:53,603 - mmdet - INFO - Epoch [56][350/750]\tlr: 2.500e-06, eta: 0:45:28, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9245, stage0_pos_acc: 42.4330, stage0_loss_bbox: 0.3621, stage0_loss_iou: 0.7845, stage0_loss_mask: 0.3508, stage1_loss_cls: 0.3980, stage1_pos_acc: 83.7483, stage1_loss_bbox: 0.1255, stage1_loss_iou: 0.2553, stage1_loss_mask: 0.1605, stage2_loss_cls: 0.2340, stage2_pos_acc: 90.6302, stage2_loss_bbox: 0.0961, stage2_loss_iou: 0.1794, stage2_loss_mask: 0.1387, stage3_loss_cls: 0.1334, stage3_pos_acc: 95.4683, stage3_loss_bbox: 0.0810, stage3_loss_iou: 0.1583, stage3_loss_mask: 0.1275, stage4_loss_cls: 0.0749, stage4_pos_acc: 97.0611, stage4_loss_bbox: 0.0827, stage4_loss_iou: 0.1566, stage4_loss_mask: 0.1305, stage5_loss_cls: 0.0639, stage5_pos_acc: 98.2833, stage5_loss_bbox: 0.0798, stage5_loss_iou: 0.1520, stage5_loss_mask: 0.1301, loss: 5.3801\n",
      "2025-07-16 20:02:12,998 - mmdet - INFO - Epoch [56][400/750]\tlr: 2.500e-06, eta: 0:45:09, time: 0.388, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9106, stage0_pos_acc: 45.8227, stage0_loss_bbox: 0.3715, stage0_loss_iou: 0.7368, stage0_loss_mask: 0.4143, stage1_loss_cls: 0.4144, stage1_pos_acc: 88.2636, stage1_loss_bbox: 0.1398, stage1_loss_iou: 0.2803, stage1_loss_mask: 0.2206, stage2_loss_cls: 0.2459, stage2_pos_acc: 91.7457, stage2_loss_bbox: 0.1224, stage2_loss_iou: 0.2300, stage2_loss_mask: 0.2103, stage3_loss_cls: 0.1429, stage3_pos_acc: 96.5063, stage3_loss_bbox: 0.1133, stage3_loss_iou: 0.2072, stage3_loss_mask: 0.1892, stage4_loss_cls: 0.0819, stage4_pos_acc: 98.6994, stage4_loss_bbox: 0.1101, stage4_loss_iou: 0.1982, stage4_loss_mask: 0.1847, stage5_loss_cls: 0.0716, stage5_pos_acc: 98.8161, stage5_loss_bbox: 0.1082, stage5_loss_iou: 0.1948, stage5_loss_mask: 0.1833, loss: 6.0821\n",
      "2025-07-16 20:02:32,060 - mmdet - INFO - Epoch [56][450/750]\tlr: 2.500e-06, eta: 0:44:50, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9448, stage0_pos_acc: 40.2563, stage0_loss_bbox: 0.3252, stage0_loss_iou: 0.7827, stage0_loss_mask: 0.4752, stage1_loss_cls: 0.4056, stage1_pos_acc: 84.7381, stage1_loss_bbox: 0.1369, stage1_loss_iou: 0.3225, stage1_loss_mask: 0.3389, stage2_loss_cls: 0.2494, stage2_pos_acc: 93.2056, stage2_loss_bbox: 0.1169, stage2_loss_iou: 0.2597, stage2_loss_mask: 0.3106, stage3_loss_cls: 0.1320, stage3_pos_acc: 96.4413, stage3_loss_bbox: 0.1037, stage3_loss_iou: 0.2380, stage3_loss_mask: 0.3012, stage4_loss_cls: 0.0834, stage4_pos_acc: 98.6000, stage4_loss_bbox: 0.0999, stage4_loss_iou: 0.2319, stage4_loss_mask: 0.2868, stage5_loss_cls: 0.0686, stage5_pos_acc: 98.7643, stage5_loss_bbox: 0.0953, stage5_loss_iou: 0.2290, stage5_loss_mask: 0.2797, loss: 6.8180\n",
      "2025-07-16 20:02:50,786 - mmdet - INFO - Epoch [56][500/750]\tlr: 2.500e-06, eta: 0:44:31, time: 0.375, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9362, stage0_pos_acc: 43.6712, stage0_loss_bbox: 0.3840, stage0_loss_iou: 0.8373, stage0_loss_mask: 0.5335, stage1_loss_cls: 0.4256, stage1_pos_acc: 84.2403, stage1_loss_bbox: 0.1592, stage1_loss_iou: 0.3224, stage1_loss_mask: 0.3552, stage2_loss_cls: 0.2572, stage2_pos_acc: 92.1387, stage2_loss_bbox: 0.1265, stage2_loss_iou: 0.2420, stage2_loss_mask: 0.2935, stage3_loss_cls: 0.1239, stage3_pos_acc: 95.8506, stage3_loss_bbox: 0.1081, stage3_loss_iou: 0.2208, stage3_loss_mask: 0.2578, stage4_loss_cls: 0.0760, stage4_pos_acc: 98.5015, stage4_loss_bbox: 0.1032, stage4_loss_iou: 0.2151, stage4_loss_mask: 0.2563, stage5_loss_cls: 0.0517, stage5_pos_acc: 99.0182, stage5_loss_bbox: 0.0998, stage5_loss_iou: 0.2097, stage5_loss_mask: 0.2521, loss: 6.8469\n",
      "2025-07-16 20:03:09,936 - mmdet - INFO - Epoch [56][550/750]\tlr: 2.500e-06, eta: 0:44:12, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9281, stage0_pos_acc: 43.7246, stage0_loss_bbox: 0.3579, stage0_loss_iou: 0.8816, stage0_loss_mask: 0.5157, stage1_loss_cls: 0.4195, stage1_pos_acc: 82.6356, stage1_loss_bbox: 0.1293, stage1_loss_iou: 0.3105, stage1_loss_mask: 0.3013, stage2_loss_cls: 0.2511, stage2_pos_acc: 90.0095, stage2_loss_bbox: 0.1003, stage2_loss_iou: 0.2266, stage2_loss_mask: 0.2658, stage3_loss_cls: 0.1277, stage3_pos_acc: 96.2840, stage3_loss_bbox: 0.0932, stage3_loss_iou: 0.2021, stage3_loss_mask: 0.2503, stage4_loss_cls: 0.0780, stage4_pos_acc: 97.9590, stage4_loss_bbox: 0.0894, stage4_loss_iou: 0.1934, stage4_loss_mask: 0.2361, stage5_loss_cls: 0.0636, stage5_pos_acc: 99.2408, stage5_loss_bbox: 0.0832, stage5_loss_iou: 0.1893, stage5_loss_mask: 0.2286, loss: 6.5228\n",
      "2025-07-16 20:03:29,070 - mmdet - INFO - Epoch [56][600/750]\tlr: 2.500e-06, eta: 0:43:53, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9393, stage0_pos_acc: 40.7617, stage0_loss_bbox: 0.3784, stage0_loss_iou: 0.8446, stage0_loss_mask: 0.6078, stage1_loss_cls: 0.4170, stage1_pos_acc: 84.3170, stage1_loss_bbox: 0.1521, stage1_loss_iou: 0.3502, stage1_loss_mask: 0.3537, stage2_loss_cls: 0.2438, stage2_pos_acc: 92.7251, stage2_loss_bbox: 0.1150, stage2_loss_iou: 0.2548, stage2_loss_mask: 0.2921, stage3_loss_cls: 0.1438, stage3_pos_acc: 95.3584, stage3_loss_bbox: 0.1005, stage3_loss_iou: 0.2320, stage3_loss_mask: 0.2847, stage4_loss_cls: 0.1048, stage4_pos_acc: 97.3295, stage4_loss_bbox: 0.0973, stage4_loss_iou: 0.2222, stage4_loss_mask: 0.2747, stage5_loss_cls: 0.0893, stage5_pos_acc: 97.6319, stage5_loss_bbox: 0.0970, stage5_loss_iou: 0.2176, stage5_loss_mask: 0.2793, loss: 7.0919\n",
      "2025-07-16 20:03:48,126 - mmdet - INFO - Epoch [56][650/750]\tlr: 2.500e-06, eta: 0:43:34, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9494, stage0_pos_acc: 42.9909, stage0_loss_bbox: 0.3385, stage0_loss_iou: 0.8289, stage0_loss_mask: 0.4126, stage1_loss_cls: 0.4170, stage1_pos_acc: 83.0504, stage1_loss_bbox: 0.1158, stage1_loss_iou: 0.2698, stage1_loss_mask: 0.2144, stage2_loss_cls: 0.2535, stage2_pos_acc: 92.5601, stage2_loss_bbox: 0.0856, stage2_loss_iou: 0.1979, stage2_loss_mask: 0.1660, stage3_loss_cls: 0.1359, stage3_pos_acc: 96.5384, stage3_loss_bbox: 0.0743, stage3_loss_iou: 0.1726, stage3_loss_mask: 0.1402, stage4_loss_cls: 0.0710, stage4_pos_acc: 97.3810, stage4_loss_bbox: 0.0721, stage4_loss_iou: 0.1667, stage4_loss_mask: 0.1414, stage5_loss_cls: 0.0472, stage5_pos_acc: 99.6000, stage5_loss_bbox: 0.0690, stage5_loss_iou: 0.1590, stage5_loss_mask: 0.1339, loss: 5.6324\n",
      "2025-07-16 20:04:07,446 - mmdet - INFO - Epoch [56][700/750]\tlr: 2.500e-06, eta: 0:43:15, time: 0.386, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9099, stage0_pos_acc: 47.1327, stage0_loss_bbox: 0.3912, stage0_loss_iou: 0.8031, stage0_loss_mask: 0.5019, stage1_loss_cls: 0.4052, stage1_pos_acc: 84.3631, stage1_loss_bbox: 0.1526, stage1_loss_iou: 0.3147, stage1_loss_mask: 0.2983, stage2_loss_cls: 0.2483, stage2_pos_acc: 93.6239, stage2_loss_bbox: 0.1212, stage2_loss_iou: 0.2377, stage2_loss_mask: 0.2331, stage3_loss_cls: 0.1329, stage3_pos_acc: 96.1338, stage3_loss_bbox: 0.1136, stage3_loss_iou: 0.2174, stage3_loss_mask: 0.2188, stage4_loss_cls: 0.0834, stage4_pos_acc: 97.6760, stage4_loss_bbox: 0.1075, stage4_loss_iou: 0.2037, stage4_loss_mask: 0.2177, stage5_loss_cls: 0.0647, stage5_pos_acc: 99.1609, stage5_loss_bbox: 0.1047, stage5_loss_iou: 0.1995, stage5_loss_mask: 0.2174, loss: 6.4984\n",
      "2025-07-16 20:04:26,419 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:04:26,419 - mmdet - INFO - Epoch [56][750/750]\tlr: 2.500e-06, eta: 0:42:55, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9451, stage0_pos_acc: 40.7965, stage0_loss_bbox: 0.3801, stage0_loss_iou: 0.8442, stage0_loss_mask: 0.4465, stage1_loss_cls: 0.4164, stage1_pos_acc: 82.7178, stage1_loss_bbox: 0.1548, stage1_loss_iou: 0.3601, stage1_loss_mask: 0.3453, stage2_loss_cls: 0.2559, stage2_pos_acc: 92.9375, stage2_loss_bbox: 0.1416, stage2_loss_iou: 0.2688, stage2_loss_mask: 0.3168, stage3_loss_cls: 0.1217, stage3_pos_acc: 97.2421, stage3_loss_bbox: 0.1300, stage3_loss_iou: 0.2443, stage3_loss_mask: 0.2966, stage4_loss_cls: 0.0802, stage4_pos_acc: 98.6833, stage4_loss_bbox: 0.1228, stage4_loss_iou: 0.2342, stage4_loss_mask: 0.2897, stage5_loss_cls: 0.0622, stage5_pos_acc: 99.3333, stage5_loss_bbox: 0.1199, stage5_loss_iou: 0.2288, stage5_loss_mask: 0.2657, loss: 7.0720\n",
      "2025-07-16 20:04:26,568 - mmdet - INFO - Saving checkpoint at 56 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 20:07:46,317 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.28s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.122\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.015\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.409\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.409\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.222\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.450\n",
      "2025-07-16 20:07:48,145 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.042\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.021\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.140\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.029\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.406\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.406\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.440\n",
      "2025-07-16 20:07:51,280 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:07:51,280 - mmdet - INFO - Epoch(val) [56][750]\tbbox_mAP: 0.0230, bbox_mAP_50: 0.0430, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.1220, bbox_mAP_m: 0.0150, bbox_mAP_l: 0.0290, bbox_mAP_copypaste: 0.023 0.043 0.023 0.122 0.015 0.029, segm_mAP: 0.0220, segm_mAP_50: 0.0420, segm_mAP_75: 0.0210, segm_mAP_s: 0.1400, segm_mAP_m: 0.0140, segm_mAP_l: 0.0290, segm_mAP_copypaste: 0.022 0.042 0.021 0.140 0.014 0.029\n",
      "2025-07-16 20:08:13,028 - mmdet - INFO - Epoch [57][50/750]\tlr: 2.500e-06, eta: 0:42:37, time: 0.435, data_time: 0.057, memory: 11264, stage0_loss_cls: 0.8748, stage0_pos_acc: 48.1292, stage0_loss_bbox: 0.3815, stage0_loss_iou: 0.8464, stage0_loss_mask: 0.5722, stage1_loss_cls: 0.4041, stage1_pos_acc: 86.8628, stage1_loss_bbox: 0.1637, stage1_loss_iou: 0.3372, stage1_loss_mask: 0.3752, stage2_loss_cls: 0.2569, stage2_pos_acc: 90.9803, stage2_loss_bbox: 0.1277, stage2_loss_iou: 0.2630, stage2_loss_mask: 0.3437, stage3_loss_cls: 0.1561, stage3_pos_acc: 96.4082, stage3_loss_bbox: 0.1085, stage3_loss_iou: 0.2423, stage3_loss_mask: 0.3339, stage4_loss_cls: 0.1013, stage4_pos_acc: 98.4167, stage4_loss_bbox: 0.1095, stage4_loss_iou: 0.2352, stage4_loss_mask: 0.3019, stage5_loss_cls: 0.0872, stage5_pos_acc: 99.1444, stage5_loss_bbox: 0.1054, stage5_loss_iou: 0.2334, stage5_loss_mask: 0.3107, loss: 7.2719\n",
      "2025-07-16 20:08:32,087 - mmdet - INFO - Epoch [57][100/750]\tlr: 2.500e-06, eta: 0:42:18, time: 0.381, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9196, stage0_pos_acc: 42.1452, stage0_loss_bbox: 0.3658, stage0_loss_iou: 0.8354, stage0_loss_mask: 0.4506, stage1_loss_cls: 0.4215, stage1_pos_acc: 84.1067, stage1_loss_bbox: 0.1337, stage1_loss_iou: 0.2988, stage1_loss_mask: 0.2600, stage2_loss_cls: 0.2517, stage2_pos_acc: 93.7772, stage2_loss_bbox: 0.1039, stage2_loss_iou: 0.2300, stage2_loss_mask: 0.2304, stage3_loss_cls: 0.1214, stage3_pos_acc: 96.5551, stage3_loss_bbox: 0.0969, stage3_loss_iou: 0.2050, stage3_loss_mask: 0.1927, stage4_loss_cls: 0.0709, stage4_pos_acc: 98.8825, stage4_loss_bbox: 0.0934, stage4_loss_iou: 0.1963, stage4_loss_mask: 0.1912, stage5_loss_cls: 0.0575, stage5_pos_acc: 99.2182, stage5_loss_bbox: 0.0913, stage5_loss_iou: 0.1908, stage5_loss_mask: 0.1871, loss: 6.1960\n",
      "2025-07-16 20:08:51,064 - mmdet - INFO - Epoch [57][150/750]\tlr: 2.500e-06, eta: 0:41:59, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8973, stage0_pos_acc: 45.1657, stage0_loss_bbox: 0.3502, stage0_loss_iou: 0.8049, stage0_loss_mask: 0.4914, stage1_loss_cls: 0.4088, stage1_pos_acc: 87.3998, stage1_loss_bbox: 0.1413, stage1_loss_iou: 0.3152, stage1_loss_mask: 0.2345, stage2_loss_cls: 0.2257, stage2_pos_acc: 94.6442, stage2_loss_bbox: 0.1133, stage2_loss_iou: 0.2408, stage2_loss_mask: 0.2265, stage3_loss_cls: 0.1147, stage3_pos_acc: 97.9340, stage3_loss_bbox: 0.1000, stage3_loss_iou: 0.2150, stage3_loss_mask: 0.2118, stage4_loss_cls: 0.0630, stage4_pos_acc: 98.9106, stage4_loss_bbox: 0.0985, stage4_loss_iou: 0.2046, stage4_loss_mask: 0.2049, stage5_loss_cls: 0.0484, stage5_pos_acc: 99.6364, stage5_loss_bbox: 0.0951, stage5_loss_iou: 0.1997, stage5_loss_mask: 0.2086, loss: 6.2141\n",
      "2025-07-16 20:09:09,906 - mmdet - INFO - Epoch [57][200/750]\tlr: 2.500e-06, eta: 0:41:40, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9792, stage0_pos_acc: 45.9909, stage0_loss_bbox: 0.4068, stage0_loss_iou: 0.9649, stage0_loss_mask: 0.6916, stage1_loss_cls: 0.4456, stage1_pos_acc: 86.4675, stage1_loss_bbox: 0.1487, stage1_loss_iou: 0.3573, stage1_loss_mask: 0.3468, stage2_loss_cls: 0.2850, stage2_pos_acc: 91.2983, stage2_loss_bbox: 0.1103, stage2_loss_iou: 0.2695, stage2_loss_mask: 0.2917, stage3_loss_cls: 0.1702, stage3_pos_acc: 96.1381, stage3_loss_bbox: 0.1000, stage3_loss_iou: 0.2438, stage3_loss_mask: 0.2656, stage4_loss_cls: 0.1026, stage4_pos_acc: 97.2405, stage4_loss_bbox: 0.0921, stage4_loss_iou: 0.2335, stage4_loss_mask: 0.2623, stage5_loss_cls: 0.0908, stage5_pos_acc: 98.4333, stage5_loss_bbox: 0.0878, stage5_loss_iou: 0.2251, stage5_loss_mask: 0.2652, loss: 7.4362\n",
      "2025-07-16 20:09:28,927 - mmdet - INFO - Epoch [57][250/750]\tlr: 2.500e-06, eta: 0:41:21, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9412, stage0_pos_acc: 43.1982, stage0_loss_bbox: 0.3632, stage0_loss_iou: 0.7689, stage0_loss_mask: 0.4076, stage1_loss_cls: 0.4285, stage1_pos_acc: 81.1991, stage1_loss_bbox: 0.1439, stage1_loss_iou: 0.2892, stage1_loss_mask: 0.2556, stage2_loss_cls: 0.2602, stage2_pos_acc: 91.3368, stage2_loss_bbox: 0.1153, stage2_loss_iou: 0.2190, stage2_loss_mask: 0.2625, stage3_loss_cls: 0.1341, stage3_pos_acc: 96.1717, stage3_loss_bbox: 0.1040, stage3_loss_iou: 0.1973, stage3_loss_mask: 0.2385, stage4_loss_cls: 0.0869, stage4_pos_acc: 98.8182, stage4_loss_bbox: 0.0979, stage4_loss_iou: 0.1903, stage4_loss_mask: 0.2374, stage5_loss_cls: 0.0594, stage5_pos_acc: 98.8591, stage5_loss_bbox: 0.0986, stage5_loss_iou: 0.1872, stage5_loss_mask: 0.2307, loss: 6.3174\n",
      "2025-07-16 20:09:47,232 - mmdet - INFO - Epoch [57][300/750]\tlr: 2.500e-06, eta: 0:41:01, time: 0.366, data_time: 0.008, memory: 11264, stage0_loss_cls: 1.0003, stage0_pos_acc: 39.9063, stage0_loss_bbox: 0.3536, stage0_loss_iou: 0.7890, stage0_loss_mask: 0.3370, stage1_loss_cls: 0.4428, stage1_pos_acc: 81.2143, stage1_loss_bbox: 0.1248, stage1_loss_iou: 0.2900, stage1_loss_mask: 0.1504, stage2_loss_cls: 0.2318, stage2_pos_acc: 92.7984, stage2_loss_bbox: 0.1019, stage2_loss_iou: 0.2032, stage2_loss_mask: 0.1611, stage3_loss_cls: 0.1229, stage3_pos_acc: 96.0952, stage3_loss_bbox: 0.0939, stage3_loss_iou: 0.1727, stage3_loss_mask: 0.1426, stage4_loss_cls: 0.0874, stage4_pos_acc: 96.8444, stage4_loss_bbox: 0.0833, stage4_loss_iou: 0.1625, stage4_loss_mask: 0.1262, stage5_loss_cls: 0.0688, stage5_pos_acc: 99.0667, stage5_loss_bbox: 0.0846, stage5_loss_iou: 0.1662, stage5_loss_mask: 0.1675, loss: 5.6644\n",
      "2025-07-16 20:10:05,962 - mmdet - INFO - Epoch [57][350/750]\tlr: 2.500e-06, eta: 0:40:42, time: 0.375, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9119, stage0_pos_acc: 38.9385, stage0_loss_bbox: 0.3445, stage0_loss_iou: 0.8026, stage0_loss_mask: 0.4164, stage1_loss_cls: 0.4063, stage1_pos_acc: 84.3619, stage1_loss_bbox: 0.1346, stage1_loss_iou: 0.2927, stage1_loss_mask: 0.2483, stage2_loss_cls: 0.2247, stage2_pos_acc: 92.4815, stage2_loss_bbox: 0.1088, stage2_loss_iou: 0.2248, stage2_loss_mask: 0.2367, stage3_loss_cls: 0.1198, stage3_pos_acc: 96.9997, stage3_loss_bbox: 0.0958, stage3_loss_iou: 0.2004, stage3_loss_mask: 0.2140, stage4_loss_cls: 0.0812, stage4_pos_acc: 98.7164, stage4_loss_bbox: 0.0929, stage4_loss_iou: 0.1938, stage4_loss_mask: 0.2111, stage5_loss_cls: 0.0682, stage5_pos_acc: 98.9704, stage5_loss_bbox: 0.0887, stage5_loss_iou: 0.1863, stage5_loss_mask: 0.2121, loss: 6.1164\n",
      "2025-07-16 20:10:24,974 - mmdet - INFO - Epoch [57][400/750]\tlr: 2.500e-06, eta: 0:40:23, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9287, stage0_pos_acc: 44.9811, stage0_loss_bbox: 0.3652, stage0_loss_iou: 0.8132, stage0_loss_mask: 0.4173, stage1_loss_cls: 0.4095, stage1_pos_acc: 84.9709, stage1_loss_bbox: 0.1281, stage1_loss_iou: 0.2820, stage1_loss_mask: 0.2244, stage2_loss_cls: 0.2464, stage2_pos_acc: 92.3896, stage2_loss_bbox: 0.0986, stage2_loss_iou: 0.2052, stage2_loss_mask: 0.1968, stage3_loss_cls: 0.0971, stage3_pos_acc: 98.5821, stage3_loss_bbox: 0.0968, stage3_loss_iou: 0.1888, stage3_loss_mask: 0.1944, stage4_loss_cls: 0.0559, stage4_pos_acc: 98.9405, stage4_loss_bbox: 0.0888, stage4_loss_iou: 0.1786, stage4_loss_mask: 0.1957, stage5_loss_cls: 0.0429, stage5_pos_acc: 99.3571, stage5_loss_bbox: 0.0906, stage5_loss_iou: 0.1775, stage5_loss_mask: 0.1899, loss: 5.9124\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 101s, ETA:     0s2025-07-16 20:15:53,990 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.29s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.061\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.394\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.394\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.433\n",
      "2025-07-16 20:15:55,822 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.34s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.83s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.043\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.060\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.010\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.238\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.437\n",
      "2025-07-16 20:15:58,976 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:15:58,976 - mmdet - INFO - Epoch(val) [57][750]\tbbox_mAP: 0.0240, bbox_mAP_50: 0.0450, bbox_mAP_75: 0.0230, bbox_mAP_s: 0.0610, bbox_mAP_m: 0.0090, bbox_mAP_l: 0.0310, bbox_mAP_copypaste: 0.024 0.045 0.023 0.061 0.009 0.031, segm_mAP: 0.0240, segm_mAP_50: 0.0430, segm_mAP_75: 0.0220, segm_mAP_s: 0.0600, segm_mAP_m: 0.0100, segm_mAP_l: 0.0320, segm_mAP_copypaste: 0.024 0.043 0.022 0.060 0.010 0.032\n",
      "2025-07-16 20:16:20,312 - mmdet - INFO - Epoch [58][50/750]\tlr: 2.500e-06, eta: 0:37:50, time: 0.426, data_time: 0.056, memory: 11264, stage0_loss_cls: 0.9087, stage0_pos_acc: 44.7603, stage0_loss_bbox: 0.3718, stage0_loss_iou: 0.8245, stage0_loss_mask: 0.3996, stage1_loss_cls: 0.4391, stage1_pos_acc: 81.5105, stage1_loss_bbox: 0.1403, stage1_loss_iou: 0.2912, stage1_loss_mask: 0.2273, stage2_loss_cls: 0.2454, stage2_pos_acc: 93.4549, stage2_loss_bbox: 0.1082, stage2_loss_iou: 0.2167, stage2_loss_mask: 0.1831, stage3_loss_cls: 0.1463, stage3_pos_acc: 95.1013, stage3_loss_bbox: 0.0944, stage3_loss_iou: 0.1869, stage3_loss_mask: 0.1684, stage4_loss_cls: 0.0660, stage4_pos_acc: 98.7155, stage4_loss_bbox: 0.0876, stage4_loss_iou: 0.1730, stage4_loss_mask: 0.1693, stage5_loss_cls: 0.0572, stage5_pos_acc: 98.9034, stage5_loss_bbox: 0.0840, stage5_loss_iou: 0.1668, stage5_loss_mask: 0.1652, loss: 5.9211\n",
      "2025-07-16 20:16:38,995 - mmdet - INFO - Epoch [58][100/750]\tlr: 2.500e-06, eta: 0:37:31, time: 0.374, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9284, stage0_pos_acc: 45.0127, stage0_loss_bbox: 0.3790, stage0_loss_iou: 0.7227, stage0_loss_mask: 0.3968, stage1_loss_cls: 0.4112, stage1_pos_acc: 84.2905, stage1_loss_bbox: 0.1450, stage1_loss_iou: 0.2896, stage1_loss_mask: 0.2414, stage2_loss_cls: 0.2226, stage2_pos_acc: 94.1786, stage2_loss_bbox: 0.1135, stage2_loss_iou: 0.2081, stage2_loss_mask: 0.1931, stage3_loss_cls: 0.1078, stage3_pos_acc: 97.7810, stage3_loss_bbox: 0.1050, stage3_loss_iou: 0.1958, stage3_loss_mask: 0.1836, stage4_loss_cls: 0.0567, stage4_pos_acc: 99.3810, stage4_loss_bbox: 0.0994, stage4_loss_iou: 0.1837, stage4_loss_mask: 0.1797, stage5_loss_cls: 0.0464, stage5_pos_acc: 99.7143, stage5_loss_bbox: 0.0955, stage5_loss_iou: 0.1810, stage5_loss_mask: 0.1790, loss: 5.8649\n",
      "2025-07-16 20:16:57,902 - mmdet - INFO - Epoch [58][150/750]\tlr: 2.500e-06, eta: 0:37:12, time: 0.378, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9328, stage0_pos_acc: 41.7933, stage0_loss_bbox: 0.3870, stage0_loss_iou: 0.9076, stage0_loss_mask: 0.6541, stage1_loss_cls: 0.4416, stage1_pos_acc: 80.7904, stage1_loss_bbox: 0.1571, stage1_loss_iou: 0.3973, stage1_loss_mask: 0.4056, stage2_loss_cls: 0.2761, stage2_pos_acc: 89.2999, stage2_loss_bbox: 0.1272, stage2_loss_iou: 0.3063, stage2_loss_mask: 0.3650, stage3_loss_cls: 0.1641, stage3_pos_acc: 96.2413, stage3_loss_bbox: 0.1133, stage3_loss_iou: 0.2836, stage3_loss_mask: 0.3511, stage4_loss_cls: 0.1119, stage4_pos_acc: 97.7524, stage4_loss_bbox: 0.1084, stage4_loss_iou: 0.2674, stage4_loss_mask: 0.3482, stage5_loss_cls: 0.0842, stage5_pos_acc: 98.4413, stage5_loss_bbox: 0.1060, stage5_loss_iou: 0.2589, stage5_loss_mask: 0.3422, loss: 7.8971\n",
      "2025-07-16 20:17:16,804 - mmdet - INFO - Epoch [58][200/750]\tlr: 2.500e-06, eta: 0:36:53, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9363, stage0_pos_acc: 43.5884, stage0_loss_bbox: 0.3136, stage0_loss_iou: 0.8052, stage0_loss_mask: 0.5069, stage1_loss_cls: 0.3946, stage1_pos_acc: 83.6398, stage1_loss_bbox: 0.1219, stage1_loss_iou: 0.3060, stage1_loss_mask: 0.3109, stage2_loss_cls: 0.2408, stage2_pos_acc: 91.7979, stage2_loss_bbox: 0.1030, stage2_loss_iou: 0.2452, stage2_loss_mask: 0.2843, stage3_loss_cls: 0.1161, stage3_pos_acc: 95.8851, stage3_loss_bbox: 0.0899, stage3_loss_iou: 0.2155, stage3_loss_mask: 0.2588, stage4_loss_cls: 0.0691, stage4_pos_acc: 97.8112, stage4_loss_bbox: 0.0839, stage4_loss_iou: 0.2054, stage4_loss_mask: 0.2595, stage5_loss_cls: 0.0545, stage5_pos_acc: 98.3716, stage5_loss_bbox: 0.0830, stage5_loss_iou: 0.2037, stage5_loss_mask: 0.2533, loss: 6.4614\n",
      "2025-07-16 20:17:35,896 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:17:35,896 - mmdet - INFO - Epoch [58][250/750]\tlr: 2.500e-06, eta: 0:36:33, time: 0.382, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9110, stage0_pos_acc: 44.0553, stage0_loss_bbox: 0.3825, stage0_loss_iou: 0.8749, stage0_loss_mask: 0.6068, stage1_loss_cls: 0.4396, stage1_pos_acc: 86.8172, stage1_loss_bbox: 0.1570, stage1_loss_iou: 0.3443, stage1_loss_mask: 0.4076, stage2_loss_cls: 0.2809, stage2_pos_acc: 91.0371, stage2_loss_bbox: 0.1258, stage2_loss_iou: 0.2672, stage2_loss_mask: 0.3486, stage3_loss_cls: 0.1432, stage3_pos_acc: 97.6602, stage3_loss_bbox: 0.1171, stage3_loss_iou: 0.2431, stage3_loss_mask: 0.3314, stage4_loss_cls: 0.0899, stage4_pos_acc: 98.7670, stage4_loss_bbox: 0.1156, stage4_loss_iou: 0.2326, stage4_loss_mask: 0.3248, stage5_loss_cls: 0.0675, stage5_pos_acc: 98.5852, stage5_loss_bbox: 0.1139, stage5_loss_iou: 0.2303, stage5_loss_mask: 0.3293, loss: 7.4849\n",
      "2025-07-16 20:17:54,778 - mmdet - INFO - Epoch [58][300/750]\tlr: 2.500e-06, eta: 0:36:14, time: 0.378, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9260, stage0_pos_acc: 40.8478, stage0_loss_bbox: 0.3813, stage0_loss_iou: 0.8131, stage0_loss_mask: 0.5014, stage1_loss_cls: 0.4084, stage1_pos_acc: 82.9132, stage1_loss_bbox: 0.1482, stage1_loss_iou: 0.3326, stage1_loss_mask: 0.3278, stage2_loss_cls: 0.2711, stage2_pos_acc: 91.3916, stage2_loss_bbox: 0.1226, stage2_loss_iou: 0.2599, stage2_loss_mask: 0.3094, stage3_loss_cls: 0.1472, stage3_pos_acc: 95.2234, stage3_loss_bbox: 0.1140, stage3_loss_iou: 0.2361, stage3_loss_mask: 0.2959, stage4_loss_cls: 0.1004, stage4_pos_acc: 97.3476, stage4_loss_bbox: 0.1127, stage4_loss_iou: 0.2320, stage4_loss_mask: 0.3001, stage5_loss_cls: 0.0762, stage5_pos_acc: 98.5636, stage5_loss_bbox: 0.1081, stage5_loss_iou: 0.2257, stage5_loss_mask: 0.2941, loss: 7.0442\n",
      "2025-07-16 20:18:13,623 - mmdet - INFO - Epoch [58][350/750]\tlr: 2.500e-06, eta: 0:35:55, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9186, stage0_pos_acc: 46.0647, stage0_loss_bbox: 0.3762, stage0_loss_iou: 0.8274, stage0_loss_mask: 0.5151, stage1_loss_cls: 0.4199, stage1_pos_acc: 80.5190, stage1_loss_bbox: 0.1491, stage1_loss_iou: 0.3300, stage1_loss_mask: 0.3540, stage2_loss_cls: 0.2464, stage2_pos_acc: 90.2230, stage2_loss_bbox: 0.1222, stage2_loss_iou: 0.2592, stage2_loss_mask: 0.3210, stage3_loss_cls: 0.1201, stage3_pos_acc: 97.1841, stage3_loss_bbox: 0.1116, stage3_loss_iou: 0.2350, stage3_loss_mask: 0.3144, stage4_loss_cls: 0.0660, stage4_pos_acc: 98.1425, stage4_loss_bbox: 0.1114, stage4_loss_iou: 0.2303, stage4_loss_mask: 0.3154, stage5_loss_cls: 0.0610, stage5_pos_acc: 98.7036, stage5_loss_bbox: 0.1078, stage5_loss_iou: 0.2232, stage5_loss_mask: 0.3063, loss: 7.0415\n",
      "2025-07-16 20:18:32,524 - mmdet - INFO - Epoch [58][400/750]\tlr: 2.500e-06, eta: 0:35:36, time: 0.378, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.8787, stage0_pos_acc: 46.4048, stage0_loss_bbox: 0.3446, stage0_loss_iou: 0.8011, stage0_loss_mask: 0.4880, stage1_loss_cls: 0.3961, stage1_pos_acc: 81.5714, stage1_loss_bbox: 0.1288, stage1_loss_iou: 0.3079, stage1_loss_mask: 0.2657, stage2_loss_cls: 0.2516, stage2_pos_acc: 87.6226, stage2_loss_bbox: 0.0991, stage2_loss_iou: 0.2159, stage2_loss_mask: 0.2145, stage3_loss_cls: 0.1342, stage3_pos_acc: 97.1976, stage3_loss_bbox: 0.0935, stage3_loss_iou: 0.1949, stage3_loss_mask: 0.2007, stage4_loss_cls: 0.0722, stage4_pos_acc: 98.8976, stage4_loss_bbox: 0.0896, stage4_loss_iou: 0.1864, stage4_loss_mask: 0.1919, stage5_loss_cls: 0.0538, stage5_pos_acc: 98.8333, stage5_loss_bbox: 0.0867, stage5_loss_iou: 0.1793, stage5_loss_mask: 0.1890, loss: 6.0644\n",
      "2025-07-16 20:18:51,389 - mmdet - INFO - Epoch [58][450/750]\tlr: 2.500e-06, eta: 0:35:17, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9473, stage0_pos_acc: 41.6321, stage0_loss_bbox: 0.4001, stage0_loss_iou: 0.8526, stage0_loss_mask: 0.4782, stage1_loss_cls: 0.4376, stage1_pos_acc: 80.2788, stage1_loss_bbox: 0.1345, stage1_loss_iou: 0.3001, stage1_loss_mask: 0.2782, stage2_loss_cls: 0.2599, stage2_pos_acc: 92.9852, stage2_loss_bbox: 0.1044, stage2_loss_iou: 0.2192, stage2_loss_mask: 0.2326, stage3_loss_cls: 0.1391, stage3_pos_acc: 96.1985, stage3_loss_bbox: 0.0921, stage3_loss_iou: 0.1917, stage3_loss_mask: 0.2090, stage4_loss_cls: 0.0889, stage4_pos_acc: 97.1690, stage4_loss_bbox: 0.0914, stage4_loss_iou: 0.1827, stage4_loss_mask: 0.2144, stage5_loss_cls: 0.0745, stage5_pos_acc: 98.7000, stage5_loss_bbox: 0.0896, stage5_loss_iou: 0.1791, stage5_loss_mask: 0.2141, loss: 6.4113\n",
      "2025-07-16 20:19:10,235 - mmdet - INFO - Epoch [58][500/750]\tlr: 2.500e-06, eta: 0:34:58, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9705, stage0_pos_acc: 38.6305, stage0_loss_bbox: 0.4002, stage0_loss_iou: 0.8942, stage0_loss_mask: 0.4742, stage1_loss_cls: 0.4182, stage1_pos_acc: 82.1461, stage1_loss_bbox: 0.1529, stage1_loss_iou: 0.3100, stage1_loss_mask: 0.2761, stage2_loss_cls: 0.2476, stage2_pos_acc: 88.7932, stage2_loss_bbox: 0.1316, stage2_loss_iou: 0.2277, stage2_loss_mask: 0.2677, stage3_loss_cls: 0.1254, stage3_pos_acc: 96.0988, stage3_loss_bbox: 0.0886, stage3_loss_iou: 0.2077, stage3_loss_mask: 0.2490, stage4_loss_cls: 0.0780, stage4_pos_acc: 98.8228, stage4_loss_bbox: 0.0815, stage4_loss_iou: 0.1996, stage4_loss_mask: 0.2452, stage5_loss_cls: 0.0598, stage5_pos_acc: 98.3561, stage5_loss_bbox: 0.0823, stage5_loss_iou: 0.1915, stage5_loss_mask: 0.2174, loss: 6.5969\n",
      "2025-07-16 20:19:29,375 - mmdet - INFO - Epoch [58][550/750]\tlr: 2.500e-06, eta: 0:34:39, time: 0.383, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.8938, stage0_pos_acc: 42.6286, stage0_loss_bbox: 0.3638, stage0_loss_iou: 0.8646, stage0_loss_mask: 0.6396, stage1_loss_cls: 0.4169, stage1_pos_acc: 88.0147, stage1_loss_bbox: 0.1459, stage1_loss_iou: 0.3460, stage1_loss_mask: 0.3665, stage2_loss_cls: 0.2521, stage2_pos_acc: 91.5631, stage2_loss_bbox: 0.1146, stage2_loss_iou: 0.2641, stage2_loss_mask: 0.3361, stage3_loss_cls: 0.1335, stage3_pos_acc: 95.8644, stage3_loss_bbox: 0.1018, stage3_loss_iou: 0.2433, stage3_loss_mask: 0.3363, stage4_loss_cls: 0.0804, stage4_pos_acc: 98.0656, stage4_loss_bbox: 0.0999, stage4_loss_iou: 0.2353, stage4_loss_mask: 0.3308, stage5_loss_cls: 0.0658, stage5_pos_acc: 99.1102, stage5_loss_bbox: 0.0990, stage5_loss_iou: 0.2300, stage5_loss_mask: 0.3280, loss: 7.2881\n",
      "2025-07-16 20:19:48,172 - mmdet - INFO - Epoch [58][600/750]\tlr: 2.500e-06, eta: 0:34:20, time: 0.376, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9275, stage0_pos_acc: 42.3883, stage0_loss_bbox: 0.3252, stage0_loss_iou: 0.8702, stage0_loss_mask: 0.4114, stage1_loss_cls: 0.4019, stage1_pos_acc: 83.6278, stage1_loss_bbox: 0.1179, stage1_loss_iou: 0.3174, stage1_loss_mask: 0.2650, stage2_loss_cls: 0.2266, stage2_pos_acc: 93.5140, stage2_loss_bbox: 0.0943, stage2_loss_iou: 0.2292, stage2_loss_mask: 0.2500, stage3_loss_cls: 0.1032, stage3_pos_acc: 97.1752, stage3_loss_bbox: 0.0935, stage3_loss_iou: 0.2179, stage3_loss_mask: 0.2501, stage4_loss_cls: 0.0598, stage4_pos_acc: 98.4011, stage4_loss_bbox: 0.0890, stage4_loss_iou: 0.2069, stage4_loss_mask: 0.2304, stage5_loss_cls: 0.0389, stage5_pos_acc: 99.1286, stage5_loss_bbox: 0.0860, stage5_loss_iou: 0.2023, stage5_loss_mask: 0.2395, loss: 6.2544\n",
      "2025-07-16 20:20:07,011 - mmdet - INFO - Epoch [58][650/750]\tlr: 2.500e-06, eta: 0:34:01, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9384, stage0_pos_acc: 42.6727, stage0_loss_bbox: 0.3457, stage0_loss_iou: 0.8453, stage0_loss_mask: 0.5500, stage1_loss_cls: 0.4154, stage1_pos_acc: 84.6379, stage1_loss_bbox: 0.1234, stage1_loss_iou: 0.3064, stage1_loss_mask: 0.2701, stage2_loss_cls: 0.2278, stage2_pos_acc: 95.0490, stage2_loss_bbox: 0.1010, stage2_loss_iou: 0.2338, stage2_loss_mask: 0.2581, stage3_loss_cls: 0.1171, stage3_pos_acc: 98.7098, stage3_loss_bbox: 0.0923, stage3_loss_iou: 0.2147, stage3_loss_mask: 0.2417, stage4_loss_cls: 0.0642, stage4_pos_acc: 99.1225, stage4_loss_bbox: 0.0894, stage4_loss_iou: 0.2096, stage4_loss_mask: 0.2424, stage5_loss_cls: 0.0512, stage5_pos_acc: 99.5960, stage5_loss_bbox: 0.0911, stage5_loss_iou: 0.2048, stage5_loss_mask: 0.2481, loss: 6.4821\n",
      "2025-07-16 20:20:25,991 - mmdet - INFO - Epoch [58][700/750]\tlr: 2.500e-06, eta: 0:33:41, time: 0.380, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8974, stage0_pos_acc: 46.0806, stage0_loss_bbox: 0.3976, stage0_loss_iou: 0.7544, stage0_loss_mask: 0.5178, stage1_loss_cls: 0.4215, stage1_pos_acc: 83.0330, stage1_loss_bbox: 0.1774, stage1_loss_iou: 0.3058, stage1_loss_mask: 0.3152, stage2_loss_cls: 0.2569, stage2_pos_acc: 91.9782, stage2_loss_bbox: 0.1508, stage2_loss_iou: 0.2451, stage2_loss_mask: 0.2808, stage3_loss_cls: 0.1407, stage3_pos_acc: 97.4939, stage3_loss_bbox: 0.1248, stage3_loss_iou: 0.2216, stage3_loss_mask: 0.2443, stage4_loss_cls: 0.0842, stage4_pos_acc: 99.1420, stage4_loss_bbox: 0.1242, stage4_loss_iou: 0.2092, stage4_loss_mask: 0.2389, stage5_loss_cls: 0.0672, stage5_pos_acc: 99.1325, stage5_loss_bbox: 0.1159, stage5_loss_iou: 0.2042, stage5_loss_mask: 0.2416, loss: 6.7374\n",
      "2025-07-16 20:20:45,453 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:20:45,454 - mmdet - INFO - Epoch [58][750/750]\tlr: 2.500e-06, eta: 0:33:22, time: 0.389, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.8975, stage0_pos_acc: 45.4579, stage0_loss_bbox: 0.3499, stage0_loss_iou: 0.7307, stage0_loss_mask: 0.3363, stage1_loss_cls: 0.3950, stage1_pos_acc: 88.0339, stage1_loss_bbox: 0.1384, stage1_loss_iou: 0.2510, stage1_loss_mask: 0.1861, stage2_loss_cls: 0.2437, stage2_pos_acc: 93.6674, stage2_loss_bbox: 0.1132, stage2_loss_iou: 0.1990, stage2_loss_mask: 0.1749, stage3_loss_cls: 0.1351, stage3_pos_acc: 94.4992, stage3_loss_bbox: 0.1039, stage3_loss_iou: 0.1801, stage3_loss_mask: 0.1827, stage4_loss_cls: 0.0939, stage4_pos_acc: 97.9605, stage4_loss_bbox: 0.0990, stage4_loss_iou: 0.1711, stage4_loss_mask: 0.1743, stage5_loss_cls: 0.0711, stage5_pos_acc: 98.9385, stage5_loss_bbox: 0.0989, stage5_loss_iou: 0.1701, stage5_loss_mask: 0.1727, loss: 5.6684\n",
      "2025-07-16 20:20:45,603 - mmdet - INFO - Saving checkpoint at 58 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 20:24:06,305 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.042\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.398\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.202\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.447\n",
      "2025-07-16 20:24:08,152 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.34s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.046\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.040\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.402\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.216\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.443\n",
      "2025-07-16 20:24:11,281 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:24:11,281 - mmdet - INFO - Epoch(val) [58][750]\tbbox_mAP: 0.0260, bbox_mAP_50: 0.0480, bbox_mAP_75: 0.0260, bbox_mAP_s: 0.0420, bbox_mAP_m: 0.0250, bbox_mAP_l: 0.0340, bbox_mAP_copypaste: 0.026 0.048 0.026 0.042 0.025 0.034, segm_mAP: 0.0260, segm_mAP_50: 0.0460, segm_mAP_75: 0.0250, segm_mAP_s: 0.0400, segm_mAP_m: 0.0250, segm_mAP_l: 0.0340, segm_mAP_copypaste: 0.026 0.046 0.025 0.040 0.025 0.034\n",
      "2025-07-16 20:24:32,542 - mmdet - INFO - Epoch [59][50/750]\tlr: 2.500e-06, eta: 0:33:04, time: 0.425, data_time: 0.055, memory: 11264, stage0_loss_cls: 0.9634, stage0_pos_acc: 40.6465, stage0_loss_bbox: 0.3846, stage0_loss_iou: 0.7769, stage0_loss_mask: 0.3522, stage1_loss_cls: 0.4249, stage1_pos_acc: 83.9086, stage1_loss_bbox: 0.1454, stage1_loss_iou: 0.2880, stage1_loss_mask: 0.1908, stage2_loss_cls: 0.2397, stage2_pos_acc: 94.9000, stage2_loss_bbox: 0.1175, stage2_loss_iou: 0.2057, stage2_loss_mask: 0.1771, stage3_loss_cls: 0.1040, stage3_pos_acc: 98.0611, stage3_loss_bbox: 0.1116, stage3_loss_iou: 0.1862, stage3_loss_mask: 0.1557, stage4_loss_cls: 0.0616, stage4_pos_acc: 98.3278, stage4_loss_bbox: 0.1071, stage4_loss_iou: 0.1769, stage4_loss_mask: 0.1310, stage5_loss_cls: 0.0473, stage5_pos_acc: 98.2444, stage5_loss_bbox: 0.1025, stage5_loss_iou: 0.1727, stage5_loss_mask: 0.1513, loss: 5.7741\n",
      "2025-07-16 20:24:51,377 - mmdet - INFO - Epoch [59][100/750]\tlr: 2.500e-06, eta: 0:32:45, time: 0.377, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9617, stage0_pos_acc: 37.3349, stage0_loss_bbox: 0.3607, stage0_loss_iou: 0.8786, stage0_loss_mask: 0.5915, stage1_loss_cls: 0.4234, stage1_pos_acc: 86.8662, stage1_loss_bbox: 0.1367, stage1_loss_iou: 0.3363, stage1_loss_mask: 0.3386, stage2_loss_cls: 0.2585, stage2_pos_acc: 92.2941, stage2_loss_bbox: 0.1067, stage2_loss_iou: 0.2623, stage2_loss_mask: 0.3570, stage3_loss_cls: 0.1420, stage3_pos_acc: 95.2587, stage3_loss_bbox: 0.0945, stage3_loss_iou: 0.2331, stage3_loss_mask: 0.3473, stage4_loss_cls: 0.0787, stage4_pos_acc: 98.2628, stage4_loss_bbox: 0.0896, stage4_loss_iou: 0.2232, stage4_loss_mask: 0.3326, stage5_loss_cls: 0.0552, stage5_pos_acc: 99.3398, stage5_loss_bbox: 0.0888, stage5_loss_iou: 0.2201, stage5_loss_mask: 0.3331, loss: 7.2501\n",
      "2025-07-16 20:25:10,196 - mmdet - INFO - Epoch [59][150/750]\tlr: 2.500e-06, eta: 0:32:26, time: 0.376, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8932, stage0_pos_acc: 43.0767, stage0_loss_bbox: 0.3774, stage0_loss_iou: 0.8640, stage0_loss_mask: 0.5657, stage1_loss_cls: 0.4147, stage1_pos_acc: 84.2661, stage1_loss_bbox: 0.1473, stage1_loss_iou: 0.3476, stage1_loss_mask: 0.3436, stage2_loss_cls: 0.2426, stage2_pos_acc: 92.4010, stage2_loss_bbox: 0.1153, stage2_loss_iou: 0.2660, stage2_loss_mask: 0.3280, stage3_loss_cls: 0.1369, stage3_pos_acc: 96.7280, stage3_loss_bbox: 0.1023, stage3_loss_iou: 0.2316, stage3_loss_mask: 0.2884, stage4_loss_cls: 0.0895, stage4_pos_acc: 98.6667, stage4_loss_bbox: 0.0990, stage4_loss_iou: 0.2194, stage4_loss_mask: 0.2707, stage5_loss_cls: 0.0700, stage5_pos_acc: 98.3000, stage5_loss_bbox: 0.0960, stage5_loss_iou: 0.2115, stage5_loss_mask: 0.2753, loss: 6.9960\n",
      "2025-07-16 20:25:29,227 - mmdet - INFO - Epoch [59][200/750]\tlr: 2.500e-06, eta: 0:32:06, time: 0.381, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.8838, stage0_pos_acc: 49.8519, stage0_loss_bbox: 0.4003, stage0_loss_iou: 0.8887, stage0_loss_mask: 0.6994, stage1_loss_cls: 0.4153, stage1_pos_acc: 85.2045, stage1_loss_bbox: 0.1686, stage1_loss_iou: 0.3755, stage1_loss_mask: 0.3654, stage2_loss_cls: 0.2447, stage2_pos_acc: 93.8260, stage2_loss_bbox: 0.1302, stage2_loss_iou: 0.2888, stage2_loss_mask: 0.3590, stage3_loss_cls: 0.1256, stage3_pos_acc: 97.6035, stage3_loss_bbox: 0.1202, stage3_loss_iou: 0.2584, stage3_loss_mask: 0.3388, stage4_loss_cls: 0.0835, stage4_pos_acc: 98.8126, stage4_loss_bbox: 0.1159, stage4_loss_iou: 0.2457, stage4_loss_mask: 0.3324, stage5_loss_cls: 0.0767, stage5_pos_acc: 99.3892, stage5_loss_bbox: 0.1126, stage5_loss_iou: 0.2381, stage5_loss_mask: 0.3183, loss: 7.5856\n",
      "2025-07-16 20:25:48,210 - mmdet - INFO - Epoch [59][250/750]\tlr: 2.500e-06, eta: 0:31:47, time: 0.380, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9485, stage0_pos_acc: 42.8139, stage0_loss_bbox: 0.3560, stage0_loss_iou: 0.8820, stage0_loss_mask: 0.3636, stage1_loss_cls: 0.3915, stage1_pos_acc: 87.6281, stage1_loss_bbox: 0.1114, stage1_loss_iou: 0.3032, stage1_loss_mask: 0.2161, stage2_loss_cls: 0.2106, stage2_pos_acc: 96.0619, stage2_loss_bbox: 0.0854, stage2_loss_iou: 0.2150, stage2_loss_mask: 0.2048, stage3_loss_cls: 0.1169, stage3_pos_acc: 97.2286, stage3_loss_bbox: 0.0801, stage3_loss_iou: 0.1864, stage3_loss_mask: 0.1771, stage4_loss_cls: 0.0693, stage4_pos_acc: 99.2143, stage4_loss_bbox: 0.0750, stage4_loss_iou: 0.1786, stage4_loss_mask: 0.1536, stage5_loss_cls: 0.0505, stage5_pos_acc: 99.4571, stage5_loss_bbox: 0.0723, stage5_loss_iou: 0.1747, stage5_loss_mask: 0.1614, loss: 5.7841\n",
      "2025-07-16 20:26:06,943 - mmdet - INFO - Epoch [59][300/750]\tlr: 2.500e-06, eta: 0:31:28, time: 0.375, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9496, stage0_pos_acc: 43.7669, stage0_loss_bbox: 0.3912, stage0_loss_iou: 0.8205, stage0_loss_mask: 0.5459, stage1_loss_cls: 0.4219, stage1_pos_acc: 83.6002, stage1_loss_bbox: 0.1646, stage1_loss_iou: 0.3123, stage1_loss_mask: 0.3479, stage2_loss_cls: 0.2737, stage2_pos_acc: 90.4050, stage2_loss_bbox: 0.1384, stage2_loss_iou: 0.2431, stage2_loss_mask: 0.2822, stage3_loss_cls: 0.1577, stage3_pos_acc: 97.0574, stage3_loss_bbox: 0.1047, stage3_loss_iou: 0.2272, stage3_loss_mask: 0.2611, stage4_loss_cls: 0.0870, stage4_pos_acc: 98.3682, stage4_loss_bbox: 0.0912, stage4_loss_iou: 0.2148, stage4_loss_mask: 0.2578, stage5_loss_cls: 0.0704, stage5_pos_acc: 99.2591, stage5_loss_bbox: 0.0876, stage5_loss_iou: 0.2100, stage5_loss_mask: 0.2558, loss: 6.9164\n",
      "2025-07-16 20:26:25,811 - mmdet - INFO - Epoch [59][350/750]\tlr: 2.500e-06, eta: 0:31:09, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9234, stage0_pos_acc: 42.5805, stage0_loss_bbox: 0.3651, stage0_loss_iou: 0.7848, stage0_loss_mask: 0.5727, stage1_loss_cls: 0.4245, stage1_pos_acc: 82.1008, stage1_loss_bbox: 0.1593, stage1_loss_iou: 0.3492, stage1_loss_mask: 0.4117, stage2_loss_cls: 0.2780, stage2_pos_acc: 90.5412, stage2_loss_bbox: 0.1361, stage2_loss_iou: 0.2715, stage2_loss_mask: 0.3722, stage3_loss_cls: 0.1438, stage3_pos_acc: 97.1604, stage3_loss_bbox: 0.1278, stage3_loss_iou: 0.2637, stage3_loss_mask: 0.3403, stage4_loss_cls: 0.0887, stage4_pos_acc: 98.6385, stage4_loss_bbox: 0.1253, stage4_loss_iou: 0.2540, stage4_loss_mask: 0.3464, stage5_loss_cls: 0.0812, stage5_pos_acc: 98.7484, stage5_loss_bbox: 0.1254, stage5_loss_iou: 0.2488, stage5_loss_mask: 0.3380, loss: 7.5322\n",
      "2025-07-16 20:26:44,735 - mmdet - INFO - Epoch [59][400/750]\tlr: 2.500e-06, eta: 0:30:50, time: 0.378, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9327, stage0_pos_acc: 38.3085, stage0_loss_bbox: 0.3029, stage0_loss_iou: 0.7712, stage0_loss_mask: 0.3925, stage1_loss_cls: 0.3961, stage1_pos_acc: 85.3030, stage1_loss_bbox: 0.1168, stage1_loss_iou: 0.2832, stage1_loss_mask: 0.2506, stage2_loss_cls: 0.2405, stage2_pos_acc: 92.7547, stage2_loss_bbox: 0.0896, stage2_loss_iou: 0.2100, stage2_loss_mask: 0.2195, stage3_loss_cls: 0.1232, stage3_pos_acc: 96.6794, stage3_loss_bbox: 0.0829, stage3_loss_iou: 0.1929, stage3_loss_mask: 0.2116, stage4_loss_cls: 0.0870, stage4_pos_acc: 98.7286, stage4_loss_bbox: 0.0748, stage4_loss_iou: 0.1799, stage4_loss_mask: 0.2035, stage5_loss_cls: 0.0709, stage5_pos_acc: 98.6571, stage5_loss_bbox: 0.0715, stage5_loss_iou: 0.1768, stage5_loss_mask: 0.2014, loss: 5.8818\n",
      "2025-07-16 20:27:03,545 - mmdet - INFO - Epoch [59][450/750]\tlr: 2.500e-06, eta: 0:30:31, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.8974, stage0_pos_acc: 50.9259, stage0_loss_bbox: 0.3405, stage0_loss_iou: 0.7302, stage0_loss_mask: 0.4365, stage1_loss_cls: 0.3918, stage1_pos_acc: 86.4314, stage1_loss_bbox: 0.1219, stage1_loss_iou: 0.2615, stage1_loss_mask: 0.2244, stage2_loss_cls: 0.2213, stage2_pos_acc: 92.9513, stage2_loss_bbox: 0.1024, stage2_loss_iou: 0.2054, stage2_loss_mask: 0.1846, stage3_loss_cls: 0.1132, stage3_pos_acc: 96.9876, stage3_loss_bbox: 0.0916, stage3_loss_iou: 0.1799, stage3_loss_mask: 0.1737, stage4_loss_cls: 0.0860, stage4_pos_acc: 97.6281, stage4_loss_bbox: 0.0844, stage4_loss_iou: 0.1681, stage4_loss_mask: 0.1615, stage5_loss_cls: 0.0802, stage5_pos_acc: 97.5069, stage5_loss_bbox: 0.0826, stage5_loss_iou: 0.1634, stage5_loss_mask: 0.1639, loss: 5.6665\n",
      "2025-07-16 20:27:22,378 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:27:22,379 - mmdet - INFO - Epoch [59][500/750]\tlr: 2.500e-06, eta: 0:30:12, time: 0.377, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.8805, stage0_pos_acc: 47.2064, stage0_loss_bbox: 0.3588, stage0_loss_iou: 0.8185, stage0_loss_mask: 0.5959, stage1_loss_cls: 0.4310, stage1_pos_acc: 85.3330, stage1_loss_bbox: 0.1517, stage1_loss_iou: 0.3403, stage1_loss_mask: 0.3526, stage2_loss_cls: 0.2794, stage2_pos_acc: 89.2803, stage2_loss_bbox: 0.1199, stage2_loss_iou: 0.2660, stage2_loss_mask: 0.3082, stage3_loss_cls: 0.1503, stage3_pos_acc: 96.5525, stage3_loss_bbox: 0.1113, stage3_loss_iou: 0.2408, stage3_loss_mask: 0.2852, stage4_loss_cls: 0.0973, stage4_pos_acc: 97.8853, stage4_loss_bbox: 0.1104, stage4_loss_iou: 0.2353, stage4_loss_mask: 0.2849, stage5_loss_cls: 0.0853, stage5_pos_acc: 98.6347, stage5_loss_bbox: 0.1066, stage5_loss_iou: 0.2292, stage5_loss_mask: 0.2870, loss: 7.1262\n",
      "2025-07-16 20:27:41,501 - mmdet - INFO - Epoch [59][550/750]\tlr: 2.500e-06, eta: 0:29:53, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9050, stage0_pos_acc: 46.1270, stage0_loss_bbox: 0.3598, stage0_loss_iou: 0.8020, stage0_loss_mask: 0.4017, stage1_loss_cls: 0.4190, stage1_pos_acc: 85.4040, stage1_loss_bbox: 0.1489, stage1_loss_iou: 0.2989, stage1_loss_mask: 0.2731, stage2_loss_cls: 0.2439, stage2_pos_acc: 91.3837, stage2_loss_bbox: 0.1268, stage2_loss_iou: 0.2293, stage2_loss_mask: 0.2435, stage3_loss_cls: 0.1211, stage3_pos_acc: 97.9302, stage3_loss_bbox: 0.1109, stage3_loss_iou: 0.2068, stage3_loss_mask: 0.2342, stage4_loss_cls: 0.0795, stage4_pos_acc: 98.8992, stage4_loss_bbox: 0.1111, stage4_loss_iou: 0.2006, stage4_loss_mask: 0.2261, stage5_loss_cls: 0.0699, stage5_pos_acc: 99.4222, stage5_loss_bbox: 0.1066, stage5_loss_iou: 0.1971, stage5_loss_mask: 0.2256, loss: 6.3415\n",
      "2025-07-16 20:28:00,571 - mmdet - INFO - Epoch [59][600/750]\tlr: 2.500e-06, eta: 0:29:34, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9057, stage0_pos_acc: 48.4593, stage0_loss_bbox: 0.3702, stage0_loss_iou: 0.8382, stage0_loss_mask: 0.4873, stage1_loss_cls: 0.4166, stage1_pos_acc: 84.6322, stage1_loss_bbox: 0.1483, stage1_loss_iou: 0.3310, stage1_loss_mask: 0.3174, stage2_loss_cls: 0.2542, stage2_pos_acc: 90.2965, stage2_loss_bbox: 0.1169, stage2_loss_iou: 0.2353, stage2_loss_mask: 0.2992, stage3_loss_cls: 0.1291, stage3_pos_acc: 97.2846, stage3_loss_bbox: 0.1054, stage3_loss_iou: 0.2171, stage3_loss_mask: 0.3230, stage4_loss_cls: 0.0718, stage4_pos_acc: 99.1108, stage4_loss_bbox: 0.1047, stage4_loss_iou: 0.2140, stage4_loss_mask: 0.3041, stage5_loss_cls: 0.0610, stage5_pos_acc: 99.6923, stage5_loss_bbox: 0.1014, stage5_loss_iou: 0.2085, stage5_loss_mask: 0.2983, loss: 6.8586\n",
      "2025-07-16 20:28:19,945 - mmdet - INFO - Epoch [59][650/750]\tlr: 2.500e-06, eta: 0:29:15, time: 0.387, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9494, stage0_pos_acc: 43.9284, stage0_loss_bbox: 0.3609, stage0_loss_iou: 0.8665, stage0_loss_mask: 0.3465, stage1_loss_cls: 0.4046, stage1_pos_acc: 83.0248, stage1_loss_bbox: 0.1270, stage1_loss_iou: 0.2773, stage1_loss_mask: 0.2094, stage2_loss_cls: 0.2579, stage2_pos_acc: 92.6162, stage2_loss_bbox: 0.1027, stage2_loss_iou: 0.2081, stage2_loss_mask: 0.1673, stage3_loss_cls: 0.1348, stage3_pos_acc: 95.8310, stage3_loss_bbox: 0.0930, stage3_loss_iou: 0.1861, stage3_loss_mask: 0.1620, stage4_loss_cls: 0.0682, stage4_pos_acc: 99.0313, stage4_loss_bbox: 0.0907, stage4_loss_iou: 0.1846, stage4_loss_mask: 0.1594, stage5_loss_cls: 0.0489, stage5_pos_acc: 99.4313, stage5_loss_bbox: 0.0888, stage5_loss_iou: 0.1799, stage5_loss_mask: 0.1575, loss: 5.8315\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.1 task/s, elapsed: 95s, ETA:     0s2025-07-16 20:32:09,581 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.27s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.44s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.122\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.033\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.385\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.387\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.207\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.427\n",
      "2025-07-16 20:32:11,379 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.73s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.020\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.394\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.394\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.220\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.221\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.432\n",
      "2025-07-16 20:32:14,266 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:32:14,266 - mmdet - INFO - Epoch(val) [59][750]\tbbox_mAP: 0.0260, bbox_mAP_50: 0.0480, bbox_mAP_75: 0.0250, bbox_mAP_s: 0.1220, bbox_mAP_m: 0.0200, bbox_mAP_l: 0.0330, bbox_mAP_copypaste: 0.026 0.048 0.025 0.122 0.020 0.033, segm_mAP: 0.0260, segm_mAP_50: 0.0470, segm_mAP_75: 0.0250, segm_mAP_s: 0.1210, segm_mAP_m: 0.0200, segm_mAP_l: 0.0340, segm_mAP_copypaste: 0.026 0.047 0.025 0.121 0.020 0.034\n",
      "2025-07-16 20:32:35,071 - mmdet - INFO - Epoch [60][50/750]\tlr: 2.500e-06, eta: 0:28:17, time: 0.416, data_time: 0.053, memory: 11264, stage0_loss_cls: 0.9030, stage0_pos_acc: 45.3174, stage0_loss_bbox: 0.3857, stage0_loss_iou: 0.8124, stage0_loss_mask: 0.5998, stage1_loss_cls: 0.4411, stage1_pos_acc: 82.3325, stage1_loss_bbox: 0.1657, stage1_loss_iou: 0.3605, stage1_loss_mask: 0.3718, stage2_loss_cls: 0.2888, stage2_pos_acc: 89.9970, stage2_loss_bbox: 0.1376, stage2_loss_iou: 0.2881, stage2_loss_mask: 0.3647, stage3_loss_cls: 0.1752, stage3_pos_acc: 93.3196, stage3_loss_bbox: 0.1291, stage3_loss_iou: 0.2673, stage3_loss_mask: 0.3354, stage4_loss_cls: 0.1197, stage4_pos_acc: 96.0359, stage4_loss_bbox: 0.1239, stage4_loss_iou: 0.2552, stage4_loss_mask: 0.3276, stage5_loss_cls: 0.0962, stage5_pos_acc: 97.8424, stage5_loss_bbox: 0.1220, stage5_loss_iou: 0.2506, stage5_loss_mask: 0.3173, loss: 7.6385\n",
      "2025-07-16 20:32:53,402 - mmdet - INFO - Epoch [60][100/750]\tlr: 2.500e-06, eta: 0:27:58, time: 0.367, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9633, stage0_pos_acc: 44.5081, stage0_loss_bbox: 0.3374, stage0_loss_iou: 0.8274, stage0_loss_mask: 0.4312, stage1_loss_cls: 0.3994, stage1_pos_acc: 89.7609, stage1_loss_bbox: 0.1120, stage1_loss_iou: 0.2973, stage1_loss_mask: 0.2486, stage2_loss_cls: 0.2179, stage2_pos_acc: 95.2547, stage2_loss_bbox: 0.0885, stage2_loss_iou: 0.2081, stage2_loss_mask: 0.2085, stage3_loss_cls: 0.0855, stage3_pos_acc: 99.0436, stage3_loss_bbox: 0.0770, stage3_loss_iou: 0.1797, stage3_loss_mask: 0.1628, stage4_loss_cls: 0.0468, stage4_pos_acc: 99.7778, stage4_loss_bbox: 0.0738, stage4_loss_iou: 0.1722, stage4_loss_mask: 0.1556, stage5_loss_cls: 0.0312, stage5_pos_acc: 99.5333, stage5_loss_bbox: 0.0735, stage5_loss_iou: 0.1681, stage5_loss_mask: 0.1586, loss: 5.7245\n",
      "2025-07-16 20:33:11,901 - mmdet - INFO - Epoch [60][150/750]\tlr: 2.500e-06, eta: 0:27:39, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9085, stage0_pos_acc: 43.7206, stage0_loss_bbox: 0.3517, stage0_loss_iou: 0.7895, stage0_loss_mask: 0.4394, stage1_loss_cls: 0.4066, stage1_pos_acc: 85.3329, stage1_loss_bbox: 0.1468, stage1_loss_iou: 0.2966, stage1_loss_mask: 0.2686, stage2_loss_cls: 0.2424, stage2_pos_acc: 92.2211, stage2_loss_bbox: 0.1198, stage2_loss_iou: 0.2331, stage2_loss_mask: 0.2543, stage3_loss_cls: 0.1157, stage3_pos_acc: 97.6123, stage3_loss_bbox: 0.1150, stage3_loss_iou: 0.2165, stage3_loss_mask: 0.2522, stage4_loss_cls: 0.0752, stage4_pos_acc: 98.8562, stage4_loss_bbox: 0.1118, stage4_loss_iou: 0.2119, stage4_loss_mask: 0.2428, stage5_loss_cls: 0.0543, stage5_pos_acc: 99.4620, stage5_loss_bbox: 0.1138, stage5_loss_iou: 0.2090, stage5_loss_mask: 0.2360, loss: 6.4115\n",
      "2025-07-16 20:33:30,421 - mmdet - INFO - Epoch [60][200/750]\tlr: 2.500e-06, eta: 0:27:20, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.8796, stage0_pos_acc: 47.3997, stage0_loss_bbox: 0.3624, stage0_loss_iou: 0.8466, stage0_loss_mask: 0.5904, stage1_loss_cls: 0.3917, stage1_pos_acc: 84.5605, stage1_loss_bbox: 0.1628, stage1_loss_iou: 0.3332, stage1_loss_mask: 0.3562, stage2_loss_cls: 0.2339, stage2_pos_acc: 92.2872, stage2_loss_bbox: 0.1348, stage2_loss_iou: 0.2557, stage2_loss_mask: 0.3187, stage3_loss_cls: 0.1612, stage3_pos_acc: 94.9245, stage3_loss_bbox: 0.1158, stage3_loss_iou: 0.2335, stage3_loss_mask: 0.2697, stage4_loss_cls: 0.0964, stage4_pos_acc: 98.5812, stage4_loss_bbox: 0.1159, stage4_loss_iou: 0.2316, stage4_loss_mask: 0.2793, stage5_loss_cls: 0.0792, stage5_pos_acc: 99.1891, stage5_loss_bbox: 0.1070, stage5_loss_iou: 0.2246, stage5_loss_mask: 0.2803, loss: 7.0604\n",
      "2025-07-16 20:33:49,211 - mmdet - INFO - Epoch [60][250/750]\tlr: 2.500e-06, eta: 0:27:01, time: 0.376, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9140, stage0_pos_acc: 47.9515, stage0_loss_bbox: 0.3867, stage0_loss_iou: 0.8029, stage0_loss_mask: 0.4420, stage1_loss_cls: 0.4135, stage1_pos_acc: 85.1281, stage1_loss_bbox: 0.1597, stage1_loss_iou: 0.2915, stage1_loss_mask: 0.2755, stage2_loss_cls: 0.2404, stage2_pos_acc: 92.9515, stage2_loss_bbox: 0.1327, stage2_loss_iou: 0.2193, stage2_loss_mask: 0.2411, stage3_loss_cls: 0.1327, stage3_pos_acc: 97.7243, stage3_loss_bbox: 0.0993, stage3_loss_iou: 0.2123, stage3_loss_mask: 0.2302, stage4_loss_cls: 0.0766, stage4_pos_acc: 98.8348, stage4_loss_bbox: 0.0926, stage4_loss_iou: 0.2027, stage4_loss_mask: 0.2216, stage5_loss_cls: 0.0604, stage5_pos_acc: 99.2833, stage5_loss_bbox: 0.0924, stage5_loss_iou: 0.1949, stage5_loss_mask: 0.2094, loss: 6.3445\n",
      "2025-07-16 20:34:08,007 - mmdet - INFO - Epoch [60][300/750]\tlr: 2.500e-06, eta: 0:26:42, time: 0.376, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9041, stage0_pos_acc: 48.5286, stage0_loss_bbox: 0.3563, stage0_loss_iou: 0.8840, stage0_loss_mask: 0.4857, stage1_loss_cls: 0.4025, stage1_pos_acc: 86.0722, stage1_loss_bbox: 0.1237, stage1_loss_iou: 0.3080, stage1_loss_mask: 0.2504, stage2_loss_cls: 0.2194, stage2_pos_acc: 95.4556, stage2_loss_bbox: 0.0992, stage2_loss_iou: 0.2290, stage2_loss_mask: 0.2343, stage3_loss_cls: 0.1103, stage3_pos_acc: 98.2008, stage3_loss_bbox: 0.0918, stage3_loss_iou: 0.2053, stage3_loss_mask: 0.2253, stage4_loss_cls: 0.0654, stage4_pos_acc: 99.7000, stage4_loss_bbox: 0.0876, stage4_loss_iou: 0.1979, stage4_loss_mask: 0.2182, stage5_loss_cls: 0.0547, stage5_pos_acc: 99.7143, stage5_loss_bbox: 0.0843, stage5_loss_iou: 0.1942, stage5_loss_mask: 0.2142, loss: 6.2457\n",
      "2025-07-16 20:34:26,494 - mmdet - INFO - Epoch [60][350/750]\tlr: 2.500e-06, eta: 0:26:23, time: 0.370, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9354, stage0_pos_acc: 41.5706, stage0_loss_bbox: 0.3481, stage0_loss_iou: 0.8238, stage0_loss_mask: 0.3660, stage1_loss_cls: 0.4133, stage1_pos_acc: 81.2984, stage1_loss_bbox: 0.1194, stage1_loss_iou: 0.2966, stage1_loss_mask: 0.2469, stage2_loss_cls: 0.2253, stage2_pos_acc: 94.0952, stage2_loss_bbox: 0.0929, stage2_loss_iou: 0.2249, stage2_loss_mask: 0.2269, stage3_loss_cls: 0.1101, stage3_pos_acc: 95.5619, stage3_loss_bbox: 0.0831, stage3_loss_iou: 0.1979, stage3_loss_mask: 0.2175, stage4_loss_cls: 0.0703, stage4_pos_acc: 97.5619, stage4_loss_bbox: 0.0750, stage4_loss_iou: 0.1843, stage4_loss_mask: 0.2036, stage5_loss_cls: 0.0525, stage5_pos_acc: 99.0952, stage5_loss_bbox: 0.0723, stage5_loss_iou: 0.1799, stage5_loss_mask: 0.2132, loss: 5.9790\n",
      "2025-07-16 20:34:45,462 - mmdet - INFO - Epoch [60][400/750]\tlr: 2.500e-06, eta: 0:26:03, time: 0.379, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9300, stage0_pos_acc: 42.5504, stage0_loss_bbox: 0.3820, stage0_loss_iou: 0.8651, stage0_loss_mask: 0.4375, stage1_loss_cls: 0.4197, stage1_pos_acc: 84.0170, stage1_loss_bbox: 0.1319, stage1_loss_iou: 0.2898, stage1_loss_mask: 0.2420, stage2_loss_cls: 0.2311, stage2_pos_acc: 93.3509, stage2_loss_bbox: 0.1061, stage2_loss_iou: 0.2091, stage2_loss_mask: 0.2130, stage3_loss_cls: 0.1073, stage3_pos_acc: 96.0842, stage3_loss_bbox: 0.0962, stage3_loss_iou: 0.1841, stage3_loss_mask: 0.1754, stage4_loss_cls: 0.0670, stage4_pos_acc: 98.2947, stage4_loss_bbox: 0.0926, stage4_loss_iou: 0.1784, stage4_loss_mask: 0.1651, stage5_loss_cls: 0.0455, stage5_pos_acc: 99.0947, stage5_loss_bbox: 0.0903, stage5_loss_iou: 0.1745, stage5_loss_mask: 0.1704, loss: 6.0039\n",
      "2025-07-16 20:35:05,021 - mmdet - INFO - Epoch [60][450/750]\tlr: 2.500e-06, eta: 0:25:44, time: 0.391, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8826, stage0_pos_acc: 46.1004, stage0_loss_bbox: 0.3405, stage0_loss_iou: 0.8105, stage0_loss_mask: 0.5087, stage1_loss_cls: 0.4021, stage1_pos_acc: 82.5215, stage1_loss_bbox: 0.1391, stage1_loss_iou: 0.3184, stage1_loss_mask: 0.3217, stage2_loss_cls: 0.2582, stage2_pos_acc: 94.8729, stage2_loss_bbox: 0.1127, stage2_loss_iou: 0.2422, stage2_loss_mask: 0.2879, stage3_loss_cls: 0.1352, stage3_pos_acc: 96.9760, stage3_loss_bbox: 0.1006, stage3_loss_iou: 0.2196, stage3_loss_mask: 0.2662, stage4_loss_cls: 0.0866, stage4_pos_acc: 99.1022, stage4_loss_bbox: 0.0975, stage4_loss_iou: 0.2122, stage4_loss_mask: 0.2707, stage5_loss_cls: 0.0733, stage5_pos_acc: 99.3381, stage5_loss_bbox: 0.0946, stage5_loss_iou: 0.2059, stage5_loss_mask: 0.2677, loss: 6.6548\n",
      "2025-07-16 20:35:24,175 - mmdet - INFO - Epoch [60][500/750]\tlr: 2.500e-06, eta: 0:25:25, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9581, stage0_pos_acc: 38.7843, stage0_loss_bbox: 0.3669, stage0_loss_iou: 0.9147, stage0_loss_mask: 0.6649, stage1_loss_cls: 0.4149, stage1_pos_acc: 81.6135, stage1_loss_bbox: 0.1341, stage1_loss_iou: 0.3853, stage1_loss_mask: 0.3640, stage2_loss_cls: 0.2500, stage2_pos_acc: 92.7191, stage2_loss_bbox: 0.1055, stage2_loss_iou: 0.2836, stage2_loss_mask: 0.3408, stage3_loss_cls: 0.1461, stage3_pos_acc: 96.2432, stage3_loss_bbox: 0.0919, stage3_loss_iou: 0.2467, stage3_loss_mask: 0.2960, stage4_loss_cls: 0.0996, stage4_pos_acc: 97.8932, stage4_loss_bbox: 0.0879, stage4_loss_iou: 0.2327, stage4_loss_mask: 0.3028, stage5_loss_cls: 0.0778, stage5_pos_acc: 98.3766, stage5_loss_bbox: 0.0868, stage5_loss_iou: 0.2274, stage5_loss_mask: 0.3164, loss: 7.3946\n",
      "2025-07-16 20:35:43,140 - mmdet - INFO - Epoch [60][550/750]\tlr: 2.500e-06, eta: 0:25:06, time: 0.379, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9471, stage0_pos_acc: 39.1738, stage0_loss_bbox: 0.3516, stage0_loss_iou: 0.7227, stage0_loss_mask: 0.3849, stage1_loss_cls: 0.4219, stage1_pos_acc: 81.2262, stage1_loss_bbox: 0.1407, stage1_loss_iou: 0.2826, stage1_loss_mask: 0.2711, stage2_loss_cls: 0.2501, stage2_pos_acc: 89.8381, stage2_loss_bbox: 0.1123, stage2_loss_iou: 0.2235, stage2_loss_mask: 0.2605, stage3_loss_cls: 0.1495, stage3_pos_acc: 97.1119, stage3_loss_bbox: 0.1005, stage3_loss_iou: 0.1984, stage3_loss_mask: 0.2430, stage4_loss_cls: 0.0797, stage4_pos_acc: 97.8167, stage4_loss_bbox: 0.0957, stage4_loss_iou: 0.1947, stage4_loss_mask: 0.2509, stage5_loss_cls: 0.0652, stage5_pos_acc: 98.4667, stage5_loss_bbox: 0.0929, stage5_loss_iou: 0.1869, stage5_loss_mask: 0.2374, loss: 6.2638\n",
      "2025-07-16 20:36:02,281 - mmdet - INFO - Epoch [60][600/750]\tlr: 2.500e-06, eta: 0:24:47, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9005, stage0_pos_acc: 46.5508, stage0_loss_bbox: 0.3716, stage0_loss_iou: 0.8223, stage0_loss_mask: 0.4710, stage1_loss_cls: 0.4142, stage1_pos_acc: 79.6438, stage1_loss_bbox: 0.1302, stage1_loss_iou: 0.2923, stage1_loss_mask: 0.2565, stage2_loss_cls: 0.2536, stage2_pos_acc: 90.5248, stage2_loss_bbox: 0.1020, stage2_loss_iou: 0.2245, stage2_loss_mask: 0.2394, stage3_loss_cls: 0.1330, stage3_pos_acc: 96.1899, stage3_loss_bbox: 0.0922, stage3_loss_iou: 0.2012, stage3_loss_mask: 0.2377, stage4_loss_cls: 0.0851, stage4_pos_acc: 97.0173, stage4_loss_bbox: 0.0925, stage4_loss_iou: 0.1954, stage4_loss_mask: 0.2298, stage5_loss_cls: 0.0710, stage5_pos_acc: 98.0920, stage5_loss_bbox: 0.0912, stage5_loss_iou: 0.1942, stage5_loss_mask: 0.2213, loss: 6.3225\n",
      "2025-07-16 20:36:21,355 - mmdet - INFO - Epoch [60][650/750]\tlr: 2.500e-06, eta: 0:24:28, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9031, stage0_pos_acc: 45.9063, stage0_loss_bbox: 0.3555, stage0_loss_iou: 0.7726, stage0_loss_mask: 0.5244, stage1_loss_cls: 0.4026, stage1_pos_acc: 86.3325, stage1_loss_bbox: 0.1553, stage1_loss_iou: 0.3140, stage1_loss_mask: 0.3534, stage2_loss_cls: 0.2370, stage2_pos_acc: 93.0802, stage2_loss_bbox: 0.1221, stage2_loss_iou: 0.2474, stage2_loss_mask: 0.3243, stage3_loss_cls: 0.1221, stage3_pos_acc: 98.2103, stage3_loss_bbox: 0.1168, stage3_loss_iou: 0.2417, stage3_loss_mask: 0.3190, stage4_loss_cls: 0.0772, stage4_pos_acc: 99.4405, stage4_loss_bbox: 0.1104, stage4_loss_iou: 0.2350, stage4_loss_mask: 0.3116, stage5_loss_cls: 0.0584, stage5_pos_acc: 99.8214, stage5_loss_bbox: 0.1111, stage5_loss_iou: 0.2322, stage5_loss_mask: 0.3064, loss: 6.9534\n",
      "2025-07-16 20:36:40,129 - mmdet - INFO - Epoch [60][700/750]\tlr: 2.500e-06, eta: 0:24:09, time: 0.375, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9076, stage0_pos_acc: 42.5106, stage0_loss_bbox: 0.3585, stage0_loss_iou: 0.8470, stage0_loss_mask: 0.6834, stage1_loss_cls: 0.4027, stage1_pos_acc: 83.3680, stage1_loss_bbox: 0.1322, stage1_loss_iou: 0.3353, stage1_loss_mask: 0.3567, stage2_loss_cls: 0.2318, stage2_pos_acc: 90.6763, stage2_loss_bbox: 0.1028, stage2_loss_iou: 0.2570, stage2_loss_mask: 0.3185, stage3_loss_cls: 0.1178, stage3_pos_acc: 97.1774, stage3_loss_bbox: 0.0897, stage3_loss_iou: 0.2260, stage3_loss_mask: 0.2982, stage4_loss_cls: 0.0680, stage4_pos_acc: 97.2195, stage4_loss_bbox: 0.0864, stage4_loss_iou: 0.2199, stage4_loss_mask: 0.2911, stage5_loss_cls: 0.0513, stage5_pos_acc: 98.7076, stage5_loss_bbox: 0.0847, stage5_loss_iou: 0.2161, stage5_loss_mask: 0.2941, loss: 6.9769\n",
      "2025-07-16 20:36:58,572 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:36:58,572 - mmdet - INFO - Epoch [60][750/750]\tlr: 2.500e-06, eta: 0:23:50, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9198, stage0_pos_acc: 43.7550, stage0_loss_bbox: 0.3495, stage0_loss_iou: 0.7971, stage0_loss_mask: 0.3942, stage1_loss_cls: 0.4169, stage1_pos_acc: 84.3279, stage1_loss_bbox: 0.1381, stage1_loss_iou: 0.2971, stage1_loss_mask: 0.2211, stage2_loss_cls: 0.2542, stage2_pos_acc: 91.9406, stage2_loss_bbox: 0.1114, stage2_loss_iou: 0.2169, stage2_loss_mask: 0.1879, stage3_loss_cls: 0.1236, stage3_pos_acc: 97.0167, stage3_loss_bbox: 0.1005, stage3_loss_iou: 0.1955, stage3_loss_mask: 0.1849, stage4_loss_cls: 0.0710, stage4_pos_acc: 98.3778, stage4_loss_bbox: 0.0997, stage4_loss_iou: 0.1905, stage4_loss_mask: 0.1914, stage5_loss_cls: 0.0630, stage5_pos_acc: 98.8778, stage5_loss_bbox: 0.0955, stage5_loss_iou: 0.1817, stage5_loss_mask: 0.1833, loss: 5.9852\n",
      "2025-07-16 20:36:58,704 - mmdet - INFO - Saving checkpoint at 60 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 103s, ETA:     0s2025-07-16 20:40:18,547 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.051\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.123\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.013\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.035\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.175\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.227\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.429\n",
      "2025-07-16 20:40:20,386 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.33s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.77s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.014\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.383\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.390\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.204\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.237\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.426\n",
      "2025-07-16 20:40:23,452 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:40:23,452 - mmdet - INFO - Epoch(val) [60][750]\tbbox_mAP: 0.0270, bbox_mAP_50: 0.0510, bbox_mAP_75: 0.0270, bbox_mAP_s: 0.1230, bbox_mAP_m: 0.0130, bbox_mAP_l: 0.0350, bbox_mAP_copypaste: 0.027 0.051 0.027 0.123 0.013 0.035, segm_mAP: 0.0260, segm_mAP_50: 0.0480, segm_mAP_75: 0.0240, segm_mAP_s: 0.1210, segm_mAP_m: 0.0140, segm_mAP_l: 0.0340, segm_mAP_copypaste: 0.026 0.048 0.024 0.121 0.014 0.034\n",
      "2025-07-16 20:40:45,001 - mmdet - INFO - Epoch [61][50/750]\tlr: 2.500e-06, eta: 0:23:31, time: 0.431, data_time: 0.057, memory: 11264, stage0_loss_cls: 0.8855, stage0_pos_acc: 44.4183, stage0_loss_bbox: 0.3249, stage0_loss_iou: 0.7362, stage0_loss_mask: 0.4642, stage1_loss_cls: 0.4248, stage1_pos_acc: 84.1427, stage1_loss_bbox: 0.1424, stage1_loss_iou: 0.2964, stage1_loss_mask: 0.2675, stage2_loss_cls: 0.2896, stage2_pos_acc: 90.0480, stage2_loss_bbox: 0.1153, stage2_loss_iou: 0.2362, stage2_loss_mask: 0.2504, stage3_loss_cls: 0.1430, stage3_pos_acc: 95.9656, stage3_loss_bbox: 0.1059, stage3_loss_iou: 0.2127, stage3_loss_mask: 0.2310, stage4_loss_cls: 0.0745, stage4_pos_acc: 97.7480, stage4_loss_bbox: 0.1018, stage4_loss_iou: 0.2071, stage4_loss_mask: 0.2319, stage5_loss_cls: 0.0577, stage5_pos_acc: 99.3575, stage5_loss_bbox: 0.1015, stage5_loss_iou: 0.2052, stage5_loss_mask: 0.2335, loss: 6.3392\n",
      "2025-07-16 20:41:03,983 - mmdet - INFO - Epoch [61][100/750]\tlr: 2.500e-06, eta: 0:23:12, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9005, stage0_pos_acc: 42.4665, stage0_loss_bbox: 0.3473, stage0_loss_iou: 0.8317, stage0_loss_mask: 0.3426, stage1_loss_cls: 0.4036, stage1_pos_acc: 79.9009, stage1_loss_bbox: 0.1295, stage1_loss_iou: 0.2915, stage1_loss_mask: 0.2389, stage2_loss_cls: 0.2271, stage2_pos_acc: 91.9066, stage2_loss_bbox: 0.1023, stage2_loss_iou: 0.2155, stage2_loss_mask: 0.2107, stage3_loss_cls: 0.1142, stage3_pos_acc: 97.4048, stage3_loss_bbox: 0.0917, stage3_loss_iou: 0.1984, stage3_loss_mask: 0.1893, stage4_loss_cls: 0.0703, stage4_pos_acc: 99.2333, stage4_loss_bbox: 0.0895, stage4_loss_iou: 0.1907, stage4_loss_mask: 0.1801, stage5_loss_cls: 0.0539, stage5_pos_acc: 99.4667, stage5_loss_bbox: 0.0890, stage5_loss_iou: 0.1868, stage5_loss_mask: 0.1828, loss: 5.8781\n",
      "2025-07-16 20:41:23,035 - mmdet - INFO - Epoch [61][150/750]\tlr: 2.500e-06, eta: 0:22:53, time: 0.381, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9302, stage0_pos_acc: 38.5772, stage0_loss_bbox: 0.3872, stage0_loss_iou: 0.8838, stage0_loss_mask: 0.6412, stage1_loss_cls: 0.4001, stage1_pos_acc: 84.5119, stage1_loss_bbox: 0.1399, stage1_loss_iou: 0.3801, stage1_loss_mask: 0.4517, stage2_loss_cls: 0.2347, stage2_pos_acc: 89.8476, stage2_loss_bbox: 0.1175, stage2_loss_iou: 0.2921, stage2_loss_mask: 0.4216, stage3_loss_cls: 0.1186, stage3_pos_acc: 96.6184, stage3_loss_bbox: 0.1038, stage3_loss_iou: 0.2638, stage3_loss_mask: 0.4155, stage4_loss_cls: 0.0697, stage4_pos_acc: 99.3611, stage4_loss_bbox: 0.0974, stage4_loss_iou: 0.2515, stage4_loss_mask: 0.3856, stage5_loss_cls: 0.0507, stage5_pos_acc: 99.2778, stage5_loss_bbox: 0.0956, stage5_loss_iou: 0.2485, stage5_loss_mask: 0.4015, loss: 7.7823\n",
      "2025-07-16 20:41:41,754 - mmdet - INFO - Epoch [61][200/750]\tlr: 2.500e-06, eta: 0:22:34, time: 0.374, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9171, stage0_pos_acc: 42.6566, stage0_loss_bbox: 0.3432, stage0_loss_iou: 0.8560, stage0_loss_mask: 0.6124, stage1_loss_cls: 0.4354, stage1_pos_acc: 83.2724, stage1_loss_bbox: 0.1287, stage1_loss_iou: 0.3489, stage1_loss_mask: 0.3118, stage2_loss_cls: 0.2536, stage2_pos_acc: 91.0811, stage2_loss_bbox: 0.1056, stage2_loss_iou: 0.2835, stage2_loss_mask: 0.2783, stage3_loss_cls: 0.1278, stage3_pos_acc: 95.6483, stage3_loss_bbox: 0.0969, stage3_loss_iou: 0.2589, stage3_loss_mask: 0.2825, stage4_loss_cls: 0.0948, stage4_pos_acc: 98.3947, stage4_loss_bbox: 0.0936, stage4_loss_iou: 0.2501, stage4_loss_mask: 0.2771, stage5_loss_cls: 0.0810, stage5_pos_acc: 99.0530, stage5_loss_bbox: 0.0958, stage5_loss_iou: 0.2492, stage5_loss_mask: 0.2890, loss: 7.0712\n",
      "2025-07-16 20:42:00,354 - mmdet - INFO - Epoch [61][250/750]\tlr: 2.500e-06, eta: 0:22:15, time: 0.372, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9482, stage0_pos_acc: 43.8991, stage0_loss_bbox: 0.3499, stage0_loss_iou: 0.8815, stage0_loss_mask: 0.4909, stage1_loss_cls: 0.4307, stage1_pos_acc: 87.7121, stage1_loss_bbox: 0.1210, stage1_loss_iou: 0.3193, stage1_loss_mask: 0.3091, stage2_loss_cls: 0.2400, stage2_pos_acc: 91.6149, stage2_loss_bbox: 0.0951, stage2_loss_iou: 0.2343, stage2_loss_mask: 0.2803, stage3_loss_cls: 0.1146, stage3_pos_acc: 97.3159, stage3_loss_bbox: 0.0893, stage3_loss_iou: 0.2151, stage3_loss_mask: 0.2531, stage4_loss_cls: 0.0823, stage4_pos_acc: 99.0015, stage4_loss_bbox: 0.0836, stage4_loss_iou: 0.2047, stage4_loss_mask: 0.2435, stage5_loss_cls: 0.0608, stage5_pos_acc: 99.3409, stage5_loss_bbox: 0.0817, stage5_loss_iou: 0.1960, stage5_loss_mask: 0.2352, loss: 6.5600\n",
      "2025-07-16 20:42:19,242 - mmdet - INFO - Epoch [61][300/750]\tlr: 2.500e-06, eta: 0:21:56, time: 0.378, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9062, stage0_pos_acc: 47.4306, stage0_loss_bbox: 0.3948, stage0_loss_iou: 0.8343, stage0_loss_mask: 0.5827, stage1_loss_cls: 0.4101, stage1_pos_acc: 86.1088, stage1_loss_bbox: 0.1814, stage1_loss_iou: 0.3439, stage1_loss_mask: 0.3425, stage2_loss_cls: 0.2406, stage2_pos_acc: 93.2121, stage2_loss_bbox: 0.1544, stage2_loss_iou: 0.2516, stage2_loss_mask: 0.3045, stage3_loss_cls: 0.1264, stage3_pos_acc: 97.2852, stage3_loss_bbox: 0.1186, stage3_loss_iou: 0.2296, stage3_loss_mask: 0.3051, stage4_loss_cls: 0.0747, stage4_pos_acc: 97.6551, stage4_loss_bbox: 0.1121, stage4_loss_iou: 0.2182, stage4_loss_mask: 0.2963, stage5_loss_cls: 0.0678, stage5_pos_acc: 98.6816, stage5_loss_bbox: 0.1005, stage5_loss_iou: 0.2138, stage5_loss_mask: 0.2838, loss: 7.0938\n",
      "2025-07-16 20:42:38,366 - mmdet - INFO - Epoch [61][350/750]\tlr: 2.500e-06, eta: 0:21:37, time: 0.382, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8846, stage0_pos_acc: 47.0455, stage0_loss_bbox: 0.3741, stage0_loss_iou: 0.8343, stage0_loss_mask: 0.7017, stage1_loss_cls: 0.3815, stage1_pos_acc: 85.3249, stage1_loss_bbox: 0.1467, stage1_loss_iou: 0.3495, stage1_loss_mask: 0.3862, stage2_loss_cls: 0.2461, stage2_pos_acc: 92.7014, stage2_loss_bbox: 0.1142, stage2_loss_iou: 0.2570, stage2_loss_mask: 0.3292, stage3_loss_cls: 0.1166, stage3_pos_acc: 96.6989, stage3_loss_bbox: 0.1065, stage3_loss_iou: 0.2373, stage3_loss_mask: 0.3207, stage4_loss_cls: 0.0763, stage4_pos_acc: 99.0260, stage4_loss_bbox: 0.1012, stage4_loss_iou: 0.2252, stage4_loss_mask: 0.3096, stage5_loss_cls: 0.0570, stage5_pos_acc: 99.2797, stage5_loss_bbox: 0.1020, stage5_loss_iou: 0.2283, stage5_loss_mask: 0.3314, loss: 7.2168\n",
      "2025-07-16 20:42:57,384 - mmdet - INFO - Epoch [61][400/750]\tlr: 2.500e-06, eta: 0:21:18, time: 0.380, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9447, stage0_pos_acc: 41.3079, stage0_loss_bbox: 0.3535, stage0_loss_iou: 0.8363, stage0_loss_mask: 0.3496, stage1_loss_cls: 0.4021, stage1_pos_acc: 88.5563, stage1_loss_bbox: 0.1219, stage1_loss_iou: 0.2661, stage1_loss_mask: 0.1649, stage2_loss_cls: 0.2243, stage2_pos_acc: 94.9893, stage2_loss_bbox: 0.0902, stage2_loss_iou: 0.1797, stage2_loss_mask: 0.1391, stage3_loss_cls: 0.1112, stage3_pos_acc: 97.5500, stage3_loss_bbox: 0.0754, stage3_loss_iou: 0.1473, stage3_loss_mask: 0.1225, stage4_loss_cls: 0.0456, stage4_pos_acc: 99.2500, stage4_loss_bbox: 0.0791, stage4_loss_iou: 0.1570, stage4_loss_mask: 0.1634, stage5_loss_cls: 0.0315, stage5_pos_acc: 99.5000, stage5_loss_bbox: 0.0791, stage5_loss_iou: 0.1560, stage5_loss_mask: 0.1595, loss: 5.4000\n",
      "2025-07-16 20:43:15,801 - mmdet - INFO - Epoch [61][450/750]\tlr: 2.500e-06, eta: 0:20:58, time: 0.368, data_time: 0.007, memory: 11264, stage0_loss_cls: 0.9156, stage0_pos_acc: 38.9071, stage0_loss_bbox: 0.3280, stage0_loss_iou: 0.7570, stage0_loss_mask: 0.2889, stage1_loss_cls: 0.3903, stage1_pos_acc: 86.6024, stage1_loss_bbox: 0.1268, stage1_loss_iou: 0.2417, stage1_loss_mask: 0.1977, stage2_loss_cls: 0.2098, stage2_pos_acc: 93.6190, stage2_loss_bbox: 0.0991, stage2_loss_iou: 0.1891, stage2_loss_mask: 0.1808, stage3_loss_cls: 0.0913, stage3_pos_acc: 98.9500, stage3_loss_bbox: 0.0895, stage3_loss_iou: 0.1727, stage3_loss_mask: 0.1870, stage4_loss_cls: 0.0451, stage4_pos_acc: 99.6667, stage4_loss_bbox: 0.0824, stage4_loss_iou: 0.1633, stage4_loss_mask: 0.1816, stage5_loss_cls: 0.0278, stage5_pos_acc: 100.0000, stage5_loss_bbox: 0.0814, stage5_loss_iou: 0.1602, stage5_loss_mask: 0.1713, loss: 5.3784\n",
      "2025-07-16 20:43:34,273 - mmdet - INFO - Epoch [61][500/750]\tlr: 2.500e-06, eta: 0:20:39, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9163, stage0_pos_acc: 42.8309, stage0_loss_bbox: 0.3670, stage0_loss_iou: 0.7697, stage0_loss_mask: 0.4459, stage1_loss_cls: 0.3874, stage1_pos_acc: 86.5784, stage1_loss_bbox: 0.1476, stage1_loss_iou: 0.2957, stage1_loss_mask: 0.2764, stage2_loss_cls: 0.2351, stage2_pos_acc: 92.1317, stage2_loss_bbox: 0.1211, stage2_loss_iou: 0.2234, stage2_loss_mask: 0.2474, stage3_loss_cls: 0.1172, stage3_pos_acc: 95.1889, stage3_loss_bbox: 0.1012, stage3_loss_iou: 0.2034, stage3_loss_mask: 0.2127, stage4_loss_cls: 0.0723, stage4_pos_acc: 97.6895, stage4_loss_bbox: 0.1012, stage4_loss_iou: 0.1993, stage4_loss_mask: 0.1945, stage5_loss_cls: 0.0523, stage5_pos_acc: 98.8228, stage5_loss_bbox: 0.0981, stage5_loss_iou: 0.1989, stage5_loss_mask: 0.1941, loss: 6.1783\n",
      "2025-07-16 20:43:53,072 - mmdet - INFO - Epoch [61][550/750]\tlr: 2.500e-06, eta: 0:20:20, time: 0.376, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8974, stage0_pos_acc: 50.9748, stage0_loss_bbox: 0.3501, stage0_loss_iou: 0.8642, stage0_loss_mask: 0.6169, stage1_loss_cls: 0.4109, stage1_pos_acc: 85.8669, stage1_loss_bbox: 0.1370, stage1_loss_iou: 0.3541, stage1_loss_mask: 0.3334, stage2_loss_cls: 0.2569, stage2_pos_acc: 93.2715, stage2_loss_bbox: 0.1036, stage2_loss_iou: 0.2494, stage2_loss_mask: 0.2673, stage3_loss_cls: 0.1217, stage3_pos_acc: 96.6346, stage3_loss_bbox: 0.0938, stage3_loss_iou: 0.2282, stage3_loss_mask: 0.2563, stage4_loss_cls: 0.0930, stage4_pos_acc: 98.2651, stage4_loss_bbox: 0.0901, stage4_loss_iou: 0.2179, stage4_loss_mask: 0.2522, stage5_loss_cls: 0.0823, stage5_pos_acc: 98.1175, stage5_loss_bbox: 0.0861, stage5_loss_iou: 0.2125, stage5_loss_mask: 0.2502, loss: 6.8255\n",
      "2025-07-16 20:44:12,171 - mmdet - INFO - Epoch [61][600/750]\tlr: 2.500e-06, eta: 0:20:01, time: 0.382, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8931, stage0_pos_acc: 41.3219, stage0_loss_bbox: 0.3369, stage0_loss_iou: 0.7523, stage0_loss_mask: 0.4317, stage1_loss_cls: 0.4042, stage1_pos_acc: 83.7520, stage1_loss_bbox: 0.1391, stage1_loss_iou: 0.2908, stage1_loss_mask: 0.2515, stage2_loss_cls: 0.2474, stage2_pos_acc: 93.9206, stage2_loss_bbox: 0.1072, stage2_loss_iou: 0.2211, stage2_loss_mask: 0.2096, stage3_loss_cls: 0.1097, stage3_pos_acc: 97.0474, stage3_loss_bbox: 0.0991, stage3_loss_iou: 0.1989, stage3_loss_mask: 0.2031, stage4_loss_cls: 0.0607, stage4_pos_acc: 99.3681, stage4_loss_bbox: 0.0954, stage4_loss_iou: 0.1841, stage4_loss_mask: 0.1953, stage5_loss_cls: 0.0549, stage5_pos_acc: 99.3681, stage5_loss_bbox: 0.0942, stage5_loss_iou: 0.1814, stage5_loss_mask: 0.1933, loss: 5.9549\n",
      "2025-07-16 20:44:31,006 - mmdet - INFO - Epoch [61][650/750]\tlr: 2.500e-06, eta: 0:19:42, time: 0.377, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9328, stage0_pos_acc: 49.0862, stage0_loss_bbox: 0.3576, stage0_loss_iou: 0.8036, stage0_loss_mask: 0.3565, stage1_loss_cls: 0.4170, stage1_pos_acc: 86.7528, stage1_loss_bbox: 0.1306, stage1_loss_iou: 0.2766, stage1_loss_mask: 0.1916, stage2_loss_cls: 0.2289, stage2_pos_acc: 95.5396, stage2_loss_bbox: 0.1041, stage2_loss_iou: 0.2001, stage2_loss_mask: 0.1700, stage3_loss_cls: 0.1171, stage3_pos_acc: 97.7547, stage3_loss_bbox: 0.0911, stage3_loss_iou: 0.1798, stage3_loss_mask: 0.1589, stage4_loss_cls: 0.0620, stage4_pos_acc: 98.6973, stage4_loss_bbox: 0.0871, stage4_loss_iou: 0.1714, stage4_loss_mask: 0.1484, stage5_loss_cls: 0.0524, stage5_pos_acc: 98.8384, stage5_loss_bbox: 0.0844, stage5_loss_iou: 0.1643, stage5_loss_mask: 0.1434, loss: 5.6295\n",
      "2025-07-16 20:44:49,891 - mmdet - INFO - Epoch [61][700/750]\tlr: 2.500e-06, eta: 0:19:23, time: 0.378, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9333, stage0_pos_acc: 44.8009, stage0_loss_bbox: 0.3493, stage0_loss_iou: 0.8045, stage0_loss_mask: 0.4050, stage1_loss_cls: 0.4050, stage1_pos_acc: 84.2763, stage1_loss_bbox: 0.1296, stage1_loss_iou: 0.3015, stage1_loss_mask: 0.2310, stage2_loss_cls: 0.2421, stage2_pos_acc: 91.1051, stage2_loss_bbox: 0.1081, stage2_loss_iou: 0.2162, stage2_loss_mask: 0.1961, stage3_loss_cls: 0.1239, stage3_pos_acc: 96.2756, stage3_loss_bbox: 0.1028, stage3_loss_iou: 0.1929, stage3_loss_mask: 0.1987, stage4_loss_cls: 0.0791, stage4_pos_acc: 98.3923, stage4_loss_bbox: 0.1003, stage4_loss_iou: 0.1854, stage4_loss_mask: 0.1895, stage5_loss_cls: 0.0756, stage5_pos_acc: 97.7590, stage5_loss_bbox: 0.0992, stage5_loss_iou: 0.1796, stage5_loss_mask: 0.1914, loss: 6.0402\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 20:48:27,933 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.30s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.403\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.246\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.440\n",
      "2025-07-16 20:48:29,778 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.80s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.048\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.140\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.029\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.401\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.404\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.213\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.274\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.435\n",
      "2025-07-16 20:48:32,937 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:48:32,938 - mmdet - INFO - Epoch(val) [61][750]\tbbox_mAP: 0.0270, bbox_mAP_50: 0.0490, bbox_mAP_75: 0.0260, bbox_mAP_s: 0.1210, bbox_mAP_m: 0.0250, bbox_mAP_l: 0.0320, bbox_mAP_copypaste: 0.027 0.049 0.026 0.121 0.025 0.032, segm_mAP: 0.0260, segm_mAP_50: 0.0480, segm_mAP_75: 0.0240, segm_mAP_s: 0.1400, segm_mAP_m: 0.0290, segm_mAP_l: 0.0320, segm_mAP_copypaste: 0.026 0.048 0.024 0.140 0.029 0.032\n",
      "2025-07-16 20:48:54,773 - mmdet - INFO - Epoch [62][50/750]\tlr: 2.500e-06, eta: 0:18:45, time: 0.436, data_time: 0.056, memory: 11264, stage0_loss_cls: 0.9205, stage0_pos_acc: 46.3569, stage0_loss_bbox: 0.3744, stage0_loss_iou: 0.9982, stage0_loss_mask: 0.8049, stage1_loss_cls: 0.4434, stage1_pos_acc: 80.7895, stage1_loss_bbox: 0.1286, stage1_loss_iou: 0.4121, stage1_loss_mask: 0.4254, stage2_loss_cls: 0.2742, stage2_pos_acc: 90.4364, stage2_loss_bbox: 0.0983, stage2_loss_iou: 0.2978, stage2_loss_mask: 0.4125, stage3_loss_cls: 0.1593, stage3_pos_acc: 95.4618, stage3_loss_bbox: 0.0874, stage3_loss_iou: 0.2630, stage3_loss_mask: 0.3464, stage4_loss_cls: 0.1015, stage4_pos_acc: 98.0301, stage4_loss_bbox: 0.0784, stage4_loss_iou: 0.2488, stage4_loss_mask: 0.3831, stage5_loss_cls: 0.0885, stage5_pos_acc: 98.8333, stage5_loss_bbox: 0.0756, stage5_loss_iou: 0.2459, stage5_loss_mask: 0.3711, loss: 8.0394\n",
      "2025-07-16 20:49:14,021 - mmdet - INFO - Epoch [62][100/750]\tlr: 2.500e-06, eta: 0:18:26, time: 0.385, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9222, stage0_pos_acc: 37.6304, stage0_loss_bbox: 0.3717, stage0_loss_iou: 0.8793, stage0_loss_mask: 0.5412, stage1_loss_cls: 0.4144, stage1_pos_acc: 86.0642, stage1_loss_bbox: 0.1198, stage1_loss_iou: 0.3225, stage1_loss_mask: 0.3057, stage2_loss_cls: 0.2421, stage2_pos_acc: 91.7117, stage2_loss_bbox: 0.0980, stage2_loss_iou: 0.2544, stage2_loss_mask: 0.2931, stage3_loss_cls: 0.1279, stage3_pos_acc: 96.8634, stage3_loss_bbox: 0.0881, stage3_loss_iou: 0.2245, stage3_loss_mask: 0.2693, stage4_loss_cls: 0.0876, stage4_pos_acc: 98.6925, stage4_loss_bbox: 0.0815, stage4_loss_iou: 0.2094, stage4_loss_mask: 0.2718, stage5_loss_cls: 0.0674, stage5_pos_acc: 98.2627, stage5_loss_bbox: 0.0795, stage5_loss_iou: 0.2048, stage5_loss_mask: 0.2701, loss: 6.7463\n",
      "2025-07-16 20:49:33,276 - mmdet - INFO - Epoch [62][150/750]\tlr: 2.500e-06, eta: 0:18:07, time: 0.385, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9572, stage0_pos_acc: 41.1351, stage0_loss_bbox: 0.3590, stage0_loss_iou: 0.8326, stage0_loss_mask: 0.4305, stage1_loss_cls: 0.4146, stage1_pos_acc: 84.3135, stage1_loss_bbox: 0.1309, stage1_loss_iou: 0.2895, stage1_loss_mask: 0.2544, stage2_loss_cls: 0.2420, stage2_pos_acc: 90.7296, stage2_loss_bbox: 0.0994, stage2_loss_iou: 0.2255, stage2_loss_mask: 0.2256, stage3_loss_cls: 0.1141, stage3_pos_acc: 96.5649, stage3_loss_bbox: 0.0906, stage3_loss_iou: 0.2067, stage3_loss_mask: 0.2215, stage4_loss_cls: 0.0672, stage4_pos_acc: 99.3450, stage4_loss_bbox: 0.0871, stage4_loss_iou: 0.1984, stage4_loss_mask: 0.2137, stage5_loss_cls: 0.0520, stage5_pos_acc: 99.6875, stage5_loss_bbox: 0.0851, stage5_loss_iou: 0.1934, stage5_loss_mask: 0.2064, loss: 6.1977\n",
      "2025-07-16 20:49:52,571 - mmdet - INFO - Epoch [62][200/750]\tlr: 2.500e-06, eta: 0:17:48, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9144, stage0_pos_acc: 45.5670, stage0_loss_bbox: 0.3403, stage0_loss_iou: 0.7826, stage0_loss_mask: 0.3737, stage1_loss_cls: 0.3806, stage1_pos_acc: 87.8515, stage1_loss_bbox: 0.1287, stage1_loss_iou: 0.2690, stage1_loss_mask: 0.2473, stage2_loss_cls: 0.2162, stage2_pos_acc: 94.8129, stage2_loss_bbox: 0.1079, stage2_loss_iou: 0.2160, stage2_loss_mask: 0.2178, stage3_loss_cls: 0.1027, stage3_pos_acc: 97.8986, stage3_loss_bbox: 0.0930, stage3_loss_iou: 0.1905, stage3_loss_mask: 0.2230, stage4_loss_cls: 0.0665, stage4_pos_acc: 98.9191, stage4_loss_bbox: 0.0901, stage4_loss_iou: 0.1856, stage4_loss_mask: 0.2171, stage5_loss_cls: 0.0637, stage5_pos_acc: 99.4781, stage5_loss_bbox: 0.0867, stage5_loss_iou: 0.1807, stage5_loss_mask: 0.2148, loss: 5.9090\n",
      "2025-07-16 20:50:12,233 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:50:12,234 - mmdet - INFO - Epoch [62][250/750]\tlr: 2.500e-06, eta: 0:17:29, time: 0.393, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9001, stage0_pos_acc: 45.9804, stage0_loss_bbox: 0.3739, stage0_loss_iou: 0.8193, stage0_loss_mask: 0.5661, stage1_loss_cls: 0.4266, stage1_pos_acc: 82.7471, stage1_loss_bbox: 0.1602, stage1_loss_iou: 0.3217, stage1_loss_mask: 0.2693, stage2_loss_cls: 0.2628, stage2_pos_acc: 91.5919, stage2_loss_bbox: 0.1267, stage2_loss_iou: 0.2453, stage2_loss_mask: 0.2194, stage3_loss_cls: 0.1617, stage3_pos_acc: 95.7384, stage3_loss_bbox: 0.1064, stage3_loss_iou: 0.2140, stage3_loss_mask: 0.1932, stage4_loss_cls: 0.0890, stage4_pos_acc: 97.6530, stage4_loss_bbox: 0.1033, stage4_loss_iou: 0.2085, stage4_loss_mask: 0.2008, stage5_loss_cls: 0.0710, stage5_pos_acc: 98.5939, stage5_loss_bbox: 0.0952, stage5_loss_iou: 0.2027, stage5_loss_mask: 0.1854, loss: 6.5227\n",
      "2025-07-16 20:50:31,597 - mmdet - INFO - Epoch [62][300/750]\tlr: 2.500e-06, eta: 0:17:10, time: 0.387, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9380, stage0_pos_acc: 39.8000, stage0_loss_bbox: 0.3848, stage0_loss_iou: 0.9292, stage0_loss_mask: 0.4920, stage1_loss_cls: 0.4067, stage1_pos_acc: 83.1167, stage1_loss_bbox: 0.1271, stage1_loss_iou: 0.3010, stage1_loss_mask: 0.2582, stage2_loss_cls: 0.2090, stage2_pos_acc: 93.9667, stage2_loss_bbox: 0.0991, stage2_loss_iou: 0.2095, stage2_loss_mask: 0.2142, stage3_loss_cls: 0.1025, stage3_pos_acc: 97.2333, stage3_loss_bbox: 0.0897, stage3_loss_iou: 0.1858, stage3_loss_mask: 0.1996, stage4_loss_cls: 0.0594, stage4_pos_acc: 98.9833, stage4_loss_bbox: 0.0855, stage4_loss_iou: 0.1763, stage4_loss_mask: 0.1979, stage5_loss_cls: 0.0493, stage5_pos_acc: 98.5167, stage5_loss_bbox: 0.0852, stage5_loss_iou: 0.1726, stage5_loss_mask: 0.1947, loss: 6.1674\n",
      "2025-07-16 20:50:51,389 - mmdet - INFO - Epoch [62][350/750]\tlr: 2.500e-06, eta: 0:16:51, time: 0.396, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9267, stage0_pos_acc: 37.9286, stage0_loss_bbox: 0.3679, stage0_loss_iou: 0.8378, stage0_loss_mask: 0.4373, stage1_loss_cls: 0.3849, stage1_pos_acc: 88.0379, stage1_loss_bbox: 0.1332, stage1_loss_iou: 0.2973, stage1_loss_mask: 0.2981, stage2_loss_cls: 0.2200, stage2_pos_acc: 92.7667, stage2_loss_bbox: 0.1019, stage2_loss_iou: 0.2240, stage2_loss_mask: 0.2645, stage3_loss_cls: 0.0997, stage3_pos_acc: 97.0139, stage3_loss_bbox: 0.0923, stage3_loss_iou: 0.2033, stage3_loss_mask: 0.2558, stage4_loss_cls: 0.0610, stage4_pos_acc: 97.7861, stage4_loss_bbox: 0.0923, stage4_loss_iou: 0.1972, stage4_loss_mask: 0.2614, stage5_loss_cls: 0.0468, stage5_pos_acc: 98.6875, stage5_loss_bbox: 0.0908, stage5_loss_iou: 0.1936, stage5_loss_mask: 0.2597, loss: 6.3475\n",
      "2025-07-16 20:51:10,633 - mmdet - INFO - Epoch [62][400/750]\tlr: 2.500e-06, eta: 0:16:32, time: 0.385, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8692, stage0_pos_acc: 50.9128, stage0_loss_bbox: 0.3419, stage0_loss_iou: 0.7626, stage0_loss_mask: 0.4218, stage1_loss_cls: 0.4010, stage1_pos_acc: 82.2348, stage1_loss_bbox: 0.1326, stage1_loss_iou: 0.2915, stage1_loss_mask: 0.2553, stage2_loss_cls: 0.2331, stage2_pos_acc: 93.3731, stage2_loss_bbox: 0.1075, stage2_loss_iou: 0.2236, stage2_loss_mask: 0.2322, stage3_loss_cls: 0.1320, stage3_pos_acc: 96.1698, stage3_loss_bbox: 0.0984, stage3_loss_iou: 0.2040, stage3_loss_mask: 0.2102, stage4_loss_cls: 0.0779, stage4_pos_acc: 98.2323, stage4_loss_bbox: 0.0935, stage4_loss_iou: 0.1933, stage4_loss_mask: 0.2124, stage5_loss_cls: 0.0719, stage5_pos_acc: 98.8120, stage5_loss_bbox: 0.0932, stage5_loss_iou: 0.1887, stage5_loss_mask: 0.2021, loss: 6.0501\n",
      "2025-07-16 20:51:29,649 - mmdet - INFO - Epoch [62][450/750]\tlr: 2.500e-06, eta: 0:16:13, time: 0.380, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8998, stage0_pos_acc: 43.5826, stage0_loss_bbox: 0.3662, stage0_loss_iou: 0.7632, stage0_loss_mask: 0.4759, stage1_loss_cls: 0.4053, stage1_pos_acc: 86.1603, stage1_loss_bbox: 0.1540, stage1_loss_iou: 0.3019, stage1_loss_mask: 0.3151, stage2_loss_cls: 0.2449, stage2_pos_acc: 93.5865, stage2_loss_bbox: 0.1318, stage2_loss_iou: 0.2509, stage2_loss_mask: 0.3159, stage3_loss_cls: 0.1378, stage3_pos_acc: 96.4845, stage3_loss_bbox: 0.1199, stage3_loss_iou: 0.2234, stage3_loss_mask: 0.2673, stage4_loss_cls: 0.0905, stage4_pos_acc: 98.0517, stage4_loss_bbox: 0.1140, stage4_loss_iou: 0.2139, stage4_loss_mask: 0.2522, stage5_loss_cls: 0.0738, stage5_pos_acc: 98.8624, stage5_loss_bbox: 0.1110, stage5_loss_iou: 0.2070, stage5_loss_mask: 0.2466, loss: 6.6822\n",
      "2025-07-16 20:51:48,419 - mmdet - INFO - Epoch [62][500/750]\tlr: 2.500e-06, eta: 0:15:53, time: 0.375, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9175, stage0_pos_acc: 42.1955, stage0_loss_bbox: 0.3689, stage0_loss_iou: 0.7908, stage0_loss_mask: 0.4552, stage1_loss_cls: 0.4048, stage1_pos_acc: 85.4857, stage1_loss_bbox: 0.1493, stage1_loss_iou: 0.3051, stage1_loss_mask: 0.2746, stage2_loss_cls: 0.2356, stage2_pos_acc: 88.6337, stage2_loss_bbox: 0.1256, stage2_loss_iou: 0.2423, stage2_loss_mask: 0.2715, stage3_loss_cls: 0.1370, stage3_pos_acc: 95.1382, stage3_loss_bbox: 0.1150, stage3_loss_iou: 0.2228, stage3_loss_mask: 0.2762, stage4_loss_cls: 0.0859, stage4_pos_acc: 98.0704, stage4_loss_bbox: 0.1113, stage4_loss_iou: 0.2128, stage4_loss_mask: 0.2679, stage5_loss_cls: 0.0694, stage5_pos_acc: 97.5056, stage5_loss_bbox: 0.1082, stage5_loss_iou: 0.2078, stage5_loss_mask: 0.2702, loss: 6.6256\n",
      "2025-07-16 20:52:06,876 - mmdet - INFO - Epoch [62][550/750]\tlr: 2.500e-06, eta: 0:15:34, time: 0.369, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.8931, stage0_pos_acc: 45.9837, stage0_loss_bbox: 0.3243, stage0_loss_iou: 0.7763, stage0_loss_mask: 0.5565, stage1_loss_cls: 0.3691, stage1_pos_acc: 85.7703, stage1_loss_bbox: 0.1365, stage1_loss_iou: 0.3140, stage1_loss_mask: 0.3223, stage2_loss_cls: 0.2195, stage2_pos_acc: 93.8576, stage2_loss_bbox: 0.1046, stage2_loss_iou: 0.2412, stage2_loss_mask: 0.2961, stage3_loss_cls: 0.1285, stage3_pos_acc: 96.5701, stage3_loss_bbox: 0.0983, stage3_loss_iou: 0.2231, stage3_loss_mask: 0.2788, stage4_loss_cls: 0.0875, stage4_pos_acc: 97.6996, stage4_loss_bbox: 0.0926, stage4_loss_iou: 0.2151, stage4_loss_mask: 0.2791, stage5_loss_cls: 0.0759, stage5_pos_acc: 98.4996, stage5_loss_bbox: 0.0905, stage5_loss_iou: 0.2115, stage5_loss_mask: 0.2735, loss: 6.6079\n",
      "2025-07-16 20:52:25,216 - mmdet - INFO - Epoch [62][600/750]\tlr: 2.500e-06, eta: 0:15:15, time: 0.367, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9173, stage0_pos_acc: 40.8320, stage0_loss_bbox: 0.3496, stage0_loss_iou: 0.6967, stage0_loss_mask: 0.4529, stage1_loss_cls: 0.4235, stage1_pos_acc: 82.3719, stage1_loss_bbox: 0.1472, stage1_loss_iou: 0.3081, stage1_loss_mask: 0.2519, stage2_loss_cls: 0.2366, stage2_pos_acc: 90.6584, stage2_loss_bbox: 0.1288, stage2_loss_iou: 0.2454, stage2_loss_mask: 0.2386, stage3_loss_cls: 0.1238, stage3_pos_acc: 96.6190, stage3_loss_bbox: 0.1213, stage3_loss_iou: 0.2202, stage3_loss_mask: 0.2267, stage4_loss_cls: 0.0770, stage4_pos_acc: 97.9325, stage4_loss_bbox: 0.1138, stage4_loss_iou: 0.2068, stage4_loss_mask: 0.2080, stage5_loss_cls: 0.0555, stage5_pos_acc: 98.9333, stage5_loss_bbox: 0.1110, stage5_loss_iou: 0.2012, stage5_loss_mask: 0.2132, loss: 6.2749\n",
      "2025-07-16 20:52:43,903 - mmdet - INFO - Epoch [62][650/750]\tlr: 2.500e-06, eta: 0:14:56, time: 0.374, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9246, stage0_pos_acc: 37.2586, stage0_loss_bbox: 0.3928, stage0_loss_iou: 0.8093, stage0_loss_mask: 0.4953, stage1_loss_cls: 0.4037, stage1_pos_acc: 83.6871, stage1_loss_bbox: 0.1614, stage1_loss_iou: 0.2992, stage1_loss_mask: 0.3300, stage2_loss_cls: 0.2167, stage2_pos_acc: 93.9281, stage2_loss_bbox: 0.1369, stage2_loss_iou: 0.2269, stage2_loss_mask: 0.3011, stage3_loss_cls: 0.1144, stage3_pos_acc: 97.1558, stage3_loss_bbox: 0.0925, stage3_loss_iou: 0.2068, stage3_loss_mask: 0.2891, stage4_loss_cls: 0.0643, stage4_pos_acc: 98.5056, stage4_loss_bbox: 0.0828, stage4_loss_iou: 0.1970, stage4_loss_mask: 0.2920, stage5_loss_cls: 0.0506, stage5_pos_acc: 99.3778, stage5_loss_bbox: 0.0818, stage5_loss_iou: 0.1932, stage5_loss_mask: 0.2911, loss: 6.6537\n",
      "2025-07-16 20:53:02,392 - mmdet - INFO - Epoch [62][700/750]\tlr: 2.500e-06, eta: 0:14:37, time: 0.370, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8901, stage0_pos_acc: 47.9266, stage0_loss_bbox: 0.3191, stage0_loss_iou: 0.7531, stage0_loss_mask: 0.4220, stage1_loss_cls: 0.3871, stage1_pos_acc: 85.0768, stage1_loss_bbox: 0.1212, stage1_loss_iou: 0.2717, stage1_loss_mask: 0.2161, stage2_loss_cls: 0.2129, stage2_pos_acc: 95.2325, stage2_loss_bbox: 0.0993, stage2_loss_iou: 0.2041, stage2_loss_mask: 0.2080, stage3_loss_cls: 0.1141, stage3_pos_acc: 99.0405, stage3_loss_bbox: 0.0923, stage3_loss_iou: 0.1894, stage3_loss_mask: 0.1890, stage4_loss_cls: 0.0714, stage4_pos_acc: 99.4452, stage4_loss_bbox: 0.0884, stage4_loss_iou: 0.1817, stage4_loss_mask: 0.1866, stage5_loss_cls: 0.0493, stage5_pos_acc: 99.8571, stage5_loss_bbox: 0.0885, stage5_loss_iou: 0.1810, stage5_loss_mask: 0.1852, loss: 5.7213\n",
      "2025-07-16 20:53:20,777 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:53:20,777 - mmdet - INFO - Epoch [62][750/750]\tlr: 2.500e-06, eta: 0:14:18, time: 0.368, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9110, stage0_pos_acc: 40.2095, stage0_loss_bbox: 0.3629, stage0_loss_iou: 0.7797, stage0_loss_mask: 0.4148, stage1_loss_cls: 0.4439, stage1_pos_acc: 83.4492, stage1_loss_bbox: 0.1397, stage1_loss_iou: 0.2949, stage1_loss_mask: 0.1760, stage2_loss_cls: 0.2668, stage2_pos_acc: 95.1508, stage2_loss_bbox: 0.1151, stage2_loss_iou: 0.2100, stage2_loss_mask: 0.1677, stage3_loss_cls: 0.1246, stage3_pos_acc: 97.7056, stage3_loss_bbox: 0.1025, stage3_loss_iou: 0.1879, stage3_loss_mask: 0.1575, stage4_loss_cls: 0.0731, stage4_pos_acc: 98.8778, stage4_loss_bbox: 0.0969, stage4_loss_iou: 0.1736, stage4_loss_mask: 0.1448, stage5_loss_cls: 0.0563, stage5_pos_acc: 99.1556, stage5_loss_bbox: 0.0940, stage5_loss_iou: 0.1694, stage5_loss_mask: 0.1429, loss: 5.8059\n",
      "2025-07-16 20:53:20,910 - mmdet - INFO - Saving checkpoint at 62 epochs\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 1.0 task/s, elapsed: 102s, ETA:     0s2025-07-16 20:56:41,170 - mmdet - INFO - Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.03s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.29s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.45s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.052\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.104\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.023\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.391\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.393\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.393\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.167\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.222\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.432\n",
      "2025-07-16 20:56:43,002 - mmdet - INFO - Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.78s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.050\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.022\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.034\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.393\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.396\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.396\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.173\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.232\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.433\n",
      "2025-07-16 20:56:46,116 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 20:56:46,116 - mmdet - INFO - Epoch(val) [62][750]\tbbox_mAP: 0.0280, bbox_mAP_50: 0.0520, bbox_mAP_75: 0.0270, bbox_mAP_s: 0.1040, bbox_mAP_m: 0.0230, bbox_mAP_l: 0.0340, bbox_mAP_copypaste: 0.028 0.052 0.027 0.104 0.023 0.034, segm_mAP: 0.0270, segm_mAP_50: 0.0500, segm_mAP_75: 0.0240, segm_mAP_s: 0.1210, segm_mAP_m: 0.0220, segm_mAP_l: 0.0340, segm_mAP_copypaste: 0.027 0.050 0.024 0.121 0.022 0.034\n",
      "2025-07-16 20:57:07,791 - mmdet - INFO - Epoch [63][50/750]\tlr: 2.500e-06, eta: 0:13:59, time: 0.433, data_time: 0.054, memory: 11264, stage0_loss_cls: 0.9034, stage0_pos_acc: 43.9309, stage0_loss_bbox: 0.3759, stage0_loss_iou: 0.8405, stage0_loss_mask: 0.4263, stage1_loss_cls: 0.4080, stage1_pos_acc: 87.9118, stage1_loss_bbox: 0.1309, stage1_loss_iou: 0.2865, stage1_loss_mask: 0.2456, stage2_loss_cls: 0.2311, stage2_pos_acc: 92.3340, stage2_loss_bbox: 0.0964, stage2_loss_iou: 0.2060, stage2_loss_mask: 0.2288, stage3_loss_cls: 0.1103, stage3_pos_acc: 97.0951, stage3_loss_bbox: 0.0946, stage3_loss_iou: 0.1864, stage3_loss_mask: 0.2177, stage4_loss_cls: 0.0712, stage4_pos_acc: 98.7515, stage4_loss_bbox: 0.0917, stage4_loss_iou: 0.1825, stage4_loss_mask: 0.2205, stage5_loss_cls: 0.0561, stage5_pos_acc: 99.1960, stage5_loss_bbox: 0.0903, stage5_loss_iou: 0.1806, stage5_loss_mask: 0.2254, loss: 6.1068\n",
      "2025-07-16 20:57:26,939 - mmdet - INFO - Epoch [63][100/750]\tlr: 2.500e-06, eta: 0:13:40, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9217, stage0_pos_acc: 40.7798, stage0_loss_bbox: 0.3945, stage0_loss_iou: 0.8883, stage0_loss_mask: 0.4686, stage1_loss_cls: 0.3967, stage1_pos_acc: 85.0808, stage1_loss_bbox: 0.1629, stage1_loss_iou: 0.3030, stage1_loss_mask: 0.3281, stage2_loss_cls: 0.2248, stage2_pos_acc: 94.2167, stage2_loss_bbox: 0.1303, stage2_loss_iou: 0.2169, stage2_loss_mask: 0.2786, stage3_loss_cls: 0.1200, stage3_pos_acc: 95.9348, stage3_loss_bbox: 0.0865, stage3_loss_iou: 0.1945, stage3_loss_mask: 0.2515, stage4_loss_cls: 0.0671, stage4_pos_acc: 98.2167, stage4_loss_bbox: 0.0817, stage4_loss_iou: 0.1828, stage4_loss_mask: 0.2181, stage5_loss_cls: 0.0503, stage5_pos_acc: 98.7500, stage5_loss_bbox: 0.0788, stage5_loss_iou: 0.1806, stage5_loss_mask: 0.2220, loss: 6.4482\n",
      "2025-07-16 20:57:46,140 - mmdet - INFO - Epoch [63][150/750]\tlr: 2.500e-06, eta: 0:13:21, time: 0.384, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9465, stage0_pos_acc: 46.7038, stage0_loss_bbox: 0.3359, stage0_loss_iou: 0.8673, stage0_loss_mask: 0.5075, stage1_loss_cls: 0.4083, stage1_pos_acc: 84.3508, stage1_loss_bbox: 0.1198, stage1_loss_iou: 0.2833, stage1_loss_mask: 0.2349, stage2_loss_cls: 0.2376, stage2_pos_acc: 91.8014, stage2_loss_bbox: 0.0928, stage2_loss_iou: 0.2093, stage2_loss_mask: 0.2195, stage3_loss_cls: 0.1111, stage3_pos_acc: 96.7198, stage3_loss_bbox: 0.0848, stage3_loss_iou: 0.1937, stage3_loss_mask: 0.2048, stage4_loss_cls: 0.0672, stage4_pos_acc: 97.5374, stage4_loss_bbox: 0.0793, stage4_loss_iou: 0.1837, stage4_loss_mask: 0.1935, stage5_loss_cls: 0.0573, stage5_pos_acc: 98.1308, stage5_loss_bbox: 0.0778, stage5_loss_iou: 0.1794, stage5_loss_mask: 0.1853, loss: 6.0810\n",
      "2025-07-16 20:58:05,502 - mmdet - INFO - Epoch [63][200/750]\tlr: 2.500e-06, eta: 0:13:02, time: 0.387, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.8733, stage0_pos_acc: 48.8218, stage0_loss_bbox: 0.3573, stage0_loss_iou: 0.8016, stage0_loss_mask: 0.4710, stage1_loss_cls: 0.3955, stage1_pos_acc: 86.0493, stage1_loss_bbox: 0.1445, stage1_loss_iou: 0.3119, stage1_loss_mask: 0.2870, stage2_loss_cls: 0.2475, stage2_pos_acc: 92.8395, stage2_loss_bbox: 0.1145, stage2_loss_iou: 0.2430, stage2_loss_mask: 0.2696, stage3_loss_cls: 0.1385, stage3_pos_acc: 96.6229, stage3_loss_bbox: 0.1075, stage3_loss_iou: 0.2278, stage3_loss_mask: 0.2477, stage4_loss_cls: 0.0826, stage4_pos_acc: 98.4687, stage4_loss_bbox: 0.1041, stage4_loss_iou: 0.2207, stage4_loss_mask: 0.2521, stage5_loss_cls: 0.0630, stage5_pos_acc: 99.5909, stage5_loss_bbox: 0.1036, stage5_loss_iou: 0.2159, stage5_loss_mask: 0.2458, loss: 6.5262\n",
      "2025-07-16 20:58:24,995 - mmdet - INFO - Epoch [63][250/750]\tlr: 2.500e-06, eta: 0:12:43, time: 0.390, data_time: 0.008, memory: 11264, stage0_loss_cls: 0.9163, stage0_pos_acc: 39.7119, stage0_loss_bbox: 0.3302, stage0_loss_iou: 0.7401, stage0_loss_mask: 0.3567, stage1_loss_cls: 0.3906, stage1_pos_acc: 85.2563, stage1_loss_bbox: 0.1295, stage1_loss_iou: 0.2604, stage1_loss_mask: 0.2234, stage2_loss_cls: 0.2199, stage2_pos_acc: 93.7143, stage2_loss_bbox: 0.1075, stage2_loss_iou: 0.1991, stage2_loss_mask: 0.2021, stage3_loss_cls: 0.0854, stage3_pos_acc: 97.6810, stage3_loss_bbox: 0.0995, stage3_loss_iou: 0.1816, stage3_loss_mask: 0.1991, stage4_loss_cls: 0.0431, stage4_pos_acc: 99.5278, stage4_loss_bbox: 0.0985, stage4_loss_iou: 0.1771, stage4_loss_mask: 0.1954, stage5_loss_cls: 0.0351, stage5_pos_acc: 99.3500, stage5_loss_bbox: 0.0962, stage5_loss_iou: 0.1731, stage5_loss_mask: 0.1987, loss: 5.6585\n",
      "2025-07-16 20:58:44,233 - mmdet - INFO - Epoch [63][300/750]\tlr: 2.500e-06, eta: 0:12:24, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8911, stage0_pos_acc: 46.7644, stage0_loss_bbox: 0.3581, stage0_loss_iou: 0.7725, stage0_loss_mask: 0.5582, stage1_loss_cls: 0.3958, stage1_pos_acc: 85.3948, stage1_loss_bbox: 0.1394, stage1_loss_iou: 0.3110, stage1_loss_mask: 0.2715, stage2_loss_cls: 0.2265, stage2_pos_acc: 95.3254, stage2_loss_bbox: 0.1078, stage2_loss_iou: 0.2255, stage2_loss_mask: 0.2317, stage3_loss_cls: 0.1046, stage3_pos_acc: 98.3361, stage3_loss_bbox: 0.0982, stage3_loss_iou: 0.2013, stage3_loss_mask: 0.2123, stage4_loss_cls: 0.0647, stage4_pos_acc: 99.3500, stage4_loss_bbox: 0.0917, stage4_loss_iou: 0.1904, stage4_loss_mask: 0.1984, stage5_loss_cls: 0.0553, stage5_pos_acc: 99.6000, stage5_loss_bbox: 0.0886, stage5_loss_iou: 0.1851, stage5_loss_mask: 0.2011, loss: 6.1806\n",
      "2025-07-16 20:59:03,609 - mmdet - INFO - Epoch [63][350/750]\tlr: 2.500e-06, eta: 0:12:05, time: 0.388, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9287, stage0_pos_acc: 43.1985, stage0_loss_bbox: 0.3502, stage0_loss_iou: 0.8197, stage0_loss_mask: 0.3796, stage1_loss_cls: 0.3846, stage1_pos_acc: 85.4477, stage1_loss_bbox: 0.1196, stage1_loss_iou: 0.2673, stage1_loss_mask: 0.1981, stage2_loss_cls: 0.2095, stage2_pos_acc: 95.9542, stage2_loss_bbox: 0.0985, stage2_loss_iou: 0.2021, stage2_loss_mask: 0.1691, stage3_loss_cls: 0.1045, stage3_pos_acc: 98.4828, stage3_loss_bbox: 0.0873, stage3_loss_iou: 0.1788, stage3_loss_mask: 0.1520, stage4_loss_cls: 0.0701, stage4_pos_acc: 99.3440, stage4_loss_bbox: 0.0853, stage4_loss_iou: 0.1681, stage4_loss_mask: 0.1570, stage5_loss_cls: 0.0649, stage5_pos_acc: 99.5074, stage5_loss_bbox: 0.0833, stage5_loss_iou: 0.1643, stage5_loss_mask: 0.1577, loss: 5.6004\n",
      "2025-07-16 20:59:23,062 - mmdet - INFO - Epoch [63][400/750]\tlr: 2.500e-06, eta: 0:11:45, time: 0.389, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9061, stage0_pos_acc: 42.3766, stage0_loss_bbox: 0.3483, stage0_loss_iou: 0.8555, stage0_loss_mask: 0.5307, stage1_loss_cls: 0.4090, stage1_pos_acc: 83.6462, stage1_loss_bbox: 0.1272, stage1_loss_iou: 0.3287, stage1_loss_mask: 0.2837, stage2_loss_cls: 0.2335, stage2_pos_acc: 94.3111, stage2_loss_bbox: 0.1067, stage2_loss_iou: 0.2510, stage2_loss_mask: 0.2681, stage3_loss_cls: 0.1209, stage3_pos_acc: 96.6089, stage3_loss_bbox: 0.0975, stage3_loss_iou: 0.2234, stage3_loss_mask: 0.2583, stage4_loss_cls: 0.0793, stage4_pos_acc: 97.9952, stage4_loss_bbox: 0.0927, stage4_loss_iou: 0.2116, stage4_loss_mask: 0.2564, stage5_loss_cls: 0.0686, stage5_pos_acc: 99.3577, stage5_loss_bbox: 0.0887, stage5_loss_iou: 0.2003, stage5_loss_mask: 0.2514, loss: 6.5975\n",
      "2025-07-16 20:59:42,368 - mmdet - INFO - Epoch [63][450/750]\tlr: 2.500e-06, eta: 0:11:26, time: 0.386, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9297, stage0_pos_acc: 45.2114, stage0_loss_bbox: 0.3600, stage0_loss_iou: 0.8483, stage0_loss_mask: 0.4828, stage1_loss_cls: 0.4099, stage1_pos_acc: 84.8452, stage1_loss_bbox: 0.1408, stage1_loss_iou: 0.3210, stage1_loss_mask: 0.3823, stage2_loss_cls: 0.2372, stage2_pos_acc: 92.4440, stage2_loss_bbox: 0.1122, stage2_loss_iou: 0.2618, stage2_loss_mask: 0.3568, stage3_loss_cls: 0.1181, stage3_pos_acc: 97.1290, stage3_loss_bbox: 0.1027, stage3_loss_iou: 0.2436, stage3_loss_mask: 0.3552, stage4_loss_cls: 0.0741, stage4_pos_acc: 98.4841, stage4_loss_bbox: 0.0976, stage4_loss_iou: 0.2355, stage4_loss_mask: 0.3466, stage5_loss_cls: 0.0614, stage5_pos_acc: 98.5754, stage5_loss_bbox: 0.0976, stage5_loss_iou: 0.2323, stage5_loss_mask: 0.3466, loss: 7.1544\n",
      "2025-07-16 21:00:01,656 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 21:00:01,657 - mmdet - INFO - Epoch [63][500/750]\tlr: 2.500e-06, eta: 0:11:07, time: 0.386, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9023, stage0_pos_acc: 40.6017, stage0_loss_bbox: 0.3509, stage0_loss_iou: 0.7860, stage0_loss_mask: 0.5224, stage1_loss_cls: 0.4048, stage1_pos_acc: 84.0966, stage1_loss_bbox: 0.1536, stage1_loss_iou: 0.3361, stage1_loss_mask: 0.3419, stage2_loss_cls: 0.2450, stage2_pos_acc: 90.6883, stage2_loss_bbox: 0.1296, stage2_loss_iou: 0.2614, stage2_loss_mask: 0.3004, stage3_loss_cls: 0.1510, stage3_pos_acc: 96.5161, stage3_loss_bbox: 0.1147, stage3_loss_iou: 0.2333, stage3_loss_mask: 0.2369, stage4_loss_cls: 0.1120, stage4_pos_acc: 97.9082, stage4_loss_bbox: 0.1087, stage4_loss_iou: 0.2287, stage4_loss_mask: 0.2150, stage5_loss_cls: 0.0923, stage5_pos_acc: 98.8165, stage5_loss_bbox: 0.1047, stage5_loss_iou: 0.2247, stage5_loss_mask: 0.2223, loss: 6.7788\n",
      "2025-07-16 21:00:21,038 - mmdet - INFO - Epoch [63][550/750]\tlr: 2.500e-06, eta: 0:10:48, time: 0.388, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9468, stage0_pos_acc: 40.0284, stage0_loss_bbox: 0.3868, stage0_loss_iou: 0.8186, stage0_loss_mask: 0.5114, stage1_loss_cls: 0.4301, stage1_pos_acc: 80.8918, stage1_loss_bbox: 0.1383, stage1_loss_iou: 0.3292, stage1_loss_mask: 0.3000, stage2_loss_cls: 0.2783, stage2_pos_acc: 87.9641, stage2_loss_bbox: 0.1131, stage2_loss_iou: 0.2424, stage2_loss_mask: 0.2673, stage3_loss_cls: 0.1521, stage3_pos_acc: 94.4149, stage3_loss_bbox: 0.1044, stage3_loss_iou: 0.2180, stage3_loss_mask: 0.2474, stage4_loss_cls: 0.0826, stage4_pos_acc: 98.6198, stage4_loss_bbox: 0.1018, stage4_loss_iou: 0.2044, stage4_loss_mask: 0.2422, stage5_loss_cls: 0.0635, stage5_pos_acc: 97.8579, stage5_loss_bbox: 0.0984, stage5_loss_iou: 0.1984, stage5_loss_mask: 0.2389, loss: 6.7143\n",
      "2025-07-16 21:00:40,200 - mmdet - INFO - Epoch [63][600/750]\tlr: 2.500e-06, eta: 0:10:29, time: 0.383, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.9240, stage0_pos_acc: 41.3520, stage0_loss_bbox: 0.3474, stage0_loss_iou: 0.8150, stage0_loss_mask: 0.4884, stage1_loss_cls: 0.4032, stage1_pos_acc: 86.2447, stage1_loss_bbox: 0.1435, stage1_loss_iou: 0.3286, stage1_loss_mask: 0.3508, stage2_loss_cls: 0.2557, stage2_pos_acc: 89.5631, stage2_loss_bbox: 0.1230, stage2_loss_iou: 0.2638, stage2_loss_mask: 0.3398, stage3_loss_cls: 0.1394, stage3_pos_acc: 96.9524, stage3_loss_bbox: 0.1124, stage3_loss_iou: 0.2416, stage3_loss_mask: 0.3413, stage4_loss_cls: 0.0957, stage4_pos_acc: 97.7771, stage4_loss_bbox: 0.1110, stage4_loss_iou: 0.2338, stage4_loss_mask: 0.3132, stage5_loss_cls: 0.0805, stage5_pos_acc: 99.5095, stage5_loss_bbox: 0.1078, stage5_loss_iou: 0.2320, stage5_loss_mask: 0.3287, loss: 7.1205\n",
      "2025-07-16 21:00:59,359 - mmdet - INFO - Epoch [63][650/750]\tlr: 2.500e-06, eta: 0:10:10, time: 0.383, data_time: 0.010, memory: 11264, stage0_loss_cls: 0.9604, stage0_pos_acc: 42.8382, stage0_loss_bbox: 0.3701, stage0_loss_iou: 0.7858, stage0_loss_mask: 0.6102, stage1_loss_cls: 0.4078, stage1_pos_acc: 84.2917, stage1_loss_bbox: 0.1554, stage1_loss_iou: 0.3383, stage1_loss_mask: 0.3606, stage2_loss_cls: 0.2458, stage2_pos_acc: 90.6299, stage2_loss_bbox: 0.1292, stage2_loss_iou: 0.2667, stage2_loss_mask: 0.3162, stage3_loss_cls: 0.1250, stage3_pos_acc: 96.3870, stage3_loss_bbox: 0.1093, stage3_loss_iou: 0.2327, stage3_loss_mask: 0.2709, stage4_loss_cls: 0.0762, stage4_pos_acc: 97.6948, stage4_loss_bbox: 0.1037, stage4_loss_iou: 0.2243, stage4_loss_mask: 0.2659, stage5_loss_cls: 0.0641, stage5_pos_acc: 98.7999, stage5_loss_bbox: 0.0982, stage5_loss_iou: 0.2199, stage5_loss_mask: 0.2682, loss: 7.0050\n",
      "2025-07-16 21:01:18,958 - mmdet - INFO - Epoch [63][700/750]\tlr: 2.500e-06, eta: 0:09:51, time: 0.392, data_time: 0.011, memory: 11264, stage0_loss_cls: 0.9068, stage0_pos_acc: 43.8633, stage0_loss_bbox: 0.3634, stage0_loss_iou: 0.8224, stage0_loss_mask: 0.5553, stage1_loss_cls: 0.3896, stage1_pos_acc: 86.2557, stage1_loss_bbox: 0.1314, stage1_loss_iou: 0.3263, stage1_loss_mask: 0.2622, stage2_loss_cls: 0.2200, stage2_pos_acc: 94.7402, stage2_loss_bbox: 0.1074, stage2_loss_iou: 0.2380, stage2_loss_mask: 0.2403, stage3_loss_cls: 0.1101, stage3_pos_acc: 97.1190, stage3_loss_bbox: 0.0937, stage3_loss_iou: 0.2171, stage3_loss_mask: 0.2296, stage4_loss_cls: 0.0681, stage4_pos_acc: 98.7950, stage4_loss_bbox: 0.0877, stage4_loss_iou: 0.2045, stage4_loss_mask: 0.2147, stage5_loss_cls: 0.0581, stage5_pos_acc: 99.1041, stage5_loss_bbox: 0.0850, stage5_loss_iou: 0.2005, stage5_loss_mask: 0.2118, loss: 6.3438\n",
      "2025-07-16 21:01:38,210 - mmdet - INFO - Exp name: ocor_swin_large_patch4_window7_fpn_300_proposals.py\n",
      "2025-07-16 21:01:38,210 - mmdet - INFO - Epoch [63][750/750]\tlr: 2.500e-06, eta: 0:09:32, time: 0.385, data_time: 0.009, memory: 11264, stage0_loss_cls: 0.8831, stage0_pos_acc: 49.5907, stage0_loss_bbox: 0.3201, stage0_loss_iou: 0.7887, stage0_loss_mask: 0.4325, stage1_loss_cls: 0.3999, stage1_pos_acc: 85.2302, stage1_loss_bbox: 0.1142, stage1_loss_iou: 0.2607, stage1_loss_mask: 0.1916, stage2_loss_cls: 0.2415, stage2_pos_acc: 90.9472, stage2_loss_bbox: 0.0919, stage2_loss_iou: 0.1954, stage2_loss_mask: 0.1761, stage3_loss_cls: 0.1198, stage3_pos_acc: 97.5440, stage3_loss_bbox: 0.0822, stage3_loss_iou: 0.1788, stage3_loss_mask: 0.1727, stage4_loss_cls: 0.0627, stage4_pos_acc: 98.9191, stage4_loss_bbox: 0.0792, stage4_loss_iou: 0.1687, stage4_loss_mask: 0.1572, stage5_loss_cls: 0.0476, stage5_pos_acc: 99.1504, stage5_loss_bbox: 0.0778, stage5_loss_iou: 0.1665, stage5_loss_mask: 0.1568, loss: 5.5654\n",
      "2025-07-16 21:01:38,347 - mmdet - INFO - Saving checkpoint at 63 epochs\n"
     ]
    }
   ],
   "source": [
    "!python tools/train.py configs/ocor/ocor_swin_large_patch4_window7_fpn_300_proposals.py \\\n",
    "    --resume-from work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals/latest.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.11/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n",
      "apex is not installed\n",
      "/environment/miniconda3/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "apex is not installed\n",
      "apex is not installed\n",
      "apex is not installed\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "/environment/miniconda3/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/environment/miniconda3/lib/python3.11/site-packages/mmcv/cnn/bricks/conv_module.py:153: UserWarning: Unnecessary conv bias before batch/instance norm\n",
      "  warnings.warn(\n",
      "load checkpoint from local path: work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals/epoch_65.pth\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 100/100, 0.5 task/s, elapsed: 198s, ETA:     0s\n",
      "Evaluating bbox...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=1.17s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.46s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.106\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.031\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.399\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.401\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.401\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.236\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.438\n",
      "\n",
      "Evaluating segm...\n",
      "/home/featurize/work/OCOR/mmdet/datasets/coco.py:450: UserWarning: The key \"bbox\" is deleted for more accurate mask AP of small/medium/large instances since v2.12.0. This does not change the overall mAP calculation.\n",
      "  warnings.warn(\n",
      "Loading and preparing results...\n",
      "DONE (t=0.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *segm*\n",
      "DONE (t=1.95s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.48s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.025\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=1000 ] = 0.046\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=1000 ] = 0.024\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.061\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.018\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.397\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=300 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=1000 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=1000 ] = 0.193\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=1000 ] = 0.241\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=1000 ] = 0.436\n",
      "OrderedDict([('bbox_mAP', 0.025), ('bbox_mAP_50', 0.047), ('bbox_mAP_75', 0.024), ('bbox_mAP_s', 0.106), ('bbox_mAP_m', 0.018), ('bbox_mAP_l', 0.031), ('bbox_mAP_copypaste', '0.025 0.047 0.024 0.106 0.018 0.031'), ('segm_mAP', 0.025), ('segm_mAP_50', 0.046), ('segm_mAP_75', 0.024), ('segm_mAP_s', 0.061), ('segm_mAP_m', 0.018), ('segm_mAP_l', 0.032), ('segm_mAP_copypaste', '0.025 0.046 0.024 0.061 0.018 0.032')])\n"
     ]
    }
   ],
   "source": [
    "!python tools/test.py \\\n",
    "  configs/ocor/ocor_swin_large_patch4_window7_fpn_300_proposals.py \\\n",
    "  work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals/epoch_65.pth \\\n",
    "  --eval bbox segm \\\n",
    "  --show-dir vis_pred \\\n",
    "  --show-score-thr 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "snqxnCS3tFwa"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ========================== 6. 将训练结果保存到 Google Drive（可选） ==========================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 将训练权重保存到 Google Drive\n",
    "!cp work_dirs/ocor_swin_large_patch4_window7_fpn_300_proposals/latest.pth /content/drive/MyDrive/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
